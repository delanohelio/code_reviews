{"pr_number": 3408, "pr_title": "[MO] - [dyn.conf] -> system tests", "pr_author": "see-quick", "pr_createdAt": "2020-07-29T11:23:16Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408", "merge_commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570250", "body": "Wouldn't it be better to make some variable for this? Because you use it in `AssertionError` too.", "bodyText": "Wouldn't it be better to make some variable for this? Because you use it in AssertionError too.", "bodyHTML": "<p dir=\"auto\">Wouldn't it be better to make some variable for this? Because you use it in <code>AssertionError</code> too.</p>", "author": "im-konge", "createdAt": "2020-07-29T20:31:37Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgzODg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462838853", "bodyText": "It was not a good approach on how to verify I have changed the it....", "author": "see-quick", "createdAt": "2020-07-30T08:34:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..d5b94d917 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,26 +175,58 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n         updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d5b94d917..8e6c33747 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,4 +238,76 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,72 +248,74 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -157,43 +189,148 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n+    }\n+\n+    /**\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDU4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570582", "body": "To the variable I mentioned above (just as note)", "bodyText": "To the variable I mentioned above (just as note)", "bodyHTML": "<p dir=\"auto\">To the variable I mentioned above (just as note)</p>", "author": "im-konge", "createdAt": "2020-07-29T20:32:13Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..d5b94d917 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,26 +175,58 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n         updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d5b94d917..8e6c33747 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,4 +238,76 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,72 +248,74 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -157,43 +189,148 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n+    }\n+\n+    /**\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462573746", "body": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\r\n\r\nAnyway the `KafkaUtils` is not needed.", "bodyText": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\nAnyway the KafkaUtils is not needed.", "bodyHTML": "<p dir=\"auto\">Just thinking -&gt; is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.</p>\n<p dir=\"auto\">Anyway the <code>KafkaUtils</code> is not needed.</p>", "author": "im-konge", "createdAt": "2020-07-29T20:38:13Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk5MDAyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462990021", "bodyText": "I have split these 2...", "author": "see-quick", "createdAt": "2020-07-30T13:19:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..d5b94d917 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,26 +175,58 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n         updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d5b94d917..8e6c33747 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,4 +238,76 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,72 +248,74 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 767abab48..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -157,43 +189,148 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n+        return true;\n+    }\n+\n+    /**\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NjQ4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463186489", "body": "`KafkaDynamicConfiguration`? typo?", "bodyText": "KafkaDynamicConfiguration? typo?", "bodyHTML": "<p dir=\"auto\"><code>KafkaDynamicConfiguration</code>? typo?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:21:14Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\nnew file mode 100644\nindex 000000000..b027f05a3\n--- /dev/null\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n", "chunk": "@@ -0,0 +1,58 @@\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+\n+    min_insync_replicas,\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    // TODO:\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_preallocate;\n+\n+    //    unclean_leader_election_enable\n+    //    message_max_bytes\n+\n+//    max_connections\n+//    max_connections_per_ip\n+//    max_connections_per_ip_overrides\n+//    log_message_downconversion_enable\n+//    metric_reporters\n+\n+    @Override\n+    public String toString() {\n+       return this.name().replaceAll(\"_\", \".\");\n+    }\n+}\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\nindex b027f05a3..bf5c3bd8e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n", "chunk": "@@ -40,19 +39,20 @@ public enum KafkaDynamicConfiguration {\n     log_index_size_max_bytes,\n     log_message_timestamp_difference_max_ms,\n     log_message_timestamp_type,\n-    log_preallocate;\n+    log_message_downconversion_enable,\n+    log_preallocate,\n \n-    //    unclean_leader_election_enable\n-    //    message_max_bytes\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n \n-//    max_connections\n-//    max_connections_per_ip\n-//    max_connections_per_ip_overrides\n-//    log_message_downconversion_enable\n-//    metric_reporters\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;\n \n     @Override\n     public String toString() {\n-       return this.name().replaceAll(\"_\", \".\");\n+        return this.name().replaceAll(\"_\", \".\");\n     }\n }\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463187718", "body": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "bodyText": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "bodyHTML": "<p dir=\"auto\">What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:23:29Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0MzM4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443388", "bodyText": "I think it's just sample from all configuration options which Kafka offers. but I am not sure.", "author": "Frawless", "createdAt": "2020-07-31T07:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MzI5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464273299", "bodyText": "Yes this is all Kafka configuration, which can by changed without triggering rolling update.", "author": "see-quick", "createdAt": "2020-08-03T08:40:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\nnew file mode 100644\nindex 000000000..b027f05a3\n--- /dev/null\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n", "chunk": "@@ -0,0 +1,58 @@\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+\n+    min_insync_replicas,\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    // TODO:\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_preallocate;\n+\n+    //    unclean_leader_election_enable\n+    //    message_max_bytes\n+\n+//    max_connections\n+//    max_connections_per_ip\n+//    max_connections_per_ip_overrides\n+//    log_message_downconversion_enable\n+//    metric_reporters\n+\n+    @Override\n+    public String toString() {\n+       return this.name().replaceAll(\"_\", \".\");\n+    }\n+}\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\nindex b027f05a3..bf5c3bd8e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n", "chunk": "@@ -40,19 +39,20 @@ public enum KafkaDynamicConfiguration {\n     log_index_size_max_bytes,\n     log_message_timestamp_difference_max_ms,\n     log_message_timestamp_type,\n-    log_preallocate;\n+    log_message_downconversion_enable,\n+    log_preallocate,\n \n-    //    unclean_leader_election_enable\n-    //    message_max_bytes\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n \n-//    max_connections\n-//    max_connections_per_ip\n-//    max_connections_per_ip_overrides\n-//    log_message_downconversion_enable\n-//    metric_reporters\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;\n \n     @Override\n     public String toString() {\n-       return this.name().replaceAll(\"_\", \".\");\n+        return this.name().replaceAll(\"_\", \".\");\n     }\n }\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java b/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\ndeleted file mode 100644\nindex bf5c3bd8e..000000000\n--- a/systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java\n+++ /dev/null\n", "chunk": "@@ -1,58 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.enums;\n-\n-/**\n- * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n- */\n-public enum KafkaDynamicConfiguration {\n-\n-    background_threads,\n-    compression_type,\n-    min_insync_replicas,\n-    unclean_leader_election_enable,\n-    message_max_bytes,\n-    metric_reporters,\n-\n-    log_flush_interval_messages,\n-    log_flush_interval_ms,\n-    log_retention_bytes,\n-    log_retention_ms,\n-    log_roll_jitter_ms,\n-    log_roll_ms,\n-    log_segment_bytes,\n-    log_segment_delete_delay_ms,\n-    log_cleaner_backoff_ms,\n-    log_cleaner_dedupe_buffer_size,\n-    log_cleaner_delete_retention_ms,\n-    log_cleaner_io_buffer_load_factor,\n-    log_cleaner_io_buffer_size,\n-    log_cleaner_io_max_bytes_per_second,\n-    log_cleaner_max_compaction_lag_ms,\n-    log_cleaner_min_cleanable_ratio,\n-    log_cleaner_min_compaction_lag_ms,\n-    log_cleaner_threads,\n-    log_cleanup_policy,\n-    log_index_interval_bytes,\n-    log_index_size_max_bytes,\n-    log_message_timestamp_difference_max_ms,\n-    log_message_timestamp_type,\n-    log_message_downconversion_enable,\n-    log_preallocate,\n-\n-    num_io_threads,\n-    num_network_threads,\n-    num_recovery_threads_per_data_dir,\n-    num_replica_fetchers,\n-\n-    max_connections,\n-    max_connections_per_ip,\n-    max_connections_per_ip_overrides;\n-\n-    @Override\n-    public String toString() {\n-        return this.name().replaceAll(\"_\", \".\");\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTU5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189599", "body": "The `unclean.leader.election.enable` should show up here as well. So maybe you can assert it too?", "bodyText": "The unclean.leader.election.enable should show up here as well. So maybe you can assert it too?", "bodyHTML": "<p dir=\"auto\">The <code>unclean.leader.election.enable</code> should show up here as well. So maybe you can assert it too?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:26:46Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,65 +45,39 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(\"true\");\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n \n         InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n \n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -68,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189987", "body": "Is this some copy paster left-over? Or what is the value of this?", "bodyText": "Is this some copy paster left-over? Or what is the value of this?", "bodyHTML": "<p dir=\"auto\">Is this some copy paster left-over? Or what is the value of this?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:27:29Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NDEyMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464274122", "bodyText": "These DynamicConfigurationIsolatedST are the Standa`s tests, which were located in the KafkaST. @stanlyDoge", "author": "see-quick", "createdAt": "2020-08-03T08:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -111,58 +89,31 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setLogging(il);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n     @Test\n     void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-                .endSpec()\n-                .done();\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        updateAndVerifyDynConf(\"true\");\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MTQxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463191418", "body": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "bodyText": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "bodyHTML": "<p dir=\"auto\">Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:30:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -111,58 +89,31 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setLogging(il);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n     @Test\n     void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-                .endSpec()\n-                .done();\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        updateAndVerifyDynConf(\"true\");\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192324", "body": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "bodyText": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "bodyHTML": "<p dir=\"auto\">A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:31:44Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NzU4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464277585", "bodyText": "I did not write these tests (just copy-paste) but I will split him if it will be possible.", "author": "see-quick", "createdAt": "2020-08-03T08:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNDAxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464834018", "bodyText": "The idea behind this test was to perform as many as possible changes in listeners. I agree the test is complicated and the environmental restrictions suck. Splitting test to smaller bits (one listener, one test) seem like a good idea.", "author": "sknot-rh", "createdAt": "2020-08-04T06:45:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -111,58 +89,31 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setLogging(il);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n     @Test\n     void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-                .endSpec()\n-                .done();\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        updateAndVerifyDynConf(\"true\");\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjY0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192644", "body": "You seem to be doing rolling updates. Please use persistent cluster.", "bodyText": "You seem to be doing rolling updates. Please use persistent cluster.", "bodyHTML": "<p dir=\"auto\">You seem to be doing rolling updates. Please use persistent cluster.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:32:27Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -111,58 +89,31 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setLogging(il);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n     @Test\n     void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-                .endSpec()\n-                .done();\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        updateAndVerifyDynConf(\"true\");\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5Mjk3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192971", "body": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "bodyText": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "bodyHTML": "<p dir=\"auto\">Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:33:11Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -225,48 +145,21 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setListeners(kl);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        updateAndVerifyDynConf(\"false\");\n     }\n \n     @Test\n-    @Tag(NODEPORT_SUPPORTED)\n     @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n     void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n                 .editSpec()\n                     .editKafka()\n                         .withNewListeners()\n                             .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n                             .endKafkaListenerExternalLoadBalancer()\n                         .endListeners()\n                         .withConfig(kafkaConfig)\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -148,24 +167,35 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n-    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n     void testDynamicConfigurationExternalTls() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463193640", "body": "Are you saying that the change here doesn't trigger RU?", "bodyText": "Are you saying that the change here doesn't trigger RU?", "bodyHTML": "<p dir=\"auto\">Are you saying that the change here doesn't trigger RU?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:34:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk5NjAyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464996028", "bodyText": "This should trigger a rolling update! Using method verifyThatRunningPodsAreStable is not a good practice here (again I did write these tests :D) and I have replaced it with the snapShot waitTillSsHasRolled.", "author": "see-quick", "createdAt": "2020-08-04T11:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -315,10 +208,10 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n+                    .withNewKafkaListenerExternalLoadBalancer()\n                         .withNewKafkaListenerAuthenticationTlsAuth()\n                         .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalLoadBalancer()\n                     .build();\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n             kafkaClusterSpec.setListeners(updatedKl);\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -208,16 +236,16 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalLoadBalancer()\n+                    .withNewKafkaListenerExternalNodePort()\n                         .withNewKafkaListenerAuthenticationTlsAuth()\n                         .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalLoadBalancer()\n+                    .endKafkaListenerExternalNodePort()\n                     .build();\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n             kafkaClusterSpec.setListeners(updatedKl);\n         });\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n \n         basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n                 basicExternalKafkaClientTls.sendMessagesTls(),\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -314,17 +272,26 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .withNewKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerAuthenticationTlsAuth()\n+                    .build()\n+            ), null));\n         });\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        // TODO: remove it ?\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n \n         basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n                 basicExternalKafkaClientTls.sendMessagesTls(),\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463194717", "body": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "bodyText": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "bodyHTML": "<p dir=\"auto\">Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:36:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MjgzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463452836", "bodyText": "Ye, I suggest to use https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests or https://junit.org/junit5/docs/current/user-guide/#writing-tests-dynamic-tests", "author": "Frawless", "createdAt": "2020-07-31T07:36:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk1MjM2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464952361", "bodyText": "Done", "author": "see-quick", "createdAt": "2020-08-04T10:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 9444a5b20..461b9b89b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -14,262 +13,68 @@ import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n \n-import java.util.Arrays;\n-\n-import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n import static io.strimzi.systemtest.Constants.REGRESSION;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n \n @Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationSharedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n \n-    @Test\n-    void testBackgroundThreads() {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n-    }\n-\n-    @Tag(ACCEPTANCE)\n-    @Test\n-    void testCompressionType() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n-\n-    }\n-\n-    @Test\n-    void testLogFlush() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n-\n-    }\n-\n-    @Test\n-    void testLogRetention() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n-    }\n-\n-    @Test\n-    void testLogSegment() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n-    }\n-\n-    @Test\n-    void testLogRoll() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n-    }\n-\n-    @Test\n-    void testLogCleaner() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n \n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n \n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+        \"log.flush.interval.ms, \" + 20,\n \n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n \n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n \n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n \n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n \n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n \n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n \n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n-    }\n-\n-    @Test\n-    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n-    }\n-\n-    @Test\n-    void testLogIndexLogMessageLogMessage() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n-    }\n-\n-    @Test\n-    void testMaxConnections() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n-    }\n-\n-    @Test\n-    void testMetricReportersMessageMaxUncleanLeaderElection() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n \n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+        // verify phase\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n+        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n     }\n \n     @BeforeAll\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 461b9b89b..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -83,6 +70,6 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n     }\n }\n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nnew file mode 100644\nindex 000000000..483712e09\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @Test\n+    void testBackgroundThreads() {\n+        // exercise phase\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n+\n+        // verify phase\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n+    }\n+\n+    @Tag(ACCEPTANCE)\n+    @Test\n+    void testCompressionType() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogFlush() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogRetention() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+    }\n+\n+    @Test\n+    void testLogSegment() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+    }\n+\n+    @Test\n+    void testLogRoll() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+    }\n+\n+    @Test\n+    void testLogCleaner() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+    }\n+\n+    @Test\n+    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+    }\n+\n+    @Test\n+    void testLogIndexLogMessageLogMessage() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+    }\n+\n+    @Test\n+    void testMaxConnections() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+    }\n+\n+    @Test\n+    void testMetricReportersMessageMaxUncleanLeaderElection() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 483712e09..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -278,6 +70,6 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n     }\n }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 6f0d5a76e..712f00643 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -71,5 +127,8 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n+\n+        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n+        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 712f00643..a50349bb1 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -127,8 +140,5 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-\n-        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex a50349bb1..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,144 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.DynamicTest;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.TestFactory;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ThreadLocalRandom;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- * DynamicConfigurationSharedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update\n- * Shared -> for each test case we same configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @TestFactory\n-    Iterator<DynamicTest> testDynConfiguration() {\n-\n-        List<DynamicTest> dynamicTests = new ArrayList<>(40);\n-\n-        String generatedTestCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        String[] testCases = generatedTestCases.split(\"\\n\");\n-\n-        for (String testCaseLine : testCases) {\n-            String[] testCase = testCaseLine.split(\",\");\n-            dynamicTests.add(DynamicTest.dynamicTest(\"Test \" + testCase[0] + \"->\" + testCase[1], () -> {\n-                // exercise phase\n-                KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, testCase[0], testCase[1]);\n-\n-                // verify phase\n-                assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, testCase[0], testCase[1]), is(true));\n-                assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), testCase[0], testCase[1]), is(true));\n-            }));\n-        }\n-\n-        return dynamicTests.iterator();\n-    }\n-\n-    /**\n-     * Method, which dynamically generate test cases based on Kafka version\n-     * @param kafkaVersion specific kafka version\n-     * @return String generated test cases\n-     */\n-    private static String generateTestCases(String kafkaVersion) {\n-\n-        StringBuilder testCases = new StringBuilder();\n-\n-        Map<String, Object> dynamicProperties = KafkaUtils.getDynamicConfigurationProperties(kafkaVersion);\n-\n-        dynamicProperties.forEach((key, value) -> {\n-            testCases.append(key);\n-            testCases.append(\", \");\n-\n-            String type = ((LinkedHashMap<String, String>) value).get(\"type\");\n-            Object stochasticChosenValue;\n-\n-            switch (type) {\n-                case \"STRING\":\n-                    if (key.equals(\"compression.type\")) {\n-                        List<String> compressionTypes = Arrays.asList(\"snappy\", \"gzip\", \"lz4\", \"zstd\");\n-\n-                        stochasticChosenValue = compressionTypes.get(ThreadLocalRandom.current().nextInt(0, compressionTypes.size() - 1));\n-                        testCases.append(stochasticChosenValue);\n-                    } else {\n-                        testCases.append(\" \");\n-                    }\n-                    break;\n-                case \"INT\":\n-                case \"LONG\":\n-                    if (key.equals(\"background.threads\") || key.equals(\"log.cleaner.io.buffer.load.factor\") ||\n-                        key.equals(\"log.retention.ms\") || key.equals(\"max.connections\") ||\n-                        key.equals(\"max.connections.per.ip\")) {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(1, 20);\n-                    } else {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(100, 50_000);\n-                    }\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"DOUBLE\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextDouble(1, 20);\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"BOOLEAN\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextInt(2) == 0 ? true : false;\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"LIST\":\n-                    // metric.reporters has default empty '\"\"'\n-                    // log.cleanup.policy = [delete, compact] -> default delete\n-\n-                    if (key.equals(\"log.cleanup.policy\")) {\n-                        stochasticChosenValue = \"[delete]\";\n-                    } else {\n-                        stochasticChosenValue = \" \";\n-                    }\n-\n-                    testCases.append(stochasticChosenValue);\n-            }\n-            testCases.append(\",\");\n-            testCases.append(\"\\n\");\n-        });\n-\n-        return testCases.toString();\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 9444a5b20..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,283 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.Arrays;\n-\n-import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @Test\n-    void testBackgroundThreads() {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n-    }\n-\n-    @Tag(ACCEPTANCE)\n-    @Test\n-    void testCompressionType() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n-\n-    }\n-\n-    @Test\n-    void testLogFlush() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n-\n-    }\n-\n-    @Test\n-    void testLogRetention() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n-    }\n-\n-    @Test\n-    void testLogSegment() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n-    }\n-\n-    @Test\n-    void testLogRoll() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n-    }\n-\n-    @Test\n-    void testLogCleaner() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n-    }\n-\n-    @Test\n-    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n-    }\n-\n-    @Test\n-    void testLogIndexLogMessageLogMessage() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n-    }\n-\n-    @Test\n-    void testMaxConnections() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n-    }\n-\n-    @Test\n-    void testMetricReportersMessageMaxUncleanLeaderElection() {\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n-\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n-\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443896", "body": "How the output of this log looks? It's formated to be easily readable ?", "bodyText": "How the output of this log looks? It's formated to be easily readable ?", "bodyHTML": "<p dir=\"auto\">How the output of this log looks? It's formated to be easily readable ?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:13:35Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NTI3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464985279", "bodyText": "Yes :)", "author": "see-quick", "createdAt": "2020-08-04T11:34:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fcca5b948..d5b94d917 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -161,11 +161,11 @@ public class KafkaUtils {\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(kafkaDynamicConfiguration, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d5b94d917..aad772f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,14 +158,14 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex aad772f4d..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -172,7 +184,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fcca5b948..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,71 +189,148 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration.toString(),\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n-            kafkaDynamicConfiguration.toString(),\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n+\n+    /**\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n+    }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444400", "body": "Maybe we should show dyn.configuration in case of error as well?", "bodyText": "Maybe we should show dyn.configuration in case of error as well?", "bodyHTML": "<p dir=\"auto\">Maybe we should show dyn.configuration in case of error as well?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:15:01Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwMTcxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465001711", "bodyText": "?", "author": "see-quick", "createdAt": "2020-08-04T12:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwNDI1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465004252", "bodyText": "If the error occurs then we show the Kafka Pod {}....", "author": "see-quick", "createdAt": "2020-08-04T12:12:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fcca5b948..d5b94d917 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -201,27 +201,31 @@ public class KafkaUtils {\n     /**\n      * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param kafkaDynamicConfiguration key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d5b94d917..8e6c33747 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,4 +238,76 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,72 +248,74 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fcca5b948..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,71 +189,148 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration.toString(), value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration.toString(),\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n-            kafkaDynamicConfiguration.toString(),\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n+\n+    /**\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n+    }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDY1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444654", "body": "Why 2 brokers instead of 3?", "bodyText": "Why 2 brokers instead of 3?", "bodyHTML": "<p dir=\"auto\">Why 2 brokers instead of 3?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:15:34Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,65 +45,39 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(\"true\");\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n \n         InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n \n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -68,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MTQ1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463451451", "body": "Are you sure we don't have similar test like this? Maybe in `ListenersST` or in `rollingupdate` ?", "bodyText": "Are you sure we don't have similar test like this? Maybe in ListenersST or in rollingupdate ?", "bodyHTML": "<p dir=\"auto\">Are you sure we don't have similar test like this? Maybe in <code>ListenersST</code> or in <code>rollingupdate</code> ?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:32:33Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -225,48 +145,21 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             kafkaClusterSpec.setListeners(kl);\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        updateAndVerifyDynConf(\"false\");\n     }\n \n     @Test\n-    @Tag(NODEPORT_SUPPORTED)\n     @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n     void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n                 .editSpec()\n                     .editKafka()\n                         .withNewListeners()\n                             .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n                             .endKafkaListenerExternalLoadBalancer()\n                         .endListeners()\n                         .withConfig(kafkaConfig)\n", "next_change": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a4d75b43b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -148,24 +167,35 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n-    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n     void testDynamicConfigurationExternalTls() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -41,238 +47,192 @@ import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.CoreMatchers.is;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n \n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n     private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 3;\n+\n+    private Map<String, Object> kafkaConfig;\n \n     @Test\n     void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n \n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n         String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n     }\n \n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalLoadBalancer()\n-                        .endKafkaListenerExternalLoadBalancer()\n-                        .withNewPlain()\n-                        .endPlain()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n-                .endSpec()\n-                .done();\n+            .endSpec()\n+            .done();\n \n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n \n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+        updateAndVerifyDynConf(kafkaConfig);\n \n         kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463453011", "body": "Indent", "bodyText": "Indent", "bodyHTML": "<p dir=\"auto\">Indent</p>", "author": "Frawless", "createdAt": "2020-07-31T07:36:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1534,332 +1524,18 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NzQ5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464987490", "bodyText": "Always fun! :D", "author": "see-quick", "createdAt": "2020-08-04T11:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex e2051b074..27171681c 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1524,11 +1524,11 @@ class KafkaST extends AbstractST {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-            .editKafka()\n-            .addToConfig(\"offsets.topic.replication.factor\", 4)\n-            .addToConfig(\"transaction.state.log.min.isr\", 4)\n-            .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-            .endKafka()\n+                .editKafka()\n+                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n+                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n+                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n+                .endKafka()\n             .endSpec().build());\n \n         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(CLUSTER_NAME, NAMESPACE,\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex 27171681c..b3a9d28ff 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1532,10 +1520,348 @@ class KafkaST extends AbstractST {\n             .endSpec().build());\n \n         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(CLUSTER_NAME, NAMESPACE,\n-            \"Kafka configuration option .* should be set to \" + replicas + \" or less because 'spec.kafka.replicas' is \" + replicas);\n+                \"Kafka configuration option .* should be set to \" + replicas + \" or less because 'spec.kafka.replicas' is \" + replicas);\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @SuppressWarnings({\"checkstyle:MethodLength\"})\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"plain\")\n+                            .withPort(9092)\n+                            .withType(KafkaListenerType.INTERNAL)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.LOADBALANCER)\n+                            .withTls(true)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n+                    new GenericKafkaListenerBuilder()\n+                            .withName(\"plain\")\n+                            .withPort(9092)\n+                            .withType(KafkaListenerType.INTERNAL)\n+                            .withTls(false)\n+                            .build(),\n+                    new GenericKafkaListenerBuilder()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(true)\n+                            .build()\n+            ), null);\n+            kafkaClusterSpec.setListeners(lst);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n+                    new GenericKafkaListenerBuilder()\n+                            .withName(\"plain\")\n+                            .withPort(9092)\n+                            .withType(KafkaListenerType.INTERNAL)\n+                            .withTls(false)\n+                            .build()\n+            ), null);\n+            kafkaClusterSpec.setListeners(lst);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .addNewGenericKafkaListener()\n+                                .withName(\"plain\")\n+                                .withPort(9092)\n+                                .withType(KafkaListenerType.INTERNAL)\n+                                .withTls(false)\n+                            .endGenericKafkaListener()\n+                            .addNewGenericKafkaListener()\n+                                .withName(\"external\")\n+                                .withPort(9094)\n+                                .withType(KafkaListenerType.LOADBALANCER)\n+                                .withTls(false)\n+                            .endGenericKafkaListener()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n+                    new GenericKafkaListenerBuilder()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(true)\n+                            .withNewKafkaListenerAuthenticationTlsAuth()\n+                            .endKafkaListenerAuthenticationTlsAuth()\n+                            .build()\n+            ), null);\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(lst);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.receiveMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n+                    new GenericKafkaListenerBuilder()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                            .build()\n+            ), null);\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(lst);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n     protected void checkKafkaConfiguration(String podNamePrefix, Map<String, Object> config, String clusterName) {\n         LOGGER.info(\"Checking kafka configuration\");\n         List<Pod> pods = kubeClient().listPodsByPrefixInName(podNamePrefix);\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex b3a9d28ff..14f2c978b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1512,356 +1505,18 @@ class KafkaST extends AbstractST {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()\n+            .addToConfig(\"offsets.topic.replication.factor\", 4)\n+            .addToConfig(\"transaction.state.log.min.isr\", 4)\n+            .addToConfig(\"transaction.state.log.replication.factor\", 4)\n+            .endKafka()\n             .endSpec().build());\n \n         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(CLUSTER_NAME, NAMESPACE,\n-                \"Kafka configuration option .* should be set to \" + replicas + \" or less because 'spec.kafka.replicas' is \" + replicas);\n+            \"Kafka configuration option .* should be set to \" + replicas + \" or less because 'spec.kafka.replicas' is \" + replicas);\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n-    @SuppressWarnings({\"checkstyle:MethodLength\"})\n-    void testDynamicConfigurationWithExternalListeners() {\n-        int kafkaReplicas = 2;\n-        int zkReplicas = 1;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n-        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n-        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n-                .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .addNewGenericKafkaListener()\n-                            .withName(\"plain\")\n-                            .withPort(9092)\n-                            .withType(KafkaListenerType.INTERNAL)\n-                            .withTls(false)\n-                        .endGenericKafkaListener()\n-                        .addNewGenericKafkaListener()\n-                            .withName(\"external\")\n-                            .withPort(9094)\n-                            .withType(KafkaListenerType.LOADBALANCER)\n-                            .withTls(true)\n-                        .endGenericKafkaListener()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-                .endSpec()\n-                .done();\n-\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n-                    new GenericKafkaListenerBuilder()\n-                            .withName(\"plain\")\n-                            .withPort(9092)\n-                            .withType(KafkaListenerType.INTERNAL)\n-                            .withTls(false)\n-                            .build(),\n-                    new GenericKafkaListenerBuilder()\n-                            .withName(\"external\")\n-                            .withPort(9094)\n-                            .withType(KafkaListenerType.NODEPORT)\n-                            .withTls(true)\n-                            .build()\n-            ), null);\n-            kafkaClusterSpec.setListeners(lst);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n-                    new GenericKafkaListenerBuilder()\n-                            .withName(\"plain\")\n-                            .withPort(9092)\n-                            .withType(KafkaListenerType.INTERNAL)\n-                            .withTls(false)\n-                            .build()\n-            ), null);\n-            kafkaClusterSpec.setListeners(lst);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        // change dynamically changeable option\n-        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(LOADBALANCER_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        int kafkaReplicas = 2;\n-        Map<String, Object> kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"default.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n-\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .addNewGenericKafkaListener()\n-                                .withName(\"plain\")\n-                                .withPort(9092)\n-                                .withType(KafkaListenerType.INTERNAL)\n-                                .withTls(false)\n-                            .endGenericKafkaListener()\n-                            .addNewGenericKafkaListener()\n-                                .withName(\"external\")\n-                                .withPort(9094)\n-                                .withType(KafkaListenerType.LOADBALANCER)\n-                                .withTls(false)\n-                            .endGenericKafkaListener()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = \"john\";\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n-                    new GenericKafkaListenerBuilder()\n-                            .withName(\"external\")\n-                            .withPort(9094)\n-                            .withType(KafkaListenerType.NODEPORT)\n-                            .withTls(true)\n-                            .withNewKafkaListenerAuthenticationTlsAuth()\n-                            .endKafkaListenerAuthenticationTlsAuth()\n-                            .build()\n-            ), null);\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(lst);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.receiveMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            ArrayOrObjectKafkaListeners lst = new ArrayOrObjectKafkaListeners(asList(\n-                    new GenericKafkaListenerBuilder()\n-                            .withName(\"external\")\n-                            .withPort(9094)\n-                            .withType(KafkaListenerType.NODEPORT)\n-                            .withTls(false)\n-                            .build()\n-            ), null);\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(lst);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n     protected void checkKafkaConfiguration(String podNamePrefix, Map<String, Object> config, String clusterName) {\n         LOGGER.info(\"Checking kafka configuration\");\n         List<Pod> pods = kubeClient().listPodsByPrefixInName(podNamePrefix);\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex e2051b074..7cda523be 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1524,11 +1507,11 @@ class KafkaST extends AbstractST {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-            .editKafka()\n-            .addToConfig(\"offsets.topic.replication.factor\", 4)\n-            .addToConfig(\"transaction.state.log.min.isr\", 4)\n-            .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-            .endKafka()\n+                .editKafka()\n+                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n+                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n+                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n+                .endKafka()\n             .endSpec().build());\n \n         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(CLUSTER_NAME, NAMESPACE,\n", "next_change": {"commit": "9913c5a904ae37877045cd63db97843655de4d00", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex 7cda523be..218ac2533 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1519,6 +1521,53 @@ class KafkaST extends AbstractST {\n         KafkaResource.kafkaClient().inNamespace(NAMESPACE).delete(kafka);\n     }\n \n+    @Test\n+    void testHostAliases() {\n+        HostAlias hostAlias = new HostAliasBuilder()\n+            .withIp(\"34.89.152.196\")\n+            .withHostnames(\"strimzi\")\n+            .build();\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewTemplate()\n+                        .withNewPod()\n+                            .withHostAliases(hostAlias)\n+                        .endPod()\n+                    .endTemplate()\n+                .endKafka()\n+                .editZookeeper()\n+                    .withNewTemplate()\n+                        .withNewPod()\n+                            .withHostAliases(hostAlias)\n+                        .endPod()\n+                    .endTemplate()\n+                .endZookeeper()\n+            .endSpec()\n+            .done();\n+\n+        List<String> pods = kubeClient().listPodNames(Labels.STRIMZI_CLUSTER_LABEL, CLUSTER_NAME);\n+\n+        for (String podName : pods) {\n+            if (!podName.contains(\"entity-operator\")) {\n+                String containerName = podName.contains(\"kafka\") ? \"kafka\" : \"zookeeper\";\n+                LOGGER.info(\"Checking host alias settings in {}, {} container\", podName, containerName);\n+\n+                LOGGER.info(\"Trying to ping strimzi.io by ping strimzi command\");\n+                String output = cmdKubeClient().execInPodContainer(false, podName, containerName, \"ping\", \"-c\", \"5\", \"strimzi\").out();\n+\n+                LOGGER.info(\"Checking output of ping\");\n+                assertThat(output, containsString(\"PING strimzi (34.89.152.196)\"));\n+                assertThat(output, containsString(\"5 packets transmitted, 5 received\"));\n+\n+                LOGGER.info(\"Checking the /etc/hosts file\");\n+                output = cmdKubeClient().execInPodContainer(false, podName, containerName, \"cat\", \"/etc/hosts\").out();\n+                assertThat(output, containsString(\"# Entries added by HostAliases.\\n34.89.152.196\\tstrimzi\"));\n+            }\n+        }\n+    }\n+\n     protected void checkKafkaConfiguration(String podNamePrefix, Map<String, Object> config, String clusterName) {\n         LOGGER.info(\"Checking kafka configuration\");\n         List<Pod> pods = kubeClient().listPodsByPrefixInName(podNamePrefix);\n", "next_change": {"commit": "68b800cbdaf1160c005f2991223a34628d0e153e", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex 218ac2533..503a6d720 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1552,18 +1552,9 @@ class KafkaST extends AbstractST {\n         for (String podName : pods) {\n             if (!podName.contains(\"entity-operator\")) {\n                 String containerName = podName.contains(\"kafka\") ? \"kafka\" : \"zookeeper\";\n-                LOGGER.info(\"Checking host alias settings in {}, {} container\", podName, containerName);\n-\n-                LOGGER.info(\"Trying to ping strimzi.io by ping strimzi command\");\n-                String output = cmdKubeClient().execInPodContainer(false, podName, containerName, \"ping\", \"-c\", \"5\", \"strimzi\").out();\n-\n-                LOGGER.info(\"Checking output of ping\");\n-                assertThat(output, containsString(\"PING strimzi (34.89.152.196)\"));\n-                assertThat(output, containsString(\"5 packets transmitted, 5 received\"));\n-\n                 LOGGER.info(\"Checking the /etc/hosts file\");\n-                output = cmdKubeClient().execInPodContainer(false, podName, containerName, \"cat\", \"/etc/hosts\").out();\n-                assertThat(output, containsString(\"# Entries added by HostAliases.\\n34.89.152.196\\tstrimzi\"));\n+                String output = cmdKubeClient().execInPodContainer(false, podName, containerName, \"cat\", \"/etc/hosts\").out();\n+                assertThat(output, containsString(etcHostsData));\n             }\n         }\n     }\n", "next_change": {"commit": "29160435430e08df7efb9437237db723379737a0", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex 503a6d720..c69408f75 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1559,6 +1568,83 @@ class KafkaST extends AbstractST {\n         }\n     }\n \n+    @Test\n+    @Tag(INTERNAL_CLIENTS_USED)\n+    void testReadOnlyRootFileSystem() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 3)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewTemplate()\n+                            .withNewKafkaContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endKafkaContainer()\n+                        .endTemplate()\n+                    .endKafka()\n+                    .editZookeeper()\n+                        .withNewTemplate()\n+                            .withNewZookeeperContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endZookeeperContainer()\n+                        .endTemplate()\n+                    .endZookeeper()\n+                    .editEntityOperator()\n+                        .withNewTemplate()\n+                            .withNewTlsSidecarContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endTlsSidecarContainer()\n+                            .withNewTopicOperatorContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endTopicOperatorContainer()\n+                            .withNewUserOperatorContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endUserOperatorContainer()\n+                        .endTemplate()\n+                    .endEntityOperator()\n+                    .editOrNewKafkaExporter()\n+                        .withNewTemplate()\n+                            .withNewContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endContainer()\n+                        .endTemplate()\n+                    .endKafkaExporter()\n+                    .editOrNewCruiseControl()\n+                        .withNewTemplate()\n+                            .withNewTlsSidecarContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endTlsSidecarContainer()\n+                            .withNewCruiseControlContainer()\n+                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())\n+                            .endCruiseControlContainer()\n+                        .endTemplate()\n+                    .endCruiseControl()\n+                .endSpec()\n+                .done();\n+\n+        KafkaUtils.waitForKafkaReady(CLUSTER_NAME);\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TEST_TOPIC_NAME).done();\n+\n+        KafkaClientsResource.deployKafkaClients(false, CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS).done();\n+\n+        String kafkaClientsPodName = kubeClient().listPodsByPrefixInName(CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS).get(0).getMetadata().getName();\n+\n+        InternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n+                .withUsingPodName(kafkaClientsPodName)\n+                .withTopicName(TEST_TOPIC_NAME)\n+                .withNamespaceName(NAMESPACE)\n+                .withClusterName(CLUSTER_NAME)\n+                .withMessageCount(MESSAGE_COUNT)\n+                .withListenerName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n+                .build();\n+\n+        LOGGER.info(\"Checking produced and consumed messages to pod:{}\", kafkaClientsPodName);\n+\n+        internalKafkaClient.checkProducedAndConsumedMessages(\n+                internalKafkaClient.sendMessagesPlain(),\n+                internalKafkaClient.receiveMessagesPlain()\n+        );\n+    }\n+\n     protected void checkKafkaConfiguration(String podNamePrefix, Map<String, Object> config, String clusterName) {\n         LOGGER.info(\"Checking kafka configuration\");\n         List<Pod> pods = kubeClient().listPodsByPrefixInName(podNamePrefix);\n", "next_change": {"commit": "2941d1aebe0011b7cbf8474594a08e94654864af", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex c69408f75..cfcb99f1e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1645,11 +1716,11 @@ class KafkaST extends AbstractST {\n         );\n     }\n \n-    protected void checkKafkaConfiguration(String podNamePrefix, Map<String, Object> config, String clusterName) {\n+    protected void checkKafkaConfiguration(String namespaceName, String podNamePrefix, Map<String, Object> config, String clusterName) {\n         LOGGER.info(\"Checking kafka configuration\");\n-        List<Pod> pods = kubeClient().listPodsByPrefixInName(podNamePrefix);\n+        List<Pod> pods = kubeClient(namespaceName).listPodsByPrefixInName(namespaceName, podNamePrefix);\n \n-        Properties properties = configMap2Properties(kubeClient().getConfigMap(clusterName + \"-kafka-config\"));\n+        Properties properties = configMap2Properties(kubeClient(namespaceName).getConfigMap(namespaceName, clusterName + \"-kafka-config\"));\n \n         for (Map.Entry<String, Object> property : config.entrySet()) {\n             String key = property.getKey();\n", "next_change": {"commit": "1511a3a4d26b760fb966d9fb965fac07437d6de5", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex cfcb99f1e..a6bd604d2 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1720,14 +1662,16 @@ class KafkaST extends AbstractST {\n         LOGGER.info(\"Checking kafka configuration\");\n         List<Pod> pods = kubeClient(namespaceName).listPodsByPrefixInName(namespaceName, podNamePrefix);\n \n-        Properties properties = configMap2Properties(kubeClient(namespaceName).getConfigMap(namespaceName, clusterName + \"-kafka-config\"));\n+        for (String cmName : StUtils.getKafkaConfigurationConfigMaps(clusterName, pods.size())) {\n+            Properties properties = configMap2Properties(kubeClient(namespaceName).getConfigMap(namespaceName, cmName));\n \n-        for (Map.Entry<String, Object> property : config.entrySet()) {\n-            String key = property.getKey();\n-            Object val = property.getValue();\n+            for (Map.Entry<String, Object> property : config.entrySet()) {\n+                String key = property.getKey();\n+                Object val = property.getValue();\n \n-            assertThat(properties.keySet().contains(key), is(true));\n-            assertThat(properties.getProperty(key), is(val));\n+                assertThat(properties.keySet().contains(key), is(true));\n+                assertThat(properties.getProperty(key), is(val));\n+            }\n         }\n \n         for (Pod pod: pods) {\n", "next_change": {"commit": "7120e347ac56cf0babc6ad85d4aae4d8835835d9", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\nindex a6bd604d2..e817d522c 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java\n", "chunk": "@@ -1632,72 +1017,28 @@ class KafkaST extends AbstractST {\n                         .endTemplate()\n                     .endCruiseControl()\n                 .endSpec()\n-                .build());\n-\n-        KafkaUtils.waitForKafkaReady(namespaceName, clusterName);\n-\n-        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n-        resourceManager.createResource(extensionContext, KafkaClientsTemplates.kafkaClients(false, clusterName + \"-\" + Constants.KAFKA_CLIENTS).build());\n-\n-        final String kafkaClientsPodName = kubeClient(namespaceName).listPodsByPrefixInName(namespaceName, clusterName + \"-\" + Constants.KAFKA_CLIENTS).get(0).getMetadata().getName();\n-\n-        InternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n-                .withUsingPodName(kafkaClientsPodName)\n-                .withTopicName(topicName)\n-                .withNamespaceName(namespaceName)\n-                .withClusterName(clusterName)\n-                .withMessageCount(MESSAGE_COUNT)\n-                .withListenerName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n                 .build();\n \n-        LOGGER.info(\"Checking produced and consumed messages to pod:{}\", kafkaClientsPodName);\n-\n-        internalKafkaClient.checkProducedAndConsumedMessages(\n-                internalKafkaClient.sendMessagesPlain(),\n-                internalKafkaClient.receiveMessagesPlain()\n-        );\n-    }\n-\n-    protected void checkKafkaConfiguration(String namespaceName, String podNamePrefix, Map<String, Object> config, String clusterName) {\n-        LOGGER.info(\"Checking kafka configuration\");\n-        List<Pod> pods = kubeClient(namespaceName).listPodsByPrefixInName(namespaceName, podNamePrefix);\n-\n-        for (String cmName : StUtils.getKafkaConfigurationConfigMaps(clusterName, pods.size())) {\n-            Properties properties = configMap2Properties(kubeClient(namespaceName).getConfigMap(namespaceName, cmName));\n-\n-            for (Map.Entry<String, Object> property : config.entrySet()) {\n-                String key = property.getKey();\n-                Object val = property.getValue();\n-\n-                assertThat(properties.keySet().contains(key), is(true));\n-                assertThat(properties.getProperty(key), is(val));\n-            }\n-        }\n+        kafka.getSpec().getEntityOperator().getTemplate().setTopicOperatorContainer(null);\n \n-        for (Pod pod: pods) {\n-            ExecResult result = cmdKubeClient(namespaceName).execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\");\n-            Properties execProperties = stringToProperties(result.out());\n+        resourceManager.createResource(extensionContext, kafka);\n \n-            for (Map.Entry<String, Object> property : config.entrySet()) {\n-                String key = property.getKey();\n-                Object val = property.getValue();\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(testStorage.getClusterName(), testStorage.getTopicName()).build());\n \n-                assertThat(execProperties.keySet().contains(key), is(true));\n-                assertThat(execProperties.getProperty(key), is(val));\n-            }\n-        }\n-    }\n+        KafkaClients kafkaClients = new KafkaClientsBuilder()\n+            .withTopicName(testStorage.getTopicName())\n+            .withBootstrapAddress(KafkaResources.plainBootstrapAddress(testStorage.getClusterName()))\n+            .withNamespaceName(testStorage.getNamespaceName())\n+            .withProducerName(testStorage.getProducerName())\n+            .withConsumerName(testStorage.getConsumerName())\n+            .withMessageCount(testStorage.getMessageCount())\n+            .build();\n \n-    void checkStorageSizeForVolumes(List<PersistentVolumeClaim> volumes, String[] diskSizes, int kafkaRepl, int diskCount) {\n-        int k = 0;\n-        for (int i = 0; i < kafkaRepl; i++) {\n-            for (int j = 0; j < diskCount; j++) {\n-                LOGGER.info(\"Checking volume {} and size of storage {}\", volumes.get(k).getMetadata().getName(),\n-                        volumes.get(k).getSpec().getResources().getRequests().get(\"storage\"));\n-                assertThat(volumes.get(k).getSpec().getResources().getRequests().get(\"storage\"), is(new Quantity(diskSizes[i])));\n-                k++;\n-            }\n-        }\n+        resourceManager.createResource(extensionContext,\n+            kafkaClients.producerStrimzi(),\n+            kafkaClients.consumerStrimzi()\n+        );\n+        ClientUtils.waitForClientsSuccess(testStorage);\n     }\n \n     void verifyVolumeNamesAndLabels(String namespaceName, String clusterName, int kafkaReplicas, int diskCountPerReplica, String diskSizeGi) {\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f4ebeda6b652e8420abf89a1f4e0a52d32b6bbd3", "committedDate": "2020-09-14 15:41:26 +0200", "message": "[systemtest] Fixes for nightlies (#3634)"}, {"oid": "9913c5a904ae37877045cd63db97843655de4d00", "committedDate": "2020-09-18 21:56:08 +0200", "message": "add tests for host aliases (#3674)"}, {"oid": "68b800cbdaf1160c005f2991223a34628d0e153e", "committedDate": "2020-10-05 23:01:34 +0200", "message": "Remove ping from tests to avoid failures on azp (#3737)"}, {"oid": "703d8cb141de03f1a5aa20c983d6f290f7a02995", "committedDate": "2020-10-07 21:09:58 +0200", "message": "Schema (#3653)"}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "efff210e5465f05a277998920b5f49dca96d1f2a", "committedDate": "2020-12-21 11:51:55 +0100", "message": "Remove the remains of the -server JVM option (#4134)"}, {"oid": "c4cecd805d3c52429edc70d07837c9db9ee699a5", "committedDate": "2021-01-06 14:50:42 +0000", "message": "feat: Namespace RBAC scope deployment option (#3845)"}, {"oid": "29160435430e08df7efb9437237db723379737a0", "committedDate": "2021-01-06 16:48:45 +0100", "message": "Make it possible to run Strimzi with read-only root filesystem - Closes #4011 (#4162)"}, {"oid": "d7706e97cecfac85a803a41b4f7a6634323408c6", "committedDate": "2021-01-08 15:30:08 +0100", "message": "[MO] - [1st step paralelism] - random names for all resources in systemtests (#4092)"}, {"oid": "4a95d1ad5679769b507fe760d1b7a4307a929023", "committedDate": "2021-01-13 13:51:07 +0100", "message": "[systemtest] Refactor of resource classes and STs - prerequisite for fabric8 upgrade (#4182)"}, {"oid": "5eb8596bb282a11529dada4261e0306ed571d18f", "committedDate": "2021-01-14 01:05:49 +0100", "message": "some tests fixes (#4240)"}, {"oid": "44b400e552ca2ed0ec6a6e7ef637c29e8cd9cbd2", "committedDate": "2021-01-19 11:22:45 +0100", "message": "Initial Kafka Streams TopicStore. (#3405)"}, {"oid": "b9994e13a35299440eb24e01aa6668bf31369a0f", "committedDate": "2021-01-21 13:48:54 +0100", "message": "Upgrade kubernetes-client to 5.0.0 (#4238)"}, {"oid": "a91f3dadd8d0adb49e96a8ebeb7bb64b4a71ee50", "committedDate": "2021-01-25 10:46:30 +0100", "message": "rename create method to createAndWaitForReadiness (#4306)"}, {"oid": "652b4755223d0e35412c7300effb3cce3755a3e6", "committedDate": "2021-03-10 17:08:58 +0100", "message": "Move install, example and Helm chart files to packaging (#4513)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "2941d1aebe0011b7cbf8474594a08e94654864af", "committedDate": "2021-04-28 13:54:22 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaST (#4802)"}, {"oid": "14eebcdb73b1b251912ff6a9fa7f306dfba0fc00", "committedDate": "2021-05-06 18:24:11 +0200", "message": "Issue 4394 - custom filter for user labels (#4791)"}, {"oid": "364978fda140a6c463fbe7d8c732e0a5a7b86cf1", "committedDate": "2021-05-07 14:24:50 +0200", "message": "[MO] - [system tests] -> removing unnecessarity test cases for host alias (#4910)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "fad06d77fc83408bc0bd416d861700bef3f48e2c", "committedDate": "2021-07-02 10:05:57 +0200", "message": "[MO] - [system test] -> rbac support (#5137)"}, {"oid": "e66ed5c23ba7066984fbcb5050ffde57d0736668", "committedDate": "2021-07-12 11:08:47 +0200", "message": "Fix up usages of Optionals (#5243)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "13c61d54bfa6354fe1e458b2f57cb44bfb020c76", "committedDate": "2021-08-10 18:26:39 +0200", "message": "Update tags and make client examples images configurable (#5392)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "b80edd0eb83be2c7f904826fea8ecb8c493550c9", "committedDate": "2021-08-16 22:12:52 +0200", "message": "Upgrade kubernetes-client dependency (#5431)"}, {"oid": "280c4df856bc628527383ad56e8a289bb11f192c", "committedDate": "2021-08-26 11:20:42 +0200", "message": "ST: Remove todo from system tests regarding the STRIMZI_RBAC_SCOPE configuration (#5477)"}, {"oid": "671ba10faea6d8d9cb91e007bd4e77bd67f8f859", "committedDate": "2021-09-15 12:12:21 +0200", "message": "Remove OpenShift templates from examples (#5548)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "97c09c6668c583eefc768b3a52cf193f39af554d", "committedDate": "2021-12-01 16:25:20 +0100", "message": "Remove StatefulSets from ZooKeeper (#5884)"}, {"oid": "399eb729201576e54367eb47a0cf07a1fd70c806", "committedDate": "2021-12-23 11:04:30 +0100", "message": "[Issue 5791] Use separate certificates for the topic operator and user operator. (#6006)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "9b25c199080d509a5927e3da1dd32419d862e921", "committedDate": "2022-01-13 17:43:57 +0100", "message": "Remove timemeasuring system from test and systemtest package (#6177)"}, {"oid": "7127543f096a0ebeea89d90e31cea062bd708fb3", "committedDate": "2022-01-19 23:02:33 +0100", "message": "Extend StrimziPodSets to Kafka brokers (#6186)"}, {"oid": "011fcdb8bf74aaf7fad8e09d76de00155ee030f7", "committedDate": "2022-02-14 15:30:28 +0100", "message": "workaround removal init (#6323)"}, {"oid": "980b681f1f9d5e940bb165210539291210dd5b00", "committedDate": "2022-02-16 13:36:48 +0100", "message": "[systemtest] Refactor of example clients -> test-clients (#6292)"}, {"oid": "1511a3a4d26b760fb966d9fb965fac07437d6de5", "committedDate": "2022-03-10 13:52:53 +0100", "message": "Per-broker configuration with Kafka PodSets (#6487)"}, {"oid": "f9fb386d20fa8b0b22adc49af749a8138413b3c0", "committedDate": "2022-03-17 14:37:27 +0100", "message": "[MO] - [system test] -> Using 3 ZooKeeper replicas instead of 2 (#6529)"}, {"oid": "98968e33ca24e92bf3fc035701901fac8de152c1", "committedDate": "2022-04-04 16:34:10 +0200", "message": "[MO] - [system test] -> proper deletion of KafkaSTs additional namespaces (#6622)"}, {"oid": "6ad176edf18faf9e71e78eed69910a5ca3e51ab1", "committedDate": "2022-04-09 13:36:26 +0200", "message": "[systemtest] Test clients exchange - kafka, log and metrics packages (#6450)"}, {"oid": "c5b8d2d3c286c6a388ed984b60953f4ab797b159", "committedDate": "2022-05-07 01:05:25 +0200", "message": "[ST Enhancement] Deletion of remaining pvcs after tests. (#6749)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "b40f0c71cbb487a793f2c288274c228abc39398c", "committedDate": "2022-10-18 09:50:24 +0200", "message": "[ST] Add message count, kafka + zk + eo STS or deployment names into the TestStorage (#7423)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "a962614e7e5b0cf5cce91048cb915dd5e318bd9d", "committedDate": "2023-01-27 11:47:13 +0100", "message": "Improve logs collection from resources deloyed during system tests execution (#7964)"}, {"oid": "52dea01abf743ad1e9ba426a73e72cf45fb14792", "committedDate": "2023-01-30 21:57:04 +0100", "message": "Fix testUOListeningOnlyUsersInSameCluster test case (#7996)"}, {"oid": "a6fa3dc69806a76eda3f1928efc00f96d0ab6593", "committedDate": "2023-02-09 10:37:51 +0100", "message": "Removes un-necessary test cases from systemtests (#8037)"}, {"oid": "946d09f0033c230b04c3acac4fc41255b5199a82", "committedDate": "2023-04-06 17:47:54 +0200", "message": "[ST] removal of test testEODeletion due to it being mostly covered in EntityOperatorReconcilerTest class (#8346)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}, {"oid": "3b6d462e38e75f6966d28875b11e7e8bd029bc13", "committedDate": "2023-04-27 12:25:19 +0200", "message": "[ST] Triage kafkaST suite, merge test focused on labels into one test.  (#8434)"}, {"oid": "7120e347ac56cf0babc6ad85d4aae4d8835835d9", "committedDate": "2023-05-04 14:51:53 +0200", "message": "[ST] Removal of duplicate tests, merging some tests together in KafkaST (#8442)"}, {"oid": "856701b3777a7d1775c9f8393a0096a7b6f4caad", "committedDate": "2023-05-09 10:58:18 +0200", "message": "[ST] Merging tests which remove parts of Entity Operator into 1 test.  (#8461)"}, {"oid": "eeb2d979ac3d4de28e864587c3ed58f9e6f792ba", "committedDate": "2023-05-11 12:55:54 +0200", "message": "[ST] Move and Improve testTopicWithoutLabels, from KafkaST to TopicST (#8475)"}, {"oid": "97de72ba4bc7690cbdcce9aff8333fc663c9bd93", "committedDate": "2023-05-15 09:42:18 +0200", "message": "[ST] KafkaST, improve testReadOnlyRootFileSystem and testJvmAndResources (#8505)"}, {"oid": "52b138134ef5a25fb28c6767a559462a351f7e80", "committedDate": "2023-05-18 14:23:08 +0200", "message": "[ST] UserST, improve checks and move testUOListeningOnlyUsersInSameCluster from KafkaST to UserST (#8523)"}]}, {"oid": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-04T11:47:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MTEzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465271135", "body": "Indent?", "bodyText": "Indent?", "bodyHTML": "<p dir=\"auto\">Indent?</p>", "author": "im-konge", "createdAt": "2020-08-04T19:14:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 51f61281b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 51f61281b..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -68,105 +75,164 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        updateAndVerifyDynConf(\"true\");\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n-    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-                .editSpec()\n-                    .editKafka()\n-                        .withNewListeners()\n-                            .withNewKafkaListenerExternalLoadBalancer()\n-                                .withTls(false)\n-                            .endKafkaListenerExternalLoadBalancer()\n-                        .endListeners()\n-                        .withConfig(kafkaConfig)\n-                    .endKafka()\n-                .endSpec()\n-                .done();\n+            .editSpec()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n         KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MjY0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465272645", "body": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "bodyText": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "bodyHTML": "<p dir=\"auto\">I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)</p>", "author": "im-konge", "createdAt": "2020-08-04T19:17:01Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 51f61281b..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -186,7 +214,6 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(CLUSTER_NAME)\n             .withMessageCount(MESSAGE_COUNT)\n-            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n             .build();\n \n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 52%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 51f61281b..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -186,7 +251,6 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(CLUSTER_NAME)\n             .withMessageCount(MESSAGE_COUNT)\n-            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n             .build();\n \n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465273689", "body": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like `parametrizedTest`? Maybe @samuel-hawker will be better for this :smile: ", "bodyText": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like parametrizedTest? Maybe @samuel-hawker will be better for this \ud83d\ude04", "bodyHTML": "<p dir=\"auto\">This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like <code>parametrizedTest</code>? Maybe <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/samuel-hawker/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samuel-hawker\">@samuel-hawker</a> will be better for this <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji></p>", "author": "im-konge", "createdAt": "2020-08-04T19:19:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+\n+        \"log.flush.interval.ms, \" + 20,\n+\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n+\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n+\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU4NTkxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465585915", "bodyText": "Already change the naming )", "author": "see-quick", "createdAt": "2020-08-05T09:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 461b9b89b..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -55,20 +49,13 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n         \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n         \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.cleaner.threads, \" + 1,\n-\n-        \"num.network.threads, \" + 2,\n-        \"testLogIndexLogMessageLogMessage, \" + 5,\n-        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n         \"log.preallocate, \" + true,\n-\n         \"max.connections, \" + 10,\n         \"max.connections.per.ip, \" + 20,\n-\n         \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048\n+        \"message.max.bytes, \" + 2048,\n     })\n-    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n         // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n \n", "next_change": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nnew file mode 100644\nindex 000000000..483712e09\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @Test\n+    void testBackgroundThreads() {\n+        // exercise phase\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n+\n+        // verify phase\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n+    }\n+\n+    @Tag(ACCEPTANCE)\n+    @Test\n+    void testCompressionType() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogFlush() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogRetention() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+    }\n+\n+    @Test\n+    void testLogSegment() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+    }\n+\n+    @Test\n+    void testLogRoll() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+    }\n+\n+    @Test\n+    void testLogCleaner() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+    }\n+\n+    @Test\n+    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+    }\n+\n+    @Test\n+    void testLogIndexLogMessageLogMessage() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+    }\n+\n+    @Test\n+    void testMaxConnections() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+    }\n+\n+    @Test\n+    void testMetricReportersMessageMaxUncleanLeaderElection() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 483712e09..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -278,6 +70,6 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n     }\n }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 6f0d5a76e..712f00643 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -71,5 +127,8 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n+\n+        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n+        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 712f00643..a50349bb1 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -127,8 +140,5 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-\n-        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex a50349bb1..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,144 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.DynamicTest;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.TestFactory;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ThreadLocalRandom;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- * DynamicConfigurationSharedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update\n- * Shared -> for each test case we same configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @TestFactory\n-    Iterator<DynamicTest> testDynConfiguration() {\n-\n-        List<DynamicTest> dynamicTests = new ArrayList<>(40);\n-\n-        String generatedTestCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        String[] testCases = generatedTestCases.split(\"\\n\");\n-\n-        for (String testCaseLine : testCases) {\n-            String[] testCase = testCaseLine.split(\",\");\n-            dynamicTests.add(DynamicTest.dynamicTest(\"Test \" + testCase[0] + \"->\" + testCase[1], () -> {\n-                // exercise phase\n-                KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, testCase[0], testCase[1]);\n-\n-                // verify phase\n-                assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, testCase[0], testCase[1]), is(true));\n-                assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), testCase[0], testCase[1]), is(true));\n-            }));\n-        }\n-\n-        return dynamicTests.iterator();\n-    }\n-\n-    /**\n-     * Method, which dynamically generate test cases based on Kafka version\n-     * @param kafkaVersion specific kafka version\n-     * @return String generated test cases\n-     */\n-    private static String generateTestCases(String kafkaVersion) {\n-\n-        StringBuilder testCases = new StringBuilder();\n-\n-        Map<String, Object> dynamicProperties = KafkaUtils.getDynamicConfigurationProperties(kafkaVersion);\n-\n-        dynamicProperties.forEach((key, value) -> {\n-            testCases.append(key);\n-            testCases.append(\", \");\n-\n-            String type = ((LinkedHashMap<String, String>) value).get(\"type\");\n-            Object stochasticChosenValue;\n-\n-            switch (type) {\n-                case \"STRING\":\n-                    if (key.equals(\"compression.type\")) {\n-                        List<String> compressionTypes = Arrays.asList(\"snappy\", \"gzip\", \"lz4\", \"zstd\");\n-\n-                        stochasticChosenValue = compressionTypes.get(ThreadLocalRandom.current().nextInt(0, compressionTypes.size() - 1));\n-                        testCases.append(stochasticChosenValue);\n-                    } else {\n-                        testCases.append(\" \");\n-                    }\n-                    break;\n-                case \"INT\":\n-                case \"LONG\":\n-                    if (key.equals(\"background.threads\") || key.equals(\"log.cleaner.io.buffer.load.factor\") ||\n-                        key.equals(\"log.retention.ms\") || key.equals(\"max.connections\") ||\n-                        key.equals(\"max.connections.per.ip\")) {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(1, 20);\n-                    } else {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(100, 50_000);\n-                    }\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"DOUBLE\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextDouble(1, 20);\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"BOOLEAN\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextInt(2) == 0 ? true : false;\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"LIST\":\n-                    // metric.reporters has default empty '\"\"'\n-                    // log.cleanup.policy = [delete, compact] -> default delete\n-\n-                    if (key.equals(\"log.cleanup.policy\")) {\n-                        stochasticChosenValue = \"[delete]\";\n-                    } else {\n-                        stochasticChosenValue = \" \";\n-                    }\n-\n-                    testCases.append(stochasticChosenValue);\n-            }\n-            testCases.append(\",\");\n-            testCases.append(\"\\n\");\n-        });\n-\n-        return testCases.toString();\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 461b9b89b..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,88 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-\n-        \"log.flush.interval.ms, \" + 20,\n-\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.cleaner.threads, \" + 1,\n-\n-        \"num.network.threads, \" + 2,\n-        \"testLogIndexLogMessageLogMessage, \" + 5,\n-        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n-        \"log.preallocate, \" + true,\n-\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048\n-    })\n-    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338023", "body": "What enum?", "bodyText": "What enum?", "bodyHTML": "<p dir=\"auto\">What enum?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:10:19Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2MTgxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466361819", "bodyText": "old code....i have changed it.", "author": "see-quick", "createdAt": "2020-08-06T12:00:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..aad772f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,14 +158,14 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex aad772f4d..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -172,7 +184,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,69 +189,68 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n                         LOGGER.error(\"Kafka configuration {}\", result);\n                         return false;\n                     }\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODkzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338931", "body": "For all these I would call it `brokerConfigName` or something. You don't care, in these methods whether the particular config supports dynamic update or not. ", "bodyText": "For all these I would call it brokerConfigName or something. You don't care, in these methods whether the particular config supports dynamic update or not.", "bodyHTML": "<p dir=\"auto\">For all these I would call it <code>brokerConfigName</code> or something. You don't care, in these methods whether the particular config supports dynamic update or not.</p>", "author": "tombentley", "createdAt": "2020-08-06T11:12:22Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..aad772f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -175,39 +175,39 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n      * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex aad772f4d..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -213,7 +224,7 @@ public class KafkaUtils {\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,69 +189,68 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n                         LOGGER.error(\"Kafka configuration {}\", result);\n                         return false;\n                     }\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTI3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339278", "body": "```suggestion\r\n     * Verifies that updated configuration was successfully changed inside Kafka pods\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n          \n          \n            \n                 * Verifies that updated configuration was successfully changed inside Kafka pods", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which, verifying </span>that <span class=\"x x-first x-last\">updating</span> configuration <span class=\"x x-first x-last\">were</span> successfully changed inside <span class=\"pl-smi\">Kafka</span> pods</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Verifies</span><span class=\"x x-last\"> </span>that <span class=\"x x-first x-last\">updated</span> configuration <span class=\"x x-first x-last\">was</span> successfully changed inside <span class=\"pl-smi\">Kafka</span> pods</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-06T11:13:07Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..aad772f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -175,39 +175,39 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n      * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex aad772f4d..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -213,7 +224,7 @@ public class KafkaUtils {\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,69 +189,68 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n                         LOGGER.error(\"Kafka configuration {}\", result);\n                         return false;\n                     }\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339727", "body": "Is there any reason for doing this via `kafka-configs.sh` and not just using an `Admin` client instance?", "bodyText": "Is there any reason for doing this via kafka-configs.sh and not just using an Admin client instance?", "bodyHTML": "<p dir=\"auto\">Is there any reason for doing this via <code>kafka-configs.sh</code> and not just using an <code>Admin</code> client instance?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:14:12Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2OTEwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466369103", "bodyText": "AdminClient has one disadvantage. If we use him we need som external listener, which is not a good for run the tests in all envinronments.", "author": "see-quick", "createdAt": "2020-08-06T12:15:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..aad772f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -175,39 +175,39 @@ public class KafkaUtils {\n      * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n      * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex aad772f4d..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -213,7 +224,7 @@ public class KafkaUtils {\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3e57a1dc6..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -158,69 +189,68 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n         KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(kafkaDynamicConfiguration, value);\n+            config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n-        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n     }\n \n     /**\n-     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            kafkaDynamicConfiguration,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n-            kafkaDynamicConfiguration,\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * Verifies that updated configuration was successfully changed inside Kafka pods\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param kafkaDynamicConfiguration key of specific property\n+     * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      * @return\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n                     String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration, value);\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n                         LOGGER.error(\"Kafka configuration {}\", result);\n                         return false;\n                     }\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466340844", "body": "Why are you calling both like this?", "bodyText": "Why are you calling both like this?", "bodyHTML": "<p dir=\"auto\">Why are you calling both like this?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:16:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NzI0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466367242", "bodyText": "This is just refactored tests, which just update the Dyn. configuration of the broker. The point is to make as many changes as possible. If I understand correctly from @stanlyDoge.", "author": "see-quick", "createdAt": "2020-08-06T12:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NTc5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466375799", "bodyText": "Yeah, as I understand the test this two lines set the same property (unclean.leader.election.enable) to false and then to true. We are checking at the end of the test, whether kafka pods rolled.\nI would consider changing another property.", "author": "sknot-rh", "createdAt": "2020-08-06T12:28:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -128,8 +131,25 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466343658", "body": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\r\n\r\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "bodyText": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "bodyHTML": "<p dir=\"auto\">Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?</p>\n<p dir=\"auto\">It's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:22:38Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MDgzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466370839", "bodyText": "It is an almost a complete list of Dyn. configuration. I picked randomly, which were not read-only and also which does not have this\n FORBIDDEN_PREFIXES = \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl., security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer., super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers;", "author": "see-quick", "createdAt": "2020-08-06T12:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MTY4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466371686", "bodyText": "Hmmm, I can create a .csv files, which will bind to specific version :)", "author": "see-quick", "createdAt": "2020-08-06T12:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NzUxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469367510", "bodyText": "No. You could include columns in the CSV defining the version range between which to test with that config, skipping versions outside that range. But the key point here is that to get complete coverage we need to automate this, which means not relying on an annotation to drive the values, but that generated file I described.", "author": "tombentley", "createdAt": "2020-08-12T15:56:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}], "type": "inlineReview", "revised_code": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nnew file mode 100644\nindex 000000000..483712e09\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @Test\n+    void testBackgroundThreads() {\n+        // exercise phase\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n+\n+        // verify phase\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n+    }\n+\n+    @Tag(ACCEPTANCE)\n+    @Test\n+    void testCompressionType() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogFlush() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogRetention() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+    }\n+\n+    @Test\n+    void testLogSegment() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+    }\n+\n+    @Test\n+    void testLogRoll() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+    }\n+\n+    @Test\n+    void testLogCleaner() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+    }\n+\n+    @Test\n+    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+    }\n+\n+    @Test\n+    void testLogIndexLogMessageLogMessage() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+    }\n+\n+    @Test\n+    void testMaxConnections() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+    }\n+\n+    @Test\n+    void testMetricReportersMessageMaxUncleanLeaderElection() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 483712e09..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -278,6 +70,6 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n     }\n }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 6f0d5a76e..712f00643 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -71,5 +127,8 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n+\n+        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n+        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 712f00643..a50349bb1 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -127,8 +140,5 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-\n-        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex a50349bb1..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,144 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.DynamicTest;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.TestFactory;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ThreadLocalRandom;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- * DynamicConfigurationSharedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update\n- * Shared -> for each test case we same configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @TestFactory\n-    Iterator<DynamicTest> testDynConfiguration() {\n-\n-        List<DynamicTest> dynamicTests = new ArrayList<>(40);\n-\n-        String generatedTestCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        String[] testCases = generatedTestCases.split(\"\\n\");\n-\n-        for (String testCaseLine : testCases) {\n-            String[] testCase = testCaseLine.split(\",\");\n-            dynamicTests.add(DynamicTest.dynamicTest(\"Test \" + testCase[0] + \"->\" + testCase[1], () -> {\n-                // exercise phase\n-                KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, testCase[0], testCase[1]);\n-\n-                // verify phase\n-                assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, testCase[0], testCase[1]), is(true));\n-                assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), testCase[0], testCase[1]), is(true));\n-            }));\n-        }\n-\n-        return dynamicTests.iterator();\n-    }\n-\n-    /**\n-     * Method, which dynamically generate test cases based on Kafka version\n-     * @param kafkaVersion specific kafka version\n-     * @return String generated test cases\n-     */\n-    private static String generateTestCases(String kafkaVersion) {\n-\n-        StringBuilder testCases = new StringBuilder();\n-\n-        Map<String, Object> dynamicProperties = KafkaUtils.getDynamicConfigurationProperties(kafkaVersion);\n-\n-        dynamicProperties.forEach((key, value) -> {\n-            testCases.append(key);\n-            testCases.append(\", \");\n-\n-            String type = ((LinkedHashMap<String, String>) value).get(\"type\");\n-            Object stochasticChosenValue;\n-\n-            switch (type) {\n-                case \"STRING\":\n-                    if (key.equals(\"compression.type\")) {\n-                        List<String> compressionTypes = Arrays.asList(\"snappy\", \"gzip\", \"lz4\", \"zstd\");\n-\n-                        stochasticChosenValue = compressionTypes.get(ThreadLocalRandom.current().nextInt(0, compressionTypes.size() - 1));\n-                        testCases.append(stochasticChosenValue);\n-                    } else {\n-                        testCases.append(\" \");\n-                    }\n-                    break;\n-                case \"INT\":\n-                case \"LONG\":\n-                    if (key.equals(\"background.threads\") || key.equals(\"log.cleaner.io.buffer.load.factor\") ||\n-                        key.equals(\"log.retention.ms\") || key.equals(\"max.connections\") ||\n-                        key.equals(\"max.connections.per.ip\")) {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(1, 20);\n-                    } else {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(100, 50_000);\n-                    }\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"DOUBLE\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextDouble(1, 20);\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"BOOLEAN\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextInt(2) == 0 ? true : false;\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"LIST\":\n-                    // metric.reporters has default empty '\"\"'\n-                    // log.cleanup.policy = [delete, compact] -> default delete\n-\n-                    if (key.equals(\"log.cleanup.policy\")) {\n-                        stochasticChosenValue = \"[delete]\";\n-                    } else {\n-                        stochasticChosenValue = \" \";\n-                    }\n-\n-                    testCases.append(stochasticChosenValue);\n-            }\n-            testCases.append(\",\");\n-            testCases.append(\"\\n\");\n-        });\n-\n-        return testCases.toString();\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466346162", "body": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "bodyText": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "bodyHTML": "<p dir=\"auto\">Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:28:05Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MjY2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466372661", "bodyText": "Everywhere, where the broker will not be triggered by change of dyn. configuration we should tag as DYNAMIC_CONFIGURATION", "author": "see-quick", "createdAt": "2020-08-06T12:22:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9248d8077..9e19f4ec2 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -245,4 +245,14 @@ public interface Constants {\n      * Tag for tests where mainly dynamic configuration is used\n      */\n     String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";\n+\n+    /**\n+     * Tag for tests which contains rolling update of resource\n+     */\n+    String ROLLING_UPDATE = \"rollingupdate\";\n+\n+    /**\n+     * Tag for tests where OLM is used for deploying CO\n+     */\n+    String OLM = \"olm\";\n }\n", "next_change": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9e19f4ec2..e43a44b56 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -255,4 +255,13 @@ public interface Constants {\n      * Tag for tests where OLM is used for deploying CO\n      */\n     String OLM = \"olm\";\n+\n+    /**\n+     * Cruise Control related parameters\n+     */\n+    String CRUISE_CONTROL_NAME = \"Cruise Control\";\n+    String CRUISE_CONTROL_CONTAINER_NAME = \"cruise-control\";\n+    String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n+    String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n+    String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n }\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9248d8077..e43a44b56 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -245,4 +245,23 @@ public interface Constants {\n      * Tag for tests where mainly dynamic configuration is used\n      */\n     String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";\n+\n+    /**\n+     * Tag for tests which contains rolling update of resource\n+     */\n+    String ROLLING_UPDATE = \"rollingupdate\";\n+\n+    /**\n+     * Tag for tests where OLM is used for deploying CO\n+     */\n+    String OLM = \"olm\";\n+\n+    /**\n+     * Cruise Control related parameters\n+     */\n+    String CRUISE_CONTROL_NAME = \"Cruise Control\";\n+    String CRUISE_CONTROL_CONTAINER_NAME = \"cruise-control\";\n+    String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n+    String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n+    String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n }\n", "next_change": {"commit": "1b90a5c68422c5ba6a381161bad59a30422216d2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex e43a44b56..c61d4e4cc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -264,4 +264,11 @@ public interface Constants {\n     String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n     String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n     String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n+\n+    /**\n+     * Default listeners names\n+     */\n+    String PLAIN_LISTENER_DEFAULT_NAME = \"plain\";\n+    String TLS_LISTENER_DEFAULT_NAME = \"tls\";\n+    String EXTERNAL_LISTENER_DEFAULT_NAME = \"external\";\n }\n", "next_change": {"commit": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex c61d4e4cc..d84e84871 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -271,4 +310,9 @@ public interface Constants {\n     String PLAIN_LISTENER_DEFAULT_NAME = \"plain\";\n     String TLS_LISTENER_DEFAULT_NAME = \"tls\";\n     String EXTERNAL_LISTENER_DEFAULT_NAME = \"external\";\n+\n+    /**\n+     * Loadbalancer finalizer config\n+     */\n+    String LOAD_BALANCER_CLEANUP = \"service.kubernetes.io/load-balancer-cleanup\";\n }\n", "next_change": {"commit": "83df898d55935e9cd01dba45c48602e1c411675a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex d84e84871..70043d65f 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -315,4 +320,9 @@ public interface Constants {\n      * Loadbalancer finalizer config\n      */\n     String LOAD_BALANCER_CLEANUP = \"service.kubernetes.io/load-balancer-cleanup\";\n+\n+    /**\n+     * Auxiliary variables for storing data across our tests\n+     */\n+    String NAMESPACE_KEY = \"NAMESPACE_NAME\";\n }\n", "next_change": {"commit": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 70043d65f..ba0339954 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -325,4 +326,9 @@ public interface Constants {\n      * Auxiliary variables for storing data across our tests\n      */\n     String NAMESPACE_KEY = \"NAMESPACE_NAME\";\n+\n+    /**\n+     * Auxiliary variable for cluster operator deployment\n+     */\n+    String WATCH_ALL_NAMESPACES = \"*\";\n }\n", "next_change": {"commit": "bb2bad568c763653a102f5023f7d5e6a435078f6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex ba0339954..a2af60f4e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -331,4 +327,13 @@ public interface Constants {\n      * Auxiliary variable for cluster operator deployment\n      */\n     String WATCH_ALL_NAMESPACES = \"*\";\n+\n+    String CLUSTER_KEY = \"CLUSTER_NAME\";\n+    String TOPIC_KEY = \"TOPIC_NAME\";\n+    String STREAM_TOPIC_KEY = \"STREAM_TOPIC_NAME\";\n+    String KAFKA_CLIENTS_KEY = \"KAFKA_CLIENTS_NAME\";\n+    String PRODUCER_KEY = \"PRODUCER_NAME\";\n+    String CONSUMER_KEY = \"CONSUMER_NAME\";\n+    String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n+    String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n }\n", "next_change": {"commit": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex a2af60f4e..87b553907 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -336,4 +350,13 @@ public interface Constants {\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n     String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n+\n+    /**\n+     * Resource constants for Cluster Operator. In case we execute more than 5 test cases in parallel we at least these configuration\n+     * (because if we use default configuration, the Cluster Operator Pod occasionally restarting because of OOM)\n+     */\n+    String CLUSTER_OPERATOR_RESOURCE_CPU_LIMITS = \"1000m\";\n+    String CLUSTER_OPERATOR_RESOURCE_MEMORY_LIMITS = \"2048Mi\";\n+    String CLUSTER_OPERATOR_RESOURCE_CPU_REQUESTS = \"200m\";\n+    String CLUSTER_OPERATOR_RESOURCE_MEMORY_REQUESTS = \"1024Mi\";\n }\n", "next_change": {"commit": "6d5695189c6fbe018ed104e461e959260cc90c4c", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 87b553907..962a76fcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -346,17 +382,14 @@ public interface Constants {\n     String TOPIC_KEY = \"TOPIC_NAME\";\n     String STREAM_TOPIC_KEY = \"STREAM_TOPIC_NAME\";\n     String KAFKA_CLIENTS_KEY = \"KAFKA_CLIENTS_NAME\";\n+    String SCRAPER_KEY = \"SCRAPER_NAME\";\n     String PRODUCER_KEY = \"PRODUCER_NAME\";\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n+    String ADMIN_KEY = \"ADMIN_NAME\";\n+    String USER_NAME_KEY = \"USER_NAME\";\n     String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n-\n-    /**\n-     * Resource constants for Cluster Operator. In case we execute more than 5 test cases in parallel we at least these configuration\n-     * (because if we use default configuration, the Cluster Operator Pod occasionally restarting because of OOM)\n-     */\n-    String CLUSTER_OPERATOR_RESOURCE_CPU_LIMITS = \"1000m\";\n-    String CLUSTER_OPERATOR_RESOURCE_MEMORY_LIMITS = \"2048Mi\";\n-    String CLUSTER_OPERATOR_RESOURCE_CPU_REQUESTS = \"200m\";\n-    String CLUSTER_OPERATOR_RESOURCE_MEMORY_REQUESTS = \"1024Mi\";\n+    String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n+    String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n+    String ENTITY_OPERATOR_NAME = \"ENTITY_OPERATOR_NAME\";\n }\n", "next_change": {"commit": "91143dc1176ea5a299a23c6066dfab74c7a0247c", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 962a76fcd..375d9a704 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -387,7 +388,7 @@ public interface Constants {\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n     String ADMIN_KEY = \"ADMIN_NAME\";\n     String USER_NAME_KEY = \"USER_NAME\";\n-    String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n+    String SCRAPER_POD_KEY = \"SCRAPER_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n     String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n     String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n", "next_change": {"commit": "c7758768427e785ad619b00ad6c5d8af669a392a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 375d9a704..9a024c9aa 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -393,4 +401,17 @@ public interface Constants {\n     String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n     String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n     String ENTITY_OPERATOR_NAME = \"ENTITY_OPERATOR_NAME\";\n+\n+    /**\n+     * Lease related resources - ClusterRole, Role, RoleBinding\n+     */\n+    String PATH_TO_LEASE_CLUSTER_ROLE = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-ClusterRole-strimzi-cluster-operator-role.yaml\";\n+    // Path after change of ClusterRole -> Role in our SetupClusterOperator class\n+    String PATH_TO_LEASE_ROLE = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-Role-strimzi-cluster-operator-role.yaml\";\n+    String PATH_TO_LEASE_ROLE_BINDING = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-RoleBinding-strimzi-cluster-operator.yaml\";\n+    Map<String, String> LEASE_FILES_AND_RESOURCES = Map.of(\n+        CLUSTER_ROLE, PATH_TO_LEASE_CLUSTER_ROLE,\n+        ROLE, PATH_TO_LEASE_ROLE,\n+        ROLE_BINDING, PATH_TO_LEASE_ROLE_BINDING\n+    );\n }\n", "next_change": {"commit": "f704cb7e881df2782685fcdc0361f42bf7f63807", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9a024c9aa..a79412f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -414,4 +455,12 @@ public interface Constants {\n         ROLE, PATH_TO_LEASE_ROLE,\n         ROLE_BINDING, PATH_TO_LEASE_ROLE_BINDING\n     );\n+\n+    /**\n+     * Cluster Operator resources config\n+     */\n+    String CO_REQUESTS_MEMORY = \"512Mi\";\n+    String CO_REQUESTS_CPU = \"200m\";\n+    String CO_LIMITS_MEMORY = \"512Mi\";\n+    String CO_LIMITS_CPU = \"1000m\";\n }\n", "next_change": {"commit": "6fcab6675a7083444742ef1b85d14d6b7f235271", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex a79412f4d..c83875c46 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -463,4 +463,9 @@ public interface Constants {\n     String CO_REQUESTS_CPU = \"200m\";\n     String CO_LIMITS_MEMORY = \"512Mi\";\n     String CO_LIMITS_CPU = \"1000m\";\n+\n+    /**\n+     * Connect build image name\n+     */\n+    String ST_CONNECT_BUILD_IMAGE_NAME = \"strimzi-sts-connect-build\";\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "39bda5352266a44e4f7d6703ef000c63b500c7a9", "committedDate": "2020-10-24 11:59:33 +0200", "message": "Remove Travis from Strimzi Operators (#3861)"}, {"oid": "1b425956e43a05ac3ebef24c40dc18bb7bbb4cdd", "committedDate": "2020-11-13 18:59:19 +0100", "message": "[MO] - [OLM] - upgrade suite (#3854)"}, {"oid": "eea72be651ccd8e7399d0daeeec17149f19f5ecf", "committedDate": "2020-12-05 00:02:31 +0100", "message": "Change images for example clients (#4055)"}, {"oid": "a6b36e4e40821258b0d442e4a5a4eb968a0c2acd", "committedDate": "2021-01-13 20:11:41 +0100", "message": "[systemtest] Add checks to deprecated MetricsST tests (#4220)"}, {"oid": "f6b19390e9d5747e8111b2682381c5af0c7f086f", "committedDate": "2021-02-21 21:58:07 +0100", "message": "Setup minikube with internal registry  (#4388)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "652b4755223d0e35412c7300effb3cce3755a3e6", "committedDate": "2021-03-10 17:08:58 +0100", "message": "Move install, example and Helm chart files to packaging (#4513)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "23714a698f000f69e9b12d89f4d0160a875765be", "committedDate": "2021-03-26 18:24:01 +0100", "message": "Test for CruiseControl rootlogger level change (#4641)"}, {"oid": "178ae2e730a3f9628f5fd6800e798629ab724c25", "committedDate": "2021-03-29 20:49:18 +0200", "message": "Issue-766: Updated reconciliation interval and zk session timeout default value (#4637)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "e2309b9989ddbdb15169a9e83a763858cb1d6230", "committedDate": "2021-04-25 12:29:57 +0200", "message": "JMX test (#4790)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "d7161c6db23d384a4e8f5ec810dbc53418ed4122", "committedDate": "2021-06-28 13:43:12 +0200", "message": "Remove KafkaConnectS2I resource and its support (#5199)"}, {"oid": "5b48b0ab3fe76f271ae9b9af0600a6ec031aa164", "committedDate": "2021-06-29 22:10:54 +0200", "message": "Fix for Metrics ConfigMap containing JSON under YML property #3986 (#5072)"}, {"oid": "5ea1789f3cadffb84d0905f38081319986a0270e", "committedDate": "2021-07-10 10:31:05 +0200", "message": "ST: Apply Roles when CO use namespace-rbac and cluster wide options properly (#5266)"}, {"oid": "bb2bad568c763653a102f5023f7d5e6a435078f6", "committedDate": "2021-07-12 21:42:38 +0200", "message": "[MO] - [Parallel namespace test] -> TracingST (#5284)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "13c61d54bfa6354fe1e458b2f57cb44bfb020c76", "committedDate": "2021-08-10 18:26:39 +0200", "message": "Update tags and make client examples images configurable (#5392)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "72f22650f0a1e3916dc82e31302a54a9b3612b31", "committedDate": "2021-09-03 12:58:36 +0200", "message": "[systemtest] Add proper wait for `Job` to be ready (#5509)"}, {"oid": "d039da67e4039d2beb82b379f55bb57f7fe37ff8", "committedDate": "2021-09-24 21:42:03 +0200", "message": "[tests] Throttle Topic create/delete/update operations KIP-599 (#5492)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "07fc066a499b799659c6cc42a537928719511c72", "committedDate": "2021-10-20 12:18:42 +0200", "message": "[systemtest] Add STs for using DrainCleaner (#5696)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d21903e1d7a923f358b0e6580de61b75b7770be1", "committedDate": "2022-01-19 10:42:23 +0100", "message": "[MO] - [system test] -> eliminate un-necessary CO creation and deletion (#6210)"}, {"oid": "da4b9447221db033c30c586637b1a0bf3190bfc9", "committedDate": "2022-02-13 23:49:25 +0100", "message": "[MO] - [system test] -> LogCollector TS & TC labels + @ParallelSuite \u2026 (#6336)"}, {"oid": "011fcdb8bf74aaf7fad8e09d76de00155ee030f7", "committedDate": "2022-02-14 15:30:28 +0100", "message": "workaround removal init (#6323)"}, {"oid": "49f032f18aed2e6fa9969b975db381b2729dc7ca", "committedDate": "2022-02-23 11:34:50 +0100", "message": "[MO] - [system test] -> propage test suite and test case controller l\u2026 (#6411)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "453f10cd29a18388f4f37a237cc8bd5982748492", "committedDate": "2022-03-14 22:27:57 +0100", "message": "ST: Add sanity profile for system tests (#6523)"}, {"oid": "1496836aa2d60194c611caa5fb1081c2154ef719", "committedDate": "2022-03-16 13:27:58 +0100", "message": "[MO] - [system test] -> parallelism + OLM fixes (#6468)"}, {"oid": "ec40708a6504da43674f25770d8fdf3ecacdcd2f", "committedDate": "2022-03-30 21:45:36 +0200", "message": "[MO] - [system test] -> SecurityST race condition fixes and correct R\u2026 (#6588)"}, {"oid": "db8804d62aaea88824885a1ffb66ad3e5a2d1e0f", "committedDate": "2022-04-19 09:49:04 +0200", "message": "[systemtest] Rewrite throttling quotas tests (#6680)"}, {"oid": "6d5695189c6fbe018ed104e461e959260cc90c4c", "committedDate": "2022-05-05 15:02:27 +0200", "message": "[MO] - [system test] -> Resource allocation of ClusterOperator, Kafka, KafkaMirrorMaker components (#6654)"}, {"oid": "c5b8d2d3c286c6a388ed984b60953f4ab797b159", "committedDate": "2022-05-07 01:05:25 +0200", "message": "[ST Enhancement] Deletion of remaining pvcs after tests. (#6749)"}, {"oid": "91143dc1176ea5a299a23c6066dfab74c7a0247c", "committedDate": "2022-05-09 09:54:27 +0200", "message": "[systemtest] Test clients exchange - remaining packages (#6756)"}, {"oid": "62f63c12a507c951b0e08ce024975a95ce305314", "committedDate": "2022-05-10 21:09:53 +0200", "message": "[systemtest] Remove `test-client` completely from the repository (#6782)"}, {"oid": "63c18914dbe95827b7cce28cd0eb7e32e1f5e077", "committedDate": "2022-05-13 15:34:45 +0200", "message": "Adding `KafkaRebalance` add/remove broker modes for rebalancing after/before scale up/down (#6800)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "5fb730e624c4942f6e71c2713f64f230caca4ffe", "committedDate": "2022-06-10 18:46:32 +0200", "message": "Move UseStrimziPodSet feature gate to beta (#6906)"}, {"oid": "5eb404fda80f572c6998f13fa3c5a60fab9b1b38", "committedDate": "2022-08-18 18:26:59 +0200", "message": "[MO] - [system test] -> Pod Security policies  (#7191)"}, {"oid": "c7758768427e785ad619b00ad6c5d8af669a392a", "committedDate": "2022-10-03 22:41:56 +0200", "message": "[systemtest] Enable multiple COs in one namespace in our tests after LeaderElection feature (#7412)"}, {"oid": "b40f0c71cbb487a793f2c288274c228abc39398c", "committedDate": "2022-10-18 09:50:24 +0200", "message": "[ST] Add message count, kafka + zk + eo STS or deployment names into the TestStorage (#7423)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "953a3c6fa0445e010da0517253286344a981b228", "committedDate": "2022-11-07 22:19:55 +0100", "message": "[systemtest] Collect StrimziPodSets in LogCollector during test failure (#7573)"}, {"oid": "d30cd390083cba6055c04629f5603cdad9ece1b8", "committedDate": "2022-12-05 18:42:15 +0100", "message": "Add `Lease` resource to the installation files to make sure it is deleted when uninstalling the operator (#7748)"}, {"oid": "b821d05ffb965dce4054cd3a855ca0e4315bd9d0", "committedDate": "2023-01-06 11:29:03 +0100", "message": "[ST] Fix issues with cert-manager webhook (#7837)"}, {"oid": "caf506b2c0007482fb697a99198ce04cf5057800", "committedDate": "2023-01-13 19:46:08 +0100", "message": "[systemtest] Add smoke test for testing all supported Kafka versions during regression pipeline (#7902)"}, {"oid": "b7d000052f593144b3fbc20b1986c9b34feb08b7", "committedDate": "2023-01-23 15:49:35 +0100", "message": "[systemtest] Update scalability tests for UO and update tags (#7930)"}, {"oid": "6b348401ff64c6045c80f3169ea222e73bcd6908", "committedDate": "2023-02-01 13:11:27 +0100", "message": "[systemtest] Refactor OLM related classes (#7999)"}, {"oid": "18b32823fce11385e014a7d4e8af30e50ba61fc6", "committedDate": "2023-02-02 14:10:09 +0100", "message": "[ST] KafkaConnector autorestart + update echo-sink-connector version (#7976)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "a858ed028a7acb545188b7cf18f45d770b9fbd8e", "committedDate": "2023-03-01 10:58:07 +0100", "message": "Make system test work with Connect stable identities  (#8113)"}, {"oid": "32c0e29ca574990b5ec20166624eca03b34d34ad", "committedDate": "2023-03-24 16:04:06 +0100", "message": "Graduate `UseStrimziPodSets` feature gate and remove `StatefulSet` support (#8273)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}, {"oid": "f704cb7e881df2782685fcdc0361f42bf7f63807", "committedDate": "2023-04-27 07:59:14 +0200", "message": "ST: Increase memory for CO to 512Mi in STs on FIPS clusters (#8439)"}, {"oid": "6fcab6675a7083444742ef1b85d14d6b7f235271", "committedDate": "2023-05-05 18:41:43 +0200", "message": "ST: Allow users to use any registry for connect build specified via env var (#8470)"}, {"oid": "67f76ea836c012dbf15fb6c54aece7740864555b", "committedDate": "2023-05-25 23:46:30 +0200", "message": "Add tag for tests which are not supported on arm (#8566)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466348498", "body": "Should we add a parameter `String dynConfProperty` to make this more general?", "bodyText": "Should we add a parameter String dynConfProperty to make this more general?", "bodyHTML": "<p dir=\"auto\">Should we add a parameter <code>String dynConfProperty</code> to make this more general?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:32:53Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NDQ3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466374475", "bodyText": "Currently, I am using only the one specific property. So it would be useless from my POV to extend it for now.", "author": "see-quick", "createdAt": "2020-08-06T12:26:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +283,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +330,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466350403", "body": "Just a nit. Should this be `execute phase`?", "bodyText": "Just a nit. Should this be execute phase?", "bodyHTML": "<p dir=\"auto\">Just a nit. Should this be <code>execute phase</code>?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:36:59Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3ODI2Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466378266", "bodyText": "No, it is exercise phase -> https://thoughtbot.com/blog/four-phase-test. I am changing the state of the SUT.", "author": "see-quick", "createdAt": "2020-08-06T12:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4MDU3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466380571", "bodyText": "Ah, ok. Thanks.", "author": "sknot-rh", "createdAt": "2020-08-06T12:37:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}], "type": "inlineReview", "revised_code": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nnew file mode 100644\nindex 000000000..483712e09\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @Test\n+    void testBackgroundThreads() {\n+        // exercise phase\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12);\n+\n+        // verify phase\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.background_threads, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.background_threads, 12), is(true));\n+    }\n+\n+    @Tag(ACCEPTANCE)\n+    @Test\n+    void testCompressionType() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"snappy\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"gzip\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"lz4\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.compression_type, \"zstd\"), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogFlush() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_flush_interval_messages, 300), is(true));\n+\n+    }\n+\n+    @Test\n+    void testLogRetention() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_ms, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_retention_bytes, 250), is(true));\n+    }\n+\n+    @Test\n+    void testLogSegment() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_bytes, 1_100), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_segment_delete_delay_ms, 400), is(true));\n+    }\n+\n+    @Test\n+    void testLogRoll() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_jitter_ms, 500), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_roll_ms, 300), is(true));\n+    }\n+\n+    @Test\n+    void testLogCleaner() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_backoff_ms, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_dedupe_buffer_size, 4_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_delete_retention_ms, 1_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_load_factor, 12), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_buffer_size, 10_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_io_max_bytes_per_second, 1.523), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_max_compaction_lag_ms, 32_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_roll_ms, 0.3);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_cleanable_ratio, 0.3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_min_compaction_lag_ms, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleaner_threads, 0), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\")), is(true));\n+    }\n+\n+    @Test\n+    void testInSyncReplicasNumIoNumNetworkNumRecoveryNumReplicaFetchers() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.min_insync_replicas, 1), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_io_threads, 4), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_network_threads, 2), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, Arrays.asList(\"compact\", \"delete\"));\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_recovery_threads_per_data_dir, 3), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_cleanup_policy, 1);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.num_replica_fetchers, 1), is(true));\n+    }\n+\n+    @Test\n+    void testLogIndexLogMessageLogMessage() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_interval_bytes, 1024), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_index_size_max_bytes, 5), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_difference_max_ms, 12_000), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_timestamp_type, \"CreateTime\"), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_message_downconversion_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.log_preallocate, true), is(true));\n+    }\n+\n+    @Test\n+    void testMaxConnections() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections, 10), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections, 10), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip, 20), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.max_connections_per_ip_overrides, \"\"), is(true));\n+    }\n+\n+    @Test\n+    void testMetricReportersMessageMaxUncleanLeaderElection() {\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.unclean_leader_election_enable, true), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048);\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.message_max_bytes, 2048), is(true));\n+\n+        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\");\n+\n+        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+        assertThat(KafkaUtils.verifyKafkaPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), KafkaDynamicConfiguration.metric_reporters, \"\"), is(true));\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 483712e09..6f0d5a76e 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -278,6 +70,6 @@ public class DynamicConfigurationSharedST extends AbstractST {\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n     }\n }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 6f0d5a76e..712f00643 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -71,5 +127,8 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n+\n+        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n+        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 712f00643..a50349bb1 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -127,8 +140,5 @@ public class DynamicConfigurationSharedST extends AbstractST {\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n         KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-\n-        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n     }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex a50349bb1..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,144 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.DynamicTest;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.TestFactory;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ThreadLocalRandom;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- * DynamicConfigurationSharedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update\n- * Shared -> for each test case we same configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @TestFactory\n-    Iterator<DynamicTest> testDynConfiguration() {\n-\n-        List<DynamicTest> dynamicTests = new ArrayList<>(40);\n-\n-        String generatedTestCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        String[] testCases = generatedTestCases.split(\"\\n\");\n-\n-        for (String testCaseLine : testCases) {\n-            String[] testCase = testCaseLine.split(\",\");\n-            dynamicTests.add(DynamicTest.dynamicTest(\"Test \" + testCase[0] + \"->\" + testCase[1], () -> {\n-                // exercise phase\n-                KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, testCase[0], testCase[1]);\n-\n-                // verify phase\n-                assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, testCase[0], testCase[1]), is(true));\n-                assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), testCase[0], testCase[1]), is(true));\n-            }));\n-        }\n-\n-        return dynamicTests.iterator();\n-    }\n-\n-    /**\n-     * Method, which dynamically generate test cases based on Kafka version\n-     * @param kafkaVersion specific kafka version\n-     * @return String generated test cases\n-     */\n-    private static String generateTestCases(String kafkaVersion) {\n-\n-        StringBuilder testCases = new StringBuilder();\n-\n-        Map<String, Object> dynamicProperties = KafkaUtils.getDynamicConfigurationProperties(kafkaVersion);\n-\n-        dynamicProperties.forEach((key, value) -> {\n-            testCases.append(key);\n-            testCases.append(\", \");\n-\n-            String type = ((LinkedHashMap<String, String>) value).get(\"type\");\n-            Object stochasticChosenValue;\n-\n-            switch (type) {\n-                case \"STRING\":\n-                    if (key.equals(\"compression.type\")) {\n-                        List<String> compressionTypes = Arrays.asList(\"snappy\", \"gzip\", \"lz4\", \"zstd\");\n-\n-                        stochasticChosenValue = compressionTypes.get(ThreadLocalRandom.current().nextInt(0, compressionTypes.size() - 1));\n-                        testCases.append(stochasticChosenValue);\n-                    } else {\n-                        testCases.append(\" \");\n-                    }\n-                    break;\n-                case \"INT\":\n-                case \"LONG\":\n-                    if (key.equals(\"background.threads\") || key.equals(\"log.cleaner.io.buffer.load.factor\") ||\n-                        key.equals(\"log.retention.ms\") || key.equals(\"max.connections\") ||\n-                        key.equals(\"max.connections.per.ip\")) {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(1, 20);\n-                    } else {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(100, 50_000);\n-                    }\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"DOUBLE\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextDouble(1, 20);\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"BOOLEAN\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextInt(2) == 0 ? true : false;\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"LIST\":\n-                    // metric.reporters has default empty '\"\"'\n-                    // log.cleanup.policy = [delete, compact] -> default delete\n-\n-                    if (key.equals(\"log.cleanup.policy\")) {\n-                        stochasticChosenValue = \"[delete]\";\n-                    } else {\n-                        stochasticChosenValue = \" \";\n-                    }\n-\n-                    testCases.append(stochasticChosenValue);\n-            }\n-            testCases.append(\",\");\n-            testCases.append(\"\\n\");\n-        });\n-\n-        return testCases.toString();\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 6f0d5a76e..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvSource;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvSource({\n-        \"background.threads, \" + 12,\n-        \"compression.type,  snappy\",\n-        \"compression.type,  gzip\",\n-        \"compression.type,  lz4\",\n-        \"compression.type,  zstd\",\n-        \"log.flush.interval.ms, \" + 20,\n-        \"log.retention.ms,  \" + 20,\n-        \"log.retention.bytes, \" + 250,\n-        \"log.segment.bytes,   \" + 1_100,\n-        \"log.segment.delete.delay.ms,  \" + 400,\n-        \"log.roll.jitter.ms, \" + 500,\n-        \"log.roll.ms, \" + 300,\n-        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n-        \"log.cleaner.delete.retention.ms, \" + 1_000,\n-        \"log.cleaner.io.buffer.load.factor, \" + 12,\n-        \"log.cleaner.io.buffer.size, \" + 10_000,\n-        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n-        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n-        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n-        \"log.preallocate, \" + true,\n-        \"max.connections, \" + 10,\n-        \"max.connections.per.ip, \" + 20,\n-        \"unclean.leader.election.enable, \" + true,\n-        \"message.max.bytes, \" + 2048,\n-    })\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQzMDk4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466430988", "body": "I think the indent is still same here.", "bodyText": "I think the indent is still same here.", "bodyHTML": "<p dir=\"auto\">I think the indent is still same here.</p>", "author": "im-konge", "createdAt": "2020-08-06T13:56:14Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -96,20 +91,28 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void testDynamicConfigurationWithExternalListeners() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639748", "body": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "bodyText": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "bodyHTML": "<p dir=\"auto\">I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:32:57Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731076", "bodyText": "Yes I will use TestKafkaVersion class to do it dynamically. :) Thanks", "author": "see-quick", "createdAt": "2020-08-10T07:33:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639927", "body": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "bodyText": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "bodyHTML": "<p dir=\"auto\">I'm not sure I understand why are we asserting these when we don't configure them anywhere.</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:19Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731092", "bodyText": "If I understand you correctly...but I configure it in the @BeforeEach phase.", "author": "see-quick", "createdAt": "2020-08-10T07:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640104", "body": "Why should this be `\"true\"`?", "bodyText": "Why should this be \"true\"?", "bodyHTML": "<p dir=\"auto\">Why should this be <code>\"true\"</code>?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:41Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841853", "bodyText": "This method (as you pointed) is setting unclean.leader.election.enable, which is dynamically changeable option. Default value is false so it is just changing its value.", "author": "sknot-rh", "createdAt": "2020-08-07T06:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkzMzczNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466933737", "bodyText": "I think the code should be improved to make it easier to understand and read even without understanding some Kafka defaults.", "author": "scholzj", "createdAt": "2020-08-07T09:37:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640208", "body": "Why should this be `\"true\"`?", "bodyText": "Why should this be \"true\"?", "bodyHTML": "<p dir=\"auto\">Why should this be <code>\"true\"</code>?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841874", "bodyText": "And after the value is changed, it is verified whether it actually changed.", "author": "sknot-rh", "createdAt": "2020-08-07T06:14:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641159", "body": "This has generic name, but it is not generic method. It should ba then maybe named `updateAndVerifyDynUncleanLeaderElectionConf`", "bodyText": "This has generic name, but it is not generic method. It should ba then maybe named updateAndVerifyDynUncleanLeaderElectionConf", "bodyHTML": "<p dir=\"auto\">This has generic name, but it is not generic method. It should ba then maybe named <code>updateAndVerifyDynUncleanLeaderElectionConf</code></p>", "author": "scholzj", "createdAt": "2020-08-06T19:35:30Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDI2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744264", "bodyText": "I have refactored that method to be more generic :)", "author": "see-quick", "createdAt": "2020-08-10T08:04:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +283,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +330,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTY3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641675", "body": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass `kafkaConfig` as parameter?", "bodyText": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass kafkaConfig as parameter?", "bodyHTML": "<p dir=\"auto\">I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass <code>kafkaConfig</code> as parameter?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:36:24Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        // change dynamically changeable option\n+        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +283,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +330,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NDUxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466644517", "body": "Can we add some Javadoc explaining what this does?", "bodyText": "Can we add some Javadoc explaining what this does?", "bodyHTML": "<p dir=\"auto\">Can we add some Javadoc explaining what this does?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:42:06Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +283,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 6d1808183..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,316 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..932ecfd55\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+\n+        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+    }\n+}\n", "next_change": {"commit": "fac2acd69f7c72748c8086553260001d86926804", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 932ecfd55..5b3df5c77 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -363,12 +332,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.5\");\n+    }\n+\n     @BeforeAll\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n \n         LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 1, 1).done();\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": {"commit": "76541b66628223a9dea92fb49d2a35b1b87f1906", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 5b3df5c77..a4d75b43b 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -344,8 +289,5 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     void setup() throws Exception {\n         ResourceManager.setClassResources();\n         installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 1, 1).done();\n     }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -255,14 +330,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         );\n     }\n \n-    private void updateAndVerifyDynConf(String dynConfValue) {\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n \n-        // change dynamically changeable option\n-        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);\n         LOGGER.info(\"Updating configuration of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n             KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466645329", "body": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "bodyText": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "bodyHTML": "<p dir=\"auto\">Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.</p>", "author": "scholzj", "createdAt": "2020-08-06T19:43:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MjU5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466842598", "bodyText": "It is copy-paste from test I wrote. At the time we had dynamic updates of kafka configuration only, we wanted to be sure the changes of logging will trigger RU. For the reasons you mentioned, it should be deleted in this PR or in my PR after rebase.", "author": "sknot-rh", "createdAt": "2020-08-07T06:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDQxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744413", "bodyText": "Removed :)", "author": "see-quick", "createdAt": "2020-08-10T08:04:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "7517de0b3496641bd930171d41daeccd54ff86ce", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..6d1808183 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,28 +66,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n", "next_change": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 95%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 6d1808183..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -87,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 54%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 95dec7333..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -67,100 +75,157 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n         kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n         assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-\n-        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n-\n-        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating logging of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setLogging(il);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n-            .editKafka()\n-                .withNewListeners()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                .endListeners()\n-                .withConfig(kafkaConfig)\n-            .endKafka()\n+                .editKafka()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n+                            .withTls(false)\n+                        .endGenericKafkaListener()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n             .endSpec()\n             .done();\n \n-        updateAndVerifyDynConf(\"true\");\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"tls\")\n+                    .withPort(9093)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(true)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n-        updateAndVerifyDynConf(\"true\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n         // Other external listeners cases are rolling because of crts\n         kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n+\n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n+            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"plain\")\n+                    .withPort(9092)\n+                    .withType(KafkaListenerType.INTERNAL)\n+                    .withTls(false)\n+                    .build(),\n+                new GenericKafkaListenerBuilder()\n+                    .withName(\"external\")\n+                    .withPort(9094)\n+                    .withType(KafkaListenerType.NODEPORT)\n+                    .withTls(true)\n+                    .build()\n+            ), null));\n         });\n \n         StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n         assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n \n-        updateAndVerifyDynConf(\"false\");\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n     }\n \n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"oid": "7517de0b3496641bd930171d41daeccd54ff86ce", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7517de0b3496641bd930171d41daeccd54ff86ce", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-10T08:06:07Z", "type": "forcePushed"}, {"oid": "22a5375f16130e257b4e13364bfd459227de4a7e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/22a5375f16130e257b4e13364bfd459227de4a7e", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-10T14:15:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NDg0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469364840", "body": "What does this tell you that the result of the update to the `Kafka` CR does not? ", "bodyText": "What does this tell you that the result of the update to the Kafka CR does not?", "bodyHTML": "<p dir=\"auto\">What does this tell you that the result of the update to the <code>Kafka</code> CR does not?</p>", "author": "tombentley", "createdAt": "2020-08-12T15:52:49Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +156,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {", "originalCommit": "defb26bb06660cf9d8eb8f07c9b899cb920b4796", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3992a395f..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -182,7 +192,6 @@ public class KafkaUtils {\n      */\n     public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n         updateSpecificConfiguration(clusterName, brokerConfigName, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3992a395f..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -182,7 +211,6 @@ public class KafkaUtils {\n      */\n     public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n         updateSpecificConfiguration(clusterName, brokerConfigName, value);\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n     /**\n", "next_change": {"commit": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..3d9183f0f 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -211,6 +214,7 @@ public class KafkaUtils {\n      */\n     public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n         updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        waitForClusterStability(clusterName);\n     }\n \n     /**\n", "next_change": {"commit": "8bcead0a21c8785e30b1ef36140208fe8379214e", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3d9183f0f..58057ce27 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -223,7 +261,7 @@ public class KafkaUtils {\n      * @param value value of specific property\n      */\n     public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and expected is {}={}\",\n             brokerConfigName,\n             KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n             brokerConfigName,\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 58057ce27..fe4ddab1e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,49 +221,53 @@ public class KafkaUtils {\n \n     /**\n      * Method which, update/replace Kafka configuration\n+     * @param namespaceName name of the namespace\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResourceInSpecificNamespace(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n             config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n-        });\n+        }, namespaceName);\n     }\n \n     /**\n      * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param namespaceName namespace name\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n-        waitForClusterStability(clusterName);\n+    public static void  updateConfigurationWithStabilityWait(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(namespaceName, clusterName, brokerConfigName, value);\n+        waitForClusterStability(namespaceName, clusterName);\n     }\n \n     /**\n      * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param namespaceName name of the namespace\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+    public synchronized static boolean verifyCrDynamicConfiguration(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and expected is {}={}\",\n             brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n             brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n      * Verifies that updated configuration was successfully changed inside Kafka pods\n+     * @param namespaceName name of the namespace\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fe4ddab1e..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -275,15 +298,16 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public synchronized static boolean verifyPodDynamicConfiguration(final String namespaceName, String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+    public synchronized static boolean verifyPodDynamicConfiguration(final String namespaceName, String scraperPodName, String bootstrapServer, String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(namespaceName, kafkaPodNamePrefix);\n+        int[] brokerId = {0};\n \n         for (Pod pod : kafkaPods) {\n \n             TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n-                    String result = cmdKubeClient(namespaceName).execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+                    String result = KafkaCmdClient.describeKafkaBrokerUsingPodCli(namespaceName, scraperPodName, bootstrapServer, brokerId[0]++);\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n", "next_change": null}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MDU3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471580579", "body": "I think it should be in kafka directory in systemtest package", "bodyText": "I think it should be in kafka directory in systemtest package", "bodyHTML": "<p dir=\"auto\">I think it should be in kafka directory in systemtest package</p>", "author": "Frawless", "createdAt": "2020-08-17T16:01:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "originalCommit": "347dd238537142b016a9fa33e8fe7c077da7583c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationSharedST.java\nsimilarity index 79%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationSharedST.java\nindex 712f00643..5cdad5b99 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationSharedST.java\n", "chunk": "@@ -2,24 +2,25 @@\n  * Copyright Strimzi authors.\n  * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n  */\n-package io.strimzi.systemtest.dynamicconfiguration;\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n \n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.systemtest.AbstractST;\n import io.strimzi.systemtest.Environment;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.FileUtils;\n import io.strimzi.systemtest.utils.TestKafkaVersion;\n import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.DynamicTest;\n import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvFileSource;\n+import org.junit.jupiter.api.TestFactory;\n \n+import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Iterator;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n", "next_change": null}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\ndeleted file mode 100644\nindex 712f00643..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java\n+++ /dev/null\n", "chunk": "@@ -1,134 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.utils.FileUtils;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.CsvFileSource;\n-\n-import java.util.Arrays;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ThreadLocalRandom;\n-\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- * DynamicConfigurationSharedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update\n- * Shared -> for each test case we same configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationSharedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n-\n-    @ParameterizedTest\n-    @CsvFileSource(resources = \"/dynamic-configuration/dynamic-configuration-test-cases.csv\")\n-    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n-        // exercise phase\n-        KafkaUtils.updateConfigurationWithStabilityWait(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue);\n-\n-        // verify phase\n-        assertThat(KafkaUtils.verifyCrDynamicConfiguration(CLUSTER_NAME, kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-        assertThat(KafkaUtils.verifyPodDynamicConfiguration(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), kafkaDynamicConfigurationKey, kafkaDynamicConfigurationValue), is(true));\n-    }\n-\n-    /**\n-     * Method, which dynamically generate test cases based on Kafka version\n-     * @param kafkaVersion specific kafka version\n-     * @return String generated test cases\n-     */\n-    private static String generateTestCases(String kafkaVersion) {\n-\n-        StringBuilder testCases = new StringBuilder();\n-\n-        Map<String, Object> dynamicProperties = KafkaUtils.getDynamicConfigurationProperties(kafkaVersion);\n-\n-        dynamicProperties.forEach((key, value) -> {\n-            testCases.append(key);\n-            testCases.append(\", \");\n-\n-            String type = ((LinkedHashMap<String, String>) value).get(\"type\");\n-            Object stochasticChosenValue;\n-\n-            switch (type) {\n-                case \"STRING\":\n-                    if (key.equals(\"compression.type\")) {\n-                        List<String> compressionTypes = Arrays.asList(\"snappy\", \"gzip\", \"lz4\", \"zstd\");\n-\n-                        stochasticChosenValue = compressionTypes.get(ThreadLocalRandom.current().nextInt(0, compressionTypes.size() - 1));\n-                        testCases.append(stochasticChosenValue);\n-                    } else {\n-                        testCases.append(\" \");\n-                    }\n-                    break;\n-                case \"INT\":\n-                case \"LONG\":\n-                    if (key.equals(\"background.threads\") || key.equals(\"log.cleaner.io.buffer.load.factor\") ||\n-                        key.equals(\"log.retention.ms\") || key.equals(\"max.connections\") ||\n-                        key.equals(\"max.connections.per.ip\")) {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(1, 20);\n-                    } else {\n-                        stochasticChosenValue = ThreadLocalRandom.current().nextInt(100, 50_000);\n-                    }\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"DOUBLE\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextDouble(1, 20);\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"BOOLEAN\":\n-                    stochasticChosenValue = ThreadLocalRandom.current().nextInt(2) == 0 ? true : false;\n-                    testCases.append(stochasticChosenValue);\n-                    break;\n-                case \"LIST\":\n-                    // metric.reporters has default empty '\"\"'\n-                    // log.cleanup.policy = [delete, compact] -> default delete\n-\n-                    if (key.equals(\"log.cleanup.policy\")) {\n-                        stochasticChosenValue = \"[delete]\";\n-                    } else {\n-                        stochasticChosenValue = \" \";\n-                    }\n-\n-                    testCases.append(stochasticChosenValue);\n-            }\n-            testCases.append(\",\");\n-            testCases.append(\"\\n\");\n-        });\n-\n-        return testCases.toString();\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-\n-        LOGGER.info(\"Deploying shared Kafka across all test cases!\");\n-        KafkaResource.kafkaEphemeral(CLUSTER_NAME, 3, 1).done();\n-\n-        String testCases = generateTestCases(TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).version());\n-        FileUtils.createCsvFile(\"../systemtest/src/test/resources/dynamic-configuration/dynamic-configuration-test-cases.csv\", testCases);\n-    }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MTUzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471581538", "body": "It should be in kafka directory of systemtest I think.", "bodyText": "It should be in kafka directory of systemtest I think.", "bodyHTML": "<p dir=\"auto\">It should be in kafka directory of systemtest I think.</p>", "author": "Frawless", "createdAt": "2020-08-17T16:03:23Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "originalCommit": "347dd238537142b016a9fa33e8fe7c077da7583c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 99%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 4c8eed075..8b18e8dbe 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -2,7 +2,7 @@\n  * Copyright Strimzi authors.\n  * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n  */\n-package io.strimzi.systemtest.dynamicconfiguration;\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n \n import io.strimzi.api.kafka.model.KafkaClusterSpec;\n import io.strimzi.api.kafka.model.KafkaResources;\n", "next_change": null}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nsimilarity index 78%\nrename from systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 4c8eed075..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -2,12 +2,13 @@\n  * Copyright Strimzi authors.\n  * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n  */\n-package io.strimzi.systemtest.dynamicconfiguration;\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n \n import io.strimzi.api.kafka.model.KafkaClusterSpec;\n import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.api.kafka.model.listener.arraylistener.ArrayOrObjectKafkaListeners;\n+import io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListenerBuilder;\n+import io.strimzi.api.kafka.model.listener.arraylistener.KafkaListenerType;\n import io.strimzi.systemtest.AbstractST;\n import io.strimzi.systemtest.Constants;\n import io.strimzi.systemtest.Environment;\n", "next_change": null}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}]}, {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/58b10ba7d48706f744cd81e4924a02eea22d660b", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-17T18:38:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472085511", "body": "Shouldn't be this annotation from spotbugs?", "bodyText": "Shouldn't be this annotation from spotbugs?", "bodyHTML": "<p dir=\"auto\">Shouldn't be this annotation from spotbugs?</p>", "author": "Frawless", "createdAt": "2020-08-18T10:44:28Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -4,6 +4,8 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;", "originalCommit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQzMjUzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476432536", "bodyText": "AFAIK everywhere we use SuppressFBWarnings. I know that FB were deprecated and replaced by SpotBugs but  from code a i cannot find any SuppressSBWarnings or something similar like this.", "author": "see-quick", "createdAt": "2020-08-25T13:05:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -4,40 +4,43 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n import io.fabric8.kubernetes.api.model.Pod;\n import io.strimzi.api.kafka.model.Kafka;\n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.api.kafka.model.status.Condition;\n import io.strimzi.api.kafka.model.status.ListenerStatus;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.ConfigModels;\n+import io.strimzi.kafka.config.model.Scope;\n import io.strimzi.systemtest.Constants;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n-import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.nio.charset.Charset;\n+import java.time.Duration;\n import java.util.Base64;\n-import java.util.LinkedHashMap;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIXES;\n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIX_EXCEPTIONS;\n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -4,39 +4,28 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;\n-import io.fabric8.kubernetes.api.model.Pod;\n import io.strimzi.api.kafka.model.Kafka;\n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.api.kafka.model.status.Condition;\n import io.strimzi.api.kafka.model.status.ListenerStatus;\n-import io.strimzi.kafka.config.model.ConfigModel;\n-import io.strimzi.kafka.config.model.ConfigModels;\n-import io.strimzi.kafka.config.model.Scope;\n import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.io.FileInputStream;\n-import java.io.IOException;\n-import java.io.InputStream;\n import java.nio.charset.Charset;\n-import java.time.Duration;\n import java.util.Base64;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n-import java.util.stream.Collectors;\n \n-import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIXES;\n-import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIX_EXCEPTIONS;\n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -34,8 +34,6 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.MatcherAssert.assertThat;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -34,6 +37,8 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -37,8 +35,6 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -31,6 +32,7 @@ import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n+import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "7decfc02e8ba8e7917ba90a42841dcda405a7508", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..a2559d8c5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -4,35 +4,39 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import io.fabric8.kubernetes.api.model.Pod;\n import io.strimzi.api.kafka.model.Kafka;\n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.api.kafka.model.status.Condition;\n import io.strimzi.api.kafka.model.status.ListenerStatus;\n import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n+import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n import java.nio.charset.Charset;\n+import java.time.Duration;\n import java.util.Base64;\n+import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n \n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex a2559d8c5..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -17,22 +20,24 @@ import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n-import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.nio.charset.Charset;\n import java.time.Duration;\n+import java.util.Arrays;\n import java.util.Base64;\n-import java.util.LinkedHashMap;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIXES;\n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIX_EXCEPTIONS;\n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -45,6 +34,8 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -34,8 +34,6 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.MatcherAssert.assertThat;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -34,6 +37,8 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -37,8 +35,6 @@ import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n \n public class KafkaUtils {\n \n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -31,6 +32,7 @@ import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n+import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "7decfc02e8ba8e7917ba90a42841dcda405a7508", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..a2559d8c5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -4,35 +4,39 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import io.fabric8.kubernetes.api.model.Pod;\n import io.strimzi.api.kafka.model.Kafka;\n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.api.kafka.model.status.Condition;\n import io.strimzi.api.kafka.model.status.ListenerStatus;\n import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n+import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n import java.nio.charset.Charset;\n+import java.time.Duration;\n import java.util.Base64;\n+import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n \n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex a2559d8c5..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -17,22 +20,24 @@ import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n-import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.nio.charset.Charset;\n import java.time.Duration;\n+import java.util.Arrays;\n import java.util.Base64;\n-import java.util.LinkedHashMap;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIXES;\n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIX_EXCEPTIONS;\n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -4,40 +4,43 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n import io.fabric8.kubernetes.api.model.Pod;\n import io.strimzi.api.kafka.model.Kafka;\n import io.strimzi.api.kafka.model.KafkaResources;\n import io.strimzi.api.kafka.model.status.Condition;\n import io.strimzi.api.kafka.model.status.ListenerStatus;\n+import io.strimzi.kafka.config.model.ConfigModel;\n+import io.strimzi.kafka.config.model.ConfigModels;\n+import io.strimzi.kafka.config.model.Scope;\n import io.strimzi.systemtest.Constants;\n import io.strimzi.systemtest.resources.ResourceManager;\n import io.strimzi.systemtest.resources.crd.KafkaResource;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils;\n import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n import io.strimzi.test.TestUtils;\n import io.strimzi.test.k8s.exceptions.KubeClusterException;\n-import io.vertx.core.json.JsonObject;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n-import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.nio.charset.Charset;\n+import java.time.Duration;\n import java.util.Base64;\n-import java.util.LinkedHashMap;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIXES;\n+import static io.strimzi.api.kafka.model.KafkaClusterSpec.FORBIDDEN_PREFIX_EXCEPTIONS;\n import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n import static io.strimzi.api.kafka.model.KafkaResources.zookeeperStatefulSetName;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.NotReady;\n import static io.strimzi.systemtest.enums.CustomResourceStatus.Ready;\n-import static io.strimzi.systemtest.resources.ResourceManager.CR_CREATION_TIMEOUT;\n import static io.strimzi.test.TestUtils.indent;\n import static io.strimzi.test.TestUtils.waitFor;\n import static io.strimzi.test.k8s.KubeClusterResource.cmdKubeClient;\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -49,6 +51,7 @@ import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n public class KafkaUtils {\n \n     private static final Logger LOGGER = LogManager.getLogger(KafkaUtils.class);\n+    private static final long DELETION_TIMEOUT = ResourceOperation.getTimeoutForResourceDeletion();\n \n     private KafkaUtils() {}\n \n", "next_change": {"commit": "2903e51d5479a7979a9bf56b80506f654753a4b2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..930c5a8db 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -55,17 +61,17 @@ public class KafkaUtils {\n \n     private KafkaUtils() {}\n \n-    public static void waitForKafkaReady(String clusterName) {\n-        waitForKafkaStatus(clusterName, Ready);\n+    public static boolean waitForKafkaReady(String clusterName) {\n+        return waitForKafkaStatus(clusterName, Ready);\n     }\n \n     public static void waitForKafkaNotReady(String clusterName) {\n         waitForKafkaStatus(clusterName, NotReady);\n     }\n \n-    public static void waitForKafkaStatus(String clusterName, Enum<?>  state) {\n+    public static boolean waitForKafkaStatus(String clusterName, Enum<?>  state) {\n         Kafka kafka = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get();\n-        ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n+        return ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n     }\n \n     /**\n", "next_change": {"commit": "a20035f511845cb88e993d93ebf3c61669b0b263", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 930c5a8db..30c399ced 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -61,17 +61,18 @@ public class KafkaUtils {\n \n     private KafkaUtils() {}\n \n-    public static boolean waitForKafkaReady(String clusterName) {\n-        return waitForKafkaStatus(clusterName, Ready);\n+    public static void waitForKafkaReady(String clusterName) {\n+        waitForKafkaStatus(clusterName, Ready);\n     }\n \n     public static void waitForKafkaNotReady(String clusterName) {\n         waitForKafkaStatus(clusterName, NotReady);\n     }\n \n-    public static boolean waitForKafkaStatus(String clusterName, Enum<?>  state) {\n-        Kafka kafka = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get();\n-        return ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n+    public static void waitForKafkaStatus(String clusterName, Enum<?>  state) {\n+        String namespace = kubeClient().getNamespace();\n+        Kafka kafka = KafkaResource.kafkaClient().inNamespace(namespace).withName(clusterName).get();\n+        ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n     }\n \n     /**\n", "next_change": {"commit": "83df898d55935e9cd01dba45c48602e1c411675a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 30c399ced..bf23d2781 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -61,18 +61,29 @@ public class KafkaUtils {\n \n     private KafkaUtils() {}\n \n-    public static void waitForKafkaReady(String clusterName) {\n-        waitForKafkaStatus(clusterName, Ready);\n+    public static boolean waitForKafkaReady(String namespaceName, String clusterName) {\n+        return waitForKafkaStatus(namespaceName, clusterName, Ready);\n     }\n \n-    public static void waitForKafkaNotReady(String clusterName) {\n-        waitForKafkaStatus(clusterName, NotReady);\n+    public static boolean waitForKafkaReady(String clusterName) {\n+        return waitForKafkaStatus(kubeClient().getNamespace(), clusterName, Ready);\n     }\n \n-    public static void waitForKafkaStatus(String clusterName, Enum<?>  state) {\n-        String namespace = kubeClient().getNamespace();\n-        Kafka kafka = KafkaResource.kafkaClient().inNamespace(namespace).withName(clusterName).get();\n-        ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n+    public static boolean waitForKafkaNotReady(String namespaceName, String clusterName) {\n+        return waitForKafkaStatus(namespaceName, clusterName, NotReady);\n+    }\n+\n+    public static boolean waitForKafkaNotReady(String clusterName) {\n+        return waitForKafkaStatus(kubeClient().getNamespace(), clusterName, NotReady);\n+    }\n+\n+    public static boolean waitForKafkaStatus(String clusterName, Enum<?>  state) {\n+        return waitForKafkaStatus(kubeClient().getNamespace(), clusterName, state);\n+    }\n+\n+    public static boolean waitForKafkaStatus(String namespaceName, String clusterName, Enum<?>  state) {\n+        Kafka kafka = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get();\n+        return ResourceManager.waitForResourceStatus(KafkaResource.kafkaClient(), kafka, state);\n     }\n \n     /**\n", "next_change": {"commit": "dfda76a1906dec690876fab5e52cf8da1496900a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bf23d2781..5d259c18d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -90,16 +91,20 @@ public class KafkaUtils {\n      * Waits for the Kafka Status to be updated after changed. It checks the generation and observed generation to\n      * ensure the status is up to date.\n      *\n+     * @param namespaceName Namespace name\n      * @param clusterName   Name of the Kafka cluster which should be checked\n      */\n-    public static void waitForKafkaStatusUpdate(String clusterName)   {\n+    public static void waitForKafkaStatusUpdate(String namespaceName, String clusterName) {\n         LOGGER.info(\"Waiting for Kafka status to be updated\");\n         TestUtils.waitFor(\"KafkaStatus update\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_STATUS_TIMEOUT, () -> {\n-            Kafka k = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get();\n+            Kafka k = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get();\n             return k.getMetadata().getGeneration() == k.getStatus().getObservedGeneration();\n         });\n     }\n \n+    public static void waitForKafkaStatusUpdate(String clusterName) {\n+        waitForKafkaStatusUpdate(kubeClient().getNamespace(), clusterName);\n+    }\n \n     public static void waitUntilKafkaStatusConditionContainsMessage(String clusterName, String namespace, String message, long timeout) {\n         TestUtils.waitFor(\"Kafka Status with message [\" + message + \"]\",\n", "next_change": {"commit": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 5d259c18d..6a366b83f 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -102,10 +97,6 @@ public class KafkaUtils {\n         });\n     }\n \n-    public static void waitForKafkaStatusUpdate(String clusterName) {\n-        waitForKafkaStatusUpdate(kubeClient().getNamespace(), clusterName);\n-    }\n-\n     public static void waitUntilKafkaStatusConditionContainsMessage(String clusterName, String namespace, String message, long timeout) {\n         TestUtils.waitFor(\"Kafka Status with message [\" + message + \"]\",\n             Constants.GLOBAL_POLL_INTERVAL, timeout, () -> {\n", "next_change": {"commit": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 6a366b83f..2f05f8915 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -97,13 +101,13 @@ public class KafkaUtils {\n         });\n     }\n \n-    public static void waitUntilKafkaStatusConditionContainsMessage(String clusterName, String namespace, String message, long timeout) {\n-        TestUtils.waitFor(\"Kafka Status with message [\" + message + \"]\",\n+    public static void waitUntilKafkaStatusConditionContainsMessage(String clusterName, String namespace, String pattern, long timeout) {\n+        TestUtils.waitFor(\"Kafka Status with message [\" + pattern + \"]\",\n             Constants.GLOBAL_POLL_INTERVAL, timeout, () -> {\n                 List<Condition> conditions = KafkaResource.kafkaClient().inNamespace(namespace).withName(clusterName).get().getStatus().getConditions();\n                 for (Condition condition : conditions) {\n                     String conditionMessage = condition.getMessage();\n-                    if (conditionMessage.matches(message)) {\n+                    if (conditionMessage.matches(pattern)) {\n                         return true;\n                     }\n                 }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNzQxNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472107416", "body": "Maybe we should use prefixes from `FORBIDDEN_PREFIXES` ?", "bodyText": "Maybe we should use prefixes from FORBIDDEN_PREFIXES ?", "bodyHTML": "<p dir=\"auto\">Maybe we should use prefixes from <code>FORBIDDEN_PREFIXES</code> ?</p>", "author": "Frawless", "createdAt": "2020-08-18T11:29:10Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,", "originalCommit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,72 +248,74 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8e6c33747..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -240,74 +261,76 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-18T15:03:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ1NTcyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472455724", "body": "I think that @im-konge is in other PR adding the tag `ROLLING_UPDATE`. I think we should use it here as well because these are closely related.", "bodyText": "I think that @im-konge is in other PR adding the tag ROLLING_UPDATE. I think we should use it here as well because these are closely related.", "bodyHTML": "<p dir=\"auto\">I think that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/im-konge/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/im-konge\">@im-konge</a> is in other PR adding the tag <code>ROLLING_UPDATE</code>. I think we should use it here as well because these are closely related.</p>", "author": "scholzj", "createdAt": "2020-08-18T20:11:50Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9248d8077..9e19f4ec2 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -245,4 +245,14 @@ public interface Constants {\n      * Tag for tests where mainly dynamic configuration is used\n      */\n     String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";\n+\n+    /**\n+     * Tag for tests which contains rolling update of resource\n+     */\n+    String ROLLING_UPDATE = \"rollingupdate\";\n+\n+    /**\n+     * Tag for tests where OLM is used for deploying CO\n+     */\n+    String OLM = \"olm\";\n }\n", "next_change": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9e19f4ec2..e43a44b56 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -255,4 +255,13 @@ public interface Constants {\n      * Tag for tests where OLM is used for deploying CO\n      */\n     String OLM = \"olm\";\n+\n+    /**\n+     * Cruise Control related parameters\n+     */\n+    String CRUISE_CONTROL_NAME = \"Cruise Control\";\n+    String CRUISE_CONTROL_CONTAINER_NAME = \"cruise-control\";\n+    String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n+    String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n+    String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n }\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9248d8077..e43a44b56 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -245,4 +245,23 @@ public interface Constants {\n      * Tag for tests where mainly dynamic configuration is used\n      */\n     String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";\n+\n+    /**\n+     * Tag for tests which contains rolling update of resource\n+     */\n+    String ROLLING_UPDATE = \"rollingupdate\";\n+\n+    /**\n+     * Tag for tests where OLM is used for deploying CO\n+     */\n+    String OLM = \"olm\";\n+\n+    /**\n+     * Cruise Control related parameters\n+     */\n+    String CRUISE_CONTROL_NAME = \"Cruise Control\";\n+    String CRUISE_CONTROL_CONTAINER_NAME = \"cruise-control\";\n+    String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n+    String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n+    String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n }\n", "next_change": {"commit": "1b90a5c68422c5ba6a381161bad59a30422216d2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex e43a44b56..c61d4e4cc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -264,4 +264,11 @@ public interface Constants {\n     String CRUISE_CONTROL_CONFIGURATION_ENV = \"CRUISE_CONTROL_CONFIGURATION\";\n     String CRUISE_CONTROL_CAPACITY_FILE_PATH = \"/tmp/capacity.json\";\n     String CRUISE_CONTROL_CONFIGURATION_FILE_PATH = \"/tmp/cruisecontrol.properties\";\n+\n+    /**\n+     * Default listeners names\n+     */\n+    String PLAIN_LISTENER_DEFAULT_NAME = \"plain\";\n+    String TLS_LISTENER_DEFAULT_NAME = \"tls\";\n+    String EXTERNAL_LISTENER_DEFAULT_NAME = \"external\";\n }\n", "next_change": {"commit": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex c61d4e4cc..d84e84871 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -271,4 +310,9 @@ public interface Constants {\n     String PLAIN_LISTENER_DEFAULT_NAME = \"plain\";\n     String TLS_LISTENER_DEFAULT_NAME = \"tls\";\n     String EXTERNAL_LISTENER_DEFAULT_NAME = \"external\";\n+\n+    /**\n+     * Loadbalancer finalizer config\n+     */\n+    String LOAD_BALANCER_CLEANUP = \"service.kubernetes.io/load-balancer-cleanup\";\n }\n", "next_change": {"commit": "83df898d55935e9cd01dba45c48602e1c411675a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex d84e84871..70043d65f 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -315,4 +320,9 @@ public interface Constants {\n      * Loadbalancer finalizer config\n      */\n     String LOAD_BALANCER_CLEANUP = \"service.kubernetes.io/load-balancer-cleanup\";\n+\n+    /**\n+     * Auxiliary variables for storing data across our tests\n+     */\n+    String NAMESPACE_KEY = \"NAMESPACE_NAME\";\n }\n", "next_change": {"commit": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 70043d65f..ba0339954 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -325,4 +326,9 @@ public interface Constants {\n      * Auxiliary variables for storing data across our tests\n      */\n     String NAMESPACE_KEY = \"NAMESPACE_NAME\";\n+\n+    /**\n+     * Auxiliary variable for cluster operator deployment\n+     */\n+    String WATCH_ALL_NAMESPACES = \"*\";\n }\n", "next_change": {"commit": "bb2bad568c763653a102f5023f7d5e6a435078f6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex ba0339954..a2af60f4e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -331,4 +327,13 @@ public interface Constants {\n      * Auxiliary variable for cluster operator deployment\n      */\n     String WATCH_ALL_NAMESPACES = \"*\";\n+\n+    String CLUSTER_KEY = \"CLUSTER_NAME\";\n+    String TOPIC_KEY = \"TOPIC_NAME\";\n+    String STREAM_TOPIC_KEY = \"STREAM_TOPIC_NAME\";\n+    String KAFKA_CLIENTS_KEY = \"KAFKA_CLIENTS_NAME\";\n+    String PRODUCER_KEY = \"PRODUCER_NAME\";\n+    String CONSUMER_KEY = \"CONSUMER_NAME\";\n+    String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n+    String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n }\n", "next_change": {"commit": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex a2af60f4e..87b553907 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -336,4 +350,13 @@ public interface Constants {\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n     String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n+\n+    /**\n+     * Resource constants for Cluster Operator. In case we execute more than 5 test cases in parallel we at least these configuration\n+     * (because if we use default configuration, the Cluster Operator Pod occasionally restarting because of OOM)\n+     */\n+    String CLUSTER_OPERATOR_RESOURCE_CPU_LIMITS = \"1000m\";\n+    String CLUSTER_OPERATOR_RESOURCE_MEMORY_LIMITS = \"2048Mi\";\n+    String CLUSTER_OPERATOR_RESOURCE_CPU_REQUESTS = \"200m\";\n+    String CLUSTER_OPERATOR_RESOURCE_MEMORY_REQUESTS = \"1024Mi\";\n }\n", "next_change": {"commit": "6d5695189c6fbe018ed104e461e959260cc90c4c", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 87b553907..962a76fcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -346,17 +382,14 @@ public interface Constants {\n     String TOPIC_KEY = \"TOPIC_NAME\";\n     String STREAM_TOPIC_KEY = \"STREAM_TOPIC_NAME\";\n     String KAFKA_CLIENTS_KEY = \"KAFKA_CLIENTS_NAME\";\n+    String SCRAPER_KEY = \"SCRAPER_NAME\";\n     String PRODUCER_KEY = \"PRODUCER_NAME\";\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n+    String ADMIN_KEY = \"ADMIN_NAME\";\n+    String USER_NAME_KEY = \"USER_NAME\";\n     String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n-\n-    /**\n-     * Resource constants for Cluster Operator. In case we execute more than 5 test cases in parallel we at least these configuration\n-     * (because if we use default configuration, the Cluster Operator Pod occasionally restarting because of OOM)\n-     */\n-    String CLUSTER_OPERATOR_RESOURCE_CPU_LIMITS = \"1000m\";\n-    String CLUSTER_OPERATOR_RESOURCE_MEMORY_LIMITS = \"2048Mi\";\n-    String CLUSTER_OPERATOR_RESOURCE_CPU_REQUESTS = \"200m\";\n-    String CLUSTER_OPERATOR_RESOURCE_MEMORY_REQUESTS = \"1024Mi\";\n+    String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n+    String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n+    String ENTITY_OPERATOR_NAME = \"ENTITY_OPERATOR_NAME\";\n }\n", "next_change": {"commit": "91143dc1176ea5a299a23c6066dfab74c7a0247c", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 962a76fcd..375d9a704 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -387,7 +388,7 @@ public interface Constants {\n     String CONSUMER_KEY = \"CONSUMER_NAME\";\n     String ADMIN_KEY = \"ADMIN_NAME\";\n     String USER_NAME_KEY = \"USER_NAME\";\n-    String KAFKA_CLIENTS_POD_KEY = \"KAFKA_CLIENTS_POD_NAME\";\n+    String SCRAPER_POD_KEY = \"SCRAPER_POD_NAME\";\n     String KAFKA_TRACING_CLIENT_KEY = \"KAFKA_TRACING_CLIENT\";\n     String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n     String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n", "next_change": {"commit": "c7758768427e785ad619b00ad6c5d8af669a392a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 375d9a704..9a024c9aa 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -393,4 +401,17 @@ public interface Constants {\n     String KAFKA_SELECTOR = \"KAFKA_SELECTOR\";\n     String ZOOKEEPER_SELECTOR = \"ZOOKEEPER_SELECTOR\";\n     String ENTITY_OPERATOR_NAME = \"ENTITY_OPERATOR_NAME\";\n+\n+    /**\n+     * Lease related resources - ClusterRole, Role, RoleBinding\n+     */\n+    String PATH_TO_LEASE_CLUSTER_ROLE = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-ClusterRole-strimzi-cluster-operator-role.yaml\";\n+    // Path after change of ClusterRole -> Role in our SetupClusterOperator class\n+    String PATH_TO_LEASE_ROLE = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-Role-strimzi-cluster-operator-role.yaml\";\n+    String PATH_TO_LEASE_ROLE_BINDING = PATH_TO_PACKAGING_INSTALL_FILES + \"/cluster-operator/022-RoleBinding-strimzi-cluster-operator.yaml\";\n+    Map<String, String> LEASE_FILES_AND_RESOURCES = Map.of(\n+        CLUSTER_ROLE, PATH_TO_LEASE_CLUSTER_ROLE,\n+        ROLE, PATH_TO_LEASE_ROLE,\n+        ROLE_BINDING, PATH_TO_LEASE_ROLE_BINDING\n+    );\n }\n", "next_change": {"commit": "f704cb7e881df2782685fcdc0361f42bf7f63807", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex 9a024c9aa..a79412f4d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -414,4 +455,12 @@ public interface Constants {\n         ROLE, PATH_TO_LEASE_ROLE,\n         ROLE_BINDING, PATH_TO_LEASE_ROLE_BINDING\n     );\n+\n+    /**\n+     * Cluster Operator resources config\n+     */\n+    String CO_REQUESTS_MEMORY = \"512Mi\";\n+    String CO_REQUESTS_CPU = \"200m\";\n+    String CO_LIMITS_MEMORY = \"512Mi\";\n+    String CO_LIMITS_CPU = \"1000m\";\n }\n", "next_change": {"commit": "6fcab6675a7083444742ef1b85d14d6b7f235271", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\nindex a79412f4d..c83875c46 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/Constants.java\n", "chunk": "@@ -463,4 +463,9 @@ public interface Constants {\n     String CO_REQUESTS_CPU = \"200m\";\n     String CO_LIMITS_MEMORY = \"512Mi\";\n     String CO_LIMITS_CPU = \"1000m\";\n+\n+    /**\n+     * Connect build image name\n+     */\n+    String ST_CONNECT_BUILD_IMAGE_NAME = \"strimzi-sts-connect-build\";\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "39bda5352266a44e4f7d6703ef000c63b500c7a9", "committedDate": "2020-10-24 11:59:33 +0200", "message": "Remove Travis from Strimzi Operators (#3861)"}, {"oid": "1b425956e43a05ac3ebef24c40dc18bb7bbb4cdd", "committedDate": "2020-11-13 18:59:19 +0100", "message": "[MO] - [OLM] - upgrade suite (#3854)"}, {"oid": "eea72be651ccd8e7399d0daeeec17149f19f5ecf", "committedDate": "2020-12-05 00:02:31 +0100", "message": "Change images for example clients (#4055)"}, {"oid": "a6b36e4e40821258b0d442e4a5a4eb968a0c2acd", "committedDate": "2021-01-13 20:11:41 +0100", "message": "[systemtest] Add checks to deprecated MetricsST tests (#4220)"}, {"oid": "f6b19390e9d5747e8111b2682381c5af0c7f086f", "committedDate": "2021-02-21 21:58:07 +0100", "message": "Setup minikube with internal registry  (#4388)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "652b4755223d0e35412c7300effb3cce3755a3e6", "committedDate": "2021-03-10 17:08:58 +0100", "message": "Move install, example and Helm chart files to packaging (#4513)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "23714a698f000f69e9b12d89f4d0160a875765be", "committedDate": "2021-03-26 18:24:01 +0100", "message": "Test for CruiseControl rootlogger level change (#4641)"}, {"oid": "178ae2e730a3f9628f5fd6800e798629ab724c25", "committedDate": "2021-03-29 20:49:18 +0200", "message": "Issue-766: Updated reconciliation interval and zk session timeout default value (#4637)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "e2309b9989ddbdb15169a9e83a763858cb1d6230", "committedDate": "2021-04-25 12:29:57 +0200", "message": "JMX test (#4790)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "d7161c6db23d384a4e8f5ec810dbc53418ed4122", "committedDate": "2021-06-28 13:43:12 +0200", "message": "Remove KafkaConnectS2I resource and its support (#5199)"}, {"oid": "5b48b0ab3fe76f271ae9b9af0600a6ec031aa164", "committedDate": "2021-06-29 22:10:54 +0200", "message": "Fix for Metrics ConfigMap containing JSON under YML property #3986 (#5072)"}, {"oid": "5ea1789f3cadffb84d0905f38081319986a0270e", "committedDate": "2021-07-10 10:31:05 +0200", "message": "ST: Apply Roles when CO use namespace-rbac and cluster wide options properly (#5266)"}, {"oid": "bb2bad568c763653a102f5023f7d5e6a435078f6", "committedDate": "2021-07-12 21:42:38 +0200", "message": "[MO] - [Parallel namespace test] -> TracingST (#5284)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "13c61d54bfa6354fe1e458b2f57cb44bfb020c76", "committedDate": "2021-08-10 18:26:39 +0200", "message": "Update tags and make client examples images configurable (#5392)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "72f22650f0a1e3916dc82e31302a54a9b3612b31", "committedDate": "2021-09-03 12:58:36 +0200", "message": "[systemtest] Add proper wait for `Job` to be ready (#5509)"}, {"oid": "d039da67e4039d2beb82b379f55bb57f7fe37ff8", "committedDate": "2021-09-24 21:42:03 +0200", "message": "[tests] Throttle Topic create/delete/update operations KIP-599 (#5492)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "07fc066a499b799659c6cc42a537928719511c72", "committedDate": "2021-10-20 12:18:42 +0200", "message": "[systemtest] Add STs for using DrainCleaner (#5696)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d21903e1d7a923f358b0e6580de61b75b7770be1", "committedDate": "2022-01-19 10:42:23 +0100", "message": "[MO] - [system test] -> eliminate un-necessary CO creation and deletion (#6210)"}, {"oid": "da4b9447221db033c30c586637b1a0bf3190bfc9", "committedDate": "2022-02-13 23:49:25 +0100", "message": "[MO] - [system test] -> LogCollector TS & TC labels + @ParallelSuite \u2026 (#6336)"}, {"oid": "011fcdb8bf74aaf7fad8e09d76de00155ee030f7", "committedDate": "2022-02-14 15:30:28 +0100", "message": "workaround removal init (#6323)"}, {"oid": "49f032f18aed2e6fa9969b975db381b2729dc7ca", "committedDate": "2022-02-23 11:34:50 +0100", "message": "[MO] - [system test] -> propage test suite and test case controller l\u2026 (#6411)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "453f10cd29a18388f4f37a237cc8bd5982748492", "committedDate": "2022-03-14 22:27:57 +0100", "message": "ST: Add sanity profile for system tests (#6523)"}, {"oid": "1496836aa2d60194c611caa5fb1081c2154ef719", "committedDate": "2022-03-16 13:27:58 +0100", "message": "[MO] - [system test] -> parallelism + OLM fixes (#6468)"}, {"oid": "ec40708a6504da43674f25770d8fdf3ecacdcd2f", "committedDate": "2022-03-30 21:45:36 +0200", "message": "[MO] - [system test] -> SecurityST race condition fixes and correct R\u2026 (#6588)"}, {"oid": "db8804d62aaea88824885a1ffb66ad3e5a2d1e0f", "committedDate": "2022-04-19 09:49:04 +0200", "message": "[systemtest] Rewrite throttling quotas tests (#6680)"}, {"oid": "6d5695189c6fbe018ed104e461e959260cc90c4c", "committedDate": "2022-05-05 15:02:27 +0200", "message": "[MO] - [system test] -> Resource allocation of ClusterOperator, Kafka, KafkaMirrorMaker components (#6654)"}, {"oid": "c5b8d2d3c286c6a388ed984b60953f4ab797b159", "committedDate": "2022-05-07 01:05:25 +0200", "message": "[ST Enhancement] Deletion of remaining pvcs after tests. (#6749)"}, {"oid": "91143dc1176ea5a299a23c6066dfab74c7a0247c", "committedDate": "2022-05-09 09:54:27 +0200", "message": "[systemtest] Test clients exchange - remaining packages (#6756)"}, {"oid": "62f63c12a507c951b0e08ce024975a95ce305314", "committedDate": "2022-05-10 21:09:53 +0200", "message": "[systemtest] Remove `test-client` completely from the repository (#6782)"}, {"oid": "63c18914dbe95827b7cce28cd0eb7e32e1f5e077", "committedDate": "2022-05-13 15:34:45 +0200", "message": "Adding `KafkaRebalance` add/remove broker modes for rebalancing after/before scale up/down (#6800)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "5fb730e624c4942f6e71c2713f64f230caca4ffe", "committedDate": "2022-06-10 18:46:32 +0200", "message": "Move UseStrimziPodSet feature gate to beta (#6906)"}, {"oid": "5eb404fda80f572c6998f13fa3c5a60fab9b1b38", "committedDate": "2022-08-18 18:26:59 +0200", "message": "[MO] - [system test] -> Pod Security policies  (#7191)"}, {"oid": "c7758768427e785ad619b00ad6c5d8af669a392a", "committedDate": "2022-10-03 22:41:56 +0200", "message": "[systemtest] Enable multiple COs in one namespace in our tests after LeaderElection feature (#7412)"}, {"oid": "b40f0c71cbb487a793f2c288274c228abc39398c", "committedDate": "2022-10-18 09:50:24 +0200", "message": "[ST] Add message count, kafka + zk + eo STS or deployment names into the TestStorage (#7423)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "953a3c6fa0445e010da0517253286344a981b228", "committedDate": "2022-11-07 22:19:55 +0100", "message": "[systemtest] Collect StrimziPodSets in LogCollector during test failure (#7573)"}, {"oid": "d30cd390083cba6055c04629f5603cdad9ece1b8", "committedDate": "2022-12-05 18:42:15 +0100", "message": "Add `Lease` resource to the installation files to make sure it is deleted when uninstalling the operator (#7748)"}, {"oid": "b821d05ffb965dce4054cd3a855ca0e4315bd9d0", "committedDate": "2023-01-06 11:29:03 +0100", "message": "[ST] Fix issues with cert-manager webhook (#7837)"}, {"oid": "caf506b2c0007482fb697a99198ce04cf5057800", "committedDate": "2023-01-13 19:46:08 +0100", "message": "[systemtest] Add smoke test for testing all supported Kafka versions during regression pipeline (#7902)"}, {"oid": "b7d000052f593144b3fbc20b1986c9b34feb08b7", "committedDate": "2023-01-23 15:49:35 +0100", "message": "[systemtest] Update scalability tests for UO and update tags (#7930)"}, {"oid": "6b348401ff64c6045c80f3169ea222e73bcd6908", "committedDate": "2023-02-01 13:11:27 +0100", "message": "[systemtest] Refactor OLM related classes (#7999)"}, {"oid": "18b32823fce11385e014a7d4e8af30e50ba61fc6", "committedDate": "2023-02-02 14:10:09 +0100", "message": "[ST] KafkaConnector autorestart + update echo-sink-connector version (#7976)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "a858ed028a7acb545188b7cf18f45d770b9fbd8e", "committedDate": "2023-03-01 10:58:07 +0100", "message": "Make system test work with Connect stable identities  (#8113)"}, {"oid": "32c0e29ca574990b5ec20166624eca03b34d34ad", "committedDate": "2023-03-24 16:04:06 +0100", "message": "Graduate `UseStrimziPodSets` feature gate and remove `StatefulSet` support (#8273)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}, {"oid": "f704cb7e881df2782685fcdc0361f42bf7f63807", "committedDate": "2023-04-27 07:59:14 +0200", "message": "ST: Increase memory for CO to 512Mi in STs on FIPS clusters (#8439)"}, {"oid": "6fcab6675a7083444742ef1b85d14d6b7f235271", "committedDate": "2023-05-05 18:41:43 +0200", "message": "ST: Allow users to use any registry for connect build specified via env var (#8470)"}, {"oid": "67f76ea836c012dbf15fb6c54aece7740864555b", "committedDate": "2023-05-25 23:46:30 +0200", "message": "Add tag for tests which are not supported on arm (#8566)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4MTA0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472481042", "body": "How does this deal with the exceptions to the forbidden prefixes?", "bodyText": "How does this deal with the exceptions to the forbidden prefixes?", "bodyHTML": "<p dir=\"auto\">How does this deal with the exceptions to the forbidden prefixes?</p>", "author": "scholzj", "createdAt": "2020-08-18T20:42:44Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -270,44 +271,51 @@ public class KafkaUtils {\n      * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return Map<String, ConfigModel> all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -240,74 +261,76 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMTI5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474731290", "body": "I think you mean `@link`.", "bodyText": "I think you mean @link.", "bodyHTML": "<p dir=\"auto\">I think you mean <code>@link</code>.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:23:51Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..c6d3a814a 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -181,7 +184,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c6d3a814a..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -170,146 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n-     * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n-     */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n-        } catch (IOException e) {\n-            e.printStackTrace();\n-        }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n-    }\n-\n-    /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n-     */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n-\n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n-            .entrySet()\n-            .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigs;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -181,7 +203,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n", "next_change": {"commit": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..3d9183f0f 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -211,6 +214,7 @@ public class KafkaUtils {\n      */\n     public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n         updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        waitForClusterStability(clusterName);\n     }\n \n     /**\n", "next_change": {"commit": "8bcead0a21c8785e30b1ef36140208fe8379214e", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 3d9183f0f..58057ce27 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -223,7 +261,7 @@ public class KafkaUtils {\n      * @param value value of specific property\n      */\n     public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and expected is {}={}\",\n             brokerConfigName,\n             KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n             brokerConfigName,\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 58057ce27..fe4ddab1e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -229,49 +221,53 @@ public class KafkaUtils {\n \n     /**\n      * Method which, update/replace Kafka configuration\n+     * @param namespaceName name of the namespace\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResourceInSpecificNamespace(clusterName, kafka -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n             config.put(brokerConfigName, value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n-        });\n+        }, namespaceName);\n     }\n \n     /**\n      * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param namespaceName namespace name\n      * @param clusterName name of the cluster where Kafka resource can be found\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n-        waitForClusterStability(clusterName);\n+    public static void  updateConfigurationWithStabilityWait(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(namespaceName, clusterName, brokerConfigName, value);\n+        waitForClusterStability(namespaceName, clusterName);\n     }\n \n     /**\n      * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param namespaceName name of the namespace\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n      */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+    public synchronized static boolean verifyCrDynamicConfiguration(final String namespaceName, String clusterName, String brokerConfigName, Object value) {\n         LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and expected is {}={}\",\n             brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n             brokerConfigName,\n             value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        return KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n     }\n \n     /**\n      * Verifies that updated configuration was successfully changed inside Kafka pods\n+     * @param namespaceName name of the namespace\n      * @param kafkaPodNamePrefix prefix of Kafka pods\n      * @param brokerConfigName key of specific property\n      * @param value value of specific property\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fe4ddab1e..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -275,15 +298,16 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public synchronized static boolean verifyPodDynamicConfiguration(final String namespaceName, String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+    public synchronized static boolean verifyPodDynamicConfiguration(final String namespaceName, String scraperPodName, String bootstrapServer, String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(namespaceName, kafkaPodNamePrefix);\n+        int[] brokerId = {0};\n \n         for (Pod pod : kafkaPods) {\n \n             TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n                 () -> {\n-                    String result = cmdKubeClient(namespaceName).execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+                    String result = KafkaCmdClient.describeKafkaBrokerUsingPodCli(namespaceName, scraperPodName, bootstrapServer, brokerId[0]++);\n \n                     LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n", "next_change": null}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMzczOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474733739", "body": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\r\n\r\n```suggestion\r\n     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\r\n```", "bodyText": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n          \n          \n            \n                 * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.", "bodyHTML": "<p dir=\"auto\">You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which load all </span>supported <span class=\"x x-first x-last\">kafka configuration </span>generated by #<span class=\"pl-smi\">KafkaConfigModelGenerator</span> in config<span class=\"pl-k\">-</span>model<span class=\"pl-k\">-</span>generator</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Loads</span><span class=\"x x-last\"> all kafka config parameters </span>supported <span class=\"x x-first\">by the given {</span><span class=\"pl-k x\">@code</span><span class=\"x x-last\"> kafkaVersion}, as </span>generated by #<span class=\"pl-smi\">KafkaConfigModelGenerator</span> in config<span class=\"pl-k\">-</span>model<span class=\"pl-k\">-</span>generator<span class=\"x x-first x-last\">.</span></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-21T14:27:58Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,26 +248,21 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n", "next_change": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..ab45879a1 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -266,7 +266,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex ab45879a1..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Verifies that updated configuration was successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n-     * @param kafkaVersion specific kafka version\n-     * @return all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Return dynamic Kafka configs supported by the the given version of Kafka.\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -240,74 +261,76 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczNDA3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474734075", "body": "rethrow a `RuntimeException`", "bodyText": "rethrow a RuntimeException", "bodyHTML": "<p dir=\"auto\">rethrow a <code>RuntimeException</code></p>", "author": "tombentley", "createdAt": "2020-08-21T14:28:33Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,26 +248,21 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n", "next_change": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..ab45879a1 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -266,7 +266,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex ab45879a1..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Verifies that updated configuration was successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n-     * @param kafkaVersion specific kafka version\n-     * @return all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Return dynamic Kafka configs supported by the the given version of Kafka.\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -240,74 +261,76 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474738256", "body": "You should be able to use the `config-model` module. This is what `KafkaConfiguration` does:\r\n\r\n```java\r\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\r\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\r\n        try {\r\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\r\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\r\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\r\n                    throw new RuntimeException(\"Incorrect version\");\r\n                }\r\n                return configModels.getConfigs();\r\n            }\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\r\n        }\r\n    }\r\n```\r\n\r\nThat will give you nicely typed `ConfigModel` to deal with, rather than `JsonObject`. Please try to refactor `KafkaConfiguration` to use a common implementation rather than copying the above code.", "bodyText": "You should be able to use the config-model module. This is what KafkaConfiguration does:\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(\"Incorrect version\");\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n        }\n    }\nThat will give you nicely typed ConfigModel to deal with, rather than JsonObject. Please try to refactor KafkaConfiguration to use a common implementation rather than copying the above code.", "bodyHTML": "<p dir=\"auto\">You should be able to use the <code>config-model</code> module. This is what <code>KafkaConfiguration</code> does:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"private Map&lt;String, ConfigModel&gt; readConfigModel(KafkaVersion kafkaVersion) {\n        String name = &quot;/kafka-&quot; + kafkaVersion.version() + &quot;-config-model.json&quot;;\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(&quot;Incorrect version&quot;);\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(&quot;Error reading from classpath resource &quot; + name, e);\n        }\n    }\"><pre><span class=\"pl-k\">private</span> <span class=\"pl-k\">Map&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">ConfigModel</span>&gt;</span> readConfigModel(<span class=\"pl-smi\">KafkaVersion</span> kafkaVersion) {\n        <span class=\"pl-smi\">String</span> name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/kafka-<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> kafkaVersion<span class=\"pl-k\">.</span>version() <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-config-model.json<span class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">try</span> {\n            <span class=\"pl-k\">try</span> (<span class=\"pl-smi\">InputStream</span> in <span class=\"pl-k\">=</span> <span class=\"pl-smi\">KafkaConfiguration</span><span class=\"pl-k\">.</span>class<span class=\"pl-k\">.</span>getResourceAsStream(name)) {\n                <span class=\"pl-smi\">ConfigModels</span> configModels <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">ObjectMapper</span>()<span class=\"pl-k\">.</span>readValue(in, <span class=\"pl-smi\">ConfigModels</span><span class=\"pl-k\">.</span>class);\n                <span class=\"pl-k\">if</span> (<span class=\"pl-k\">!</span>kafkaVersion<span class=\"pl-k\">.</span>version()<span class=\"pl-k\">.</span>equals(configModels<span class=\"pl-k\">.</span>getVersion())) {\n                    <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">RuntimeException</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Incorrect version<span class=\"pl-pds\">\"</span></span>);\n                }\n                <span class=\"pl-k\">return</span> configModels<span class=\"pl-k\">.</span>getConfigs();\n            }\n        } <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">IOException</span> e) {\n            <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">RuntimeException</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Error reading from classpath resource <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> name, e);\n        }\n    }</pre></div>\n<p dir=\"auto\">That will give you nicely typed <code>ConfigModel</code> to deal with, rather than <code>JsonObject</code>. Please try to refactor <code>KafkaConfiguration</code> to use a common implementation rather than copying the above code.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:35:04Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ4ODM5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476488398", "bodyText": "Done :)", "author": "see-quick", "createdAt": "2020-08-25T14:22:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..44a0fdd31 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -242,26 +248,21 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n", "next_change": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 44a0fdd31..ab45879a1 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -266,7 +266,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex ab45879a1..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Verifies that updated configuration was successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n-     * @param kafkaVersion specific kafka version\n-     * @return all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Return dynamic Kafka configs supported by the the given version of Kafka.\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 404a2059c..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -240,74 +261,76 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return all supported kafka properties\n      */\n-    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n     @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n-                    !(\n-                        a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n-            )\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n+\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n+\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n+\n+                return isClusterWideOrPerBroker;\n+            })\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..c56279c9e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -333,4 +334,13 @@ public class KafkaUtils {\n \n         return dynamicConfigsWithExceptions;\n     }\n+\n+    /**\n+     * Generated random name for the Kafka resource based on prefix\n+     * @param clusterName name prefix\n+     * @return name with prefix and random salt\n+     */\n+    public static String generateRandomNameOfKafka(String clusterName) {\n+        return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n+    }\n }\n", "next_change": {"commit": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c56279c9e..8a7060651 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -343,4 +343,15 @@ public class KafkaUtils {\n     public static String generateRandomNameOfKafka(String clusterName) {\n         return clusterName + \"-\" + new Random().nextInt(Integer.MAX_VALUE);\n     }\n+\n+    public static String getVersionFromKafkaPodLibs(String kafkaPodName) {\n+        String command = \"ls libs | grep -Po 'kafka_\\\\d+.\\\\d+-\\\\K(\\\\d+.\\\\d+.\\\\d+)(?=.*jar)' | head -1 | cut -d \\\"-\\\" -f2\";\n+        return cmdKubeClient().execInPodContainer(\n+            kafkaPodName,\n+            \"kafka\",\n+            \"/bin/bash\",\n+            \"-c\",\n+            command\n+        ).out().trim();\n+    }\n }\n", "next_change": {"commit": "a547519d4eae659c733db9c5875f76093f61d15f", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 8a7060651..b5e64a39d 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -354,4 +356,21 @@ public class KafkaUtils {\n             command\n         ).out().trim();\n     }\n+\n+    public static void waitForKafkaDeletion(String kafkaClusterName) {\n+        LOGGER.info(\"Waiting for deletion of Kafka:{}\", kafkaClusterName);\n+        TestUtils.waitFor(\"Kafka deletion \" + kafkaClusterName, Constants.POLL_INTERVAL_FOR_RESOURCE_READINESS, DELETION_TIMEOUT,\n+            () -> {\n+                if (KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get() == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getStatefulSet(KafkaResources.zookeeperStatefulSetName(kafkaClusterName)) == null &&\n+                    kubeClient().getDeployment(KafkaResources.entityOperatorDeploymentName(kafkaClusterName)) == null) {\n+                    return true;\n+                } else {\n+                    cmdKubeClient().deleteByName(Kafka.RESOURCE_KIND, kafkaClusterName);\n+                    return false;\n+                }\n+            },\n+            () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n+    }\n }\n", "next_change": {"commit": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex b5e64a39d..543aca4e8 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -373,4 +378,22 @@ public class KafkaUtils {\n             },\n             () -> LOGGER.info(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(kafkaClusterName).get()));\n     }\n+\n+    public static String changeOrRemoveKafkaVersion(File file, String version) {\n+        YAMLMapper mapper = new YAMLMapper();\n+        try {\n+            JsonNode node = mapper.readTree(file);\n+            ObjectNode kafkaNode = (ObjectNode) node.at(\"/spec/kafka\");\n+            if (version == null) {\n+                kafkaNode.remove(\"version\");\n+                ((ObjectNode) kafkaNode.get(\"config\")).remove(\"log.message.format.version\");\n+            } else if (!version.equals(\"\")) {\n+                kafkaNode.put(\"version\", version);\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n+            }\n+            return mapper.writeValueAsString(node);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n", "next_change": {"commit": "96493c56e9e35c24d148b663c13197bca07d7856", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 543aca4e8..829d7203e 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -391,6 +395,12 @@ public class KafkaUtils {\n                 kafkaNode.put(\"version\", version);\n                 ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", version.substring(0, 3));\n             }\n+            if (logMessageFormat != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"log.message.format.version\", logMessageFormat);\n+            }\n+            if (interBrokerProtocol != null) {\n+                ((ObjectNode) kafkaNode.get(\"config\")).put(\"inter.broker.protocol.version\", interBrokerProtocol);\n+            }\n             return mapper.writeValueAsString(node);\n         } catch (IOException e) {\n             throw new RuntimeException(e);\n", "next_change": {"commit": "1e67c880e01dea157376b2bf3a02903b976db3ef", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 829d7203e..631657bcd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -406,4 +465,16 @@ public class KafkaUtils {\n             throw new RuntimeException(e);\n         }\n     }\n+\n+    public static String namespacedPlainBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9092);\n+    }\n+\n+    public static String namespacedTlsBootstrapAddress(String clusterName, String namespace) {\n+        return namespacedBootstrapAddress(clusterName, namespace, 9093);\n+    }\n+\n+    private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n+        return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n+    }\n }\n", "next_change": {"commit": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 631657bcd..c2b3b65ab 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -477,4 +481,29 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+    /**\n+     * Kafka scripts related methods\n+     */\n+    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n+        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n+                \"--describe\",\n+                \"--bootstrap-server\",\n+                \"localhost:9092\",\n+                \"--group\",\n+                consumerGroup)\n+            .out()\n+            .trim();\n+\n+        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n+\n+        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n+        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n+        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n+\n+        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n+\n+\n+        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n+    }\n }\n", "next_change": {"commit": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c2b3b65ab..c9bcb5b39 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -481,29 +502,4 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n-\n-    /**\n-     * Kafka scripts related methods\n-     */\n-    public static int getCurrentOffsets(String podName, String topicName, String consumerGroup) {\n-        String offsetOutput = cmdKubeClient().execInPod(podName, \"/opt/kafka/bin/kafka-consumer-groups.sh\",\n-                \"--describe\",\n-                \"--bootstrap-server\",\n-                \"localhost:9092\",\n-                \"--group\",\n-                consumerGroup)\n-            .out()\n-            .trim();\n-\n-        String replaced = offsetOutput.replaceAll(\"\\\\s\\\\s+\", \" \");\n-\n-        List<String> lines = Arrays.asList(replaced.split(\"\\n\"));\n-        List<String> headers = Arrays.asList(lines.get(0).split(\" \"));\n-        List<String> matchingLine = Arrays.asList(lines.stream().filter(line -> line.contains(topicName)).findFirst().get().split(\" \"));\n-\n-        Map<String, String> valuesMap = IntStream.range(0, headers.size()).boxed().collect(Collectors.toMap(headers::get, matchingLine::get));\n-\n-\n-        return Integer.parseInt(valuesMap.get(\"CURRENT-OFFSET\"));\n-    }\n }\n", "next_change": {"commit": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex c9bcb5b39..4869f0ef5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -502,4 +502,24 @@ public class KafkaUtils {\n     private static String namespacedBootstrapAddress(String clusterName, String namespace, int port) {\n         return KafkaResources.bootstrapServiceName(clusterName) + \".\" + namespace + \".svc:\" + port;\n     }\n+\n+\n+    public static String bootstrapAddressFromStatus(String clusterName, String namespaceName, String listenerName) {\n+\n+        List<ListenerStatus> listenerStatusList = KafkaResource.kafkaClient().inNamespace(namespaceName).withName(clusterName).get().getStatus().getListeners();\n+\n+        if (listenerStatusList == null || listenerStatusList.size() < 1) {\n+            LOGGER.error(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+            throw new RuntimeException(\"There is no Kafka external listener specified in the Kafka CR Status\");\n+        } else if (listenerName == null) {\n+            LOGGER.info(\"Listener name is not specified. Picking the first one from the Kafka Status.\");\n+            return listenerStatusList.get(0).getBootstrapServers();\n+        }\n+\n+        return listenerStatusList.stream().filter(listener -> listener.getName().equals(listenerName))\n+                .findFirst()\n+                .orElseThrow(RuntimeException::new)\n+                .getBootstrapServers();\n+    }\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474740874", "body": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\r\n\r\n```\r\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")\r\n```", "bodyText": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")", "bodyHTML": "<p dir=\"auto\">Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"cmdKubeClient().execKafkaConfigsInPod(podName, &quot;--entity-type brokers --entity-name 0 --describe&quot;)\"><pre><code>cmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")\n</code></pre></div>", "author": "tombentley", "createdAt": "2020-08-21T14:39:34Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc2Njk4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474766980", "bodyText": "Yeah, will do in another PR and also create an issue for that.", "author": "see-quick", "createdAt": "2020-08-21T15:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA=="}], "type": "inlineReview", "revised_code": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex 8b18e8dbe..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,321 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.kafka.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-/**\n- * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update.\n- * Isolated -> for each test case we have different configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..8b18e8dbe\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(kafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+    }\n+\n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+    }\n+}\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "d7706e97cecfac85a803a41b4f7a6634323408c6", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 8b18e8dbe..476afa947 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -68,23 +72,23 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .endSpec()\n             .done();\n \n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(clusterName)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n         kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n         updateAndVerifyDynConf(kafkaConfig);\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(clusterName)).getData().get(\"server.config\");\n         assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n         assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n", "next_change": {"commit": "2903e51d5479a7979a9bf56b80506f654753a4b2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 476afa947..fe011dd9f 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -97,9 +102,13 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(ROLLING_UPDATE)\n-    @Test\n-    void testUpdateToExternalListenerCausesRollingRestart() {\n-        KafkaResource.kafkaPersistent(clusterName, KAFKA_REPLICAS, 1)\n+    @IsolatedTest(\"Using more tha one Kafka cluster in one namespace\")\n+    void testUpdateToExternalListenerCausesRollingRestart(ExtensionContext extensionContext) {\n+        String clusterName = mapWithClusterNames.get(extensionContext.getDisplayName());\n+        Map<String, Object> deepCopyOfShardKafkaConfig = kafkaConfig.entrySet().stream()\n+            .collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue()));\n+\n+        resourceManager.createResource(extensionContext, KafkaTemplates.kafkaPersistent(clusterName, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n                     .editListeners()\n", "next_change": {"commit": "33da771f49456935ab6f2122695db4f925879c96", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex fe011dd9f..97a34b4b5 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -111,14 +111,24 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         resourceManager.createResource(extensionContext, KafkaTemplates.kafkaPersistent(clusterName, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .editListeners()\n-                        .addNewGenericKafkaListener()\n-                            .withName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n-                            .withPort(9094)\n-                            .withType(KafkaListenerType.NODEPORT)\n-                            .withTls(false)\n-                        .endGenericKafkaListener()\n-                    .endListeners()\n+                    .withListeners(new GenericKafkaListenerBuilder()\n+                                .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n+                                .withPort(9092)\n+                                .withType(KafkaListenerType.INTERNAL)\n+                                .withTls(false)\n+                                .build(),\n+                            new GenericKafkaListenerBuilder()\n+                                .withName(Constants.TLS_LISTENER_DEFAULT_NAME)\n+                                .withPort(9093)\n+                                .withType(KafkaListenerType.INTERNAL)\n+                                .withTls(true)\n+                                .build(),\n+                            new GenericKafkaListenerBuilder()\n+                                .withName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n+                                .withPort(9094)\n+                                .withType(KafkaListenerType.NODEPORT)\n+                                .withTls(false)\n+                                .build())\n                     .withConfig(deepCopyOfShardKafkaConfig)\n                 .endKafka()\n             .endSpec()\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 64%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex 97a34b4b5..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -134,21 +142,21 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .endSpec()\n             .build());\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        String kafkaConfigurationFromPod = cmdKubeClient().namespace(namespace).execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n         deepCopyOfShardKafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        updateAndVerifyDynConf(clusterName, deepCopyOfShardKafkaConfig);\n+        updateAndVerifyDynConf(namespace, clusterName, deepCopyOfShardKafkaConfig);\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        kafkaConfigurationFromPod = cmdKubeClient().namespace(namespace).execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n+        Map<String, String> kafkaPods = PodUtils.podSnapshot(namespace, kafkaSelector);\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n \n-        KafkaResource.replaceKafkaResource(clusterName, k -> {\n+        KafkaResource.replaceKafkaResourceInSpecificNamespace(clusterName, k -> {\n             k.getSpec().getKafka().setListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "030f7b8946b1598b830783002ec2396a98b1dc4a", "committedDate": "2020-09-18 20:03:39 +0200", "message": "[systemtest] Fixes of tests in shared profile (#3660)"}, {"oid": "703d8cb141de03f1a5aa20c983d6f290f7a02995", "committedDate": "2020-10-07 21:09:58 +0200", "message": "Schema (#3653)"}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "d7706e97cecfac85a803a41b4f7a6634323408c6", "committedDate": "2021-01-08 15:30:08 +0100", "message": "[MO] - [1st step paralelism] - random names for all resources in systemtests (#4092)"}, {"oid": "4a95d1ad5679769b507fe760d1b7a4307a929023", "committedDate": "2021-01-13 13:51:07 +0100", "message": "[systemtest] Refactor of resource classes and STs - prerequisite for fabric8 upgrade (#4182)"}, {"oid": "a91f3dadd8d0adb49e96a8ebeb7bb64b4a71ee50", "committedDate": "2021-01-25 10:46:30 +0100", "message": "rename create method to createAndWaitForReadiness (#4306)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "62d42bc58481e07543817c049e9ad2e9bd373050", "committedDate": "2021-09-23 13:44:50 +0200", "message": "[systemtest] Rewrite External Kafka Clients (#5590)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MzQ5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474743497", "body": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about _withExternalListeners_, it's that changing an external listeners config causes a RU. So the name would be better as `testUpdateToExternalListenerCausesRollingRestart`", "bodyText": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about withExternalListeners, it's that changing an external listeners config causes a RU. So the name would be better as testUpdateToExternalListenerCausesRollingRestart", "bodyHTML": "<p dir=\"auto\">I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about <em>withExternalListeners</em>, it's that changing an external listeners config causes a RU. So the name would be better as <code>testUpdateToExternalListenerCausesRollingRestart</code></p>", "author": "tombentley", "createdAt": "2020-08-21T14:44:04Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 8b18e8dbe..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -92,8 +93,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex c55ed69b0..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,324 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.kafka.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.Constants.ROLLING_UPDATE;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-/**\n- * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update.\n- * Isolated -> for each test case we have different configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(ROLLING_UPDATE)\n-    @Test\n-    void testUpdateToExternalListenerCausesRollingRestart() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    @Tag(ROLLING_UPDATE)\n-    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..8b18e8dbe\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(kafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+    }\n+\n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+    }\n+}\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 8b18e8dbe..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -92,17 +96,19 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     }\n \n     @Tag(NODEPORT_SUPPORTED)\n+    @Tag(ROLLING_UPDATE)\n     @Test\n-    void testDynamicConfigurationWithExternalListeners() {\n+    void testUpdateToExternalListenerCausesRollingRestart() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": {"commit": "1b90a5c68422c5ba6a381161bad59a30422216d2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 09a3e6dac..a95f820a6 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -104,7 +104,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n                 .editKafka()\n                     .editListeners()\n                         .addNewGenericKafkaListener()\n-                            .withName(\"external\")\n+                            .withName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n                             .withPort(9094)\n                             .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n", "next_change": {"commit": "4a95d1ad5679769b507fe760d1b7a4307a929023", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a95f820a6..b5bf017fa 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -113,23 +113,23 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n             .endSpec()\n-            .done();\n+            .build());\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n         kafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n         updateAndVerifyDynConf(kafkaConfig);\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n \n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+        KafkaResource.replaceKafkaResource(clusterName, k -> {\n             k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n", "next_change": {"commit": "33da771f49456935ab6f2122695db4f925879c96", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex b5bf017fa..97a34b4b5 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -130,7 +149,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n \n         KafkaResource.replaceKafkaResource(clusterName, k -> {\n-            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+            k.getSpec().getKafka().setListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n                     .withPort(9092)\n", "next_change": {"commit": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 97a34b4b5..f508b96af 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -145,7 +146,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n+        Map<String, String> kafkaPods = PodUtils.podSnapshot(kafkaSelector);\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n \n         KafkaResource.replaceKafkaResource(clusterName, k -> {\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 72%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex f508b96af..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -135,21 +142,21 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .endSpec()\n             .build());\n \n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        String kafkaConfigurationFromPod = cmdKubeClient().namespace(namespace).execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n \n         deepCopyOfShardKafkaConfig.put(\"unclean.leader.election.enable\", true);\n \n-        updateAndVerifyDynConf(clusterName, deepCopyOfShardKafkaConfig);\n+        updateAndVerifyDynConf(namespace, clusterName, deepCopyOfShardKafkaConfig);\n \n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        kafkaConfigurationFromPod = cmdKubeClient().namespace(namespace).execInPod(KafkaResources.kafkaPodName(clusterName, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n         assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n \n         // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = PodUtils.podSnapshot(kafkaSelector);\n+        Map<String, String> kafkaPods = PodUtils.podSnapshot(namespace, kafkaSelector);\n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n \n-        KafkaResource.replaceKafkaResource(clusterName, k -> {\n+        KafkaResource.replaceKafkaResourceInSpecificNamespace(clusterName, k -> {\n             k.getSpec().getKafka().setListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.PLAIN_LISTENER_DEFAULT_NAME)\n", "next_change": null}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "030f7b8946b1598b830783002ec2396a98b1dc4a", "committedDate": "2020-09-18 20:03:39 +0200", "message": "[systemtest] Fixes of tests in shared profile (#3660)"}, {"oid": "703d8cb141de03f1a5aa20c983d6f290f7a02995", "committedDate": "2020-10-07 21:09:58 +0200", "message": "Schema (#3653)"}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "d7706e97cecfac85a803a41b4f7a6634323408c6", "committedDate": "2021-01-08 15:30:08 +0100", "message": "[MO] - [1st step paralelism] - random names for all resources in systemtests (#4092)"}, {"oid": "4a95d1ad5679769b507fe760d1b7a4307a929023", "committedDate": "2021-01-13 13:51:07 +0100", "message": "[systemtest] Refactor of resource classes and STs - prerequisite for fabric8 upgrade (#4182)"}, {"oid": "a91f3dadd8d0adb49e96a8ebeb7bb64b4a71ee50", "committedDate": "2021-01-25 10:46:30 +0100", "message": "rename create method to createAndWaitForReadiness (#4306)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "62d42bc58481e07543817c049e9ad2e9bd373050", "committedDate": "2021-09-23 13:44:50 +0200", "message": "[systemtest] Rewrite External Kafka Clients (#5590)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0NDA1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474744054", "body": "Same comment about javadoc or method name.", "bodyText": "Same comment about javadoc or method name.", "bodyHTML": "<p dir=\"auto\">Same comment about javadoc or method name.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:44:57Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 8b18e8dbe..c55ed69b0 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -186,7 +188,8 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\ndeleted file mode 100644\nindex c55ed69b0..000000000\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ /dev/null\n", "chunk": "@@ -1,324 +0,0 @@\n-/*\n- * Copyright Strimzi authors.\n- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n- */\n-package io.strimzi.systemtest.kafka.dynamicconfiguration;\n-\n-import io.strimzi.api.kafka.model.KafkaClusterSpec;\n-import io.strimzi.api.kafka.model.KafkaResources;\n-import io.strimzi.api.kafka.model.listener.KafkaListeners;\n-import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n-import io.strimzi.systemtest.AbstractST;\n-import io.strimzi.systemtest.Constants;\n-import io.strimzi.systemtest.Environment;\n-import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n-import io.strimzi.systemtest.resources.ResourceManager;\n-import io.strimzi.systemtest.resources.crd.KafkaResource;\n-import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n-import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n-import io.strimzi.systemtest.utils.TestKafkaVersion;\n-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n-import org.apache.kafka.common.security.auth.SecurityProtocol;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Tag;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n-import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n-import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n-import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n-import static io.strimzi.systemtest.Constants.REGRESSION;\n-import static io.strimzi.systemtest.Constants.ROLLING_UPDATE;\n-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n-\n-/**\n- * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n- * trigger rolling update.\n- * Isolated -> for each test case we have different configuration of Kafka resource\n- */\n-@Tag(REGRESSION)\n-@Tag(DYNAMIC_CONFIGURATION)\n-public class DynamicConfigurationIsolatedST extends AbstractST {\n-\n-    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n-    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n-    private static final int KAFKA_REPLICAS = 1;\n-\n-    private Map<String, Object> kafkaConfig;\n-\n-    @Test\n-    void testSimpleDynamicConfiguration() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        LOGGER.info(\"Verify values after update\");\n-        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n-        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n-        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n-        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n-    }\n-\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(ROLLING_UPDATE)\n-    @Test\n-    void testUpdateToExternalListenerCausesRollingRestart() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                        .withNewPlain()\n-                        .endPlain()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Edit listeners - this should cause RU (because of new crts)\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                    .endKafkaListenerExternalNodePort()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"compression.type\", \"snappy\");\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n-\n-        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n-        // Other external listeners cases are rolling because of crts\n-        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            KafkaListeners kl = new KafkaListenersBuilder()\n-                    .withNewPlain()\n-                    .endPlain()\n-                    .build();\n-            kafkaClusterSpec.setListeners(kl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n-\n-        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n-\n-        updateAndVerifyDynConf(kafkaConfig);\n-\n-        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n-    }\n-\n-    @Test\n-    @Tag(NODEPORT_SUPPORTED)\n-    @Tag(EXTERNAL_CLIENTS_USED)\n-    @Tag(ROLLING_UPDATE)\n-    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n-            .editSpec()\n-                .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n-                            .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n-                    .endListeners()\n-                    .withConfig(kafkaConfig)\n-                .endKafka()\n-            .endSpec()\n-            .done();\n-\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withKafkaUsername(USER_NAME)\n-            .withSecurityProtocol(SecurityProtocol.SSL)\n-            .build();\n-\n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n-            .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n-            .withMessageCount(MESSAGE_COUNT)\n-            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n-            .build();\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n-\n-        basicExternalKafkaClientTls.setKafkaUsername(userName);\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withNewKafkaListenerAuthenticationTlsAuth()\n-                        .endKafkaListenerAuthenticationTlsAuth()\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientTls.sendMessagesTls(),\n-                basicExternalKafkaClientTls.sendMessagesTls()\n-        );\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n-        });\n-\n-        LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaListeners updatedKl = new KafkaListenersBuilder()\n-                    .withNewKafkaListenerExternalNodePort()\n-                        .withTls(false)\n-                    .endKafkaListenerExternalNodePort()\n-                    .build();\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setListeners(updatedKl);\n-        });\n-\n-        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n-\n-        assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n-        });\n-\n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-                basicExternalKafkaClientPlain.sendMessagesPlain(),\n-                basicExternalKafkaClientPlain.receiveMessagesPlain()\n-        );\n-    }\n-\n-    /**\n-     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n-     * @param kafkaConfig specific kafka configuration, which will be changed\n-     */\n-    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n-\n-        LOGGER.info(\"Updating configuration of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n-            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n-            kafkaClusterSpec.setConfig(kafkaConfig);\n-        });\n-\n-        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n-        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n-    }\n-\n-    @BeforeEach\n-    void setupEach() {\n-        kafkaConfig = new HashMap<>();\n-        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n-        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n-        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n-    }\n-\n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n-}\n", "next_change": {"commit": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nnew file mode 100644\nindex 000000000..8b18e8dbe\n--- /dev/null\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    /**\n+     * UpdateAndVerifyDynConf, change the kafka configuration and verify that no rolling update were triggered\n+     * @param kafkaConfig specific kafka configuration, which will be changed\n+     */\n+    private void updateAndVerifyDynConf(Map<String, Object> kafkaConfig) {\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(kafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+    }\n+\n+    @BeforeEach\n+    void setupEach() {\n+        kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion());\n+    }\n+\n+    @BeforeAll\n+    void setup() throws Exception {\n+        ResourceManager.setClassResources();\n+        installClusterOperator(NAMESPACE);\n+    }\n+}\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 8b18e8dbe..09a3e6dac 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -186,14 +214,18 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n     @Test\n     @Tag(NODEPORT_SUPPORTED)\n     @Tag(EXTERNAL_CLIENTS_USED)\n-    void testDynamicConfigurationExternalTls() {\n+    @Tag(ROLLING_UPDATE)\n+    void testUpdateToExternalListenerCausesRollingRestartUsingExternalClients() {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .withNewListeners()\n-                        .withNewKafkaListenerExternalNodePort()\n+                    .editListeners()\n+                        .addNewGenericKafkaListener()\n+                            .withName(\"external\")\n+                            .withPort(9094)\n+                            .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n-                        .endKafkaListenerExternalNodePort()\n+                        .endGenericKafkaListener()\n                     .endListeners()\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n", "next_change": {"commit": "1b90a5c68422c5ba6a381161bad59a30422216d2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 09a3e6dac..a95f820a6 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -219,9 +219,9 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n             .editSpec()\n                 .editKafka()\n-                    .editListeners()\n+                    .withNewListeners()\n                         .addNewGenericKafkaListener()\n-                            .withName(\"external\")\n+                            .withName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n                             .withPort(9094)\n                             .withType(KafkaListenerType.NODEPORT)\n                             .withTls(false)\n", "next_change": {"commit": "4a95d1ad5679769b507fe760d1b7a4307a929023", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex a95f820a6..b5bf017fa 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -230,20 +230,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n                     .withConfig(kafkaConfig)\n                 .endKafka()\n             .endSpec()\n-            .done();\n+            .build());\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n \n-        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+        KafkaTopicResource.create(KafkaTopicResource.topic(clusterName, TOPIC_NAME).build());\n+        KafkaUserResource.create(KafkaUserResource.tlsUser(clusterName, USER_NAME).build());\n \n         String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+        KafkaUserResource.create(KafkaUserResource.tlsUser(clusterName, userName).build());\n \n         BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n             .withTopicName(TOPIC_NAME)\n             .withNamespaceName(NAMESPACE)\n-            .withClusterName(CLUSTER_NAME)\n+            .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withKafkaUsername(userName)\n             .withSecurityProtocol(SecurityProtocol.SSL)\n", "next_change": {"commit": "2903e51d5479a7979a9bf56b80506f654753a4b2", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex b5bf017fa..fe011dd9f 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -227,21 +242,18 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n                             .withTls(false)\n                         .endGenericKafkaListener()\n                     .endListeners()\n-                    .withConfig(kafkaConfig)\n+                    .withConfig(deepCopyOfShardKafkaConfig)\n                 .endKafka()\n             .endSpec()\n             .build());\n \n         Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n \n-        KafkaTopicResource.create(KafkaTopicResource.topic(clusterName, TOPIC_NAME).build());\n-        KafkaUserResource.create(KafkaUserResource.tlsUser(clusterName, USER_NAME).build());\n-\n-        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n-        KafkaUserResource.create(KafkaUserResource.tlsUser(clusterName, userName).build());\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n+        resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(clusterName, userName).build());\n \n         BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n+            .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n", "next_change": {"commit": "62d42bc58481e07543817c049e9ad2e9bd373050", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex fe011dd9f..474c9b756 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -252,7 +260,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n         resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(clusterName, userName).build());\n \n-        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+        ExternalKafkaClient externalKafkaClientTls = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n", "next_change": {"commit": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 474c9b756..19d6a4f37 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -262,7 +262,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         ExternalKafkaClient externalKafkaClientTls = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(NAMESPACE)\n+            .withNamespaceName(INFRA_NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withKafkaUsername(userName)\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 69%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex 19d6a4f37..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -255,14 +267,14 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .endSpec()\n             .build());\n \n-        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(clusterName));\n+        Map<String, String> kafkaPods = PodUtils.podSnapshot(namespace, kafkaSelector);\n \n-        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName).build());\n-        resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(clusterName, userName).build());\n+        resourceManager.createResource(extensionContext, KafkaTopicTemplates.topic(clusterName, topicName, namespace).build());\n+        resourceManager.createResource(extensionContext, KafkaUserTemplates.tlsUser(namespace, clusterName, userName).build());\n \n         ExternalKafkaClient externalKafkaClientTls = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(INFRA_NAMESPACE)\n+            .withNamespaceName(namespace)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withKafkaUsername(userName)\n", "next_change": null}]}}]}}, {"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex fe011dd9f..474c9b756 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -262,7 +270,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .withListenerName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n             .build();\n \n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+        ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n", "next_change": {"commit": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 474c9b756..19d6a4f37 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -272,7 +272,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(NAMESPACE)\n+            .withNamespaceName(INFRA_NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 69%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex 19d6a4f37..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -272,7 +284,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(INFRA_NAMESPACE)\n+            .withNamespaceName(namespace)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n", "next_change": null}]}}]}}]}}, {"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex b5bf017fa..fe011dd9f 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -251,7 +263,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .build();\n \n         BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n-            .withTopicName(TOPIC_NAME)\n+            .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n", "next_change": {"commit": "62d42bc58481e07543817c049e9ad2e9bd373050", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex fe011dd9f..474c9b756 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -262,7 +270,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .withListenerName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n             .build();\n \n-        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+        ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n             .withNamespaceName(NAMESPACE)\n             .withClusterName(clusterName)\n", "next_change": {"commit": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex 474c9b756..19d6a4f37 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -272,7 +272,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(NAMESPACE)\n+            .withNamespaceName(INFRA_NAMESPACE)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 69%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex 19d6a4f37..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -272,7 +284,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n \n         ExternalKafkaClient externalKafkaClientPlain = new ExternalKafkaClient.Builder()\n             .withTopicName(topicName)\n-            .withNamespaceName(INFRA_NAMESPACE)\n+            .withNamespaceName(namespace)\n             .withClusterName(clusterName)\n             .withMessageCount(MESSAGE_COUNT)\n             .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n", "next_change": null}]}}]}}, {"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nindex fe011dd9f..474c9b756 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n", "chunk": "@@ -271,20 +279,20 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n             .withListenerName(Constants.EXTERNAL_LISTENER_DEFAULT_NAME)\n             .build();\n \n-        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n-            basicExternalKafkaClientPlain.sendMessagesPlain(),\n-            basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        externalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+            externalKafkaClientPlain.sendMessagesPlain(),\n+            externalKafkaClientPlain.receiveMessagesPlain()\n         );\n \n         assertThrows(Exception.class, () -> {\n-            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n-            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            externalKafkaClientTls.sendMessagesTls();\n+            externalKafkaClientTls.receiveMessagesTls();\n             LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n         });\n \n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n         KafkaResource.replaceKafkaResource(clusterName, k -> {\n-            k.getSpec().getKafka().setListeners(new ArrayOrObjectKafkaListeners(Arrays.asList(\n+            k.getSpec().getKafka().setListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.TLS_LISTENER_DEFAULT_NAME)\n                     .withPort(9093)\n", "next_change": {"commit": "199c8d15edfccb3f12894a1459064bf6136da623", "changed_code": [{"header": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nsimilarity index 69%\nrename from systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\nrename to systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\nindex 474c9b756..b557c092d 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfST.java\n", "chunk": "@@ -291,7 +303,7 @@ public class DynamicConfigurationIsolatedST extends AbstractST {\n         });\n \n         LOGGER.info(\"Updating listeners of Kafka cluster\");\n-        KafkaResource.replaceKafkaResource(clusterName, k -> {\n+        KafkaResource.replaceKafkaResourceInSpecificNamespace(clusterName, k -> {\n             k.getSpec().getKafka().setListeners(Arrays.asList(\n                 new GenericKafkaListenerBuilder()\n                     .withName(Constants.TLS_LISTENER_DEFAULT_NAME)\n", "next_change": null}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "030f7b8946b1598b830783002ec2396a98b1dc4a", "committedDate": "2020-09-18 20:03:39 +0200", "message": "[systemtest] Fixes of tests in shared profile (#3660)"}, {"oid": "703d8cb141de03f1a5aa20c983d6f290f7a02995", "committedDate": "2020-10-07 21:09:58 +0200", "message": "Schema (#3653)"}, {"oid": "1b90a5c68422c5ba6a381161bad59a30422216d2", "committedDate": "2020-10-10 13:08:53 +0200", "message": "[MO] - [system test] -> test suite of multiple listeners (#3651)"}, {"oid": "d7706e97cecfac85a803a41b4f7a6634323408c6", "committedDate": "2021-01-08 15:30:08 +0100", "message": "[MO] - [1st step paralelism] - random names for all resources in systemtests (#4092)"}, {"oid": "4a95d1ad5679769b507fe760d1b7a4307a929023", "committedDate": "2021-01-13 13:51:07 +0100", "message": "[systemtest] Refactor of resource classes and STs - prerequisite for fabric8 upgrade (#4182)"}, {"oid": "a91f3dadd8d0adb49e96a8ebeb7bb64b4a71ee50", "committedDate": "2021-01-25 10:46:30 +0100", "message": "rename create method to createAndWaitForReadiness (#4306)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "3bd79ba3850e1f599408b792cf0a0bfcc6242bcf", "committedDate": "2021-06-14 23:59:56 +0200", "message": "[MO] - [system tests] -> correct installation for cluster-wide and si\u2026 (#5105)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "73a9efb537edebd4db0e52ca58707725cd932292", "committedDate": "2021-08-02 10:57:36 +0200", "message": "ST: Change deployment of Roles/RoleBingins to fix namespace-rbac pipelines (#5333)"}, {"oid": "62d42bc58481e07543817c049e9ad2e9bd373050", "committedDate": "2021-09-23 13:44:50 +0200", "message": "[systemtest] Rewrite External Kafka Clients (#5590)"}, {"oid": "9d7d0056f24d3e3e9a0c88f720bdcb94176bad6f", "committedDate": "2021-10-15 12:51:49 +0200", "message": "[MO] - [package-wide parallelism] -> deployment of operator, parallel suite mechanism, Bridge package support (#5446)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}]}, {"oid": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-21T18:19:14Z", "type": "forcePushed"}, {"oid": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-26T13:48:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3Njg1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r478376856", "body": "```suggestion\r\n     * Return dynamic Kafka configs supported by the the given version of Kafka.\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which process all supported configs by Kafka and filter all which are not dynamic\n          \n          \n            \n                 * Return dynamic Kafka configs supported by the the given version of Kafka.", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which process all</span> supported <span class=\"x x-first x-last\">configs </span>by <span class=\"pl-smi x x-first\">Kafka</span><span class=\"x x-last\"> and filter all which are not dynamic</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Return</span><span class=\"x\"> dynamic </span><span class=\"pl-smi x\">Kafka</span><span class=\"x x-last\"> configs</span> supported by <span class=\"x x-first\">the the given version of </span><span class=\"pl-smi x\">Kafka</span><span class=\"x x-last\">.</span></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-27T12:21:58Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -157,4 +170,152 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic", "originalCommit": "308c3ecdb4ea1d3e0389c44e4da596f89bf08acd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76de14021f24172b40ce8bc26d3bceb3babb323d", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex e9b1203f3..ab45879a1 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -266,7 +266,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n", "next_change": {"commit": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex ab45879a1..827a8a392 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -174,148 +180,45 @@ public class KafkaUtils {\n     /**\n      * Method which, update/replace Kafka configuration\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, (kafka) -> {\n             LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n             Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n-            config.put(brokerConfigName, value);\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n             kafka.getSpec().getKafka().setConfig(config);\n             LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n         });\n     }\n \n     /**\n-     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n      * with stability and ensures after update of Kafka resource there will be not rolling update\n      * @param clusterName name of the cluster where Kafka resource can be found\n-     * @param brokerConfigName key of specific property\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n-        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    /**\n-     * Verifies that updated configuration was successfully changed inside Kafka CR\n-     * @param brokerConfigName key of specific property\n-     * @param value value of specific property\n-     */\n-    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n-        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n-            brokerConfigName,\n-            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n-            brokerConfigName,\n-            value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n \n     /**\n-     * Verifies that updated configuration was successfully changed inside Kafka pods\n-     * @param kafkaPodNamePrefix prefix of Kafka pods\n-     * @param brokerConfigName key of specific property\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n-     * @return\n-     * true = if specific property match the excepted property\n-     * false = if specific property doesn't match the excepted property\n-     */\n-    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n-\n-        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n-\n-        for (Pod pod : kafkaPods) {\n-\n-            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n-                () -> {\n-                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n-\n-                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n-\n-                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n-                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n-                        LOGGER.error(\"Kafka configuration {}\", result);\n-                        return false;\n-                    }\n-                    return true;\n-                });\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\n-     * @param kafkaVersion specific kafka version\n-     * @return all supported kafka properties\n-     */\n-    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n-        String name = TestUtils.USER_PATH + \"/../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n-        try {\n-            try (InputStream in = new FileInputStream(name)) {\n-                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n-                if (!kafkaVersion.equals(configModels.getVersion())) {\n-                    throw new RuntimeException(\"Incorrect version\");\n-                }\n-                return configModels.getConfigs();\n-            }\n-        } catch (IOException e) {\n-            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n-        }\n-    }\n-\n-    /**\n-     * Return dynamic Kafka configs supported by the the given version of Kafka.\n-     * @param kafkaVersion specific kafka version\n-     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n-    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n-\n-        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n-\n-        LOGGER.info(\"This is configs {}\", configs.toString());\n-\n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> {\n-                String[] prefixKey = a.getKey().split(\"\\\\.\");\n-\n-                // filter all which is Scope = ClusterWide or PerBroker\n-                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n-\n-                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n-                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n-                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n-                }\n-\n-                return isClusterWideOrPerBroker;\n-            })\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n-\n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n-            .entrySet()\n-            .stream()\n-            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n-\n-        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n-\n-        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n-        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n-\n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n-\n-        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n \n-        return dynamicConfigsWithExceptions;\n+        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n     }\n }\n", "next_change": {"commit": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 827a8a392..4e56e9ae5 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -205,20 +203,18 @@ public class KafkaUtils {\n         PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n     }\n \n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n-\n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n-    }\n-\n     /**\n      * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n-        KafkaUtils.updateConfigurationWithStabilityWait(\"my-cluster\", kafkaDynamicConfiguration, value);\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        assertThat(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(\"my-cluster\").get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()), is(value));\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+        }\n     }\n }\n", "next_change": {"commit": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 4e56e9ae5..bc260e4a9 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -204,17 +209,39 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * Method, which encapsulates the update phase of dyn. configuration of Kafka CR + verifying that updating configuration were successfully changed inside Kafka CR\n      * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n      * @param value value of specific property\n      */\n-    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean replaceAndVerifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        // exercise phase\n         KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n \n-        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-        if (!result) {\n-            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));\n+            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+                LOGGER.error(\"Kafka configuration {}\", result);\n+                return false;\n+            }\n         }\n+        return true;\n     }\n }\n", "next_change": {"commit": "7183c843117f568922ac13319fb0281e40d1aabd", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex bc260e4a9..d147538d7 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -234,10 +233,13 @@ public class KafkaUtils {\n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"cat /tmp/strimzi.properties\").out();\n \n-            if (!result.contains(kafkaDynamicConfiguration.toString() + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod, kafkaDynamicConfiguration.toString(), value);\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n                 LOGGER.error(\"Kafka configuration {}\", result);\n                 return false;\n             }\n", "next_change": {"commit": "e095f29aaafd8abfd9b8a1975033b711292393a3", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex d147538d7..babbd3990 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -228,21 +230,25 @@ public class KafkaUtils {\n      * true = if specific property match the excepted property\n      * false = if specific property doesn't match the excepted property\n      */\n-    public static boolean verifyKafkaPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n \n         List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n \n         for (Pod pod : kafkaPods) {\n \n-            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n \n-            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n \n-            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n-                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n-                LOGGER.error(\"Kafka configuration {}\", result);\n-                return false;\n-            }\n+                    if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n         }\n         return true;\n     }\n", "next_change": {"commit": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex babbd3990..2f6c2d315 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -252,4 +256,75 @@ public class KafkaUtils {\n         }\n         return true;\n     }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+            )\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        return dynamicConfigs;\n+    }\n }\n", "next_change": {"commit": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 2f6c2d315..fac69def6 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -260,71 +261,93 @@ public class KafkaUtils {\n     /**\n      * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n      * @param kafkaVersion specific kafka version\n-     * @return JsonObject all supported kafka properties\n+     * @return Map<String, ConfigModel> all supported kafka properties\n      */\n-    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n-\n-        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n-        byte[] data = new byte[0];\n-\n-        try (FileInputStream fis = new FileInputStream(file)) {\n-\n-            data = new byte[(int) file.length()];\n-            fis.read(data);\n-\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n         } catch (IOException e) {\n-            e.printStackTrace();\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n         }\n-\n-        String kafkaConfigs = new String(data, Charset.defaultCharset());\n-\n-        return new JsonObject(kafkaConfigs);\n     }\n \n     /**\n      * Method, which process all supported configs by Kafka and filter all which are not dynamic\n      * @param kafkaVersion specific kafka version\n-     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     * @return all dynamic properties for specific kafka version\n      */\n-    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\"})\n-    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, ConfigModel> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n+\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // forbidden prefix exceptions\n+                a.getKey().startsWith(\"zookeeper.connection.timeout.ms\") ||\n+                a.getKey().startsWith(\"ssl.cipher.suites\") ||\n+                a.getKey().startsWith(\"ssl.protocol\") ||\n+                a.getKey().startsWith(\"ssl.enabled.protocols\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.num.partitions\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.replication.factor\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.retention.ms\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.retries\") ||\n+                a.getKey().startsWith(\"cruise.control.metrics.topic.auto.create.timeout.ms\"))\n+//                a.getKey().contains(FORBIDDEN_PREFIX_EXCEPTIONS)) //  this doesn't work\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n-        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n-            .getMap()\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n             .filter(a ->\n-                // ignoring everything which is READ_ONLY\n-                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n-                    // filtering configs with following prefixes\n-                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n-                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n-                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(a.getValue().getScope() == Scope.READ_ONLY) &&\n                     !(\n                         a.getKey().startsWith(\"listeners\") ||\n-                            a.getKey().startsWith(\"advertised\") ||\n-                            a.getKey().startsWith(\"broker\") ||\n-                            a.getKey().startsWith(\"listener\") ||\n-                            a.getKey().startsWith(\"host.name\") ||\n-                            a.getKey().startsWith(\"port\") ||\n-                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n-                            a.getKey().startsWith(\"sasl\") ||\n-                            a.getKey().startsWith(\"ssl\") ||\n-                            a.getKey().startsWith(\"security\") ||\n-                            a.getKey().startsWith(\"password\") ||\n-                            a.getKey().startsWith(\"principal.builder.class\") ||\n-                            a.getKey().startsWith(\"log.dir\") ||\n-                            a.getKey().startsWith(\"zookeeper.connect\") ||\n-                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n-                            a.getKey().startsWith(\"authorizer\") ||\n-                            a.getKey().startsWith(\"super.user\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n-                            a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                        a.getKey().startsWith(\"advertised\") ||\n+                        a.getKey().startsWith(\"broker\") ||\n+                        a.getKey().startsWith(\"listener\") ||\n+                        a.getKey().startsWith(\"host.name\") ||\n+                        a.getKey().startsWith(\"port\") ||\n+                        a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                        a.getKey().startsWith(\"sasl\") ||\n+                        a.getKey().startsWith(\"ssl\") ||\n+                        a.getKey().startsWith(\"security\") ||\n+                        a.getKey().startsWith(\"password\") ||\n+                        a.getKey().startsWith(\"principal.builder.class\") ||\n+                        a.getKey().startsWith(\"log.dir\") ||\n+                        a.getKey().startsWith(\"zookeeper.connect\") ||\n+                        a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                        a.getKey().startsWith(\"authorizer\") ||\n+                        a.getKey().startsWith(\"super.user\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.topic\") ||\n+                        a.getKey().startsWith(\"cruise.control.metrics.reporter.bootstrap.servers\"))\n+                //   !a.getKey().contains(FORBIDDEN_PREFIXES) // this doesn't work\n+\n             )\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        return dynamicConfigs;\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n+\n+        dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n+        dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n+\n+        dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n+\n+        return dynamicConfigsWithExceptions;\n     }\n }\n", "next_change": {"commit": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex fac69def6..62ca2c0bc 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -346,6 +318,8 @@ public class KafkaUtils {\n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+\n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n         return dynamicConfigsWithExceptions;\n", "next_change": {"commit": "fe509f09a63587f1103f9d178e25094c00fb47d6", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 62ca2c0bc..5d4f7a0bf 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -291,34 +290,44 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"This is configs {}\", configs.toString());\n \n-        List<String> forbiddenPrefixesExceptions = Arrays.asList(FORBIDDEN_PREFIX_EXCEPTIONS.split(\"\\\\s*,+\\\\s*\"));\n+        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n \n-        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n+        Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> forbiddenPrefixesExceptions.contains(a.getKey()))\n-            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+            .filter(a -> {\n+                String[] prefixKey = a.getKey().split(\"\\\\.\");\n \n-        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n+                // filter all which is Scope = ClusterWide or PerBroker\n+                boolean isClusterWideOrPerBroker = a.getValue().getScope() == Scope.CLUSTER_WIDE || a.getValue().getScope() == Scope.PER_BROKER;\n \n-        List<String> forbiddenPrefixes = Arrays.asList(FORBIDDEN_PREFIXES.split(\"\\\\s*,+\\\\s*\"));\n+                if (prefixKey[0].equals(\"ssl\") || prefixKey[0].equals(\"sasl\") || prefixKey[0].equals(\"advertised\") ||\n+                    prefixKey[0].equals(\"listeners\") || prefixKey[0].equals(\"listener\")) {\n+                    return isClusterWideOrPerBroker && !FORBIDDEN_PREFIXES.contains(prefixKey[0]);\n+                }\n \n-        Map<String, ConfigModel> dynamicConfigs = configs\n+                return isClusterWideOrPerBroker;\n+            })\n+            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+\n+        Map<String, ConfigModel> forbiddenExceptionsConfigs = configs\n             .entrySet()\n             .stream()\n-            .filter(a -> !(a.getValue().getScope() == Scope.READ_ONLY) && !forbiddenPrefixes.contains(a.getKey()))\n+            .filter(a -> FORBIDDEN_PREFIX_EXCEPTIONS.contains(a.getKey()))\n             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n \n-        LOGGER.info(\"This is dynamic-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is size of forbidden-exception-configs size {}\", forbiddenExceptionsConfigs.size());\n \n         Map<String, ConfigModel> dynamicConfigsWithExceptions = new HashMap<>();\n \n         dynamicConfigsWithExceptions.putAll(dynamicConfigs);\n         dynamicConfigsWithExceptions.putAll(forbiddenExceptionsConfigs);\n \n-        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigs.size());\n+        LOGGER.info(\"This is dynamic-configs with forbidden-exception-configs size {}\", dynamicConfigsWithExceptions.size());\n \n         dynamicConfigsWithExceptions.forEach((key, value) -> LOGGER.info(key + \" -> \"  + value));\n \n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "4d4a467a0a7baf763708d8169d878ad098a2e410", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex e9b1203f3..200080efd 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -266,7 +281,7 @@ public class KafkaUtils {\n     }\n \n     /**\n-     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * Return dynamic Kafka configs supported by the the given version of Kafka.\n      * @param kafkaVersion specific kafka version\n      * @return all dynamic properties for specific kafka version\n      */\n", "next_change": {"commit": "8bcead0a21c8785e30b1ef36140208fe8379214e", "changed_code": [{"header": "diff --git a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\nindex 200080efd..58057ce27 100644\n--- a/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n+++ b/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java\n", "chunk": "@@ -290,9 +332,9 @@ public class KafkaUtils {\n \n         Map<String, ConfigModel> configs = KafkaUtils.readConfigModel(kafkaVersion);\n \n-        LOGGER.info(\"This is configs {}\", configs.toString());\n+        LOGGER.info(\"Kafka config {}\", configs.toString());\n \n-        LOGGER.info(\"This is all kafka configs with size {}\", configs.size());\n+        LOGGER.info(\"Number of all kafka configs {}\", configs.size());\n \n         Map<String, ConfigModel> dynamicConfigs = configs\n             .entrySet()\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410", "message": "Merge commit", "committedDate": null}, {"oid": "f19cf77ad7949942d8152b5a6390dd4c8d898cc4", "committedDate": "2020-11-11 16:14:22 +0100", "message": "Rework RecoveryST and azp based on it (#3941)"}, {"oid": "042ab82551ca9162e6cb40680fc3be84df5cdedb", "committedDate": "2020-11-12 20:28:28 +0100", "message": "better way how to get version of kafka (#3947)"}, {"oid": "a547519d4eae659c733db9c5875f76093f61d15f", "committedDate": "2020-11-18 16:24:56 +0100", "message": "[systemtest] Test for owner reference of CA secrets (#3954)"}, {"oid": "ca7f7893687336914e4246d55a6e71aa985ef6ce", "committedDate": "2020-12-12 00:42:35 +0100", "message": "[systemtest] Tests for NetworkPolicy enhancements (#4085)"}, {"oid": "d344999808f45e75939a1ab0e9e934a011b0dc4d", "committedDate": "2021-02-10 16:37:52 +0100", "message": "ST: Add new upgrade tests and improve current methods (#4368)"}, {"oid": "96493c56e9e35c24d148b663c13197bca07d7856", "committedDate": "2021-02-25 22:43:13 +0100", "message": "ST: Use cmd client for deploy in upgrade tests (#4453)"}, {"oid": "2903e51d5479a7979a9bf56b80506f654753a4b2", "committedDate": "2021-03-21 10:44:36 +0100", "message": "[MO] - [2nd-3rd step paralelism] -> templates, re-worked resources, re-writed \u2200 tests (#4137)"}, {"oid": "eef3b1c0666ca46fbf2c12b905689bcf14551852", "committedDate": "2021-03-25 22:17:55 +0100", "message": "[systemtest] Make upgrade work with new CRDs (#4608)"}, {"oid": "69e77ce8d5918c25048a253f91f4bca8e89028d9", "committedDate": "2021-04-06 17:18:55 +0200", "message": "ST: Enable loadbalancer tests for aws and cover finalizer testing (#4633)"}, {"oid": "a20035f511845cb88e993d93ebf3c61669b0b263", "committedDate": "2021-04-06 18:58:43 +0200", "message": "Add cold/offline backup script (#4459)"}, {"oid": "83df898d55935e9cd01dba45c48602e1c411675a", "committedDate": "2021-04-15 21:41:37 +0200", "message": "[MO] - [Parallel namespace tests] -> namespace reduction + mirrormaker package + LogSettingsST (#4726)"}, {"oid": "768c042e648e909e4e16fa6f7e036b45b111b24d", "committedDate": "2021-04-16 18:25:54 +0200", "message": "[MO] - [Parallel namespace test] -> KafkaRollerST, AlternativeRecST (#4764)"}, {"oid": "3684cd5345b21842152f66c8a2203b651f8b4bb5", "committedDate": "2021-04-20 17:06:53 +0200", "message": "[MO] - [Parallel namespace test] -> RollingUpdateST (#4768)"}, {"oid": "16f35949c91648ec3ad8f11b0e386e91c28d59eb", "committedDate": "2021-04-24 14:53:16 +0200", "message": "ST: Downgrade Strimzi without upgraded Kafka (#4785)"}, {"oid": "dfda76a1906dec690876fab5e52cf8da1496900a", "committedDate": "2021-04-24 15:19:03 +0200", "message": "[MO] - [Parallel namespace test] -> ListenersST (#4801)"}, {"oid": "bcd88f0fe49f2171316a70a52834f9cc849c6815", "committedDate": "2021-04-29 11:56:50 +0200", "message": "[MO] - [Parallel namespace test] -> SecurityST' (#4845)"}, {"oid": "b5452f45d8ce66ad773d6fa22386c0200c59db4f", "committedDate": "2021-05-06 19:30:50 +0200", "message": "[Issue 4630] Removed non-array listeners support from Cluster Operator (#4908)"}, {"oid": "8bcead0a21c8785e30b1ef36140208fe8379214e", "committedDate": "2021-05-25 15:48:19 +0200", "message": "Various small updates to test log statements (#5008)"}, {"oid": "33da771f49456935ab6f2122695db4f925879c96", "committedDate": "2021-06-25 01:10:24 +0200", "message": "Remove the APIs not supported in v1beta2 (#5175)"}, {"oid": "a89f9b466a79b36d49b6b7fcdd120ad9b1c6cec4", "committedDate": "2021-08-14 15:28:02 +0200", "message": "Removal of dead code in systemtests package (#5280)"}, {"oid": "a7d8249172a2c71be98ce1abc48f910eb1f3ea85", "committedDate": "2021-11-13 23:44:24 +0100", "message": "[systemtest] Remove StatefulSet checks in methods where are not needed (#5840)"}, {"oid": "1e67c880e01dea157376b2bf3a02903b976db3ef", "committedDate": "2021-11-18 09:55:25 +0100", "message": "KMM2 should not be ready when incorrectly configured (#5733)"}, {"oid": "87a7366fb3e2b12fd8e8e583bf9da53fc9ca6e01", "committedDate": "2021-12-22 08:25:56 +0100", "message": "Fix wait util (#6060)"}, {"oid": "199c8d15edfccb3f12894a1459064bf6136da623", "committedDate": "2022-01-12 14:37:35 +0100", "message": "[MO] - \ud83d\udd31 package-wide parallelism \ud83d\udd31 (#6034)"}, {"oid": "d20d0a135182f7f56e485674cfe542858509bcb4", "committedDate": "2022-01-16 14:09:37 +0100", "message": "Update spotbugs and checkstyle (#6165)"}, {"oid": "bc1fb6d1f3ee7bb797e7637a9df177c79c77ebac", "committedDate": "2022-01-25 22:34:20 +0100", "message": "Added the name field and suggestion over the PR (#5777)"}, {"oid": "4f052d4b10b97294ad79b390c19417a75d2fbd31", "committedDate": "2022-03-10 15:43:58 +0100", "message": "rename method, init exchange (#6430)"}, {"oid": "9e4381081621f3a3cf732506939a41b7d44d218d", "committedDate": "2022-05-26 13:50:55 +0200", "message": "ST: Execute system tests with KRaft mode (#6865)"}, {"oid": "24de5b000d167d9c583c31da8f898bf16fffc389", "committedDate": "2022-06-08 10:33:14 +0200", "message": "ST: Enable tests with simple auth and UO (#6883)"}, {"oid": "b522e01ca2d4dbe40f3315507ff4c92b21f975aa", "committedDate": "2022-06-13 09:08:57 +0200", "message": "[systemtest] Use different pod than Kafka for executing all Kafka scripts (#6917)"}, {"oid": "2f5d788038343dd7931d91024bb1ec73ef3405e7", "committedDate": "2022-10-27 23:38:48 +0200", "message": "Cluster-IP listener to expose Kafka through per-broker services (#7365)"}, {"oid": "7e3754ba3fa1cc3a6013b75c858c7daec8ab6fe3", "committedDate": "2022-11-23 14:25:38 +0100", "message": "System test for cluster role split for cluster wide operator with lim\u2026 (#7603)"}, {"oid": "240ce5beba8d862043edc7ab8294c62187fdcbf7", "committedDate": "2022-12-23 18:19:27 +0100", "message": "[ST] Unspecified namespace removal (#7555)"}, {"oid": "303d2a189ddfdf32c892bd430b2e66d7fd82f491", "committedDate": "2023-02-23 09:18:50 +0100", "message": "[systemtest] Fix routes tests in `ListenersST` and add `route` tag (#8138)"}, {"oid": "f1da58ec70bf6bdc5e610f19e863d9327c398bfa", "committedDate": "2023-04-12 16:42:46 +0200", "message": "[systemtest] Remove StatefulSet from tests (#8344)"}]}, {"oid": "76de14021f24172b40ce8bc26d3bceb3babb323d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76de14021f24172b40ce8bc26d3bceb3babb323d", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-07T09:03:22Z", "type": "forcePushed"}, {"oid": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "message": "[MO] - [dyn.conf] -> tests draft\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/959776c5b0016187d4f31d166bdb1aaa6b973c50", "message": "[MO] - [tests] -> additional tests\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "message": "adding pod verification + changing the way how to verify\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "7183c843117f568922ac13319fb0281e40d1aabd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7183c843117f568922ac13319fb0281e40d1aabd", "message": "create shared, iso + separate phases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "faa36204a6d0a281eb1f3e232040f6675fd4d853", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/faa36204a6d0a281eb1f3e232040f6675fd4d853", "message": "some nits\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "e095f29aaafd8abfd9b8a1975033b711292393a3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e095f29aaafd8abfd9b8a1975033b711292393a3", "message": "[MO] -> changes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "fac2acd69f7c72748c8086553260001d86926804", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fac2acd69f7c72748c8086553260001d86926804", "message": "parametrized tst\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "message": "updatee\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "76541b66628223a9dea92fb49d2a35b1b87f1906", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76541b66628223a9dea92fb49d2a35b1b87f1906", "message": "[MO] - adding new profile + tests separation\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "f47afcab2df41630f38fa641f4567d8bda5cb3bc", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f47afcab2df41630f38fa641f4567d8bda5cb3bc", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "message": "RU fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "message": "removing un-used toString()\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "9c8ccb0a22dde9b835f5631e29881736159fc2ba", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9c8ccb0a22dde9b835f5631e29881736159fc2ba", "message": "fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "message": "[MO] - commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "message": "[MO] dyn\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/280900459f501a8cc4e97a9d5a489d268c5ccb0f", "message": "[MO] - fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "f331eb558309ec2340070057f9ab4fd3f8daf250", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f331eb558309ec2340070057f9ab4fd3f8daf250", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "message": "resolve commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "60da26d546e118073bf7edf2d19f6a5ab3761ef8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/60da26d546e118073bf7edf2d19f6a5ab3761ef8", "message": "description\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "message": "space\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7b4f05888d312f2167e5ac74927e73d78665eb1a", "message": "automatical generation of test cases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "964470971f3e228be1dcb17efd8156cc1ef95007", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/964470971f3e228be1dcb17efd8156cc1ef95007", "message": "update fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "message": "test factory removing csv file\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "message": "supress unchecked\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "message": "move\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "222629b3400b246e42bf0627e1091fbea41850fa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/222629b3400b246e42bf0627e1091fbea41850fa", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "0234793179c6624771a00322a0150d7eedb7ac05", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0234793179c6624771a00322a0150d7eedb7ac05", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "7decfc02e8ba8e7917ba90a42841dcda405a7508", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7decfc02e8ba8e7917ba90a42841dcda405a7508", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "message": "dependecies\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "message": "sd\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ff69976bca9ce196e746465f8f444bbb5d584eeb", "message": "fix all\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "message": "dsds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "fe509f09a63587f1103f9d178e25094c00fb47d6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fe509f09a63587f1103f9d178e25094c00fb47d6", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "683cf641b3b84c75858d28950653dcb5c526adc3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/683cf641b3b84c75858d28950653dcb5c526adc3", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "61c88a6c58a3b515eccca7d620aa128f192727d4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/61c88a6c58a3b515eccca7d620aa128f192727d4", "message": "done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "1ff1560d339af68e38b497e676ed7f847bb349b7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1ff1560d339af68e38b497e676ed7f847bb349b7", "message": "ds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "message": "this\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "message": "dpj\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "163425f0d185336b2edefd0b9f3c1e96d613bce8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/163425f0d185336b2edefd0b9f3c1e96d613bce8", "message": "last change\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "237b7528382790286d1251af98c3069a6043a497", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/237b7528382790286d1251af98c3069a6043a497", "message": "Tom commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "f674e4ccfd53635b09216a64c39c276b73d5f714", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f674e4ccfd53635b09216a64c39c276b73d5f714", "message": "check\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T14:34:28Z", "type": "commit"}, {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T14:34:28Z", "type": "forcePushed"}, {"oid": "f2ce488bde5d2749f61f27611840297fef8c6542", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f2ce488bde5d2749f61f27611840297fef8c6542", "message": "last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T17:41:28Z", "type": "commit"}]}