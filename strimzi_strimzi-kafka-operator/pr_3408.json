{"pr_number": 3408, "pr_title": "[MO] - [dyn.conf] -> system tests", "pr_author": "see-quick", "pr_createdAt": "2020-07-29T11:23:16Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570250", "body": "Wouldn't it be better to make some variable for this? Because you use it in `AssertionError` too.", "bodyText": "Wouldn't it be better to make some variable for this? Because you use it in AssertionError too.", "bodyHTML": "<p dir=\"auto\">Wouldn't it be better to make some variable for this? Because you use it in <code>AssertionError</code> too.</p>", "author": "im-konge", "createdAt": "2020-07-29T20:31:37Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgzODg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462838853", "bodyText": "It was not a good approach on how to verify I have changed the it....", "author": "see-quick", "createdAt": "2020-07-30T08:34:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDU4Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570582", "body": "To the variable I mentioned above (just as note)", "bodyText": "To the variable I mentioned above (just as note)", "bodyHTML": "<p dir=\"auto\">To the variable I mentioned above (just as note)</p>", "author": "im-konge", "createdAt": "2020-07-29T20:32:13Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462573746", "body": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\r\n\r\nAnyway the `KafkaUtils` is not needed.", "bodyText": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\nAnyway the KafkaUtils is not needed.", "bodyHTML": "<p dir=\"auto\">Just thinking -&gt; is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.</p>\n<p dir=\"auto\">Anyway the <code>KafkaUtils</code> is not needed.</p>", "author": "im-konge", "createdAt": "2020-07-29T20:38:13Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);", "originalCommit": "95e0f621c7e649132ed6849083368832adb6970a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk5MDAyMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462990021", "bodyText": "I have split these 2...", "author": "see-quick", "createdAt": "2020-07-30T13:19:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NjQ4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463186489", "body": "`KafkaDynamicConfiguration`? typo?", "bodyText": "KafkaDynamicConfiguration? typo?", "bodyHTML": "<p dir=\"auto\"><code>KafkaDynamicConfiguration</code>? typo?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:21:14Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463187718", "body": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "bodyText": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "bodyHTML": "<p dir=\"auto\">What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:23:29Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0MzM4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443388", "bodyText": "I think it's just sample from all configuration options which Kafka offers. but I am not sure.", "author": "Frawless", "createdAt": "2020-07-31T07:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MzI5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464273299", "bodyText": "Yes this is all Kafka configuration, which can by changed without triggering rolling update.", "author": "see-quick", "createdAt": "2020-08-03T08:40:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTU5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189599", "body": "The `unclean.leader.election.enable` should show up here as well. So maybe you can assert it too?", "bodyText": "The unclean.leader.election.enable should show up here as well. So maybe you can assert it too?", "bodyHTML": "<p dir=\"auto\">The <code>unclean.leader.election.enable</code> should show up here as well. So maybe you can assert it too?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:26:46Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189987", "body": "Is this some copy paster left-over? Or what is the value of this?", "bodyText": "Is this some copy paster left-over? Or what is the value of this?", "bodyHTML": "<p dir=\"auto\">Is this some copy paster left-over? Or what is the value of this?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:27:29Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NDEyMg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464274122", "bodyText": "These DynamicConfigurationIsolatedST are the Standa`s tests, which were located in the KafkaST. @stanlyDoge", "author": "see-quick", "createdAt": "2020-08-03T08:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MTQxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463191418", "body": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "bodyText": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "bodyHTML": "<p dir=\"auto\">Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:30:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192324", "body": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "bodyText": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "bodyHTML": "<p dir=\"auto\">A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:31:44Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NzU4NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464277585", "bodyText": "I did not write these tests (just copy-paste) but I will split him if it will be possible.", "author": "see-quick", "createdAt": "2020-08-03T08:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNDAxOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464834018", "bodyText": "The idea behind this test was to perform as many as possible changes in listeners. I agree the test is complicated and the environmental restrictions suck. Splitting test to smaller bits (one listener, one test) seem like a good idea.", "author": "sknot-rh", "createdAt": "2020-08-04T06:45:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjY0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192644", "body": "You seem to be doing rolling updates. Please use persistent cluster.", "bodyText": "You seem to be doing rolling updates. Please use persistent cluster.", "bodyHTML": "<p dir=\"auto\">You seem to be doing rolling updates. Please use persistent cluster.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:32:27Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5Mjk3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192971", "body": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "bodyText": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "bodyHTML": "<p dir=\"auto\">Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:33:11Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463193640", "body": "Are you saying that the change here doesn't trigger RU?", "bodyText": "Are you saying that the change here doesn't trigger RU?", "bodyHTML": "<p dir=\"auto\">Are you saying that the change here doesn't trigger RU?</p>", "author": "scholzj", "createdAt": "2020-07-30T18:34:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk5NjAyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464996028", "bodyText": "This should trigger a rolling update! Using method verifyThatRunningPodsAreStable is not a good practice here (again I did write these tests :D) and I have replaced it with the snapShot waitTillSsHasRolled.", "author": "see-quick", "createdAt": "2020-08-04T11:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463194717", "body": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "bodyText": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "bodyHTML": "<p dir=\"auto\">Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.</p>", "author": "scholzj", "createdAt": "2020-07-30T18:36:26Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MjgzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463452836", "bodyText": "Ye, I suggest to use https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests or https://junit.org/junit5/docs/current/user-guide/#writing-tests-dynamic-tests", "author": "Frawless", "createdAt": "2020-07-31T07:36:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk1MjM2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464952361", "bodyText": "Done", "author": "see-quick", "createdAt": "2020-08-04T10:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443896", "body": "How the output of this log looks? It's formated to be easily readable ?", "bodyText": "How the output of this log looks? It's formated to be easily readable ?", "bodyHTML": "<p dir=\"auto\">How the output of this log looks? It's formated to be easily readable ?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:13:35Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NTI3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464985279", "bodyText": "Yes :)", "author": "see-quick", "createdAt": "2020-08-04T11:34:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444400", "body": "Maybe we should show dyn.configuration in case of error as well?", "bodyText": "Maybe we should show dyn.configuration in case of error as well?", "bodyHTML": "<p dir=\"auto\">Maybe we should show dyn.configuration in case of error as well?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:15:01Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwMTcxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465001711", "bodyText": "?", "author": "see-quick", "createdAt": "2020-08-04T12:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwNDI1Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465004252", "bodyText": "If the error occurs then we show the Kafka Pod {}....", "author": "see-quick", "createdAt": "2020-08-04T12:12:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDY1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444654", "body": "Why 2 brokers instead of 3?", "bodyText": "Why 2 brokers instead of 3?", "bodyHTML": "<p dir=\"auto\">Why 2 brokers instead of 3?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:15:34Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MTQ1MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463451451", "body": "Are you sure we don't have similar test like this? Maybe in `ListenersST` or in `rollingupdate` ?", "bodyText": "Are you sure we don't have similar test like this? Maybe in ListenersST or in rollingupdate ?", "bodyHTML": "<p dir=\"auto\">Are you sure we don't have similar test like this? Maybe in <code>ListenersST</code> or in <code>rollingupdate</code> ?</p>", "author": "Frawless", "createdAt": "2020-07-31T07:32:33Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463453011", "body": "Indent", "bodyText": "Indent", "bodyHTML": "<p dir=\"auto\">Indent</p>", "author": "Frawless", "createdAt": "2020-07-31T07:36:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1534,332 +1524,18 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()", "originalCommit": "d2b6a98e958649fdc656a2032c6c8a42f3923eca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NzQ5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464987490", "bodyText": "Always fun! :D", "author": "see-quick", "createdAt": "2020-08-04T11:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ=="}], "type": "inlineReview"}, {"oid": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-04T11:47:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MTEzNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465271135", "body": "Indent?", "bodyText": "Indent?", "bodyHTML": "<p dir=\"auto\">Indent?</p>", "author": "im-konge", "createdAt": "2020-08-04T19:14:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MjY0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465272645", "body": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "bodyText": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "bodyHTML": "<p dir=\"auto\">I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)</p>", "author": "im-konge", "createdAt": "2020-08-04T19:17:01Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465273689", "body": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like `parametrizedTest`? Maybe @samuel-hawker will be better for this :smile: ", "bodyText": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like parametrizedTest? Maybe @samuel-hawker will be better for this \ud83d\ude04", "bodyHTML": "<p dir=\"auto\">This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like <code>parametrizedTest</code>? Maybe <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/samuel-hawker/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samuel-hawker\">@samuel-hawker</a> will be better for this <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji></p>", "author": "im-konge", "createdAt": "2020-08-04T19:19:07Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+\n+        \"log.flush.interval.ms, \" + 20,\n+\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n+\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n+\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {", "originalCommit": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU4NTkxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465585915", "bodyText": "Already change the naming )", "author": "see-quick", "createdAt": "2020-08-05T09:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338023", "body": "What enum?", "bodyText": "What enum?", "bodyHTML": "<p dir=\"auto\">What enum?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:10:19Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2MTgxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466361819", "bodyText": "old code....i have changed it.", "author": "see-quick", "createdAt": "2020-08-06T12:00:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODkzMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338931", "body": "For all these I would call it `brokerConfigName` or something. You don't care, in these methods whether the particular config supports dynamic update or not. ", "bodyText": "For all these I would call it brokerConfigName or something. You don't care, in these methods whether the particular config supports dynamic update or not.", "bodyHTML": "<p dir=\"auto\">For all these I would call it <code>brokerConfigName</code> or something. You don't care, in these methods whether the particular config supports dynamic update or not.</p>", "author": "tombentley", "createdAt": "2020-08-06T11:12:22Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTI3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339278", "body": "```suggestion\r\n     * Verifies that updated configuration was successfully changed inside Kafka pods\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n          \n          \n            \n                 * Verifies that updated configuration was successfully changed inside Kafka pods", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which, verifying </span>that <span class=\"x x-first x-last\">updating</span> configuration <span class=\"x x-first x-last\">were</span> successfully changed inside <span class=\"pl-smi\">Kafka</span> pods</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Verifies</span><span class=\"x x-last\"> </span>that <span class=\"x x-first x-last\">updated</span> configuration <span class=\"x x-first x-last\">was</span> successfully changed inside <span class=\"pl-smi\">Kafka</span> pods</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-06T11:13:07Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339727", "body": "Is there any reason for doing this via `kafka-configs.sh` and not just using an `Admin` client instance?", "bodyText": "Is there any reason for doing this via kafka-configs.sh and not just using an Admin client instance?", "bodyHTML": "<p dir=\"auto\">Is there any reason for doing this via <code>kafka-configs.sh</code> and not just using an <code>Admin</code> client instance?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:14:12Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2OTEwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466369103", "bodyText": "AdminClient has one disadvantage. If we use him we need som external listener, which is not a good for run the tests in all envinronments.", "author": "see-quick", "createdAt": "2020-08-06T12:15:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466340844", "body": "Why are you calling both like this?", "bodyText": "Why are you calling both like this?", "bodyHTML": "<p dir=\"auto\">Why are you calling both like this?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:16:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NzI0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466367242", "bodyText": "This is just refactored tests, which just update the Dyn. configuration of the broker. The point is to make as many changes as possible. If I understand correctly from @stanlyDoge.", "author": "see-quick", "createdAt": "2020-08-06T12:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NTc5OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466375799", "bodyText": "Yeah, as I understand the test this two lines set the same property (unclean.leader.election.enable) to false and then to true. We are checking at the end of the test, whether kafka pods rolled.\nI would consider changing another property.", "author": "sknot-rh", "createdAt": "2020-08-06T12:28:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466343658", "body": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\r\n\r\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "bodyText": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "bodyHTML": "<p dir=\"auto\">Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?</p>\n<p dir=\"auto\">It's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?</p>", "author": "tombentley", "createdAt": "2020-08-06T11:22:38Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MDgzOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466370839", "bodyText": "It is an almost a complete list of Dyn. configuration. I picked randomly, which were not read-only and also which does not have this\n FORBIDDEN_PREFIXES = \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl., security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer., super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers;", "author": "see-quick", "createdAt": "2020-08-06T12:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MTY4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466371686", "bodyText": "Hmmm, I can create a .csv files, which will bind to specific version :)", "author": "see-quick", "createdAt": "2020-08-06T12:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NzUxMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469367510", "bodyText": "No. You could include columns in the CSV defining the version range between which to test with that config, skipping versions outside that range. But the key point here is that to get complete coverage we need to automate this, which means not relying on an annotation to drive the values, but that generated file I described.", "author": "tombentley", "createdAt": "2020-08-12T15:56:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466346162", "body": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "bodyText": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "bodyHTML": "<p dir=\"auto\">Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:28:05Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MjY2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466372661", "bodyText": "Everywhere, where the broker will not be triggered by change of dyn. configuration we should tag as DYNAMIC_CONFIGURATION", "author": "see-quick", "createdAt": "2020-08-06T12:22:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466348498", "body": "Should we add a parameter `String dynConfProperty` to make this more general?", "bodyText": "Should we add a parameter String dynConfProperty to make this more general?", "bodyHTML": "<p dir=\"auto\">Should we add a parameter <code>String dynConfProperty</code> to make this more general?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:32:53Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NDQ3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466374475", "bodyText": "Currently, I am using only the one specific property. So it would be useless from my POV to extend it for now.", "author": "see-quick", "createdAt": "2020-08-06T12:26:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466350403", "body": "Just a nit. Should this be `execute phase`?", "bodyText": "Just a nit. Should this be execute phase?", "bodyHTML": "<p dir=\"auto\">Just a nit. Should this be <code>execute phase</code>?</p>", "author": "sknot-rh", "createdAt": "2020-08-06T11:36:59Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "originalCommit": "64d1ac624b2f41b7401f52d4bd2054a8dc893294", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3ODI2Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466378266", "bodyText": "No, it is exercise phase -> https://thoughtbot.com/blog/four-phase-test. I am changing the state of the SUT.", "author": "see-quick", "createdAt": "2020-08-06T12:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4MDU3MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466380571", "bodyText": "Ah, ok. Thanks.", "author": "sknot-rh", "createdAt": "2020-08-06T12:37:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQzMDk4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466430988", "body": "I think the indent is still same here.", "bodyText": "I think the indent is still same here.", "bodyHTML": "<p dir=\"auto\">I think the indent is still same here.</p>", "author": "im-konge", "createdAt": "2020-08-06T13:56:14Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639748", "body": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "bodyText": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "bodyHTML": "<p dir=\"auto\">I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:32:57Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731076", "bodyText": "Yes I will use TestKafkaVersion class to do it dynamically. :) Thanks", "author": "see-quick", "createdAt": "2020-08-10T07:33:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639927", "body": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "bodyText": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "bodyHTML": "<p dir=\"auto\">I'm not sure I understand why are we asserting these when we don't configure them anywhere.</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:19Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA5Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731092", "bodyText": "If I understand you correctly...but I configure it in the @BeforeEach phase.", "author": "see-quick", "createdAt": "2020-08-10T07:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640104", "body": "Why should this be `\"true\"`?", "bodyText": "Why should this be \"true\"?", "bodyHTML": "<p dir=\"auto\">Why should this be <code>\"true\"</code>?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:41Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg1Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841853", "bodyText": "This method (as you pointed) is setting unclean.leader.election.enable, which is dynamically changeable option. Default value is false so it is just changing its value.", "author": "sknot-rh", "createdAt": "2020-08-07T06:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkzMzczNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466933737", "bodyText": "I think the code should be improved to make it easier to understand and read even without understanding some Kafka defaults.", "author": "scholzj", "createdAt": "2020-08-07T09:37:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640208", "body": "Why should this be `\"true\"`?", "bodyText": "Why should this be \"true\"?", "bodyHTML": "<p dir=\"auto\">Why should this be <code>\"true\"</code>?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:33:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841874", "bodyText": "And after the value is changed, it is verified whether it actually changed.", "author": "sknot-rh", "createdAt": "2020-08-07T06:14:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641159", "body": "This has generic name, but it is not generic method. It should ba then maybe named `updateAndVerifyDynUncleanLeaderElectionConf`", "bodyText": "This has generic name, but it is not generic method. It should ba then maybe named updateAndVerifyDynUncleanLeaderElectionConf", "bodyHTML": "<p dir=\"auto\">This has generic name, but it is not generic method. It should ba then maybe named <code>updateAndVerifyDynUncleanLeaderElectionConf</code></p>", "author": "scholzj", "createdAt": "2020-08-06T19:35:30Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDI2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744264", "bodyText": "I have refactored that method to be more generic :)", "author": "see-quick", "createdAt": "2020-08-10T08:04:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTY3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641675", "body": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass `kafkaConfig` as parameter?", "bodyText": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass kafkaConfig as parameter?", "bodyHTML": "<p dir=\"auto\">I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass <code>kafkaConfig</code> as parameter?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:36:24Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        // change dynamically changeable option\n+        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NDUxNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466644517", "body": "Can we add some Javadoc explaining what this does?", "bodyText": "Can we add some Javadoc explaining what this does?", "bodyHTML": "<p dir=\"auto\">Can we add some Javadoc explaining what this does?</p>", "author": "scholzj", "createdAt": "2020-08-06T19:42:06Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466645329", "body": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "bodyText": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "bodyHTML": "<p dir=\"auto\">Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.</p>", "author": "scholzj", "createdAt": "2020-08-06T19:43:32Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "originalCommit": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MjU5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466842598", "bodyText": "It is copy-paste from test I wrote. At the time we had dynamic updates of kafka configuration only, we wanted to be sure the changes of logging will trigger RU. For the reasons you mentioned, it should be deleted in this PR or in my PR after rebase.", "author": "sknot-rh", "createdAt": "2020-08-07T06:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDQxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744413", "bodyText": "Removed :)", "author": "see-quick", "createdAt": "2020-08-10T08:04:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}], "type": "inlineReview"}, {"oid": "7517de0b3496641bd930171d41daeccd54ff86ce", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7517de0b3496641bd930171d41daeccd54ff86ce", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-10T08:06:07Z", "type": "forcePushed"}, {"oid": "22a5375f16130e257b4e13364bfd459227de4a7e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/22a5375f16130e257b4e13364bfd459227de4a7e", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-10T14:15:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NDg0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469364840", "body": "What does this tell you that the result of the update to the `Kafka` CR does not? ", "bodyText": "What does this tell you that the result of the update to the Kafka CR does not?", "bodyHTML": "<p dir=\"auto\">What does this tell you that the result of the update to the <code>Kafka</code> CR does not?</p>", "author": "tombentley", "createdAt": "2020-08-12T15:52:49Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +156,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {", "originalCommit": "defb26bb06660cf9d8eb8f07c9b899cb920b4796", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MDU3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471580579", "body": "I think it should be in kafka directory in systemtest package", "bodyText": "I think it should be in kafka directory in systemtest package", "bodyHTML": "<p dir=\"auto\">I think it should be in kafka directory in systemtest package</p>", "author": "Frawless", "createdAt": "2020-08-17T16:01:52Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "originalCommit": "347dd238537142b016a9fa33e8fe7c077da7583c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MTUzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471581538", "body": "It should be in kafka directory of systemtest I think.", "bodyText": "It should be in kafka directory of systemtest I think.", "bodyHTML": "<p dir=\"auto\">It should be in kafka directory of systemtest I think.</p>", "author": "Frawless", "createdAt": "2020-08-17T16:03:23Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "originalCommit": "347dd238537142b016a9fa33e8fe7c077da7583c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/58b10ba7d48706f744cd81e4924a02eea22d660b", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-17T18:38:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472085511", "body": "Shouldn't be this annotation from spotbugs?", "bodyText": "Shouldn't be this annotation from spotbugs?", "bodyHTML": "<p dir=\"auto\">Shouldn't be this annotation from spotbugs?</p>", "author": "Frawless", "createdAt": "2020-08-18T10:44:28Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -4,6 +4,8 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;", "originalCommit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQzMjUzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476432536", "bodyText": "AFAIK everywhere we use SuppressFBWarnings. I know that FB were deprecated and replaced by SpotBugs but  from code a i cannot find any SuppressSBWarnings or something similar like this.", "author": "see-quick", "createdAt": "2020-08-25T13:05:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNzQxNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472107416", "body": "Maybe we should use prefixes from `FORBIDDEN_PREFIXES` ?", "bodyText": "Maybe we should use prefixes from FORBIDDEN_PREFIXES ?", "bodyHTML": "<p dir=\"auto\">Maybe we should use prefixes from <code>FORBIDDEN_PREFIXES</code> ?</p>", "author": "Frawless", "createdAt": "2020-08-18T11:29:10Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,", "originalCommit": "58b10ba7d48706f744cd81e4924a02eea22d660b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-18T15:03:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ1NTcyNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472455724", "body": "I think that @im-konge is in other PR adding the tag `ROLLING_UPDATE`. I think we should use it here as well because these are closely related.", "bodyText": "I think that @im-konge is in other PR adding the tag ROLLING_UPDATE. I think we should use it here as well because these are closely related.", "bodyHTML": "<p dir=\"auto\">I think that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/im-konge/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/im-konge\">@im-konge</a> is in other PR adding the tag <code>ROLLING_UPDATE</code>. I think we should use it here as well because these are closely related.</p>", "author": "scholzj", "createdAt": "2020-08-18T20:11:50Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4MTA0Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472481042", "body": "How does this deal with the exceptions to the forbidden prefixes?", "bodyText": "How does this deal with the exceptions to the forbidden prefixes?", "bodyHTML": "<p dir=\"auto\">How does this deal with the exceptions to the forbidden prefixes?</p>", "author": "scholzj", "createdAt": "2020-08-18T20:42:44Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMTI5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474731290", "body": "I think you mean `@link`.", "bodyText": "I think you mean @link.", "bodyHTML": "<p dir=\"auto\">I think you mean <code>@link</code>.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:23:51Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMzczOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474733739", "body": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\r\n\r\n```suggestion\r\n     * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.\r\n```", "bodyText": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n          \n          \n            \n                 * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.", "bodyHTML": "<p dir=\"auto\">You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which load all </span>supported <span class=\"x x-first x-last\">kafka configuration </span>generated by #<span class=\"pl-smi\">KafkaConfigModelGenerator</span> in config<span class=\"pl-k\">-</span>model<span class=\"pl-k\">-</span>generator</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Loads</span><span class=\"x x-last\"> all kafka config parameters </span>supported <span class=\"x x-first\">by the given {</span><span class=\"pl-k x\">@code</span><span class=\"x x-last\"> kafkaVersion}, as </span>generated by #<span class=\"pl-smi\">KafkaConfigModelGenerator</span> in config<span class=\"pl-k\">-</span>model<span class=\"pl-k\">-</span>generator<span class=\"x x-first x-last\">.</span></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-21T14:27:58Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczNDA3NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474734075", "body": "rethrow a `RuntimeException`", "bodyText": "rethrow a RuntimeException", "bodyHTML": "<p dir=\"auto\">rethrow a <code>RuntimeException</code></p>", "author": "tombentley", "createdAt": "2020-08-21T14:28:33Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474738256", "body": "You should be able to use the `config-model` module. This is what `KafkaConfiguration` does:\r\n\r\n```java\r\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\r\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\r\n        try {\r\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\r\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\r\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\r\n                    throw new RuntimeException(\"Incorrect version\");\r\n                }\r\n                return configModels.getConfigs();\r\n            }\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\r\n        }\r\n    }\r\n```\r\n\r\nThat will give you nicely typed `ConfigModel` to deal with, rather than `JsonObject`. Please try to refactor `KafkaConfiguration` to use a common implementation rather than copying the above code.", "bodyText": "You should be able to use the config-model module. This is what KafkaConfiguration does:\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(\"Incorrect version\");\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n        }\n    }\nThat will give you nicely typed ConfigModel to deal with, rather than JsonObject. Please try to refactor KafkaConfiguration to use a common implementation rather than copying the above code.", "bodyHTML": "<p dir=\"auto\">You should be able to use the <code>config-model</code> module. This is what <code>KafkaConfiguration</code> does:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"private Map&lt;String, ConfigModel&gt; readConfigModel(KafkaVersion kafkaVersion) {\n        String name = &quot;/kafka-&quot; + kafkaVersion.version() + &quot;-config-model.json&quot;;\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(&quot;Incorrect version&quot;);\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(&quot;Error reading from classpath resource &quot; + name, e);\n        }\n    }\"><pre><span class=\"pl-k\">private</span> <span class=\"pl-k\">Map&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">ConfigModel</span>&gt;</span> readConfigModel(<span class=\"pl-smi\">KafkaVersion</span> kafkaVersion) {\n        <span class=\"pl-smi\">String</span> name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/kafka-<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> kafkaVersion<span class=\"pl-k\">.</span>version() <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-config-model.json<span class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">try</span> {\n            <span class=\"pl-k\">try</span> (<span class=\"pl-smi\">InputStream</span> in <span class=\"pl-k\">=</span> <span class=\"pl-smi\">KafkaConfiguration</span><span class=\"pl-k\">.</span>class<span class=\"pl-k\">.</span>getResourceAsStream(name)) {\n                <span class=\"pl-smi\">ConfigModels</span> configModels <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">ObjectMapper</span>()<span class=\"pl-k\">.</span>readValue(in, <span class=\"pl-smi\">ConfigModels</span><span class=\"pl-k\">.</span>class);\n                <span class=\"pl-k\">if</span> (<span class=\"pl-k\">!</span>kafkaVersion<span class=\"pl-k\">.</span>version()<span class=\"pl-k\">.</span>equals(configModels<span class=\"pl-k\">.</span>getVersion())) {\n                    <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">RuntimeException</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Incorrect version<span class=\"pl-pds\">\"</span></span>);\n                }\n                <span class=\"pl-k\">return</span> configModels<span class=\"pl-k\">.</span>getConfigs();\n            }\n        } <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">IOException</span> e) {\n            <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">RuntimeException</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Error reading from classpath resource <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> name, e);\n        }\n    }</pre></div>\n<p dir=\"auto\">That will give you nicely typed <code>ConfigModel</code> to deal with, rather than <code>JsonObject</code>. Please try to refactor <code>KafkaConfiguration</code> to use a common implementation rather than copying the above code.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:35:04Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ4ODM5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476488398", "bodyText": "Done :)", "author": "see-quick", "createdAt": "2020-08-25T14:22:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474740874", "body": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\r\n\r\n```\r\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")\r\n```", "bodyText": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")", "bodyHTML": "<p dir=\"auto\">Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"cmdKubeClient().execKafkaConfigsInPod(podName, &quot;--entity-type brokers --entity-name 0 --describe&quot;)\"><pre><code>cmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")\n</code></pre></div>", "author": "tombentley", "createdAt": "2020-08-21T14:39:34Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc2Njk4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474766980", "bodyText": "Yeah, will do in another PR and also create an issue for that.", "author": "see-quick", "createdAt": "2020-08-21T15:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MzQ5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474743497", "body": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about _withExternalListeners_, it's that changing an external listeners config causes a RU. So the name would be better as `testUpdateToExternalListenerCausesRollingRestart`", "bodyText": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about withExternalListeners, it's that changing an external listeners config causes a RU. So the name would be better as testUpdateToExternalListenerCausesRollingRestart", "bodyHTML": "<p dir=\"auto\">I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about <em>withExternalListeners</em>, it's that changing an external listeners config causes a RU. So the name would be better as <code>testUpdateToExternalListenerCausesRollingRestart</code></p>", "author": "tombentley", "createdAt": "2020-08-21T14:44:04Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0NDA1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474744054", "body": "Same comment about javadoc or method name.", "bodyText": "Same comment about javadoc or method name.", "bodyHTML": "<p dir=\"auto\">Same comment about javadoc or method name.</p>", "author": "tombentley", "createdAt": "2020-08-21T14:44:57Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "originalCommit": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-21T18:19:14Z", "type": "forcePushed"}, {"oid": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-08-26T13:48:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3Njg1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r478376856", "body": "```suggestion\r\n     * Return dynamic Kafka configs supported by the the given version of Kafka.\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which process all supported configs by Kafka and filter all which are not dynamic\n          \n          \n            \n                 * Return dynamic Kafka configs supported by the the given version of Kafka.", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Method</span><span class=\"x x-last\">, which process all</span> supported <span class=\"x x-first x-last\">configs </span>by <span class=\"pl-smi x x-first\">Kafka</span><span class=\"x x-last\"> and filter all which are not dynamic</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi x x-first\">Return</span><span class=\"x\"> dynamic </span><span class=\"pl-smi x\">Kafka</span><span class=\"x x-last\"> configs</span> supported by <span class=\"x x-first\">the the given version of </span><span class=\"pl-smi x\">Kafka</span><span class=\"x x-last\">.</span></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "tombentley", "createdAt": "2020-08-27T12:21:58Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -157,4 +170,152 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic", "originalCommit": "308c3ecdb4ea1d3e0389c44e4da596f89bf08acd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "76de14021f24172b40ce8bc26d3bceb3babb323d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76de14021f24172b40ce8bc26d3bceb3babb323d", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-07T09:03:22Z", "type": "forcePushed"}, {"oid": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "message": "[MO] - [dyn.conf] -> tests draft\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/959776c5b0016187d4f31d166bdb1aaa6b973c50", "message": "[MO] - [tests] -> additional tests\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "message": "adding pod verification + changing the way how to verify\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:55:59Z", "type": "commit"}, {"oid": "7183c843117f568922ac13319fb0281e40d1aabd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7183c843117f568922ac13319fb0281e40d1aabd", "message": "create shared, iso + separate phases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "faa36204a6d0a281eb1f3e232040f6675fd4d853", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/faa36204a6d0a281eb1f3e232040f6675fd4d853", "message": "some nits\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "e095f29aaafd8abfd9b8a1975033b711292393a3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e095f29aaafd8abfd9b8a1975033b711292393a3", "message": "[MO] -> changes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "fac2acd69f7c72748c8086553260001d86926804", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fac2acd69f7c72748c8086553260001d86926804", "message": "parametrized tst\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "message": "updatee\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "76541b66628223a9dea92fb49d2a35b1b87f1906", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76541b66628223a9dea92fb49d2a35b1b87f1906", "message": "[MO] - adding new profile + tests separation\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "f47afcab2df41630f38fa641f4567d8bda5cb3bc", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f47afcab2df41630f38fa641f4567d8bda5cb3bc", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "message": "RU fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "message": "removing un-used toString()\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "9c8ccb0a22dde9b835f5631e29881736159fc2ba", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9c8ccb0a22dde9b835f5631e29881736159fc2ba", "message": "fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "message": "[MO] - commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "message": "[MO] dyn\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/280900459f501a8cc4e97a9d5a489d268c5ccb0f", "message": "[MO] - fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "f331eb558309ec2340070057f9ab4fd3f8daf250", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f331eb558309ec2340070057f9ab4fd3f8daf250", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "message": "resolve commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "60da26d546e118073bf7edf2d19f6a5ab3761ef8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/60da26d546e118073bf7edf2d19f6a5ab3761ef8", "message": "description\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "message": "space\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7b4f05888d312f2167e5ac74927e73d78665eb1a", "message": "automatical generation of test cases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "964470971f3e228be1dcb17efd8156cc1ef95007", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/964470971f3e228be1dcb17efd8156cc1ef95007", "message": "update fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:39Z", "type": "commit"}, {"oid": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "message": "test factory removing csv file\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "message": "supress unchecked\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "message": "move\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "222629b3400b246e42bf0627e1091fbea41850fa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/222629b3400b246e42bf0627e1091fbea41850fa", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "0234793179c6624771a00322a0150d7eedb7ac05", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0234793179c6624771a00322a0150d7eedb7ac05", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "7decfc02e8ba8e7917ba90a42841dcda405a7508", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7decfc02e8ba8e7917ba90a42841dcda405a7508", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "message": "dependecies\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "message": "sd\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ff69976bca9ce196e746465f8f444bbb5d584eeb", "message": "fix all\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "message": "dsds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "fe509f09a63587f1103f9d178e25094c00fb47d6", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fe509f09a63587f1103f9d178e25094c00fb47d6", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "683cf641b3b84c75858d28950653dcb5c526adc3", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/683cf641b3b84c75858d28950653dcb5c526adc3", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "61c88a6c58a3b515eccca7d620aa128f192727d4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/61c88a6c58a3b515eccca7d620aa128f192727d4", "message": "done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "1ff1560d339af68e38b497e676ed7f847bb349b7", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1ff1560d339af68e38b497e676ed7f847bb349b7", "message": "ds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "message": "this\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "message": "dpj\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "163425f0d185336b2edefd0b9f3c1e96d613bce8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/163425f0d185336b2edefd0b9f3c1e96d613bce8", "message": "last change\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "237b7528382790286d1251af98c3069a6043a497", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/237b7528382790286d1251af98c3069a6043a497", "message": "Tom commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "f674e4ccfd53635b09216a64c39c276b73d5f714", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f674e4ccfd53635b09216a64c39c276b73d5f714", "message": "check\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T10:56:40Z", "type": "commit"}, {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T14:34:28Z", "type": "commit"}, {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T14:34:28Z", "type": "forcePushed"}, {"oid": "f2ce488bde5d2749f61f27611840297fef8c6542", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f2ce488bde5d2749f61f27611840297fef8c6542", "message": "last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-09-08T17:41:28Z", "type": "commit"}]}