{"pr_number": 1510, "pr_title": "Kafka support", "pr_author": "jbescos", "pr_createdAt": "2020-03-13T12:09:39Z", "pr_url": "https://github.com/oracle/helidon/pull/1510", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392898324", "body": "Helidon has a flat package structure - each module can only use one package (and `spi` if needed). ", "bodyText": "Helidon has a flat package structure - each module can only use one package (and spi if needed).", "bodyHTML": "<p dir=\"auto\">Helidon has a flat package structure - each module can only use one package (and <code>spi</code> if needed).</p>", "author": "tomas-langer", "createdAt": "2020-03-16T09:56:40Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzE5Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457197", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-18T15:57:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392916808", "body": "This method should be private (or package local)", "bodyText": "This method should be private (or package local)", "bodyHTML": "<p dir=\"auto\">This method should be private (or package local)</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:30:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MDU3Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397780576", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392918072", "body": "public method with no javadoc.\r\nAlso Helidon uses getters without `get` verb, so the method should be called `consumers`.\r\nI am not sure it should be public at all. Please minimize number of public methods that are not implementing interface methods.", "bodyText": "public method with no javadoc.\nAlso Helidon uses getters without get verb, so the method should be called consumers.\nI am not sure it should be public at all. Please minimize number of public methods that are not implementing interface methods.", "bodyHTML": "<p dir=\"auto\">public method with no javadoc.<br>\nAlso Helidon uses getters without <code>get</code> verb, so the method should be called <code>consumers</code>.<br>\nI am not sure it should be public at all. Please minimize number of public methods that are not implementing interface methods.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:31:56Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTAxMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781011", "bodyText": "It is for testing purposes, to check that all resources are closed. Not it has package visibility and other name without get.", "author": "jbescos", "createdAt": "2020-03-25T11:23:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392919773", "body": "You are creating a new thread pool for each publisher. That is probably not intended.", "bodyText": "You are creating a new thread pool for each publisher. That is probably not intended.", "bodyHTML": "<p dir=\"auto\">You are creating a new thread pool for each publisher. That is probably not intended.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:33:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTM3Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781377", "bodyText": "Right, now it reuses a scheduler thread pool", "author": "jbescos", "createdAt": "2020-03-25T11:23:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392920553", "body": "In this case you do not close the producer.", "bodyText": "In this case you do not close the producer.", "bodyHTML": "<p dir=\"auto\">In this case you do not close the producer.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:34:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                simpleKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {\n+                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTU0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781549", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:24:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921003", "body": "No backpressure support may cause issues in reactive environment.", "bodyText": "No backpressure support may cause issues in reactive environment.", "bodyHTML": "<p dir=\"auto\">No backpressure support may cause issues in reactive environment.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:35:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781784", "bodyText": "It is now configured a configured parameter.", "author": "jbescos", "createdAt": "2020-03-25T11:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n", "chunk": "@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921592", "body": "This class should not be public maybe?", "bodyText": "This class should not be public maybe?", "bodyHTML": "<p dir=\"auto\">This class should not be public maybe?</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:36:01Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjEyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782120", "bodyText": "It has packaged visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -28,8 +29,9 @@ import org.eclipse.microprofile.reactive.messaging.Message;\n  * @param <K> kafka record key type\n  * @param <V> kafka record value type\n  */\n-public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n \n+    private static final Logger LOGGER = Logger.getLogger(KafkaMessage.class.getName());\n     private ConsumerRecord<K, V> consumerRecord;\n     private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392930784", "body": "Getter should not use `get`, also I guess this should not be public.", "bodyText": "Getter should not use get, also I guess this should not be public.", "bodyHTML": "<p dir=\"auto\">Getter should not use <code>get</code>, also I guess this should not be public.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:47:09Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjI3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782270", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -47,7 +49,7 @@ public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n         return consumerRecord;\n     }\n \n-    public CompletableFuture<Void> getAckFuture() {\n+    CompletableFuture<Void> ackFuture() {\n         return ackFuture;\n     }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931269", "body": "Constructors may never be public in Helidon, unless required by CDI or JAX-RS.\r\nWe use factory methods (if need to be public).\r\n", "bodyText": "Constructors may never be public in Helidon, unless required by CDI or JAX-RS.\nWe use factory methods (if need to be public).", "bodyHTML": "<p dir=\"auto\">Constructors may never be public in Helidon, unless required by CDI or JAX-RS.<br>\nWe use factory methods (if need to be public).</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:03Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MzAyMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397783021", "bodyText": "It has package visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -38,7 +40,7 @@ public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n      *\n      * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n      */\n-    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+    KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n         this.consumerRecord = consumerRecord;\n     }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931585", "body": "Is there no support for ack in Kafka itself? This basically makes the `ack` method a no-op.", "bodyText": "Is there no support for ack in Kafka itself? This basically makes the ack method a no-op.", "bodyHTML": "<p dir=\"auto\">Is there no support for ack in Kafka itself? This basically makes the <code>ack</code> method a no-op.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:36Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {\n+        return ackFuture;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ackFuture.complete(null);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTU4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791588", "bodyText": "I spoke with @danielkec about this. The thing is that polling from Kafka is a blocking operation, and in reactive streams we cannot have threads blocked. So we need some way to make it in non-blocking way.\nThe workaround to deal with this is the BackPressureLayer. There we are buffering events coming from polling. That KafkaMessage.ack() is communication with BackPressureLayer, instead of doing it with Kafka.", "author": "jbescos", "createdAt": "2020-03-25T11:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -47,7 +49,7 @@ public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n         return consumerRecord;\n     }\n \n-    public CompletableFuture<Void> getAckFuture() {\n+    CompletableFuture<Void> ackFuture() {\n         return ackFuture;\n     }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392932046", "body": "The name is not good. ", "bodyText": "The name is not good.", "bodyHTML": "<p dir=\"auto\">The name is not good.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:49:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934566", "bodyText": "Also do you expect users to use this class? If not, it must not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTg3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791871", "bodyText": "It is not public and it is renamed.", "author": "jbescos", "createdAt": "2020-03-25T11:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nsimilarity index 81%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nindex a4f89b8d0..1b0d30245 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\n", "chunk": "@@ -27,7 +26,7 @@ import org.reactivestreams.Subscriber;\n  * @param <K> kafka record key type\n  * @param <V> kafka record value type\n  */\n-public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n+class BasicPublisher<K, V> extends EmittingPublisher<KafkaMessage<K, V>> {\n \n     private Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher;\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934828", "body": "Public constructor cannot be used. Also if class is not to be public, this method will not be public.", "bodyText": "Public constructor cannot be used. Also if class is not to be public, this method will not be public.", "bodyHTML": "<p dir=\"auto\">Public constructor cannot be used. Also if class is not to be public, this method will not be public.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:33Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n+\n+    private Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher;\n+\n+    /**\n+     * Create new Reactive Streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+     *\n+     * @param publisher {@link java.util.function.Consumer}\n+     */\n+    public SimplePublisher(Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MjEzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397792139", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nsimilarity index 81%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nindex a4f89b8d0..1b0d30245 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\n", "chunk": "@@ -36,12 +35,13 @@ public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n      *\n      * @param publisher {@link java.util.function.Consumer}\n      */\n-    public SimplePublisher(Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher) {\n+    BasicPublisher(Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher) {\n         this.publisher = publisher;\n     }\n \n     @Override\n-    public void subscribe(Subscriber<? super KafkaMessage<K, V>> s) {\n-        publisher.accept(s);\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        super.subscribe(subscriber);\n+        publisher.accept(subscriber);\n     }\n }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936058", "body": "I am not sure why this class implements Properties - is this used to send to Kafka itself?\r\nIf not, this class should not implement `Properties`. \r\nI am not sure if the properties are \"free\" - if so, use an internal `Map<String, String>` to store them.\r\nIf not free, use explicit fields to store such configuration.\r\n", "bodyText": "I am not sure why this class implements Properties - is this used to send to Kafka itself?\nIf not, this class should not implement Properties.\nI am not sure if the properties are \"free\" - if so, use an internal Map<String, String> to store them.\nIf not free, use explicit fields to store such configuration.", "bodyHTML": "<p dir=\"auto\">I am not sure why this class implements Properties - is this used to send to Kafka itself?<br>\nIf not, this class should not implement <code>Properties</code>.<br>\nI am not sure if the properties are \"free\" - if so, use an internal <code>Map&lt;String, String&gt;</code> to store them.<br>\nIf not free, use explicit fields to store such configuration.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:56:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjQyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936422", "bodyText": "Do not use get verb in getters.", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzM1MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937350", "bodyText": "Helidon classes (except for very specific cases) must be immutable. Properties is not immutable.", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTMwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809301", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T12:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n", "chunk": "@@ -51,41 +52,44 @@ import io.helidon.config.Config;\n  *\n  * @see io.helidon.config.Config\n  */\n-class KafkaConfigProperties extends Properties {\n+class HelidonToKafkaConfigParser {\n \n     /**\n      * Topic or topics delimited by commas.\n      */\n-    static final String TOPIC_NAME = \"topic\";\n+    private static final String TOPIC_NAME = \"topic\";\n \n-    /**\n-     * Consumer group id.\n-     */\n-    static final String GROUP_ID = \"group.id\";\n+    private HelidonToKafkaConfigParser() {\n+    }\n \n     /**\n-     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n-     * underscores in keys are translated to dots.\n-     *\n-     * @param config parent config of kafka key\n+     * Builds the Kafka properties from Helidon Config.\n+     * @param config\n+     * @return the Kafka properties\n      */\n-    KafkaConfigProperties(Config config) {\n-        config.asNodeList().get().forEach(this::addProperty);\n+    static Properties toProperties(Config config) {\n+        Properties prop = new Properties();\n+        config.asNodeList().get().forEach(entry -> addProperty(entry, prop));\n+        return prop;\n     }\n \n     /**\n      * Split comma separated topic names.\n-     *\n+     * @param kafkaProperties\n      * @return list of topic names\n      */\n-    public List<String> getTopicNameList() {\n-        return Arrays.stream(getProperty(TOPIC_NAME)\n-                .split(\",\"))\n-                .map(String::trim)\n-                .collect(Collectors.toList());\n+    static List<String> topicNameList(Properties kafkaProperties) {\n+        String topics = kafkaProperties.getProperty(TOPIC_NAME);\n+        if (topics != null) {\n+            return Arrays.stream(topics.split(\",\"))\n+                    .map(String::trim)\n+                    .collect(Collectors.toList());\n+        } else {\n+            return Collections.emptyList();\n+        }\n     }\n \n-    private void addProperty(Config c) {\n+    private static void addProperty(Config c, Properties prop) {\n         String key = c.traverse().map(m -> m.key().parent().name() + \".\" + m.key().name())\n                 .collect(Collectors.joining(\".\"));\n         if (key.isEmpty()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936328", "body": "This should happen at the time this instance is created and stored in a field.", "bodyText": "This should happen at the time this instance is created and stored in a field.", "bodyHTML": "<p dir=\"auto\">This should happen at the time this instance is created and stored in a field.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:30Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTc1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809754", "bodyText": "This class is not instanced anymore. It contains some utility static methods.", "author": "jbescos", "createdAt": "2020-03-25T12:17:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n", "chunk": "@@ -51,41 +52,44 @@ import io.helidon.config.Config;\n  *\n  * @see io.helidon.config.Config\n  */\n-class KafkaConfigProperties extends Properties {\n+class HelidonToKafkaConfigParser {\n \n     /**\n      * Topic or topics delimited by commas.\n      */\n-    static final String TOPIC_NAME = \"topic\";\n+    private static final String TOPIC_NAME = \"topic\";\n \n-    /**\n-     * Consumer group id.\n-     */\n-    static final String GROUP_ID = \"group.id\";\n+    private HelidonToKafkaConfigParser() {\n+    }\n \n     /**\n-     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n-     * underscores in keys are translated to dots.\n-     *\n-     * @param config parent config of kafka key\n+     * Builds the Kafka properties from Helidon Config.\n+     * @param config\n+     * @return the Kafka properties\n      */\n-    KafkaConfigProperties(Config config) {\n-        config.asNodeList().get().forEach(this::addProperty);\n+    static Properties toProperties(Config config) {\n+        Properties prop = new Properties();\n+        config.asNodeList().get().forEach(entry -> addProperty(entry, prop));\n+        return prop;\n     }\n \n     /**\n      * Split comma separated topic names.\n-     *\n+     * @param kafkaProperties\n      * @return list of topic names\n      */\n-    public List<String> getTopicNameList() {\n-        return Arrays.stream(getProperty(TOPIC_NAME)\n-                .split(\",\"))\n-                .map(String::trim)\n-                .collect(Collectors.toList());\n+    static List<String> topicNameList(Properties kafkaProperties) {\n+        String topics = kafkaProperties.getProperty(TOPIC_NAME);\n+        if (topics != null) {\n+            return Arrays.stream(topics.split(\",\"))\n+                    .map(String::trim)\n+                    .collect(Collectors.toList());\n+        } else {\n+            return Collections.emptyList();\n+        }\n     }\n \n-    private void addProperty(Config c) {\n+    private static void addProperty(Config c, Properties prop) {\n         String key = c.traverse().map(m -> m.key().parent().name() + \".\" + m.key().name())\n                 .collect(Collectors.joining(\".\"));\n         if (key.isEmpty()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937012", "body": "If you want to get everything into a map, just use `config.detach().asMap().ifPresent(map -> ...)`", "bodyText": "If you want to get everything into a map, just use config.detach().asMap().ifPresent(map -> ...)", "bodyHTML": "<p dir=\"auto\">If you want to get everything into a map, just use <code>config.detach().asMap().ifPresent(map -&gt; ...)</code></p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:58:46Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTczOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875739", "bodyText": "Great, thanks", "author": "jbescos", "createdAt": "2020-03-25T14:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n", "chunk": "@@ -51,41 +52,44 @@ import io.helidon.config.Config;\n  *\n  * @see io.helidon.config.Config\n  */\n-class KafkaConfigProperties extends Properties {\n+class HelidonToKafkaConfigParser {\n \n     /**\n      * Topic or topics delimited by commas.\n      */\n-    static final String TOPIC_NAME = \"topic\";\n+    private static final String TOPIC_NAME = \"topic\";\n \n-    /**\n-     * Consumer group id.\n-     */\n-    static final String GROUP_ID = \"group.id\";\n+    private HelidonToKafkaConfigParser() {\n+    }\n \n     /**\n-     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n-     * underscores in keys are translated to dots.\n-     *\n-     * @param config parent config of kafka key\n+     * Builds the Kafka properties from Helidon Config.\n+     * @param config\n+     * @return the Kafka properties\n      */\n-    KafkaConfigProperties(Config config) {\n-        config.asNodeList().get().forEach(this::addProperty);\n+    static Properties toProperties(Config config) {\n+        Properties prop = new Properties();\n+        config.asNodeList().get().forEach(entry -> addProperty(entry, prop));\n+        return prop;\n     }\n \n     /**\n      * Split comma separated topic names.\n-     *\n+     * @param kafkaProperties\n      * @return list of topic names\n      */\n-    public List<String> getTopicNameList() {\n-        return Arrays.stream(getProperty(TOPIC_NAME)\n-                .split(\",\"))\n-                .map(String::trim)\n-                .collect(Collectors.toList());\n+    static List<String> topicNameList(Properties kafkaProperties) {\n+        String topics = kafkaProperties.getProperty(TOPIC_NAME);\n+        if (topics != null) {\n+            return Arrays.stream(topics.split(\",\"))\n+                    .map(String::trim)\n+                    .collect(Collectors.toList());\n+        } else {\n+            return Collections.emptyList();\n+        }\n     }\n \n-    private void addProperty(Config c) {\n+    private static void addProperty(Config c, Properties prop) {\n         String key = c.traverse().map(m -> m.key().parent().name() + \".\" + m.key().name())\n                 .collect(Collectors.joining(\".\"));\n         if (key.isEmpty()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937487", "body": "Why is this class public?", "bodyText": "Why is this class public?", "bodyHTML": "<p dir=\"auto\">Why is this class public?</p>", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:35Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+public class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTg0Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875842", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\nsimilarity index 89%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\nindex 51a8a4488..e9b36146e 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\n", "chunk": "@@ -25,7 +25,7 @@ import org.apache.kafka.common.TopicPartition;\n /**\n  * Waiting latch for partition assigment, after that is consumer ready to receive.\n  */\n-public class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {\n+class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {\n \n     PartitionsAssignedLatch() {\n         super(1);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392938671", "body": "If I understand correctly, we want an implementation for Microprofile reactive messaging. In such a case, this class should not be public.\r\nAlso the name is not very good.\r\n", "bodyText": "If I understand correctly, we want an implementation for Microprofile reactive messaging. In such a case, this class should not be public.\nAlso the name is not very good.", "bodyHTML": "<p dir=\"auto\">If I understand correctly, we want an implementation for Microprofile reactive messaging. In such a case, this class should not be public.<br>\nAlso the name is not very good.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T11:01:42Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzU1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457551", "bodyText": "I have changed the name and visibility", "author": "jbescos", "createdAt": "2020-03-18T15:57:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939334", "body": "Never use public constructors in Helidon.\r\nIf this class should be public (in which case the name should change), use a `Builder` pattern as in other Helidon classes.\r\nYou may have a static factory method, such as `create(String, Config)`, but should be limited to one such method.\r\n", "bodyText": "Never use public constructors in Helidon.\nIf this class should be public (in which case the name should change), use a Builder pattern as in other Helidon classes.\nYou may have a static factory method, such as create(String, Config), but should be limited to one such method.", "bodyHTML": "<p dir=\"auto\">Never use public constructors in Helidon.<br>\nIf this class should be public (in which case the name should change), use a <code>Builder</code> pattern as in other Helidon classes.<br>\nYou may have a static factory method, such as <code>create(String, Config)</code>, but should be limited to one such method.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T11:02:53Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTE2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461168", "bodyText": "I think that class was designed with 2 purposes:\n\nTo be integrated with cdi\nTo be used from any other code to consume from Kafka.\n\nI have simplified this class for the first point. There is only one constructor.", "author": "jbescos", "createdAt": "2020-03-18T16:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939603", "body": "Creating a new executor service for each request is wrong use of executors.", "bodyText": "Creating a new executor service for each request is wrong use of executors.", "bodyHTML": "<p dir=\"auto\">Creating a new executor service for each request is wrong use of executors.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T11:03:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTg1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461854", "bodyText": "This part is out.", "author": "jbescos", "createdAt": "2020-03-18T16:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392940357", "body": "Info messages should be only printed if we know it will happen only once per runtime of Helidon. I am not sure this is the case.", "bodyText": "Info messages should be only printed if we know it will happen only once per runtime of Helidon. I am not sure this is the case.", "bodyHTML": "<p dir=\"auto\">Info messages should be only printed if we know it will happen only once per runtime of Helidon. I am not sure this is the case.</p>", "author": "tomas-langer", "createdAt": "2020-03-16T11:04:49Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n+    }\n+\n+    /**\n+     * Execute supplied consumer by provided executor service for each received record.\n+     *\n+     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n+     * @param customTopics    Can be null, list of topics appended to the list from configuration\n+     * @param function        Consumer method executed in new thread for each received record\n+     * @return The Future's get method will return null when consumer is closed\n+     */\n+    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n+                                  Consumer<ConsumerRecord<K, V>> function) {\n+        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MjEwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394462103", "bodyText": "This is removed too.", "author": "jbescos", "createdAt": "2020-03-18T16:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392942335", "body": "This seems to be mutable (and wrongly).\r\nIf the consumer is only for a single use, make sure you construct the instance with all the configuration, then have a method to start listening. This will simplify a lot of checks.\r\nAlso not sure why there is `executorService` and `externalExecutorService`\r\n", "bodyText": "This seems to be mutable (and wrongly).\nIf the consumer is only for a single use, make sure you construct the instance with all the configuration, then have a method to start listening. This will simplify a lot of checks.\nAlso not sure why there is executorService and externalExecutorService", "bodyHTML": "<p dir=\"auto\">This seems to be mutable (and wrongly).<br>\nIf the consumer is only for a single use, make sure you construct the instance with all the configuration, then have a method to start listening. This will simplify a lot of checks.<br>\nAlso not sure why there is <code>executorService</code> and <code>externalExecutorService</code></p>", "author": "tomas-langer", "createdAt": "2020-03-16T11:08:32Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2ODY4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394468687", "bodyText": "Now it is simpler.\nRegarding the executors, I followed your suggestion about usage of one unique ScheduledExecutorService that is shared between all the kafka consumers.", "author": "jbescos", "createdAt": "2020-03-18T16:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394455046", "body": "I am not sure if this while is acceptable because of CPU usage. However I put it there because the time frame must be very small (few milliseconds) or zero.\r\n\r\nLet me know if you prefer other way, with count down latch for example. ", "bodyText": "I am not sure if this while is acceptable because of CPU usage. However I put it there because the time frame must be very small (few milliseconds) or zero.\nLet me know if you prefer other way, with count down latch for example.", "bodyHTML": "<p dir=\"auto\">I am not sure if this while is acceptable because of CPU usage. However I put it there because the time frame must be very small (few milliseconds) or zero.</p>\n<p dir=\"auto\">Let me know if you prefer other way, with count down latch for example.</p>", "author": "jbescos", "createdAt": "2020-03-18T15:54:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber, \n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0, \n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        while (running.get()) {", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0NTQ2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394845460", "bodyText": "I will do this with a synchronize.", "author": "jbescos", "createdAt": "2020-03-19T07:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0Njc1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394846756", "bodyText": "In this case I think it is much better to use a lock.", "author": "jbescos", "createdAt": "2020-03-19T08:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 3300bb42a..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -132,50 +126,51 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n     public void close() {\n         // Stops pooling\n         consumer.wakeup();\n-        while (running.get()) {\n-            // Waits till previous task finishes (in case it is still running)\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            consumer.close();\n+            publisher.complete();\n+        } catch (RuntimeException e) {\n+            publisher.fail(e);\n+        } finally {\n+            taskLock.unlock();\n         }\n-        LOGGER.fine(\"Closing kafka consumer\");\n-        consumer.close();\n     }\n \n     //Move to messaging incoming connector\n     private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n     }\n \n     private final class BackPressureLayer implements Runnable {\n \n         private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n         private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-        private final long poolTimeout;\n+        private final long pollTimeout;\n \n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n-            this.subscriber = subscriber;\n-            this.poolTimeout = poolTimeout;\n+        private BackPressureLayer(long pollTimeout) {\n+            this.pollTimeout = pollTimeout;\n         }\n \n         @Override\n         public void run() {\n-            if(!running.getAndSet(true)) {\n-                try {\n+            try {\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n                     waitForAcksAndPoll();\n                     ConsumerRecord<K, V> cr;\n                     while ((cr = backPressureBuffer.poll()) != null) {\n                         KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.getAckFuture());\n-                        runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                        ackFutures.add(kafkaMessage.ackFuture());\n+                        runInNewContext(() -> publisher.emit(kafkaMessage));\n                     }\n-                } finally {\n-                    running.set(false);\n                 }\n+            } finally {\n+                taskLock.unlock();\n             }\n         }\n \n", "next_change": {"commit": "10612d66d9b6094133053f2f2778682db3616ef3", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..134453e1f 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -159,47 +171,46 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         @Override\n         public void run() {\n             try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n                 taskLock.lock();\n                 if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    if (backPressureBuffer.isEmpty()) {\n+                        try {\n+                            consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                        } catch (WakeupException e) {\n+                            LOGGER.fine(\"It was requested to stop polling from channel\");\n+                        }\n+                    } else {\n+                        long totalToEmit = requests.get();\n+                        // Avoid index out bound exceptions\n+                        long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                        for (long i = 0; i < eventsToEmit; i++) {\n+                            ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                            // Unfortunately KafkaConsumer is not thread safe, so the commit must happen in this thread.\n+                            // KafkaMessage will notify ACK to this thread via Callback\n+                            KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, entry -> pendingCommits.add(entry));\n+                            runInNewContext(() -> publisher.emit(kafkaMessage));\n+                            requests.decrementAndGet();\n+                        }\n+                        if (eventsToEmit != 0) {\n+                            LOGGER.fine(String.format(\"Emitted %s of %s. Buffer size: %s\", eventsToEmit, totalToEmit, backPressureBuffer.size()));\n+                        }\n                     }\n                 }\n+                // Commit ACKs\n+                Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+                Entry<TopicPartition, OffsetAndMetadata> entry;\n+                while ((entry = pendingCommits.poll()) != null) {\n+                    offsets.put(entry.getKey(), entry.getValue());\n+                }\n+                consumer.commitSync(offsets);\n+                if (!offsets.isEmpty()) {\n+                    LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n+                }\n             } finally {\n                 taskLock.unlock();\n             }\n         }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n     }\n \n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nsimilarity index 60%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 134453e1f..92768f3ac 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -203,14 +128,89 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                 while ((entry = pendingCommits.poll()) != null) {\n                     offsets.put(entry.getKey(), entry.getValue());\n                 }\n-                consumer.commitSync(offsets);\n+                kafkaConsumer.commitSync(offsets);\n                 if (!offsets.isEmpty()) {\n                     LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n                 }\n+            } catch (Exception e) {\n+                emiter.fail(e);\n             } finally {\n                 taskLock.unlock();\n             }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            kafkaConsumer.close();\n+            if (!pendingCommits.isEmpty()) {\n+                LOGGER.warning(pendingCommits.size() + \" events were not commited to Kafka\");\n+            }\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n         }\n     }\n \n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     * \n+     * Note: after creating a KafkaPublisher you must always {@link #close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when {@link #execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    public static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);\n+        long pollTimeout = config.get(POLL_TIMEOUT).asLong().orElse(50L);\n+        long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().orElse(100L);\n+        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics, pollTimeout, periodExecutions);\n+        publisher.execute();\n+        return publisher;\n+    }\n }\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394479326", "body": "Uf this was my super bad idea, we have to do something about this, it basically ignores backpressure. Something like this would be much better:\r\nhttps://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java\r\n\r\nBut that depends on protected RS with SequentialSubscriber,\r\nI expect David to remove SequentialSubscriber from RS implemetation in #1511 so it gets little more complicated then.", "bodyText": "Uf this was my super bad idea, we have to do something about this, it basically ignores backpressure. Something like this would be much better:\nhttps://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java\nBut that depends on protected RS with SequentialSubscriber,\nI expect David to remove SequentialSubscriber from RS implemetation in #1511 so it gets little more complicated then.", "bodyHTML": "<p dir=\"auto\">Uf this was my super bad idea, we have to do something about this, it basically ignores backpressure. Something like this would be much better:<br>\n<a href=\"https://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java\">https://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java</a></p>\n<p dir=\"auto\">But that depends on protected RS with SequentialSubscriber,<br>\nI expect David to remove SequentialSubscriber from RS implemetation in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"580793513\" data-permission-text=\"Title is private\" data-url=\"https://github.com/oracle/helidon/issues/1511\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/oracle/helidon/pull/1511/hovercard\" href=\"https://github.com/oracle/helidon/pull/1511\">#1511</a> so it gets little more complicated then.</p>", "author": "danielkec", "createdAt": "2020-03-18T16:27:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjQwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876405", "bodyText": "I integrated the EmittingSubscriber", "author": "jbescos", "createdAt": "2020-03-25T14:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 3300bb42a..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -89,25 +100,8 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      *\n      * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n      */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    // Pushing Kafka consumer doesn't support requests.\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    BasicKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber, \n-                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0, \n-                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n-        }));\n+    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     /**\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 7018434cb..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,205 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989380", "body": "This field is never used", "bodyText": "This field is never used", "bodyHTML": "<p dir=\"auto\">This field is never used</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:29:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjUwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876504", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -41,47 +42,57 @@ import org.apache.kafka.common.errors.WakeupException;\n import org.eclipse.microprofile.reactive.messaging.Message;\n import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n \n /**\n  * Basic Kafka consumer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaConsumer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n     private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final KafkaConfigProperties properties;\n-    private final Config config;\n     private final Lock taskLock = new ReentrantLock();\n     private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final List<String> topicNameList;\n-    // It is not thread safe. It needs to be closed in the same thread it reads events.\n-    // We need to keep the reference here to be able to wake up from pooling when shuting down\n     private final KafkaConsumer<K, V> consumer;\n     private final ScheduledExecutorService scheduler;\n+    private final BasicPublisher<K, V> publisher;\n+\n+    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n+            long pollTimeout, long periodExecutions) {\n+        this.consumer = consumer;\n+        this.scheduler = scheduler;\n+        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n+            consumer.subscribe(topics, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n+                    periodExecutions, TimeUnit.MILLISECONDS);\n+        });\n+    }\n \n     /**\n      * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n      * @param config Helidon {@link io.helidon.config.Config config}\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n-    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n-        this.config = config;\n-        this.properties = new KafkaConfigProperties(config);\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumer = new KafkaConsumer<>(properties);\n-        this.scheduler = scheduler;\n+    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n+        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n+            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+                    pollTimeout, periodExecutions);\n+        }\n     }\n \n     /**\n", "next_change": {"commit": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..1a78261a3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -83,14 +84,14 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n     static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n         if (topics.isEmpty()) {\n             throw new IllegalArgumentException(\"The topic is a required configuration value\");\n         } else {\n             long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n             long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n                     pollTimeout, periodExecutions);\n         }\n     }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 1a78261a3..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,207 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        LOGGER.fine(\"Consumer. Add ack future\");\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Consumer. Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989805", "body": "Please do not `extends Properties` in `KafkaConfigProperties`.\r\nAdd a method `toProperties` to that class that would return the properties required by `KafkaConsumer`", "bodyText": "Please do not extends Properties in KafkaConfigProperties.\nAdd a method toProperties to that class that would return the properties required by KafkaConsumer", "bodyHTML": "<p dir=\"auto\">Please do not <code>extends Properties</code> in <code>KafkaConfigProperties</code>.<br>\nAdd a method <code>toProperties</code> to that class that would return the properties required by <code>KafkaConsumer</code></p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:30:02Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NzY5MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397877691", "bodyText": "I implemented it differently", "author": "jbescos", "createdAt": "2020-03-25T14:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -41,47 +42,57 @@ import org.apache.kafka.common.errors.WakeupException;\n import org.eclipse.microprofile.reactive.messaging.Message;\n import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n \n /**\n  * Basic Kafka consumer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaConsumer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n     private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final KafkaConfigProperties properties;\n-    private final Config config;\n     private final Lock taskLock = new ReentrantLock();\n     private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final List<String> topicNameList;\n-    // It is not thread safe. It needs to be closed in the same thread it reads events.\n-    // We need to keep the reference here to be able to wake up from pooling when shuting down\n     private final KafkaConsumer<K, V> consumer;\n     private final ScheduledExecutorService scheduler;\n+    private final BasicPublisher<K, V> publisher;\n+\n+    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n+            long pollTimeout, long periodExecutions) {\n+        this.consumer = consumer;\n+        this.scheduler = scheduler;\n+        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n+            consumer.subscribe(topics, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n+                    periodExecutions, TimeUnit.MILLISECONDS);\n+        });\n+    }\n \n     /**\n      * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n      * @param config Helidon {@link io.helidon.config.Config config}\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n-    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n-        this.config = config;\n-        this.properties = new KafkaConfigProperties(config);\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumer = new KafkaConsumer<>(properties);\n-        this.scheduler = scheduler;\n+    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n+        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n+            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+                    pollTimeout, periodExecutions);\n+        }\n     }\n \n     /**\n", "next_change": {"commit": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..1a78261a3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -83,14 +84,14 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n     static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n         if (topics.isEmpty()) {\n             throw new IllegalArgumentException(\"The topic is a required configuration value\");\n         } else {\n             long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n             long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n                     pollTimeout, periodExecutions);\n         }\n     }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 1a78261a3..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,207 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        LOGGER.fine(\"Consumer. Add ack future\");\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Consumer. Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990527", "body": "config is an immutable snapshot - read configuration options when creating this instance. ", "bodyText": "config is an immutable snapshot - read configuration options when creating this instance.", "bodyHTML": "<p dir=\"auto\">config is an immutable snapshot - read configuration options when creating this instance.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:31:20Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDk2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990969", "bodyText": "Not sure - is this about \"polling\" or \"pooling\"?\nIf this is how often we poll Kafka for changes, then the correct key should be poll-timeout and constant POLL_TIMEOUT.", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODgzMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878833", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -89,25 +100,8 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      *\n      * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n      */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    // Pushing Kafka consumer doesn't support requests.\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    BasicKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n-                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n-                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n-        }));\n+    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     /**\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 7018434cb..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,205 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991351", "body": "If you return a constant from `Optional.orElseGet`, then use `Optional.orElse`", "bodyText": "If you return a constant from Optional.orElseGet, then use Optional.orElse", "bodyHTML": "<p dir=\"auto\">If you return a constant from <code>Optional.orElseGet</code>, then use <code>Optional.orElse</code></p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:53Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODA4Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878082", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -89,25 +100,8 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      *\n      * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n      */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    // Pushing Kafka consumer doesn't support requests.\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    BasicKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n-                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n-                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n-        }));\n+    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     /**\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 7018434cb..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,205 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991673", "body": "Method should be package private", "bodyText": "Method should be package private", "bodyHTML": "<p dir=\"auto\">Method should be package private</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:27Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODkyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878922", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -41,47 +42,57 @@ import org.apache.kafka.common.errors.WakeupException;\n import org.eclipse.microprofile.reactive.messaging.Message;\n import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n \n /**\n  * Basic Kafka consumer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaConsumer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n     private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final KafkaConfigProperties properties;\n-    private final Config config;\n     private final Lock taskLock = new ReentrantLock();\n     private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final List<String> topicNameList;\n-    // It is not thread safe. It needs to be closed in the same thread it reads events.\n-    // We need to keep the reference here to be able to wake up from pooling when shuting down\n     private final KafkaConsumer<K, V> consumer;\n     private final ScheduledExecutorService scheduler;\n+    private final BasicPublisher<K, V> publisher;\n+\n+    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n+            long pollTimeout, long periodExecutions) {\n+        this.consumer = consumer;\n+        this.scheduler = scheduler;\n+        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n+            consumer.subscribe(topics, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n+                    periodExecutions, TimeUnit.MILLISECONDS);\n+        });\n+    }\n \n     /**\n      * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n      * @param config Helidon {@link io.helidon.config.Config config}\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n-    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n-        this.config = config;\n-        this.properties = new KafkaConfigProperties(config);\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumer = new KafkaConsumer<>(properties);\n-        this.scheduler = scheduler;\n+    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n+        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n+            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+                    pollTimeout, periodExecutions);\n+        }\n     }\n \n     /**\n", "next_change": {"commit": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..1a78261a3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -83,14 +84,14 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n      */\n     static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n         if (topics.isEmpty()) {\n             throw new IllegalArgumentException(\"The topic is a required configuration value\");\n         } else {\n             long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n             long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n+            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n                     pollTimeout, periodExecutions);\n         }\n     }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 1a78261a3..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,207 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaConfig), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        LOGGER.fine(\"Consumer. Add ack future\");\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Consumer. Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -89,25 +100,8 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      *\n      * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n      */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    // Pushing Kafka consumer doesn't support requests.\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    BasicKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n-                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n-                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n-        }));\n+    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     /**\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 7018434cb..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,205 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991780", "body": "Method should be package private", "bodyText": "Method should be package private", "bodyHTML": "<p dir=\"auto\">Method should be package private</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:39Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTE0MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879140", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -119,7 +113,7 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n      * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n      * @throws java.util.concurrent.TimeoutException if the timeout is reached\n      */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n         if (!partitionsAssignedLatch.await(timeout, unit)) {\n             throw new TimeoutException(\"Timeout for subscription reached\");\n         }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\ndeleted file mode 100644\nindex 7018434cb..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ /dev/null\n", "chunk": "@@ -1,205 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-\n-/**\n- * Basic Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private final Lock taskLock = new ReentrantLock();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final KafkaConsumer<K, V> consumer;\n-    private final ScheduledExecutorService scheduler;\n-    private final BasicPublisher<K, V> publisher;\n-\n-    private BasicKafkaConsumer(KafkaConsumer<K, V> consumer, ScheduledExecutorService scheduler, List<String> topics,\n-            long pollTimeout, long periodExecutions) {\n-        this.consumer = consumer;\n-        this.scheduler = scheduler;\n-        this.publisher = new BasicPublisher<K, V>(subscriber -> {\n-            consumer.subscribe(topics, partitionsAssignedLatch);\n-            scheduler.scheduleAtFixedRate(new BackPressureLayer(pollTimeout), 0,\n-                    periodExecutions, TimeUnit.MILLISECONDS);\n-        });\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n-     */\n-    static <K, V> BasicKafkaConsumer<K, V> create(Config config, ScheduledExecutorService scheduler){\n-        Properties kafkaProperties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaProperties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            long pollTimeout = config.get(POLL_TIMEOUT).asLong().asOptional().orElse(50L);\n-            long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElse(100L);\n-            return new BasicKafkaConsumer<K, V>(new KafkaConsumer<>(kafkaProperties), scheduler, topics,\n-                    pollTimeout, periodExecutions);\n-        }\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n-     */\n-    @Override\n-    public void close() {\n-        // Stops pooling\n-        consumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            consumer.close();\n-            publisher.complete();\n-        } catch (RuntimeException e) {\n-            publisher.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final long pollTimeout;\n-\n-        private BackPressureLayer(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-        }\n-\n-        @Override\n-        public void run() {\n-            try {\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n-                    }\n-                }\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394992404", "body": "If you unlock a lock, another thread may lock it before you close the consumer.\r\nThe taskLock should be unlocked in a `finally` block after `consumer.close()`", "bodyText": "If you unlock a lock, another thread may lock it before you close the consumer.\nThe taskLock should be unlocked in a finally block after consumer.close()", "bodyHTML": "<p dir=\"auto\">If you unlock a lock, another thread may lock it before you close the consumer.<br>\nThe taskLock should be unlocked in a <code>finally</code> block after <code>consumer.close()</code></p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:34:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAyNDgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395024831", "bodyText": "In this case there is no other thread that could run later because the scheduler was stopped before forever. Still I'm thinking there is a very rare scenario that:\n\nScheduler started a task, and it doesn't reach the lock.\nShutdown is executed.\nclose() is executed and blocks. So task of point 1 is waiting.\nTask run and fails with unexpected error because the kafka connection is closed.\n\nSo a part of doing what you said, I will modify BackPressureLayer to check !scheduler.isShutdown()", "author": "jbescos", "createdAt": "2020-03-19T13:30:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -133,45 +127,47 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         // Stops pooling\n         consumer.wakeup();\n         // Wait that current task finishes in case it is still running\n-        taskLock.lock();\n-        taskLock.unlock();\n-        LOGGER.fine(\"Closing kafka consumer\");\n-        consumer.close();\n+        try {\n+            taskLock.lock();\n+            consumer.close();\n+            publisher.complete();\n+        } catch (RuntimeException e) {\n+            publisher.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n     }\n \n     //Move to messaging incoming connector\n     private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n     }\n \n     private final class BackPressureLayer implements Runnable {\n \n         private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n         private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-        private final long poolTimeout;\n+        private final long pollTimeout;\n \n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n-            this.subscriber = subscriber;\n-            this.poolTimeout = poolTimeout;\n+        private BackPressureLayer(long pollTimeout) {\n+            this.pollTimeout = pollTimeout;\n         }\n \n         @Override\n         public void run() {\n             try {\n                 taskLock.lock();\n-                waitForAcksAndPoll();\n-                ConsumerRecord<K, V> cr;\n-                while ((cr = backPressureBuffer.poll()) != null) {\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n+                    waitForAcksAndPoll();\n+                    ConsumerRecord<K, V> cr;\n+                    while ((cr = backPressureBuffer.poll()) != null) {\n+                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                        ackFutures.add(kafkaMessage.ackFuture());\n+                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    }\n                 }\n             } finally {\n                 taskLock.unlock();\n", "next_change": {"commit": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..1a78261a3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -165,6 +166,7 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                     ConsumerRecord<K, V> cr;\n                     while ((cr = backPressureBuffer.poll()) != null) {\n                         KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                        LOGGER.fine(\"Consumer. Add ack future\");\n                         ackFutures.add(kafkaMessage.ackFuture());\n                         runInNewContext(() -> publisher.emit(kafkaMessage));\n                     }\n", "next_change": {"commit": "07a11a58331466f830337cf71cd033aec1022418", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 1a78261a3..4400d9bf3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -166,7 +165,6 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                     ConsumerRecord<K, V> cr;\n                     while ((cr = backPressureBuffer.poll()) != null) {\n                         KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        LOGGER.fine(\"Consumer. Add ack future\");\n                         ackFutures.add(kafkaMessage.ackFuture());\n                         runInNewContext(() -> publisher.emit(kafkaMessage));\n                     }\n", "next_change": {"commit": "10612d66d9b6094133053f2f2778682db3616ef3", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 4400d9bf3..134453e1f 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -159,47 +171,46 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         @Override\n         public void run() {\n             try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n                 taskLock.lock();\n                 if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    if (backPressureBuffer.isEmpty()) {\n+                        try {\n+                            consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                        } catch (WakeupException e) {\n+                            LOGGER.fine(\"It was requested to stop polling from channel\");\n+                        }\n+                    } else {\n+                        long totalToEmit = requests.get();\n+                        // Avoid index out bound exceptions\n+                        long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                        for (long i = 0; i < eventsToEmit; i++) {\n+                            ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                            // Unfortunately KafkaConsumer is not thread safe, so the commit must happen in this thread.\n+                            // KafkaMessage will notify ACK to this thread via Callback\n+                            KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, entry -> pendingCommits.add(entry));\n+                            runInNewContext(() -> publisher.emit(kafkaMessage));\n+                            requests.decrementAndGet();\n+                        }\n+                        if (eventsToEmit != 0) {\n+                            LOGGER.fine(String.format(\"Emitted %s of %s. Buffer size: %s\", eventsToEmit, totalToEmit, backPressureBuffer.size()));\n+                        }\n                     }\n                 }\n+                // Commit ACKs\n+                Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+                Entry<TopicPartition, OffsetAndMetadata> entry;\n+                while ((entry = pendingCommits.poll()) != null) {\n+                    offsets.put(entry.getKey(), entry.getValue());\n+                }\n+                consumer.commitSync(offsets);\n+                if (!offsets.isEmpty()) {\n+                    LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n+                }\n             } finally {\n                 taskLock.unlock();\n             }\n         }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Consumer. Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n     }\n \n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nsimilarity index 60%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 134453e1f..92768f3ac 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -203,14 +128,89 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                 while ((entry = pendingCommits.poll()) != null) {\n                     offsets.put(entry.getKey(), entry.getValue());\n                 }\n-                consumer.commitSync(offsets);\n+                kafkaConsumer.commitSync(offsets);\n                 if (!offsets.isEmpty()) {\n                     LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n                 }\n+            } catch (Exception e) {\n+                emiter.fail(e);\n             } finally {\n                 taskLock.unlock();\n             }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            kafkaConsumer.close();\n+            if (!pendingCommits.isEmpty()) {\n+                LOGGER.warning(pendingCommits.size() + \" events were not commited to Kafka\");\n+            }\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n         }\n     }\n \n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     * \n+     * Note: after creating a KafkaPublisher you must always {@link #close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when {@link #execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    public static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);\n+        long pollTimeout = config.get(POLL_TIMEOUT).asLong().orElse(50L);\n+        long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().orElse(100L);\n+        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics, pollTimeout, periodExecutions);\n+        publisher.execute();\n+        return publisher;\n+    }\n }\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394994736", "body": "Please replace with:\r\n```\r\nContext.Builder contextBuilder = Context.builder()\r\n                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\r\n\r\nContexts.context().ifPresent(contextBuilder::parent);\r\n        \r\nContexts.runInContext(contextBuilder.build(), runnable);\r\n```", "bodyText": "Please replace with:\nContext.Builder contextBuilder = Context.builder()\n                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n\nContexts.context().ifPresent(contextBuilder::parent);\n        \nContexts.runInContext(contextBuilder.build(), runnable);", "bodyHTML": "<p dir=\"auto\">Please replace with:</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"Context.Builder contextBuilder = Context.builder()\n                .id(String.format(&quot;kafka-message-%s:&quot;, UUID.randomUUID().toString()));\n\nContexts.context().ifPresent(contextBuilder::parent);\n        \nContexts.runInContext(contextBuilder.build(), runnable);\"><pre><code>Context.Builder contextBuilder = Context.builder()\n                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n\nContexts.context().ifPresent(contextBuilder::parent);\n        \nContexts.runInContext(contextBuilder.build(), runnable);\n</code></pre></div>", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:03Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTI2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879260", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -133,45 +127,47 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         // Stops pooling\n         consumer.wakeup();\n         // Wait that current task finishes in case it is still running\n-        taskLock.lock();\n-        taskLock.unlock();\n-        LOGGER.fine(\"Closing kafka consumer\");\n-        consumer.close();\n+        try {\n+            taskLock.lock();\n+            consumer.close();\n+            publisher.complete();\n+        } catch (RuntimeException e) {\n+            publisher.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n     }\n \n     //Move to messaging incoming connector\n     private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n     }\n \n     private final class BackPressureLayer implements Runnable {\n \n         private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n         private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-        private final long poolTimeout;\n+        private final long pollTimeout;\n \n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n-            this.subscriber = subscriber;\n-            this.poolTimeout = poolTimeout;\n+        private BackPressureLayer(long pollTimeout) {\n+            this.pollTimeout = pollTimeout;\n         }\n \n         @Override\n         public void run() {\n             try {\n                 taskLock.lock();\n-                waitForAcksAndPoll();\n-                ConsumerRecord<K, V> cr;\n-                while ((cr = backPressureBuffer.poll()) != null) {\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n+                    waitForAcksAndPoll();\n+                    ConsumerRecord<K, V> cr;\n+                    while ((cr = backPressureBuffer.poll()) != null) {\n+                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                        ackFutures.add(kafkaMessage.ackFuture());\n+                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    }\n                 }\n             } finally {\n                 taskLock.unlock();\n", "next_change": {"commit": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..1a78261a3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -165,6 +166,7 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                     ConsumerRecord<K, V> cr;\n                     while ((cr = backPressureBuffer.poll()) != null) {\n                         KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                        LOGGER.fine(\"Consumer. Add ack future\");\n                         ackFutures.add(kafkaMessage.ackFuture());\n                         runInNewContext(() -> publisher.emit(kafkaMessage));\n                     }\n", "next_change": {"commit": "07a11a58331466f830337cf71cd033aec1022418", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 1a78261a3..4400d9bf3 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -166,7 +165,6 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                     ConsumerRecord<K, V> cr;\n                     while ((cr = backPressureBuffer.poll()) != null) {\n                         KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        LOGGER.fine(\"Consumer. Add ack future\");\n                         ackFutures.add(kafkaMessage.ackFuture());\n                         runInNewContext(() -> publisher.emit(kafkaMessage));\n                     }\n", "next_change": {"commit": "10612d66d9b6094133053f2f2778682db3616ef3", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 4400d9bf3..134453e1f 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -159,47 +171,46 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         @Override\n         public void run() {\n             try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n                 taskLock.lock();\n                 if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    if (backPressureBuffer.isEmpty()) {\n+                        try {\n+                            consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                        } catch (WakeupException e) {\n+                            LOGGER.fine(\"It was requested to stop polling from channel\");\n+                        }\n+                    } else {\n+                        long totalToEmit = requests.get();\n+                        // Avoid index out bound exceptions\n+                        long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                        for (long i = 0; i < eventsToEmit; i++) {\n+                            ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                            // Unfortunately KafkaConsumer is not thread safe, so the commit must happen in this thread.\n+                            // KafkaMessage will notify ACK to this thread via Callback\n+                            KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, entry -> pendingCommits.add(entry));\n+                            runInNewContext(() -> publisher.emit(kafkaMessage));\n+                            requests.decrementAndGet();\n+                        }\n+                        if (eventsToEmit != 0) {\n+                            LOGGER.fine(String.format(\"Emitted %s of %s. Buffer size: %s\", eventsToEmit, totalToEmit, backPressureBuffer.size()));\n+                        }\n                     }\n                 }\n+                // Commit ACKs\n+                Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+                Entry<TopicPartition, OffsetAndMetadata> entry;\n+                while ((entry = pendingCommits.poll()) != null) {\n+                    offsets.put(entry.getKey(), entry.getValue());\n+                }\n+                consumer.commitSync(offsets);\n+                if (!offsets.isEmpty()) {\n+                    LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n+                }\n             } finally {\n                 taskLock.unlock();\n             }\n         }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Consumer. Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n     }\n \n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nsimilarity index 60%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 134453e1f..92768f3ac 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -203,14 +128,89 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                 while ((entry = pendingCommits.poll()) != null) {\n                     offsets.put(entry.getKey(), entry.getValue());\n                 }\n-                consumer.commitSync(offsets);\n+                kafkaConsumer.commitSync(offsets);\n                 if (!offsets.isEmpty()) {\n                     LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n                 }\n+            } catch (Exception e) {\n+                emiter.fail(e);\n             } finally {\n                 taskLock.unlock();\n             }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            kafkaConsumer.close();\n+            if (!pendingCommits.isEmpty()) {\n+                LOGGER.warning(pendingCommits.size() + \" events were not commited to Kafka\");\n+            }\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n         }\n     }\n \n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     * \n+     * Note: after creating a KafkaPublisher you must always {@link #close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when {@link #execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    public static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);\n+        long pollTimeout = config.get(POLL_TIMEOUT).asLong().orElse(50L);\n+        long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().orElse(100L);\n+        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics, pollTimeout, periodExecutions);\n+        publisher.execute();\n+        return publisher;\n+    }\n }\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995234", "body": "Definitely `pollTimeout` not `poolTimeout`", "bodyText": "Definitely pollTimeout not poolTimeout", "bodyHTML": "<p dir=\"auto\">Definitely <code>pollTimeout</code> not <code>poolTimeout</code></p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);\n+    }\n+\n+    private final class BackPressureLayer implements Runnable {\n+\n+        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n+        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n+        private final long poolTimeout;\n+\n+        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n+            this.subscriber = subscriber;\n+            this.poolTimeout = poolTimeout;\n+        }\n+\n+        @Override\n+        public void run() {\n+            try {\n+                taskLock.lock();\n+                waitForAcksAndPoll();\n+                ConsumerRecord<K, V> cr;\n+                while ((cr = backPressureBuffer.poll()) != null) {\n+                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                    ackFutures.add(kafkaMessage.getAckFuture());\n+                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                }\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }\n+\n+        /**\n+         * Naive impl of back pressure wise lazy poll.\n+         * Wait for the last batch of records to be acknowledged before commit and another poll.\n+         */\n+        private void waitForAcksAndPoll() {\n+            if (backPressureBuffer.isEmpty()) {\n+                try {\n+                    if (!ackFutures.isEmpty()) {\n+                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n+                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n+                        ackFutures.clear();\n+                        consumer.commitSync();\n+                    }\n+                    consumer.poll(Duration.ofMillis(poolTimeout)).forEach(backPressureBuffer::add);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTM4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879386", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -191,9 +187,12 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                         ackFutures.clear();\n                         consumer.commitSync();\n                     }\n-                    consumer.poll(Duration.ofMillis(poolTimeout)).forEach(backPressureBuffer::add);\n+                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                    if (!backPressureBuffer.isEmpty()) {\n+                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n+                    }\n                 } catch (WakeupException e) {\n-                    LOGGER.info(\"It was requested to stop pooling from channel\");\n+                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n                 } catch (InterruptedException | ExecutionException e) {\n                     LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n                 }\n", "next_change": {"commit": "10612d66d9b6094133053f2f2778682db3616ef3", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 7018434cb..134453e1f 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n", "chunk": "@@ -159,47 +171,46 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n         @Override\n         public void run() {\n             try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n                 taskLock.lock();\n                 if (!scheduler.isShutdown() && !publisher.isCancelled()) {\n-                    waitForAcksAndPoll();\n-                    ConsumerRecord<K, V> cr;\n-                    while ((cr = backPressureBuffer.poll()) != null) {\n-                        KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                        ackFutures.add(kafkaMessage.ackFuture());\n-                        runInNewContext(() -> publisher.emit(kafkaMessage));\n+                    if (backPressureBuffer.isEmpty()) {\n+                        try {\n+                            consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                        } catch (WakeupException e) {\n+                            LOGGER.fine(\"It was requested to stop polling from channel\");\n+                        }\n+                    } else {\n+                        long totalToEmit = requests.get();\n+                        // Avoid index out bound exceptions\n+                        long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                        for (long i = 0; i < eventsToEmit; i++) {\n+                            ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                            // Unfortunately KafkaConsumer is not thread safe, so the commit must happen in this thread.\n+                            // KafkaMessage will notify ACK to this thread via Callback\n+                            KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, entry -> pendingCommits.add(entry));\n+                            runInNewContext(() -> publisher.emit(kafkaMessage));\n+                            requests.decrementAndGet();\n+                        }\n+                        if (eventsToEmit != 0) {\n+                            LOGGER.fine(String.format(\"Emitted %s of %s. Buffer size: %s\", eventsToEmit, totalToEmit, backPressureBuffer.size()));\n+                        }\n                     }\n                 }\n+                // Commit ACKs\n+                Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+                Entry<TopicPartition, OffsetAndMetadata> entry;\n+                while ((entry = pendingCommits.poll()) != null) {\n+                    offsets.put(entry.getKey(), entry.getValue());\n+                }\n+                consumer.commitSync(offsets);\n+                if (!offsets.isEmpty()) {\n+                    LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n+                }\n             } finally {\n                 taskLock.unlock();\n             }\n         }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                    if (!backPressureBuffer.isEmpty()) {\n-                        LOGGER.fine(String.format(\"Buffered %s\", backPressureBuffer.size()));\n-                    }\n-                } catch (WakeupException e) {\n-                    LOGGER.fine(\"It was requested to stop pooling from channel\");\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n     }\n \n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nsimilarity index 60%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 134453e1f..92768f3ac 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -203,14 +128,89 @@ class BasicKafkaConsumer<K, V> implements Closeable {\n                 while ((entry = pendingCommits.poll()) != null) {\n                     offsets.put(entry.getKey(), entry.getValue());\n                 }\n-                consumer.commitSync(offsets);\n+                kafkaConsumer.commitSync(offsets);\n                 if (!offsets.isEmpty()) {\n                     LOGGER.fine(String.format(\"%s events were ACK: \", offsets.size()));\n                 }\n+            } catch (Exception e) {\n+                emiter.fail(e);\n             } finally {\n                 taskLock.unlock();\n             }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            kafkaConsumer.close();\n+            if (!pendingCommits.isEmpty()) {\n+                LOGGER.warning(pendingCommits.size() + \" events were not commited to Kafka\");\n+            }\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n         }\n     }\n \n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     * \n+     * Note: after creating a KafkaPublisher you must always {@link #close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when {@link #execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    public static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);\n+        long pollTimeout = config.get(POLL_TIMEOUT).asLong().orElse(50L);\n+        long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().orElse(100L);\n+        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics, pollTimeout, periodExecutions);\n+        publisher.execute();\n+        return publisher;\n+    }\n }\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995494", "body": "method should be package local", "bodyText": "method should be package local", "bodyHTML": "<p dir=\"auto\">method should be package local</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:22Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTc5OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995798", "bodyText": "(and all other public methods in this class)", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTUzNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879536", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -30,110 +28,62 @@ import io.helidon.config.Config;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n import org.apache.kafka.clients.producer.RecordMetadata;\n-import org.apache.kafka.common.header.Header;\n \n /**\n  * Basic Kafka producer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}.\n- * <p>\n- * Usage:\n- * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n- *             .produce(\"Hello world!\");\n- * }</pre>\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaProducer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final KafkaConfigProperties properties;\n+    private final List<String> topics;\n     private final KafkaProducer<K, V> producer;\n \n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    BasicKafkaProducer(Config config) {\n-        properties = new KafkaConfigProperties(config);\n-        producer = new KafkaProducer<>(properties);\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n     }\n \n     /**\n-     * Send record to all provided topics,\n-     * blocking until all records are acknowledged by broker.\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n-     * @param value Will be serialized by <b>value.serializer</b> class\n-     *              defined in {@link KafkaConfigProperties configuration}\n-     * @return Server acknowledged metadata about sent topics\n+     * @param config Helidon {@link io.helidon.config.Config config}\n      */\n-    public List<RecordMetadata> produce(V value) {\n-        List<Future<RecordMetadata>> futureRecords =\n-                produceAsync(null, null, null, null, value, null);\n-        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n-\n-        for (Future<RecordMetadata> future : futureRecords) {\n-            try {\n-                metadataList.add(future.get());\n-            } catch (InterruptedException | ExecutionException e) {\n-                throw new RuntimeException(\"Failed to send topic\", e);\n-            }\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n         }\n-        return metadataList;\n-    }\n-\n-    /**\n-     * Produce asynchronously.\n-     *\n-     * @param value value to be produced\n-     * @return list of futures\n-     */\n-    public List<Future<RecordMetadata>> produceAsync(V value) {\n-        return produceAsync(null, null, null, null, value, null);\n     }\n \n     /**\n      * Send record to all provided topics, don't wait for server acknowledgement.\n      *\n-     * @param customTopics Can be null, list of topics appended to the list from configuration,\n-     *                     record will be sent to all topics iteratively\n-     * @param partition    Can be null, if key is also null topic is sent to random partition\n-     * @param timestamp    Can be null System.currentTimeMillis() is used then\n-     * @param key          Can be null, if not, topics are grouped to partitions by key\n      * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @param headers      Can be null, custom headers for additional meta information if needed\n      * @return Futures of server acknowledged metadata about sent topics\n      */\n-    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n-                                                     Integer partition,\n-                                                     Long timestamp,\n-                                                     K key,\n-                                                     V value,\n-                                                     Iterable<Header> headers) {\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");\n-            return Collections.emptyList();\n-        }\n-\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(mergedTopics.size());\n-\n-        for (String topic : mergedTopics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, partition, timestamp, key, value, headers);\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n             recordMetadataFutures.add(producer.send(record));\n         }\n         return recordMetadataFutures;\n     }\n \n+    /**\n+     * Closes the Kafka producer.\n+     */\n     @Override\n     public void close() {\n         LOGGER.fine(\"Closing kafka producer\");\n", "next_change": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 16ccf02fe..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nnew file mode 100644\nindex 000000000..7b1886500\n--- /dev/null\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see HelidonToKafkaConfigParser\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final List<String> topics;\n+    private final KafkaProducer<K, V> producer;\n+\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n+    }\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n+        }\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n+            recordMetadataFutures.add(producer.send(record));\n+        }\n+        return recordMetadataFutures;\n+    }\n+\n+    /**\n+     * Closes the Kafka producer.\n+     */\n+    @Override\n+    public void close() {\n+        LOGGER.fine(\"Closing kafka producer\");\n+        producer.close();\n+    }\n+}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 7b1886500..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996367", "body": "Using `null` is not encouraged in Helidon. Would be better to send correct defaults rather than nulls - this is very error prone", "bodyText": "Using null is not encouraged in Helidon. Would be better to send correct defaults rather than nulls - this is very error prone", "bodyHTML": "<p dir=\"auto\">Using <code>null</code> is not encouraged in Helidon. Would be better to send correct defaults rather than nulls - this is very error prone</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:41:54Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTY4OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879689", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -30,110 +28,62 @@ import io.helidon.config.Config;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n import org.apache.kafka.clients.producer.RecordMetadata;\n-import org.apache.kafka.common.header.Header;\n \n /**\n  * Basic Kafka producer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}.\n- * <p>\n- * Usage:\n- * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n- *             .produce(\"Hello world!\");\n- * }</pre>\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaProducer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final KafkaConfigProperties properties;\n+    private final List<String> topics;\n     private final KafkaProducer<K, V> producer;\n \n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    BasicKafkaProducer(Config config) {\n-        properties = new KafkaConfigProperties(config);\n-        producer = new KafkaProducer<>(properties);\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n     }\n \n     /**\n-     * Send record to all provided topics,\n-     * blocking until all records are acknowledged by broker.\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n-     * @param value Will be serialized by <b>value.serializer</b> class\n-     *              defined in {@link KafkaConfigProperties configuration}\n-     * @return Server acknowledged metadata about sent topics\n+     * @param config Helidon {@link io.helidon.config.Config config}\n      */\n-    public List<RecordMetadata> produce(V value) {\n-        List<Future<RecordMetadata>> futureRecords =\n-                produceAsync(null, null, null, null, value, null);\n-        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n-\n-        for (Future<RecordMetadata> future : futureRecords) {\n-            try {\n-                metadataList.add(future.get());\n-            } catch (InterruptedException | ExecutionException e) {\n-                throw new RuntimeException(\"Failed to send topic\", e);\n-            }\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n         }\n-        return metadataList;\n-    }\n-\n-    /**\n-     * Produce asynchronously.\n-     *\n-     * @param value value to be produced\n-     * @return list of futures\n-     */\n-    public List<Future<RecordMetadata>> produceAsync(V value) {\n-        return produceAsync(null, null, null, null, value, null);\n     }\n \n     /**\n      * Send record to all provided topics, don't wait for server acknowledgement.\n      *\n-     * @param customTopics Can be null, list of topics appended to the list from configuration,\n-     *                     record will be sent to all topics iteratively\n-     * @param partition    Can be null, if key is also null topic is sent to random partition\n-     * @param timestamp    Can be null System.currentTimeMillis() is used then\n-     * @param key          Can be null, if not, topics are grouped to partitions by key\n      * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @param headers      Can be null, custom headers for additional meta information if needed\n      * @return Futures of server acknowledged metadata about sent topics\n      */\n-    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n-                                                     Integer partition,\n-                                                     Long timestamp,\n-                                                     K key,\n-                                                     V value,\n-                                                     Iterable<Header> headers) {\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");\n-            return Collections.emptyList();\n-        }\n-\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(mergedTopics.size());\n-\n-        for (String topic : mergedTopics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, partition, timestamp, key, value, headers);\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n             recordMetadataFutures.add(producer.send(record));\n         }\n         return recordMetadataFutures;\n     }\n \n+    /**\n+     * Closes the Kafka producer.\n+     */\n     @Override\n     public void close() {\n         LOGGER.fine(\"Closing kafka producer\");\n", "next_change": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 16ccf02fe..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nnew file mode 100644\nindex 000000000..7b1886500\n--- /dev/null\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see HelidonToKafkaConfigParser\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final List<String> topics;\n+    private final KafkaProducer<K, V> producer;\n+\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n+    }\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n+        }\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n+            recordMetadataFutures.add(producer.send(record));\n+        }\n+        return recordMetadataFutures;\n+    }\n+\n+    /**\n+     * Closes the Kafka producer.\n+     */\n+    @Override\n+    public void close() {\n+        LOGGER.fine(\"Closing kafka producer\");\n+        producer.close();\n+    }\n+}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 7b1886500..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996949", "body": "This is an error - throw an exception", "bodyText": "This is an error - throw an exception", "bodyHTML": "<p dir=\"auto\">This is an error - throw an exception</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:42:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);\n+        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n+\n+        for (Future<RecordMetadata> future : futureRecords) {\n+            try {\n+                metadataList.add(future.get());\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new RuntimeException(\"Failed to send topic\", e);\n+            }\n+        }\n+        return metadataList;\n+    }\n+\n+    /**\n+     * Produce asynchronously.\n+     *\n+     * @param value value to be produced\n+     * @return list of futures\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(V value) {\n+        return produceAsync(null, null, null, null, value, null);\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param customTopics Can be null, list of topics appended to the list from configuration,\n+     *                     record will be sent to all topics iteratively\n+     * @param partition    Can be null, if key is also null topic is sent to random partition\n+     * @param timestamp    Can be null System.currentTimeMillis() is used then\n+     * @param key          Can be null, if not, topics are grouped to partitions by key\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @param headers      Can be null, custom headers for additional meta information if needed\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n+                                                     Integer partition,\n+                                                     Long timestamp,\n+                                                     K key,\n+                                                     V value,\n+                                                     Iterable<Header> headers) {\n+\n+        List<String> mergedTopics = new ArrayList<>();\n+        mergedTopics.addAll(properties.getTopicNameList());\n+        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n+\n+        if (mergedTopics.isEmpty()) {\n+            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDg4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880887", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -30,110 +28,62 @@ import io.helidon.config.Config;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerRecord;\n import org.apache.kafka.clients.producer.RecordMetadata;\n-import org.apache.kafka.common.header.Header;\n \n /**\n  * Basic Kafka producer covering basic use-cases.\n  * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}.\n- * <p>\n- * Usage:\n- * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n- *             .produce(\"Hello world!\");\n- * }</pre>\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n  *\n  * @param <K> Key type\n  * @param <V> Value type\n- * @see KafkaConfigProperties\n+ * @see HelidonToKafkaConfigParser\n  * @see io.helidon.config.Config\n  */\n class BasicKafkaProducer<K, V> implements Closeable {\n \n     private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final KafkaConfigProperties properties;\n+    private final List<String> topics;\n     private final KafkaProducer<K, V> producer;\n \n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    BasicKafkaProducer(Config config) {\n-        properties = new KafkaConfigProperties(config);\n-        producer = new KafkaProducer<>(properties);\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n     }\n \n     /**\n-     * Send record to all provided topics,\n-     * blocking until all records are acknowledged by broker.\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n      *\n-     * @param value Will be serialized by <b>value.serializer</b> class\n-     *              defined in {@link KafkaConfigProperties configuration}\n-     * @return Server acknowledged metadata about sent topics\n+     * @param config Helidon {@link io.helidon.config.Config config}\n      */\n-    public List<RecordMetadata> produce(V value) {\n-        List<Future<RecordMetadata>> futureRecords =\n-                produceAsync(null, null, null, null, value, null);\n-        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n-\n-        for (Future<RecordMetadata> future : futureRecords) {\n-            try {\n-                metadataList.add(future.get());\n-            } catch (InterruptedException | ExecutionException e) {\n-                throw new RuntimeException(\"Failed to send topic\", e);\n-            }\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n         }\n-        return metadataList;\n-    }\n-\n-    /**\n-     * Produce asynchronously.\n-     *\n-     * @param value value to be produced\n-     * @return list of futures\n-     */\n-    public List<Future<RecordMetadata>> produceAsync(V value) {\n-        return produceAsync(null, null, null, null, value, null);\n     }\n \n     /**\n      * Send record to all provided topics, don't wait for server acknowledgement.\n      *\n-     * @param customTopics Can be null, list of topics appended to the list from configuration,\n-     *                     record will be sent to all topics iteratively\n-     * @param partition    Can be null, if key is also null topic is sent to random partition\n-     * @param timestamp    Can be null System.currentTimeMillis() is used then\n-     * @param key          Can be null, if not, topics are grouped to partitions by key\n      * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @param headers      Can be null, custom headers for additional meta information if needed\n      * @return Futures of server acknowledged metadata about sent topics\n      */\n-    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n-                                                     Integer partition,\n-                                                     Long timestamp,\n-                                                     K key,\n-                                                     V value,\n-                                                     Iterable<Header> headers) {\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");\n-            return Collections.emptyList();\n-        }\n-\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(mergedTopics.size());\n-\n-        for (String topic : mergedTopics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, partition, timestamp, key, value, headers);\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n             recordMetadataFutures.add(producer.send(record));\n         }\n         return recordMetadataFutures;\n     }\n \n+    /**\n+     * Closes the Kafka producer.\n+     */\n     @Override\n     public void close() {\n         LOGGER.fine(\"Closing kafka producer\");\n", "next_change": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 16ccf02fe..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Properties;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Properties properties = HelidonToKafkaConfigParser.toProperties(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(properties);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(properties));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nnew file mode 100644\nindex 000000000..7b1886500\n--- /dev/null\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n", "chunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see HelidonToKafkaConfigParser\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final List<String> topics;\n+    private final KafkaProducer<K, V> producer;\n+\n+    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n+        this.topics = topics;\n+        this.producer = producer;\n+    }\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link HelidonToKafkaConfigParser example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        } else {\n+            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n+        }\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    List<Future<RecordMetadata>> produceAsync(V value) {\n+        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n+            recordMetadataFutures.add(producer.send(record));\n+        }\n+        return recordMetadataFutures;\n+    }\n+\n+    /**\n+     * Closes the Kafka producer.\n+     */\n+    @Override\n+    public void close() {\n+        LOGGER.fine(\"Closing kafka producer\");\n+        producer.close();\n+    }\n+}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\ndeleted file mode 100644\nindex 7b1886500..000000000\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ /dev/null\n", "chunk": "@@ -1,92 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.microprofile.connectors.kafka;\n-\n-import java.io.Closeable;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.Future;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.apache.kafka.clients.producer.RecordMetadata;\n-\n-/**\n- * Basic Kafka producer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link HelidonToKafkaConfigParser}.\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see HelidonToKafkaConfigParser\n- * @see io.helidon.config.Config\n- */\n-class BasicKafkaProducer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n-    private final List<String> topics;\n-    private final KafkaProducer<K, V> producer;\n-\n-    private BasicKafkaProducer(List<String> topics, KafkaProducer<K, V> producer) {\n-        this.topics = topics;\n-        this.producer = producer;\n-    }\n-\n-    /**\n-     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n-     * see configuration {@link HelidonToKafkaConfigParser example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    static <K, V> BasicKafkaProducer<K, V> create(Config config){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n-        } else {\n-            return new BasicKafkaProducer<K, V>(topics, new KafkaProducer<>(kafkaConfig));\n-        }\n-    }\n-\n-    /**\n-     * Send record to all provided topics, don't wait for server acknowledgement.\n-     *\n-     * @param value        Will be serialized by value.serializer class defined in configuration\n-     * @return Futures of server acknowledged metadata about sent topics\n-     */\n-    List<Future<RecordMetadata>> produceAsync(V value) {\n-        List<Future<RecordMetadata>> recordMetadataFutures = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            ProducerRecord<K, V> record = new ProducerRecord<K, V>(topic, value);\n-            recordMetadataFutures.add(producer.send(record));\n-        }\n-        return recordMetadataFutures;\n-    }\n-\n-    /**\n-     * Closes the Kafka producer.\n-     */\n-    @Override\n-    public void close() {\n-        LOGGER.fine(\"Closing kafka producer\");\n-        producer.close();\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394997949", "body": "This is never used.", "bodyText": "This is never used.", "bodyHTML": "<p dir=\"auto\">This is never used.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:44:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDk4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880988", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:07:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 69%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex 8c3b1bee4..8d33fee53 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n", "chunk": "@@ -51,41 +52,44 @@ import io.helidon.config.Config;\n  *\n  * @see io.helidon.config.Config\n  */\n-class KafkaConfigProperties extends Properties {\n+class HelidonToKafkaConfigParser {\n \n     /**\n      * Topic or topics delimited by commas.\n      */\n-    static final String TOPIC_NAME = \"topic\";\n+    private static final String TOPIC_NAME = \"topic\";\n \n-    /**\n-     * Consumer group id.\n-     */\n-    static final String GROUP_ID = \"group.id\";\n+    private HelidonToKafkaConfigParser() {\n+    }\n \n     /**\n-     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n-     * underscores in keys are translated to dots.\n-     *\n-     * @param config parent config of kafka key\n+     * Builds the Kafka properties from Helidon Config.\n+     * @param config\n+     * @return the Kafka properties\n      */\n-    KafkaConfigProperties(Config config) {\n-        config.asNodeList().get().forEach(this::addProperty);\n+    static Properties toProperties(Config config) {\n+        Properties prop = new Properties();\n+        config.asNodeList().get().forEach(entry -> addProperty(entry, prop));\n+        return prop;\n     }\n \n     /**\n      * Split comma separated topic names.\n-     *\n+     * @param kafkaProperties\n      * @return list of topic names\n      */\n-    public List<String> getTopicNameList() {\n-        return Arrays.stream(getProperty(TOPIC_NAME)\n-                .split(\",\"))\n-                .map(String::trim)\n-                .collect(Collectors.toList());\n+    static List<String> topicNameList(Properties kafkaProperties) {\n+        String topics = kafkaProperties.getProperty(TOPIC_NAME);\n+        if (topics != null) {\n+            return Arrays.stream(topics.split(\",\"))\n+                    .map(String::trim)\n+                    .collect(Collectors.toList());\n+        } else {\n+            return Collections.emptyList();\n+        }\n     }\n \n-    private void addProperty(Config c) {\n+    private static void addProperty(Config c, Properties prop) {\n         String key = c.traverse().map(m -> m.key().parent().name() + \".\" + m.key().name())\n                 .collect(Collectors.joining(\".\"));\n         if (key.isEmpty()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394998973", "body": "This is not the correct way to do this (I mentioned this in my previous review.", "bodyText": "This is not the correct way to do this (I mentioned this in my previous review.", "bodyHTML": "<p dir=\"auto\">This is not the correct way to do this (I mentioned this in my previous review.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:46:33Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)\n+                .split(\",\"))\n+                .map(String::trim)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private void addProperty(Config c) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MjgwMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882800", "bodyText": "Sorry I didn't notice github was hiding some comments.", "author": "jbescos", "createdAt": "2020-03-25T14:09:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 69%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex 8c3b1bee4..8d33fee53 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n", "chunk": "@@ -51,41 +52,44 @@ import io.helidon.config.Config;\n  *\n  * @see io.helidon.config.Config\n  */\n-class KafkaConfigProperties extends Properties {\n+class HelidonToKafkaConfigParser {\n \n     /**\n      * Topic or topics delimited by commas.\n      */\n-    static final String TOPIC_NAME = \"topic\";\n+    private static final String TOPIC_NAME = \"topic\";\n \n-    /**\n-     * Consumer group id.\n-     */\n-    static final String GROUP_ID = \"group.id\";\n+    private HelidonToKafkaConfigParser() {\n+    }\n \n     /**\n-     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n-     * underscores in keys are translated to dots.\n-     *\n-     * @param config parent config of kafka key\n+     * Builds the Kafka properties from Helidon Config.\n+     * @param config\n+     * @return the Kafka properties\n      */\n-    KafkaConfigProperties(Config config) {\n-        config.asNodeList().get().forEach(this::addProperty);\n+    static Properties toProperties(Config config) {\n+        Properties prop = new Properties();\n+        config.asNodeList().get().forEach(entry -> addProperty(entry, prop));\n+        return prop;\n     }\n \n     /**\n      * Split comma separated topic names.\n-     *\n+     * @param kafkaProperties\n      * @return list of topic names\n      */\n-    public List<String> getTopicNameList() {\n-        return Arrays.stream(getProperty(TOPIC_NAME)\n-                .split(\",\"))\n-                .map(String::trim)\n-                .collect(Collectors.toList());\n+    static List<String> topicNameList(Properties kafkaProperties) {\n+        String topics = kafkaProperties.getProperty(TOPIC_NAME);\n+        if (topics != null) {\n+            return Arrays.stream(topics.split(\",\"))\n+                    .map(String::trim)\n+                    .collect(Collectors.toList());\n+        } else {\n+            return Collections.emptyList();\n+        }\n     }\n \n-    private void addProperty(Config c) {\n+    private static void addProperty(Config c, Properties prop) {\n         String key = c.traverse().map(m -> m.key().parent().name() + \".\" + m.key().name())\n                 .collect(Collectors.joining(\".\"));\n         if (key.isEmpty()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999609", "body": "Please use Helidon `ScheduledThreadPoolSupplier`, that is already fully configurable using Helidon `Config`", "bodyText": "Please use Helidon ScheduledThreadPoolSupplier, that is already fully configurable using Helidon Config", "bodyHTML": "<p dir=\"auto\">Please use Helidon <code>ScheduledThreadPoolSupplier</code>, that is already fully configurable using Helidon <code>Config</code></p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTY4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999686", "bodyText": "Also supports context propagation.", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDM4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000387", "bodyText": "scheduler = ScheduledThreadPoolSupplier.builder()\n                .threadNamePrefix(\"kafka-\")\n                .config(config)\n                .build()\n                .get();", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MTQ3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397881471", "bodyText": "Thanks for the information, I have modified it to that", "author": "jbescos", "createdAt": "2020-03-25T14:07:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -52,16 +51,16 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      * Microprofile messaging Kafka connector name.\n      */\n     static final String CONNECTOR_NAME = \"helidon-kafka\";\n-    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n     private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n     private final ScheduledExecutorService scheduler;\n-    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n-    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n \n     @Inject\n     KafkaConnectorFactory(Config config) {\n-        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n-                .asInt().asOptional().orElseGet(() -> 10));\n+        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n+        .config(config).build().get();\n     }\n \n     /**\n", "next_change": {"commit": "c63e74cbc47b8f480e92e3d0c804576d1b144061", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex e1e683ab2..12ccba402 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -57,8 +57,13 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     private final ScheduledExecutorService scheduler;\n     private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n \n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n     @Inject\n-    KafkaConnectorFactory(Config config) {\n+    public KafkaConnectorFactory(Config config) {\n         scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n         .config(config).build().get();\n     }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 77%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex 12ccba402..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -63,9 +61,12 @@ public class KafkaConnectorFactory implements IncomingConnectorFactory, Outgoing\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n     @Inject\n-    public KafkaConnectorFactory(Config config) {\n-        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n-        .config(config).build().get();\n+    public KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n     }\n \n     /**\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000643", "body": "This method should be private or package local.", "bodyText": "This method should be private or package local.", "bodyHTML": "<p dir=\"auto\">This method should be private or package local.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4Mjk0OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882948", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -52,16 +51,16 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      * Microprofile messaging Kafka connector name.\n      */\n     static final String CONNECTOR_NAME = \"helidon-kafka\";\n-    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n     private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n     private final ScheduledExecutorService scheduler;\n-    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n-    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n \n     @Inject\n     KafkaConnectorFactory(Config config) {\n-        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n-                .asInt().asOptional().orElseGet(() -> 10));\n+        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n+        .config(config).build().get();\n     }\n \n     /**\n", "next_change": {"commit": "c63e74cbc47b8f480e92e3d0c804576d1b144061", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex e1e683ab2..12ccba402 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -57,8 +57,13 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     private final ScheduledExecutorService scheduler;\n     private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n \n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n     @Inject\n-    KafkaConnectorFactory(Config config) {\n+    public KafkaConnectorFactory(Config config) {\n         scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n         .config(config).build().get();\n     }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 77%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex 12ccba402..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -63,9 +61,12 @@ public class KafkaConnectorFactory implements IncomingConnectorFactory, Outgoing\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n     @Inject\n-    public KafkaConnectorFactory(Config config) {\n-        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n-        .config(config).build().get();\n+    public KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n     }\n \n     /**\n", "next_change": null}]}}]}}, {"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -69,62 +68,50 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param event termination event\n      */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n-        BasicKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n+        List<Exception> failed = new LinkedList<>();\n+        Closeable closeable;\n+        while ((closeable = resourcesToClose.poll()) != null) {\n+            try {\n+                closeable.close();\n+            } catch (Exception e) {\n+                // Continue closing\n+                failed.add(e);\n+            }\n         }\n-        BasicKafkaProducer<Object, Object> producer;\n-        while ((producer = producers.poll()) != null) {\n-            producer.close();\n+        if (failed.isEmpty()) {\n+            LOGGER.fine(\"KafkaConnectorFactory terminated successfuly\");\n+        } else {\n+            // Inform about the errors\n+            failed.forEach(e -> LOGGER.log(Level.SEVERE, \"An error happened closing resource\", e));\n         }\n-        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n     }\n \n-    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n+    /**\n+     * Gets the open resources for testing verification purposes.\n+     * @return the opened resources\n+     */\n+    Queue<Closeable> resources(){\n+        return resourcesToClose;\n     }\n \n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n-        consumers.add(basicKafkaConsumer);\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n+        resourcesToClose.add(basicKafkaConsumer);\n         return basicKafkaConsumer.createPushPublisherBuilder();\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n-        producers.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                LOGGER.fine(\"On next received \" + message.getPayload());\n-                basicKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                LOGGER.fine(\"Subscriber has finished\");\n-                basicKafkaProducer.close();\n-            }\n-        });\n+        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n+        resourcesToClose.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n     }\n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 75%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex e1e683ab2..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -101,17 +107,15 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n-        resourcesToClose.add(basicKafkaConsumer);\n-        return basicKafkaConsumer.createPushPublisherBuilder();\n+        KafkaPublisher<Object, Object> publisher = KafkaPublisher.build(scheduler, helidonConfig);\n+        resourcesToClose.add(publisher);\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n-        resourcesToClose.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n+        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTEwMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001102", "body": "Do not use `info` log level so much. If you want to log an info message, just log a single one in this method.", "bodyText": "Do not use info log level so much. If you want to log an info message, just log a single one in this method.", "bodyHTML": "<p dir=\"auto\">Do not use <code>info</code> log level so much. If you want to log an info message, just log a single one in this method.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:35Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -69,62 +68,50 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param event termination event\n      */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n-        BasicKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n+        List<Exception> failed = new LinkedList<>();\n+        Closeable closeable;\n+        while ((closeable = resourcesToClose.poll()) != null) {\n+            try {\n+                closeable.close();\n+            } catch (Exception e) {\n+                // Continue closing\n+                failed.add(e);\n+            }\n         }\n-        BasicKafkaProducer<Object, Object> producer;\n-        while ((producer = producers.poll()) != null) {\n-            producer.close();\n+        if (failed.isEmpty()) {\n+            LOGGER.fine(\"KafkaConnectorFactory terminated successfuly\");\n+        } else {\n+            // Inform about the errors\n+            failed.forEach(e -> LOGGER.log(Level.SEVERE, \"An error happened closing resource\", e));\n         }\n-        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n     }\n \n-    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n+    /**\n+     * Gets the open resources for testing verification purposes.\n+     * @return the opened resources\n+     */\n+    Queue<Closeable> resources(){\n+        return resourcesToClose;\n     }\n \n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n-        consumers.add(basicKafkaConsumer);\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n+        resourcesToClose.add(basicKafkaConsumer);\n         return basicKafkaConsumer.createPushPublisherBuilder();\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n-        producers.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                LOGGER.fine(\"On next received \" + message.getPayload());\n-                basicKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                LOGGER.fine(\"Subscriber has finished\");\n-                basicKafkaProducer.close();\n-            }\n-        });\n+        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n+        resourcesToClose.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n     }\n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 75%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex e1e683ab2..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -101,17 +107,15 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n-        resourcesToClose.add(basicKafkaConsumer);\n-        return basicKafkaConsumer.createPushPublisherBuilder();\n+        KafkaPublisher<Object, Object> publisher = KafkaPublisher.build(scheduler, helidonConfig);\n+        resourcesToClose.add(publisher);\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n-        resourcesToClose.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n+        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001252", "body": "Method should be package local.", "bodyText": "Method should be package local.", "bodyHTML": "<p dir=\"auto\">Method should be package local.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzM1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883353", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -69,62 +68,50 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param event termination event\n      */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n-        BasicKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n+        List<Exception> failed = new LinkedList<>();\n+        Closeable closeable;\n+        while ((closeable = resourcesToClose.poll()) != null) {\n+            try {\n+                closeable.close();\n+            } catch (Exception e) {\n+                // Continue closing\n+                failed.add(e);\n+            }\n         }\n-        BasicKafkaProducer<Object, Object> producer;\n-        while ((producer = producers.poll()) != null) {\n-            producer.close();\n+        if (failed.isEmpty()) {\n+            LOGGER.fine(\"KafkaConnectorFactory terminated successfuly\");\n+        } else {\n+            // Inform about the errors\n+            failed.forEach(e -> LOGGER.log(Level.SEVERE, \"An error happened closing resource\", e));\n         }\n-        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n     }\n \n-    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n+    /**\n+     * Gets the open resources for testing verification purposes.\n+     * @return the opened resources\n+     */\n+    Queue<Closeable> resources(){\n+        return resourcesToClose;\n     }\n \n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n-        consumers.add(basicKafkaConsumer);\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n+        resourcesToClose.add(basicKafkaConsumer);\n         return basicKafkaConsumer.createPushPublisherBuilder();\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n-        producers.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                LOGGER.fine(\"On next received \" + message.getPayload());\n-                basicKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                LOGGER.fine(\"Subscriber has finished\");\n-                basicKafkaProducer.close();\n-            }\n-        });\n+        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n+        resourcesToClose.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n     }\n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 75%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex e1e683ab2..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -101,17 +107,15 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n-        resourcesToClose.add(basicKafkaConsumer);\n-        return basicKafkaConsumer.createPushPublisherBuilder();\n+        KafkaPublisher<Object, Object> publisher = KafkaPublisher.build(scheduler, helidonConfig);\n+        resourcesToClose.add(publisher);\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n-        resourcesToClose.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n+        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001708", "body": "You should close the producer in `onError` as well, as `onComplete` may never be called.", "bodyText": "You should close the producer in onError as well, as onComplete may never be called.", "bodyHTML": "<p dir=\"auto\">You should close the producer in <code>onError</code> as well, as <code>onComplete</code> may never be called.</p>", "author": "tomas-langer", "createdAt": "2020-03-19T12:51:42Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n+        consumers.add(basicKafkaConsumer);\n+        return basicKafkaConsumer.createPushPublisherBuilder();\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n+        producers.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                LOGGER.fine(\"On next received \" + message.getPayload());\n+                basicKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883566", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -69,62 +68,50 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param event termination event\n      */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n-        BasicKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n+        List<Exception> failed = new LinkedList<>();\n+        Closeable closeable;\n+        while ((closeable = resourcesToClose.poll()) != null) {\n+            try {\n+                closeable.close();\n+            } catch (Exception e) {\n+                // Continue closing\n+                failed.add(e);\n+            }\n         }\n-        BasicKafkaProducer<Object, Object> producer;\n-        while ((producer = producers.poll()) != null) {\n-            producer.close();\n+        if (failed.isEmpty()) {\n+            LOGGER.fine(\"KafkaConnectorFactory terminated successfuly\");\n+        } else {\n+            // Inform about the errors\n+            failed.forEach(e -> LOGGER.log(Level.SEVERE, \"An error happened closing resource\", e));\n         }\n-        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n     }\n \n-    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n+    /**\n+     * Gets the open resources for testing verification purposes.\n+     * @return the opened resources\n+     */\n+    Queue<Closeable> resources(){\n+        return resourcesToClose;\n     }\n \n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n-        consumers.add(basicKafkaConsumer);\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n+        resourcesToClose.add(basicKafkaConsumer);\n         return basicKafkaConsumer.createPushPublisherBuilder();\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n-        producers.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                LOGGER.fine(\"On next received \" + message.getPayload());\n-                basicKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                LOGGER.fine(\"Subscriber has finished\");\n-                basicKafkaProducer.close();\n-            }\n-        });\n+        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n+        resourcesToClose.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n     }\n }\n", "next_change": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nsimilarity index 75%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nindex e1e683ab2..e98e25bb4 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -101,17 +107,15 @@ class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = BasicKafkaConsumer.create(helidonConfig, scheduler);\n-        resourcesToClose.add(basicKafkaConsumer);\n-        return basicKafkaConsumer.createPushPublisherBuilder();\n+        KafkaPublisher<Object, Object> publisher = KafkaPublisher.build(scheduler, helidonConfig);\n+        resourcesToClose.add(publisher);\n+        return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        long backpressure = helidonConfig.get(BACKPRESSURE_SIZE_KEY).asLong().asOptional().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        BasicKafkaProducer<Object, Object> basicKafkaProducer = BasicKafkaProducer.create(helidonConfig);\n-        resourcesToClose.add(basicKafkaProducer);\n-        return ReactiveStreams.fromSubscriber(new BasicSubscriber<>(basicKafkaProducer, backpressure));\n+        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n", "next_change": null}]}}]}}, {"oid": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "url": "https://github.com/oracle/helidon/commit/e260c32d99f3f0078ee691ebf7063b02e7aba188", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T11:18:43Z", "type": "forcePushed"}, {"oid": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "url": "https://github.com/oracle/helidon/commit/c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T12:07:54Z", "type": "forcePushed"}, {"oid": "07a11a58331466f830337cf71cd033aec1022418", "url": "https://github.com/oracle/helidon/commit/07a11a58331466f830337cf71cd033aec1022418", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T13:59:28Z", "type": "forcePushed"}, {"oid": "c63e74cbc47b8f480e92e3d0c804576d1b144061", "url": "https://github.com/oracle/helidon/commit/c63e74cbc47b8f480e92e3d0c804576d1b144061", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-26T11:51:26Z", "type": "forcePushed"}, {"oid": "10612d66d9b6094133053f2f2778682db3616ef3", "url": "https://github.com/oracle/helidon/commit/10612d66d9b6094133053f2f2778682db3616ef3", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-27T23:33:51Z", "type": "forcePushed"}, {"oid": "aad54c4aefca8c3c7235adc20db2515630a88681", "url": "https://github.com/oracle/helidon/commit/aad54c4aefca8c3c7235adc20db2515630a88681", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T06:42:11Z", "type": "forcePushed"}, {"oid": "ede74fc9dedaffdf9b79684633b7b33e341ff5db", "url": "https://github.com/oracle/helidon/commit/ede74fc9dedaffdf9b79684633b7b33e341ff5db", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T07:05:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMjY2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r400112666", "body": "NPE rethrow was intentional, as its reserved as a signal for upstream [(\u00a72.13)](https://github.com/reactive-streams/reactive-streams-jvm#2.13) and this is not the place we are able to solve kafka client errors. On the other hand I am more and more convinced we should remove abstraction layer between `BasicKafkaConsumer`, `BasicKafkaPublisher`,  `EmittingPublisher` and create one specialized `KafkaConsumingPublisher`, which can be used in both Helidon MP and SE", "bodyText": "NPE rethrow was intentional, as its reserved as a signal for upstream (\u00a72.13) and this is not the place we are able to solve kafka client errors. On the other hand I am more and more convinced we should remove abstraction layer between BasicKafkaConsumer, BasicKafkaPublisher,  EmittingPublisher and create one specialized KafkaConsumingPublisher, which can be used in both Helidon MP and SE", "bodyHTML": "<p dir=\"auto\">NPE rethrow was intentional, as its reserved as a signal for upstream <a href=\"https://github.com/reactive-streams/reactive-streams-jvm#2.13\">(\u00a72.13)</a> and this is not the place we are able to solve kafka client errors. On the other hand I am more and more convinced we should remove abstraction layer between <code>BasicKafkaConsumer</code>, <code>BasicKafkaPublisher</code>,  <code>EmittingPublisher</code> and create one specialized <code>KafkaConsumingPublisher</code>, which can be used in both Helidon MP and SE</p>", "author": "danielkec", "createdAt": "2020-03-30T11:17:04Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Optional<Callback<Long>> requestsCallback;\n+\n+    protected EmittingPublisher(Optional<Callback<Long>> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.ifPresent(callback -> callback.nofity(n));\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {\n+                    return false;\n+                }\n+                try {\n+                    publisher.subscriber.onNext(item);\n+                    return true;\n+                } catch (Throwable t) {\n+                    publisher.fail(t);\n+                    return false;\n+                }", "originalCommit": "dcf791bbc62232b546d3e492a4e9bfb127a7a7a3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex 223745ddb..d3f796647 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -146,6 +150,7 @@ class EmittingPublisher<T> implements Publisher<T> {\n                     publisher.subscriber.onNext(item);\n                     return true;\n                 } catch (Throwable t) {\n+                    // We need to catch the error here because emit is invoked in other context\n                     publisher.fail(t);\n                     return false;\n                 }\n", "next_change": {"commit": "89106e8b66fc27f890e8515fc484f72082c1a23b", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex d3f796647..f1595a402 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -159,13 +159,15 @@ class EmittingPublisher<T> implements Publisher<T> {\n         CANCELLED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                throw new IllegalStateException(\"Emitter is cancelled!\");\n+                // No-op\n+                return false;\n             }\n         },\n         COMPLETED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                throw new IllegalStateException(\"Emitter is completed!\");\n+                // No-op\n+                return false;\n             }\n         };\n \n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex f1595a402..198b0eb5a 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -159,15 +156,13 @@ class EmittingPublisher<T> implements Publisher<T> {\n         CANCELLED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                // No-op\n-                return false;\n+                throw new IllegalStateException(\"Emitter is cancelled!\");\n             }\n         },\n         COMPLETED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                // No-op\n-                return false;\n+                throw new IllegalStateException(\"Emitter is completed!\");\n             }\n         };\n \n", "next_change": {"commit": "32be94abc930b1516e6eca952fd4e379619563c1", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex 198b0eb5a..f1595a402 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -156,13 +159,15 @@ class EmittingPublisher<T> implements Publisher<T> {\n         CANCELLED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                throw new IllegalStateException(\"Emitter is cancelled!\");\n+                // No-op\n+                return false;\n             }\n         },\n         COMPLETED {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                throw new IllegalStateException(\"Emitter is completed!\");\n+                // No-op\n+                return false;\n             }\n         };\n \n", "next_change": null}]}}]}}]}}]}}, {"oid": "4f234782d06edb4522b570f6f3c42e6d90e41866", "url": "https://github.com/oracle/helidon/commit/4f234782d06edb4522b570f6f3c42e6d90e41866", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:43:40Z", "type": "forcePushed"}, {"oid": "1a55afff628b5ccfb12aac8637b96b8550f77569", "url": "https://github.com/oracle/helidon/commit/1a55afff628b5ccfb12aac8637b96b8550f77569", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:53:47Z", "type": "forcePushed"}, {"oid": "47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "url": "https://github.com/oracle/helidon/commit/47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T18:45:28Z", "type": "forcePushed"}, {"oid": "434d47cdac68eedd818d8e4e74d60b553343d6f2", "url": "https://github.com/oracle/helidon/commit/434d47cdac68eedd818d8e4e74d60b553343d6f2", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T06:24:43Z", "type": "forcePushed"}, {"oid": "7fc5933aa86e4078b94479a1f6138996f560a714", "url": "https://github.com/oracle/helidon/commit/7fc5933aa86e4078b94479a1f6138996f560a714", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T07:16:57Z", "type": "forcePushed"}, {"oid": "e594ef5c5ada615d6aa957e7109f10fdab5caf09", "url": "https://github.com/oracle/helidon/commit/e594ef5c5ada615d6aa957e7109f10fdab5caf09", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T10:39:01Z", "type": "forcePushed"}, {"oid": "f2e4290553f1092f9033a01b928e13843519e282", "url": "https://github.com/oracle/helidon/commit/f2e4290553f1092f9033a01b928e13843519e282", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T06:23:31Z", "type": "forcePushed"}, {"oid": "82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "url": "https://github.com/oracle/helidon/commit/82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "message": "Refactoring\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T07:28:03Z", "type": "forcePushed"}, {"oid": "716fd4729baca6548038a27ada9f6dbe6bd9acac", "url": "https://github.com/oracle/helidon/commit/716fd4729baca6548038a27ada9f6dbe6bd9acac", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T08:14:58Z", "type": "forcePushed"}, {"oid": "89106e8b66fc27f890e8515fc484f72082c1a23b", "url": "https://github.com/oracle/helidon/commit/89106e8b66fc27f890e8515fc484f72082c1a23b", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:44:17Z", "type": "forcePushed"}, {"oid": "3aac54995e0518abe8d0da8c50bc26fb69ebc591", "url": "https://github.com/oracle/helidon/commit/3aac54995e0518abe8d0da8c50bc26fb69ebc591", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:45:30Z", "type": "forcePushed"}, {"oid": "cd91f520f05970477ea967c68ef7119789ccf874", "url": "https://github.com/oracle/helidon/commit/cd91f520f05970477ea967c68ef7119789ccf874", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T09:46:39Z", "type": "forcePushed"}, {"oid": "fd9112ad67622206ce02eaeec03751c1199671e9", "url": "https://github.com/oracle/helidon/commit/fd9112ad67622206ce02eaeec03751c1199671e9", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T11:04:04Z", "type": "forcePushed"}, {"oid": "22dc4e2f129a748097971fa3e1cf8999ba083ce4", "url": "https://github.com/oracle/helidon/commit/22dc4e2f129a748097971fa3e1cf8999ba083ce4", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T12:33:05Z", "type": "forcePushed"}, {"oid": "e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "url": "https://github.com/oracle/helidon/commit/e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:38:27Z", "type": "forcePushed"}, {"oid": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "url": "https://github.com/oracle/helidon/commit/f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:40:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MjYyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406472620", "body": "Spotbugs is \"picky\", `maxEvents` doesn't have to be volatile at all. Kafka publisher should be unbounded in runtime so `maxEvents` shoudn't be needed. It can be tested like this:\r\n```java\r\npublic Publisher<KafkaMessage<String, Long>> createPublisher(long elements) {\r\n...\r\n        return ReactiveStreams.fromPublisher(\r\n                KafkaPublisher.build(Executors.newScheduledThreadPool(2), \r\n                        kafkaConsumer, \r\n                        Arrays.asList(TEST_TOPIC_1), \r\n                        1L, \r\n                        POLL_TIMEOUT, \r\n                        true))\r\n                .limit(elements);\r\n```", "bodyText": "Spotbugs is \"picky\", maxEvents doesn't have to be volatile at all. Kafka publisher should be unbounded in runtime so maxEvents shoudn't be needed. It can be tested like this:\npublic Publisher<KafkaMessage<String, Long>> createPublisher(long elements) {\n...\n        return ReactiveStreams.fromPublisher(\n                KafkaPublisher.build(Executors.newScheduledThreadPool(2), \n                        kafkaConsumer, \n                        Arrays.asList(TEST_TOPIC_1), \n                        1L, \n                        POLL_TIMEOUT, \n                        true))\n                .limit(elements);", "bodyHTML": "<p dir=\"auto\">Spotbugs is \"picky\", <code>maxEvents</code> doesn't have to be volatile at all. Kafka publisher should be unbounded in runtime so <code>maxEvents</code> shoudn't be needed. It can be tested like this:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"public Publisher&lt;KafkaMessage&lt;String, Long&gt;&gt; createPublisher(long elements) {\n...\n        return ReactiveStreams.fromPublisher(\n                KafkaPublisher.build(Executors.newScheduledThreadPool(2), \n                        kafkaConsumer, \n                        Arrays.asList(TEST_TOPIC_1), \n                        1L, \n                        POLL_TIMEOUT, \n                        true))\n                .limit(elements);\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">Publisher&lt;<span class=\"pl-k\">KafkaMessage&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">Long</span>&gt;</span>&gt;</span> createPublisher(<span class=\"pl-k\">long</span> elements) {\n<span class=\"pl-c1\">...</span>\n        <span class=\"pl-k\">return</span> <span class=\"pl-smi\">ReactiveStreams</span><span class=\"pl-k\">.</span>fromPublisher(\n                <span class=\"pl-smi\">KafkaPublisher</span><span class=\"pl-k\">.</span>build(<span class=\"pl-smi\">Executors</span><span class=\"pl-k\">.</span>newScheduledThreadPool(<span class=\"pl-c1\">2</span>), \n                        kafkaConsumer, \n                        <span class=\"pl-smi\">Arrays</span><span class=\"pl-k\">.</span>asList(<span class=\"pl-c1\">TEST_TOPIC_1</span>), \n                        <span class=\"pl-c1\">1L</span>, \n                        <span class=\"pl-c1\">POLL_TIMEOUT</span>, \n                        <span class=\"pl-c1\">true</span>))\n                .limit(elements);</pre></div>", "author": "danielkec", "createdAt": "2020-04-09T20:56:46Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..4ba9a73d2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -130,11 +131,14 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n                                     break;\n                                 }\n                                 ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n                                 if (!autoCommit) {\n                                     TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n                                             kafkaMessage.getPayload().partition());\n                                     pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                } else {\n+                                    kafkaCommit.complete(null);\n                                 }\n                                 // Note that next execution will reach the user code inside @Incoming method.\n                                 // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n", "next_change": {"commit": "b0c66b2fff95eb07754c409c655227e897c886c5", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 4ba9a73d2..6d3dc5dc7 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -144,7 +137,6 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n                                 // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n                                 runInNewContext(() ->  emiter.emit(kafkaMessage));\n                                 requests.decrementAndGet();\n-                                maxEvents--;\n                             }\n                         }\n                     } else {\n", "next_change": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nsimilarity index 79%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 6d3dc5dc7..3bb4a673a 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -133,10 +132,8 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n                                 } else {\n                                     kafkaCommit.complete(null);\n                                 }\n-                                // Note that next execution will reach the user code inside @Incoming method.\n-                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n-                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n                                 requests.decrementAndGet();\n+                                CompletableFuture.runAsync(() -> runInNewContext(() ->  emiter.emit(kafkaMessage)));\n                             }\n                         }\n                     } else {\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406483079", "body": "Wakeup should normally get you from polling loop, but in our case its inside the scheduled runnable, It can be better to stop scheduler before wakeup and in the catch block jump out:\r\n```java\r\n} catch (WakeupException e) {\r\n  LOGGER.fine(() -> \"It was requested to stop polling from channel\");\r\n  return;\r\n}\r\n```", "bodyText": "Wakeup should normally get you from polling loop, but in our case its inside the scheduled runnable, It can be better to stop scheduler before wakeup and in the catch block jump out:\n} catch (WakeupException e) {\n  LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n  return;\n}", "bodyHTML": "<p dir=\"auto\">Wakeup should normally get you from polling loop, but in our case its inside the scheduled runnable, It can be better to stop scheduler before wakeup and in the catch block jump out:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"} catch (WakeupException e) {\n  LOGGER.fine(() -&gt; &quot;It was requested to stop polling from channel&quot;);\n  return;\n}\"><pre>} <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">WakeupException</span> e) {\n  <span class=\"pl-c1\">LOGGER</span><span class=\"pl-k\">.</span>fine(() <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>It was requested to stop polling from channel<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-k\">return</span>;\n}</pre></div>", "author": "danielkec", "createdAt": "2020-04-09T21:18:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5MTQ3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r407991472", "bodyText": "You already stopped it in connector, sorry", "author": "danielkec", "createdAt": "2020-04-14T09:22:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "b0c66b2fff95eb07754c409c655227e897c886c5", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..6d3dc5dc7 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -226,13 +226,15 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n         // Wait that current task finishes in case it is still running\n         try {\n             taskLock.lock();\n-            processACK();\n             LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n             // Terminate waiting ACKs\n-            pendingCommits.values().stream().flatMap(List::stream).forEach(message -> message.wakeUp());\n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n             kafkaConsumer.close();\n             emiter.complete();\n         } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n             emiter.fail(e);\n         } finally {\n             taskLock.unlock();\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nsimilarity index 52%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 6d3dc5dc7..8b4438b18 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -232,10 +223,9 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n             .forEach(message ->\n             message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n             kafkaConsumer.close();\n-            emiter.complete();\n+            emitter.complete();\n         } catch (RuntimeException e) {\n-            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n-            emiter.fail(e);\n+            emitter.fail(e);\n         } finally {\n             taskLock.unlock();\n         }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4NDE1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406484151", "body": "Can be swallowed by used code, would be great to log it", "bodyText": "Can be swallowed by used code, would be great to log it", "bodyHTML": "<p dir=\"auto\">Can be swallowed by used code, would be great to log it</p>", "author": "danielkec", "createdAt": "2020-04-09T21:20:31Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            processACK();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream).forEach(message -> message.wakeUp());\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..4ba9a73d2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -229,10 +236,13 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n             processACK();\n             LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n             // Terminate waiting ACKs\n-            pendingCommits.values().stream().flatMap(List::stream).forEach(message -> message.wakeUp());\n+            pendingCommits.values().stream().flatMap(List::stream).\n+            forEach(message -> \n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n             kafkaConsumer.close();\n             emiter.complete();\n         } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n             emiter.fail(e);\n         } finally {\n             taskLock.unlock();\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nsimilarity index 52%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 4ba9a73d2..8b4438b18 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -200,50 +196,36 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n                 }\n             }\n             if (!messagesToCommit.isEmpty()) {\n-                Optional<RuntimeException> exception = commitInKafka(offsets);\n-                messagesToCommit.stream().forEach(message -> {\n-                    exception.ifPresentOrElse(\n-                            ex -> message.kafkaCommit().completeExceptionally(ex),\n-                            () -> message.kafkaCommit().complete(null));\n-                });\n+                LOGGER.fine(() -> String.format(\"Offsets %s\", offsets));\n+                try {\n+                    kafkaConsumer.commitSync(offsets);\n+                    messagesToCommit.stream().forEach(message -> message.kafkaCommit().complete(null));\n+                } catch (RuntimeException e) {\n+                    LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+                    messagesToCommit.stream().forEach(message -> message.kafkaCommit().completeExceptionally(e));\n+                }\n             }\n         }\n     }\n \n-    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n-        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n-        LOGGER.fine(() -> String.format(\"%s\", offsets));\n-        try {\n-            kafkaConsumer.commitSync(offsets);\n-            LOGGER.fine(() -> \"The commit was successful\");\n-            return Optional.empty();\n-        } catch (RuntimeException e) {\n-            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n-            return Optional.of(e);\n-        }\n-    }\n-\n     /**\n      * Closes the connections to Kafka and stops to process new events.\n      */\n-    @Override\n-    public void close() {\n+    public void stop() {\n         // Stops pooling\n         kafkaConsumer.wakeup();\n         // Wait that current task finishes in case it is still running\n         try {\n             taskLock.lock();\n-            processACK();\n             LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n             // Terminate waiting ACKs\n-            pendingCommits.values().stream().flatMap(List::stream).\n-            forEach(message -> \n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n             message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n             kafkaConsumer.close();\n-            emiter.complete();\n+            emitter.complete();\n         } catch (RuntimeException e) {\n-            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n-            emiter.fail(e);\n+            emitter.fail(e);\n         } finally {\n             taskLock.unlock();\n         }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjc4MjE0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406782143", "body": "ack is going to be called frequently, can we avoid spinning up ForkJoinPool and reuse emit loop we already have? I know it introduces another queue ", "bodyText": "ack is going to be called frequently, can we avoid spinning up ForkJoinPool and reuse emit loop we already have? I know it introduces another queue", "bodyHTML": "<p dir=\"auto\">ack is going to be called frequently, can we avoid spinning up ForkJoinPool and reuse emit loop we already have? I know it introduces another queue</p>", "author": "danielkec", "createdAt": "2020-04-10T14:27:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaMessage.class.getName());\n+    private final ConsumerRecord<K, V> consumerRecord;\n+    private final AtomicBoolean ack = new AtomicBoolean(false);\n+    private final CountDownLatch waitForCommit;\n+    private final long millisWaitingTimeout;\n+    private final AtomicReference<Exception> ackException = new AtomicReference<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     * @param autoCommit when false it will ack will wait till it is really commited in Kafka,\n+     *        otherwise there is no waiting time because it was committed already.\n+     * @param millisWaitingTimeout this is the time in milliseconds that the ack will be waiting\n+     *        the commit in Kafka. Applies only if autoCommit is false.\n+     */\n+    KafkaMessage(ConsumerRecord<K, V> consumerRecord, boolean autoCommit, long millisWaitingTimeout) {\n+        this.consumerRecord = consumerRecord;\n+        this.waitForCommit = new CountDownLatch(autoCommit ? 0 : 1);\n+        this.millisWaitingTimeout = millisWaitingTimeout;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ack.set(true);\n+        return CompletableFuture.runAsync(() -> {", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "800055dd4a6dc3ded303ab27234f22af832ded5b", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 8486d63ff..c4c7d4d97 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -67,7 +67,7 @@ class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n \n     @Override\n     public CompletionStage<Void> ack() {\n-        ack.set(true);\n+        ack = true;\n         return CompletableFuture.runAsync(() -> {\n             boolean inTime = false;\n             try {\n", "next_change": {"commit": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex c4c7d4d97..6887b9b2b 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -68,24 +61,7 @@ class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n     @Override\n     public CompletionStage<Void> ack() {\n         ack = true;\n-        return CompletableFuture.runAsync(() -> {\n-            boolean inTime = false;\n-            try {\n-                LOGGER.fine(() -> consumerRecord.value() + \" is waiting for ACK\");\n-                inTime = waitForCommit.await(millisWaitingTimeout, TimeUnit.MILLISECONDS);\n-                if (!inTime) {\n-                    throw new TimeoutException(\"ACK timed out\");\n-                }\n-            } catch (InterruptedException | TimeoutException e) {\n-                ackException.set(e);\n-                LOGGER.log(Level.SEVERE, \"ACK terminated unexpectedly\", e);\n-            }\n-            LOGGER.fine(() -> consumerRecord.value() + \" finished ACK\");\n-            Exception e = ackException.get();\n-            if (e != null) {\n-                throw new CompletionException(e);\n-            }\n-        });\n+        return kafkaCommit.orTimeout(millisWaitingTimeout, TimeUnit.MILLISECONDS);\n     }\n \n     @Override\n", "next_change": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaMessage.java\nsimilarity index 92%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaMessage.java\nindex 6887b9b2b..9d479037d 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaMessage.java\n", "chunk": "@@ -60,7 +60,7 @@ class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n \n     @Override\n     public CompletionStage<Void> ack() {\n-        ack = true;\n+        ack.getAndSet(true);\n         return kafkaCommit.orTimeout(millisWaitingTimeout, TimeUnit.MILLISECONDS);\n     }\n \n", "next_change": null}]}}]}}]}}, {"oid": "800055dd4a6dc3ded303ab27234f22af832ded5b", "url": "https://github.com/oracle/helidon/commit/800055dd4a6dc3ded303ab27234f22af832ded5b", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T05:19:31Z", "type": "forcePushed"}, {"oid": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "url": "https://github.com/oracle/helidon/commit/9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:36:01Z", "type": "forcePushed"}, {"oid": "bb915d2406926c4a51ac9237bf797680511beb7a", "url": "https://github.com/oracle/helidon/commit/bb915d2406926c4a51ac9237bf797680511beb7a", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:39:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODExODAzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408118039", "body": "This is my bug, it should have had a bottom bound\r\n```java\r\nif (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {\r\n```", "bodyText": "This is my bug, it should have had a bottom bound\nif (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {", "bodyHTML": "<p dir=\"auto\">This is my bug, it should have had a bottom bound</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"if (publisher.requested.getAndUpdate(r -&gt; r &gt; 0 ? r - 1 : 0) &lt; 1) {\"><pre><span class=\"pl-k\">if</span> (publisher<span class=\"pl-k\">.</span>requested<span class=\"pl-k\">.</span>getAndUpdate(r <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> r <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">?</span> r <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">:</span> <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1</span>) {</pre></div>", "author": "danielkec", "createdAt": "2020-04-14T13:02:50Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    protected EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.nofity(n);\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+                EmittingPublisher.this.subscriber = null;\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    boolean isTerminated() {\n+        return terminated.get();\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b0c66b2fff95eb07754c409c655227e897c886c5", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex f1595a402..0bf164f73 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -143,7 +143,7 @@ class EmittingPublisher<T> implements Publisher<T> {\n         READY_TO_EMIT {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                if (publisher.requested.getAndDecrement() < 1) {\n+                if (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {\n                     return false;\n                 }\n                 try {\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nsimilarity index 87%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nindex 0bf164f73..abcd93924 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -143,9 +147,6 @@ class EmittingPublisher<T> implements Publisher<T> {\n         READY_TO_EMIT {\n             @Override\n             <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n-                if (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {\n-                    return false;\n-                }\n                 try {\n                     publisher.subscriber.onNext(item);\n                     return true;\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMjc4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408132788", "body": "Needs to be public to be usable in Helidon SE ", "bodyText": "Needs to be public to be usable in Helidon SE", "bodyHTML": "<p dir=\"auto\">Needs to be public to be usable in Helidon SE</p>", "author": "danielkec", "createdAt": "2020-04-14T13:24:24Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nsimilarity index 91%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 8f5e862cb..05948b418 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -61,7 +61,7 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n     @Inject\n-    KafkaConnector(Config config) {\n+    public KafkaConnector(Config config) {\n         scheduler = ScheduledThreadPoolSupplier.builder()\n                 .threadNamePrefix(\"kafka-\")\n                 .config(config)\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMzE4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408133180", "body": "Needs to be public to be usable in Helidon SE ", "bodyText": "Needs to be public to be usable in Helidon SE", "bodyHTML": "<p dir=\"auto\">Needs to be public to be usable in Helidon SE</p>", "author": "danielkec", "createdAt": "2020-04-14T13:24:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "changed_code": [{"header": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nsimilarity index 91%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 8f5e862cb..05948b418 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -70,12 +70,13 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n     }\n \n     /**\n-     * Called when container is terminated.\n+     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n+     * to terminate the messaging and release Kafka connections.\n      *\n      * @param event termination event\n      */\n-    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.fine(\"Terminating KafkaConnector...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n         List<Exception> failed = new LinkedList<>();\n", "next_change": null}]}}, {"oid": "b0c66b2fff95eb07754c409c655227e897c886c5", "url": "https://github.com/oracle/helidon/commit/b0c66b2fff95eb07754c409c655227e897c886c5", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T13:36:36Z", "type": "forcePushed"}, {"oid": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "url": "https://github.com/oracle/helidon/commit/29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T14:27:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408194418", "body": "Backpressure is not driven by anything here, use `.request(Long.MAX)` instead pls. no need for request counting", "bodyText": "Backpressure is not driven by anything here, use .request(Long.MAX) instead pls. no need for request counting", "bodyHTML": "<p dir=\"auto\">Backpressure is not driven by anything here, use <code>.request(Long.MAX)</code> instead pls. no need for request counting</p>", "author": "danielkec", "createdAt": "2020-04-14T14:45:38Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNTM0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408315349", "bodyText": "Backpressure has its place\n\nwhen reading messages from Kafka and delivering them to consumers, we should not deliver more than requested\nwhen writing message to Kafka and reading them from producers, we should not request more than we can send at that time\n\nIn both cases, requesting Long.MAX may result in memory issues, as the messages need to be buffered, or in thread issues, as you would block threads.", "author": "tomas-langer", "createdAt": "2020-04-14T17:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408339334", "bodyText": "No doubt about that, but without taking result of producer.produceAsync(message.getPayload()); in to the account its just unbounded stream with more bureaucracy", "author": "danielkec", "createdAt": "2020-04-14T18:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0MzYwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408343601", "bodyText": "Jorge already came with nice idea of combining callbacks", "author": "danielkec", "createdAt": "2020-04-14T18:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}], "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -35,19 +38,21 @@ import org.reactivestreams.Subscription;\n  *\n  * @param <T> kafka record value type\n  */\n-class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n     private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n     private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n     private final long backpressure;\n+    private final Producer<K, V> producer;\n+    private final List<String> topics;\n     private final AtomicLong backpressureCounter = new AtomicLong();\n-    private final BasicKafkaProducer<?, T> producer;\n     private Subscription subscription;\n \n-    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+    private KafkaSubscriber(Producer<K, V> producer, List<String> topics, long backpressure){\n         this.backpressure = backpressure;\n         this.producer = producer;\n+        this.topics = topics;\n     }\n \n     @Override\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 23abff5e0..bea7961b0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -35,29 +35,32 @@ import org.reactivestreams.Subscriber;\n import org.reactivestreams.Subscription;\n /**\n  * Reactive streams subscriber implementation.\n- *\n- * @param <T> kafka record value type\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n  */\n-class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n+public class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n     private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n-    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+\n     private final long backpressure;\n-    private final Producer<K, V> producer;\n+    private final Supplier<Producer<K, V>> producerSupplier;\n     private final List<String> topics;\n     private final AtomicLong backpressureCounter = new AtomicLong();\n+\n     private Subscription subscription;\n+    private Producer<K, V> kafkaProducer;\n \n-    private KafkaSubscriber(Producer<K, V> producer, List<String> topics, long backpressure){\n+    private KafkaSubscriber(Supplier<Producer<K, V>> producerSupplier, List<String> topics, long backpressure){\n         this.backpressure = backpressure;\n-        this.producer = producer;\n+        this.producerSupplier = producerSupplier;\n         this.topics = topics;\n     }\n \n     @Override\n     public void onSubscribe(Subscription subscription) {\n         if (this.subscription == null) {\n+            this.kafkaProducer = producerSupplier.get();\n             this.subscription = subscription;\n             this.subscription.request(backpressure);\n         } else {\n", "next_change": {"commit": "e17c7ab5db7ddb6861e52dd90796cf77806556ec", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex bea7961b0..c49f38778 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -59,11 +59,16 @@ public class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n \n     @Override\n     public void onSubscribe(Subscription subscription) {\n-        if (this.subscription == null) {\n-            this.kafkaProducer = producerSupplier.get();\n-            this.subscription = subscription;\n-            this.subscription.request(backpressure);\n-        } else {\n+        try {\n+            if (this.subscription == null) {\n+                this.kafkaProducer = producerSupplier.get();\n+                this.subscription = subscription;\n+                this.subscription.request(backpressure);\n+            } else {\n+                subscription.cancel();\n+            }\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Cannot start the Kafka producer\", e);\n             subscription.cancel();\n         }\n     }\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\ndeleted file mode 100644\nindex c49f38778..000000000\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ /dev/null\n", "chunk": "@@ -1,222 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.connectors.kafka;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.function.Supplier;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.Producer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-/**\n- * Reactive streams subscriber implementation.\n- * @param <K> kafka record key type\n- * @param <V> kafka record value type\n- */\n-public class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n-\n-    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n-    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n-\n-    private final long backpressure;\n-    private final Supplier<Producer<K, V>> producerSupplier;\n-    private final List<String> topics;\n-    private final AtomicLong backpressureCounter = new AtomicLong();\n-\n-    private Subscription subscription;\n-    private Producer<K, V> kafkaProducer;\n-\n-    private KafkaSubscriber(Supplier<Producer<K, V>> producerSupplier, List<String> topics, long backpressure){\n-        this.backpressure = backpressure;\n-        this.producerSupplier = producerSupplier;\n-        this.topics = topics;\n-    }\n-\n-    @Override\n-    public void onSubscribe(Subscription subscription) {\n-        try {\n-            if (this.subscription == null) {\n-                this.kafkaProducer = producerSupplier.get();\n-                this.subscription = subscription;\n-                this.subscription.request(backpressure);\n-            } else {\n-                subscription.cancel();\n-            }\n-        } catch (RuntimeException e) {\n-            LOGGER.log(Level.SEVERE, \"Cannot start the Kafka producer\", e);\n-            subscription.cancel();\n-        }\n-    }\n-\n-    @Override\n-    public void onNext(Message<V> message) {\n-        Objects.requireNonNull(message);\n-        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n-            futureList.add(completableFuture);\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n-            kafkaProducer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    subscription.cancel();\n-                    completableFuture.completeExceptionally(exception);\n-                } else {\n-                    completableFuture.complete(null);\n-                }\n-            });\n-        }\n-        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n-        .whenComplete((success, exception) -> {\n-            if (exception == null) {\n-                message.ack().whenComplete((a, b) -> {\n-                    if (backpressureCounter.incrementAndGet() == backpressure) {\n-                        backpressureCounter.set(0);\n-                        subscription.request(backpressure);\n-                    }\n-                });\n-            }\n-        });\n-    }\n-\n-    @Override\n-    public void onError(Throwable t) {\n-        Objects.requireNonNull(t);\n-        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        kafkaProducer.close();\n-    }\n-\n-    @Override\n-    public void onComplete() {\n-        LOGGER.fine(() -> \"Subscriber has finished\");\n-        kafkaProducer.close();\n-    }\n-\n-    /**\n-     * A builder for KafkaSubscriber.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @return builder to create a new instance\n-     */\n-    public static <K, V> Builder<K, V> builder() {\n-        return new Builder<>();\n-    }\n-\n-    /**\n-     * Load this builder from a configuration.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @param config configuration to load from\n-     * @return updated builder instance\n-     */\n-    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n-        return (KafkaSubscriber<K, V>) builder().config(config).build();\n-    }\n-\n-    /**\n-     * Fluent API builder for {@link KafkaSubscriber}.\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     */\n-    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n-\n-        private Supplier<Producer<K, V>> producerSupplier;\n-        private List<String> topics;\n-        private long backpressure = 5L;\n-\n-        private Builder() {\n-        }\n-\n-        @Override\n-        public KafkaSubscriber<K, V> build() {\n-            if (Objects.isNull(topics) || topics.isEmpty()) {\n-                throw new IllegalArgumentException(\"The topic is a required value\");\n-            }\n-            if (Objects.isNull(producerSupplier)) {\n-                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n-            }\n-            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n-        }\n-\n-        /**\n-         * Load this builder from a configuration.\n-         *\n-         * @param config configuration to load from\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> config(Config config) {\n-            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n-            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n-            topics(kafkaConfig.topics());\n-            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n-            return this;\n-        }\n-\n-        /**\n-         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n-         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param producerSupplier\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n-            this.producerSupplier = producerSupplier;\n-            return this;\n-        }\n-\n-        /**\n-         * Specifies the number of messages that are requested after processing them.\n-         *\n-         * The default value is 5.\n-         *\n-         * @param backpressure\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> backpressure(long backpressure) {\n-            this.backpressure = backpressure;\n-            return this;\n-        }\n-\n-        /**\n-         * The list of topics the messages should be sent to.\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param topics\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> topics(List<String> topics) {\n-            this.topics = topics;\n-            return this;\n-        }\n-    }\n-\n-}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nnew file mode 100644\nindex 000000000..f43e2d062\n--- /dev/null\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private final long backpressure;\n+    private final Producer<K, V> producer;\n+    private final List<String> topics;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(Producer<K, V> producer, List<String> topics, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+        this.topics = topics;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<V> message) {\n+        Objects.requireNonNull(message);\n+        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n+            futureList.add(completableFuture);\n+            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n+            producer.send(record, (metadata, exception) -> {\n+                if (exception != null) {\n+                    subscription.cancel();\n+                    completableFuture.completeExceptionally(exception);\n+                } else {\n+                    completableFuture.complete(null);\n+                }\n+            });\n+        }\n+        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n+        .whenComplete((success, exception) -> {\n+            if (exception == null) {\n+                message.ack().whenComplete((a, b) -> {\n+                    if (backpressureCounter.incrementAndGet() == backpressure) {\n+                        backpressureCounter.set(0);\n+                        subscription.request(backpressure);\n+                    }\n+                });\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n+        return new KafkaSubscriberBuilder<>(producer, topics);\n+    }\n+\n+    /**\n+     * Fluent API builder for {@link KafkaSubscriber}.\n+     */\n+    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private final Producer<K, V> producer;\n+        private final List<String> topics;\n+        private long backpressure = 5L;\n+\n+        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n+            this.producer = producer;\n+            this.topics = topics;\n+        }\n+\n+        @Override\n+        public KafkaSubscriber<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producer, topics, backpressure);\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> config(Config config) {\n+            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n+            return this;\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+            this.backpressure = backpressure;\n+            return this;\n+        }\n+    }\n+\n+}\n", "next_change": {"commit": "7081c593461d66f7eea754455b70d825a6a7cd96", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex f43e2d062..c49f38778 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -96,50 +107,116 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     public void onError(Throwable t) {\n         Objects.requireNonNull(t);\n         LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        producer.close();\n+        kafkaProducer.close();\n     }\n \n     @Override\n     public void onComplete() {\n-        LOGGER.fine(\"Subscriber has finished\");\n-        producer.close();\n+        LOGGER.fine(() -> \"Subscriber has finished\");\n+        kafkaProducer.close();\n+    }\n+\n+    /**\n+     * A builder for KafkaSubscriber.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @return builder to create a new instance\n+     */\n+    public static <K, V> Builder<K, V> builder() {\n+        return new Builder<>();\n     }\n \n-    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n-        return new KafkaSubscriberBuilder<>(producer, topics);\n+    /**\n+     * Load this builder from a configuration.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param config configuration to load from\n+     * @return updated builder instance\n+     */\n+    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n+        return (KafkaSubscriber<K, V>) builder().config(config).build();\n     }\n \n     /**\n      * Fluent API builder for {@link KafkaSubscriber}.\n+     * @param <K> Key type\n+     * @param <V> Value type\n      */\n-    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n \n-        private final Producer<K, V> producer;\n-        private final List<String> topics;\n+        private Supplier<Producer<K, V>> producerSupplier;\n+        private List<String> topics;\n         private long backpressure = 5L;\n \n-        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n-            this.producer = producer;\n-            this.topics = topics;\n+        private Builder() {\n         }\n \n         @Override\n         public KafkaSubscriber<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            return new KafkaSubscriber<>(producer, topics, backpressure);\n+            if (Objects.isNull(producerSupplier)) {\n+                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n         }\n \n-        KafkaSubscriberBuilder<K, V> config(Config config) {\n+        /**\n+         * Load this builder from a configuration.\n+         *\n+         * @param config configuration to load from\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> config(Config config) {\n+            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n+            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n+            topics(kafkaConfig.topics());\n             config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n             return this;\n         }\n \n-        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+        /**\n+         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n+         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param producerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n+            this.producerSupplier = producerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the number of messages that are requested after processing them.\n+         *\n+         * The default value is 5.\n+         *\n+         * @param backpressure\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> backpressure(long backpressure) {\n             this.backpressure = backpressure;\n             return this;\n         }\n+\n+        /**\n+         * The list of topics the messages should be sent to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI5MDY3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408290670", "body": "When you do less than warning levels of logging, please use the lambda approach:\r\n`Logger.fine(() -> String.format(\"...\", n))` - otherwise the string formatting is evaluated for every single request.", "bodyText": "When you do less than warning levels of logging, please use the lambda approach:\nLogger.fine(() -> String.format(\"...\", n)) - otherwise the string formatting is evaluated for every single request.", "bodyHTML": "<p dir=\"auto\">When you do less than warning levels of logging, please use the lambda approach:<br>\n<code>Logger.fine(() -&gt; String.format(\"...\", n))</code> - otherwise the string formatting is evaluated for every single request.</p>", "author": "tomas-langer", "createdAt": "2020-04-14T16:56:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nindex 872b32b38..c2eb941f7 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -56,7 +56,7 @@ class EmittingPublisher<T> implements Publisher<T> {\n                 if (n < 1) {\n                     fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n                 }\n-                LOGGER.fine(String.format(\"Request %s events\", n));\n+                LOGGER.fine(() -> String.format(\"Request %s events\", n));\n                 requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n                 requestsCallback.nofity(n);\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nindex c2eb941f7..abcd93924 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -36,41 +40,41 @@ import org.reactivestreams.Subscription;\n class EmittingPublisher<T> implements Publisher<T> {\n \n     private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n-    private Subscriber<? super T> subscriber;\n+\n     private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n-    private final AtomicLong requested = new AtomicLong();\n     private final AtomicBoolean terminated = new AtomicBoolean();\n-    private final Callback<Long> requestsCallback;\n+    private final Consumer<Long> requestsCallback;\n+\n+    private Flow.Subscriber<? super T> subscriber;\n \n-    EmittingPublisher(Callback<Long> requestsCallback) {\n+    EmittingPublisher(Consumer<Long> requestsCallback) {\n         this.requestsCallback = requestsCallback;\n     }\n \n     @Override\n     public void subscribe(Subscriber<? super T> subscriber) {\n         Objects.requireNonNull(subscriber, \"subscriber is null\");\n-        this.subscriber = subscriber;\n         subscriber.onSubscribe(new Subscription() {\n             @Override\n-            public void request(final long n) {\n+            public void request(long n) {\n                 if (n < 1) {\n                     fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n                 }\n                 LOGGER.fine(() -> String.format(\"Request %s events\", n));\n-                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n-                requestsCallback.nofity(n);\n+                requestsCallback.accept(n);\n             }\n \n             @Override\n             public void cancel() {\n-                LOGGER.fine(\"Subscription cancelled\");\n+                LOGGER.fine(() -> \"Subscription cancelled\");\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n                 state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n                 EmittingPublisher.this.subscriber = null;\n             }\n \n         });\n+        this.subscriber = SequentialSubscriber.create(FlowAdapters.toFlowSubscriber(subscriber));\n     }\n \n     /**\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nsimilarity index 82%\nrename from messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex abcd93924..198b0eb5a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n", "chunk": "@@ -40,41 +37,40 @@ import org.reactivestreams.Subscription;\n class EmittingPublisher<T> implements Publisher<T> {\n \n     private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n-\n+    private Subscriber<? super T> subscriber;\n     private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n     private final AtomicBoolean terminated = new AtomicBoolean();\n-    private final Consumer<Long> requestsCallback;\n-\n-    private Flow.Subscriber<? super T> subscriber;\n+    private final Optional<Callback<Long>> requestsCallback;\n \n-    EmittingPublisher(Consumer<Long> requestsCallback) {\n+    protected EmittingPublisher(Optional<Callback<Long>> requestsCallback) {\n         this.requestsCallback = requestsCallback;\n     }\n \n     @Override\n     public void subscribe(Subscriber<? super T> subscriber) {\n         Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n         subscriber.onSubscribe(new Subscription() {\n             @Override\n-            public void request(long n) {\n+            public void request(final long n) {\n                 if (n < 1) {\n                     fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n                 }\n-                LOGGER.fine(() -> String.format(\"Request %s events\", n));\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n-                requestsCallback.accept(n);\n+                requestsCallback.ifPresent(callback -> callback.nofity(n));\n             }\n \n             @Override\n             public void cancel() {\n-                LOGGER.fine(() -> \"Subscription cancelled\");\n+                LOGGER.fine(\"Subscription cancelled\");\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n                 state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n-                EmittingPublisher.this.subscriber = null;\n             }\n \n         });\n-        this.subscriber = SequentialSubscriber.create(FlowAdapters.toFlowSubscriber(subscriber));\n     }\n \n     /**\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408311905", "body": "If this class should be part of Helidon SE, then the public constructor is an issue.\r\nAlso you need to add a Builder for any configurable options - in Helidon SE, most of things that can be done using `Config` should be doable using `Builder` and vice-versa", "bodyText": "If this class should be part of Helidon SE, then the public constructor is an issue.\nAlso you need to add a Builder for any configurable options - in Helidon SE, most of things that can be done using Config should be doable using Builder and vice-versa", "bodyHTML": "<p dir=\"auto\">If this class should be part of Helidon SE, then the public constructor is an issue.<br>\nAlso you need to add a Builder for any configurable options - in Helidon SE, most of things that can be done using <code>Config</code> should be doable using <code>Builder</code> and vice-versa</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:30:06Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMjk2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408312966", "bodyText": "For correct SE/MP split, I think you should:\n\nRemove public constructor from this class\nCreate a builder that also supports config through config(Config) method as other builders in Helidon\nAdd a CDI extension for Kafka connector to another module, that would create the connector instance for MP messaging correctly injecting values using CDI", "author": "tomas-langer", "createdAt": "2020-04-14T17:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 05948b418..23cf93c25 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -60,8 +63,9 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n+    // FIXME Need to create a builder\n     @Inject\n-    public KafkaConnector(Config config) {\n+    KafkaConnector(Config config) {\n         scheduler = ScheduledThreadPoolSupplier.builder()\n                 .threadNamePrefix(\"kafka-\")\n                 .config(config)\n", "next_change": {"commit": "443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 23cf93c25..86e84ab6a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -63,7 +63,6 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n-    // FIXME Need to create a builder\n     @Inject\n     KafkaConnector(Config config) {\n         scheduler = ScheduledThreadPoolSupplier.builder()\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nsimilarity index 64%\nrename from messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 86e84ab6a..12ccba402 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -64,22 +63,18 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n     @Inject\n-    KafkaConnector(Config config) {\n-        scheduler = ScheduledThreadPoolSupplier.builder()\n-                .threadNamePrefix(\"kafka-\")\n-                .config(config)\n-                .build()\n-                .get();\n+    public KafkaConnectorFactory(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n+        .config(config).build().get();\n     }\n \n     /**\n-     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n-     * to terminate the messaging and release Kafka connections.\n+     * Called when container is terminated.\n      *\n      * @param event termination event\n      */\n     void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.fine(\"Terminating KafkaConnector...\");\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n         List<Exception> failed = new LinkedList<>();\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzIwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313201", "body": "`@Observes` methods should not be public", "bodyText": "@Observes methods should not be public", "bodyHTML": "<p dir=\"auto\"><code>@Observes</code> methods should not be public</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:32:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n+     * to terminate the messaging and release Kafka connections.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 05948b418..23cf93c25 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -75,7 +79,7 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      *\n      * @param event termination event\n      */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n         LOGGER.fine(\"Terminating KafkaConnector...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n", "next_change": {"commit": "127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 23cf93c25..4bb1e6977 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -80,25 +79,7 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      * @param event termination event\n      */\n     void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        LOGGER.fine(\"Terminating KafkaConnector...\");\n-        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n-        scheduler.shutdown();\n-        List<Exception> failed = new LinkedList<>();\n-        Closeable closeable;\n-        while ((closeable = resourcesToClose.poll()) != null) {\n-            try {\n-                closeable.close();\n-            } catch (Exception e) {\n-                // Continue closing\n-                failed.add(e);\n-            }\n-        }\n-        if (failed.isEmpty()) {\n-            LOGGER.fine(\"KafkaConnectorFactory terminated successfuly\");\n-        } else {\n-            // Inform about the errors\n-            failed.forEach(e -> LOGGER.log(Level.SEVERE, \"An error happened closing resource\", e));\n-        }\n+        close();\n     }\n \n     /**\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 4bb1e6977..368e1035f 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n", "chunk": "@@ -79,38 +76,27 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      * @param event termination event\n      */\n     void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        close();\n+        stop();\n     }\n \n     /**\n      * Gets the open resources for testing verification purposes.\n      * @return the opened resources\n      */\n-    Queue<Closeable> resources(){\n-        return resourcesToClose;\n+    Queue<KafkaPublisher<?, ?>> resources(){\n+        return resources;\n     }\n \n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        Map<String, Object> kafkaConfig  = HelidonToKafkaConfigParser.toMap(helidonConfig);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        KafkaPublisher<Object, Object> publisher = KafkaPublisher.KafkaPublisherBuilder\n-                .builderSE(scheduler, new KafkaConsumer<>(kafkaConfig), topics)\n-                .config(helidonConfig).build();\n-        resourcesToClose.add(publisher);\n+        KafkaPublisher<Object, Object> publisher = KafkaPublisher.builder().config((Config) config).scheduler(scheduler).build();\n+        resources.add(publisher);\n         return ReactiveStreams.fromPublisher(publisher);\n     }\n \n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        Map<String, Object> kafkaConfig  = HelidonToKafkaConfigParser.toMap(helidonConfig);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        KafkaSubscriber<Object, Object> subscriber = KafkaSubscriber.KafkaSubscriberBuilder\n-                .builderSE(new KafkaProducer<>(kafkaConfig), topics)\n-                .config(helidonConfig).build();\n-        return ReactiveStreams.fromSubscriber(subscriber);\n+        return ReactiveStreams.fromSubscriber(KafkaSubscriber.create((Config) config));\n     }\n \n     /**\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nsimilarity index 66%\nrename from messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 368e1035f..12ccba402 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n", "chunk": "@@ -61,65 +63,25 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n      * @param config Helidon {@link io.helidon.config.Config config}\n      */\n     @Inject\n-    KafkaConnector(Config config) {\n-        scheduler = ScheduledThreadPoolSupplier.builder()\n-                .threadNamePrefix(\"kafka-\")\n-                .config(config)\n-                .build()\n-                .get();\n+    public KafkaConnectorFactory(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder().threadNamePrefix(\"kafka-\")\n+        .config(config).build().get();\n     }\n \n     /**\n-     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n-     * to terminate the messaging and release Kafka connections.\n+     * Called when container is terminated.\n      *\n      * @param event termination event\n      */\n     void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        stop();\n-    }\n-\n-    /**\n-     * Gets the open resources for testing verification purposes.\n-     * @return the opened resources\n-     */\n-    Queue<KafkaPublisher<?, ?>> resources(){\n-        return resources;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        KafkaPublisher<Object, Object> publisher = KafkaPublisher.builder().config((Config) config).scheduler(scheduler).build();\n-        resources.add(publisher);\n-        return ReactiveStreams.fromPublisher(publisher);\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        return ReactiveStreams.fromSubscriber(KafkaSubscriber.create((Config) config));\n-    }\n-\n-    /**\n-     * Creates a new instance of KafkaConnector with the required configuration.\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     * @return the new instance\n-     */\n-    public static KafkaConnector create(Config config) {\n-        return new KafkaConnector(config);\n-    }\n-\n-    /**\n-     * Stops the KafkaConnector and all the jobs and resources related to it.\n-     */\n-    void stop() {\n-        LOGGER.fine(() -> \"Terminating KafkaConnector...\");\n+        LOGGER.fine(\"Terminating KafkaConnectorFactory...\");\n         // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n         scheduler.shutdown();\n         List<Exception> failed = new LinkedList<>();\n-        KafkaPublisher<?, ?> resource;\n-        while ((resource = resources.poll()) != null) {\n+        Closeable closeable;\n+        while ((closeable = resourcesToClose.poll()) != null) {\n             try {\n-                resource.stop();\n+                closeable.close();\n             } catch (Exception e) {\n                 // Continue closing\n                 failed.add(e);\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzk2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313968", "body": "This is a use case for a builder. ", "bodyText": "This is a use case for a builder.", "bodyHTML": "<p dir=\"auto\">This is a use case for a builder.</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:33:30Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,312 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                } else {\n+                                    kafkaCommit.complete(null);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * Those events that are committed will make the {@link KafkaMessage#ack()}\n+     * to complete.\n+     */\n+    private void processACK() {\n+        if (!autoCommit) {\n+            Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+            List<KafkaMessage<K, V>> messagesToCommit = new LinkedList<>();\n+            // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+            for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+                // No need to sort it, offsets are consumed in order\n+                List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+                Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+                KafkaMessage<K, V> highest = null;\n+                while (iterator.hasNext()) {\n+                    KafkaMessage<K, V> element = iterator.next();\n+                    if (element.isAck()) {\n+                        messagesToCommit.add(element);\n+                        highest = element;\n+                        iterator.remove();\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                if (highest != null) {\n+                    OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                    LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                    offsets.put(entry.getKey(), offset);\n+                }\n+            }\n+            if (!messagesToCommit.isEmpty()) {\n+                Optional<RuntimeException> exception = commitInKafka(offsets);\n+                messagesToCommit.stream().forEach(message -> {\n+                    exception.ifPresentOrElse(\n+                            ex -> message.kafkaCommit().completeExceptionally(ex),\n+                            () -> message.kafkaCommit().complete(null));\n+                });\n+            }\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n+        LOGGER.fine(() -> \"Closed\");\n+    }\n+\n+    //Move to messaging incoming connector\n+    protected void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     *\n+     * Note: after creating a KafkaPublisher you must always\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 3e7e4d74a..3bb4a673a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -270,43 +267,75 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n     }\n \n     /**\n-     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n-     *\n-     * Note: after creating a KafkaPublisher you must always\n-     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} it to avoid resource leaks.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @param scheduler It will trigger the task execution when\n-     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#execute()} is invoked\n-     * @param config With the KafkaPublisher required parameters\n-     * @return A new instance of ReactiveKafkaPublisher\n+     * Fluent API builder for {@link KafkaPublisher}.\n      */\n-    static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+    static final class KafkaPublisherBuilder<K, V> implements io.helidon.common.Builder<KafkaPublisher<K, V>> {\n+\n+        private long pollTimeout = 50L;\n+        private long periodExecutions = 100L;\n+        private boolean autoCommit = true;\n+        private long ackTimeout = Long.MAX_VALUE;\n+        private int limitNoAck = Integer.MAX_VALUE;\n+        private final List<String> topics;\n+        private final ScheduledExecutorService scheduler;\n+        private final Consumer<K, V> kafkaConsumer;\n+\n+        private KafkaPublisherBuilder(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+                List<String> topics) {\n+            this.scheduler = scheduler;\n+            this.kafkaConsumer = kafkaConsumer;\n+            this.topics = topics;\n+        }\n+\n+        static <K, V> KafkaPublisherBuilder<K, V> builderSE(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+                List<String> topics) {\n+            return new KafkaPublisherBuilder<>(scheduler, kafkaConsumer, topics);\n+        }\n+\n+        KafkaPublisherBuilder<K, V> config(Config config) {\n+            config.get(POLL_TIMEOUT).asLong().ifPresent(this::pollTimeout);\n+            config.get(PERIOD_EXECUTIONS).asLong().ifPresent(this::periodExecutions);\n+            config.get(ENABLE_AUTOCOMMIT).asBoolean().ifPresent(this::autoCommit);\n+            config.get(ACK_TIMEOUT).asLong().ifPresent(this::ackTimeout);\n+            config.get(LIMIT_NO_ACK).asInt().ifPresent(this::limitNoAck);\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> periodExecutions(long periodExecutions) {\n+            this.periodExecutions = periodExecutions;\n+            return this;\n         }\n-        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);\n-        long pollTimeout = config.get(POLL_TIMEOUT).asLong().orElse(50L);\n-        long periodExecutions = config.get(PERIOD_EXECUTIONS).asLong().orElse(100L);\n-        // Default Kafka value is true.\n-        boolean autoCommit = config.get(ENABLE_AUTOCOMMIT).asBoolean().orElse(true);\n-        long ackTimeout = config.get(ACK_TIMEOUT).asLong().orElse(Long.MAX_VALUE);\n-        int limitNoAck = config.get(LIMIT_NO_ACK).asInt().orElse(Integer.MAX_VALUE);\n-        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n-                pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n-        publisher.execute();\n-        return publisher;\n-    }\n \n-    // For testing purposes\n-    static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n-            List<String> topics, long pollTimeout, long periodExecutions, boolean autoCommit){\n-        KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n-                pollTimeout, periodExecutions, autoCommit, Long.MAX_VALUE, Integer.MAX_VALUE);\n-        publisher.execute();\n-        return publisher;\n+        KafkaPublisherBuilder<K, V> pollTimeout(long pollTimeout) {\n+            this.pollTimeout = pollTimeout;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> autoCommit(boolean autoCommit) {\n+            this.autoCommit = autoCommit;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> ackTimeout(long ackTimeout) {\n+            this.ackTimeout = ackTimeout;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> limitNoAck(int limitNoAck) {\n+            this.limitNoAck = limitNoAck;\n+            return this;\n+        }\n+\n+        @Override\n+        public KafkaPublisher<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n+                    pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n+            publisher.execute();\n+            return publisher;\n+        }\n     }\n+\n }\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 3bb4a673a..8b4438b18 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -301,39 +320,139 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> periodExecutions(long periodExecutions) {\n+        /**\n+         * Defines how to instantiate the KafkaConsumer. It will be invoked\n+         * in {@link KafkaPublisher#subscribe(Subscriber)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param consumerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> consumerSupplier(Supplier<Consumer<K, V>> consumerSupplier) {\n+            this.consumerSupplier = consumerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * The list of topics to subscribe to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n+\n+        /**\n+         * Specify a scheduler that will read ad process messages coming from Kafka.\n+         * Is it intended that this scheduler is reused for other tasks.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param scheduler\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> scheduler(ScheduledExecutorService scheduler) {\n+            this.scheduler = scheduler;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the period in milliseconds between successive scheduler executions.\n+         * The default value is 100 milliseconds.\n+         *\n+         * @param periodExecutions\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> periodExecutions(long periodExecutions) {\n             this.periodExecutions = periodExecutions;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> pollTimeout(long pollTimeout) {\n+        /**\n+         * Specifies maximum time in milliseconds to block polling messages from Kafka.\n+         * The default value is 50 milliseconds.\n+         *\n+         * @param pollTimeout\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> pollTimeout(long pollTimeout) {\n             this.pollTimeout = pollTimeout;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> autoCommit(boolean autoCommit) {\n+        /**\n+         * This flag defines the strategy of committing messages to Kafka.\n+         * When true, the messages are committed in the moment they are polled from Kafka.\n+         * When false, the messages are committed when {@link KafkaMessage#ack()} is invoked.\n+         *\n+         * This value is mandatory to be specified and it must be consistent with the value enable.auto.commit\n+         * in Kafka properties. Failing to do this will result the next scenarios:\n+         * - For autoCommit = true and enable.auto.commit = false, messages will never be committed in Kafka.\n+         * - For autoCommit = false and enable.auto.commit = true, all messages will be committed and\n+         * {@link KafkaMessage#ack()} will have no effect.\n+         *\n+         * @param autoCommit\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> autoCommit(boolean autoCommit) {\n             this.autoCommit = autoCommit;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> ackTimeout(long ackTimeout) {\n+        /**\n+         * This value applies only when autoCommit is set to false.\n+         * It defines the maximum time in milliseconds that {@link KafkaMessage#ack()} will be waiting\n+         * for the commit in Kafka.\n+         *\n+         * The default value is Long.MAX_VALUE\n+         *\n+         * @param ackTimeout\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> ackTimeout(long ackTimeout) {\n             this.ackTimeout = ackTimeout;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> limitNoAck(int limitNoAck) {\n+        /**\n+         * This value applies only when autoCommit is set to false.\n+         * It specifies the limit of messages waiting to be committed in Kafka.\n+         * If this value is overflown, the KafkaPublisher will notify a failure.\n+         *\n+         * The intention of this value is to fail gracefully when there are many pending commits,\n+         * instead of failing with OutOfMemoryError.\n+         *\n+         * @param limitNoAck\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> limitNoAck(int limitNoAck) {\n             this.limitNoAck = limitNoAck;\n             return this;\n         }\n \n         @Override\n         public KafkaPublisher<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n+            if (Objects.isNull(autoCommit)) {\n+                String message =\n+                    String.format(\"The autoCommit is a required value and be equals to KafkaProperty %s\", ENABLE_AUTOCOMMIT);\n+                throw new IllegalArgumentException(message);\n+            }\n+            if (Objects.isNull(scheduler)) {\n+                throw new IllegalArgumentException(\"The scheduler is a required value\");\n+            }\n+            if (Objects.isNull(consumerSupplier)) {\n+                throw new IllegalArgumentException(\"The kafkaConsumerSupplier is a required value\");\n+            }\n+            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, consumerSupplier, topics,\n                     pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n-            publisher.execute();\n             return publisher;\n         }\n     }\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\ndeleted file mode 100644\nindex 8b4438b18..000000000\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n+++ /dev/null\n", "chunk": "@@ -1,460 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.connectors.kafka;\n-\n-import java.time.Duration;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-import java.util.Objects;\n-import java.util.Queue;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReentrantLock;\n-import java.util.function.Supplier;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.consumer.Consumer;\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.reactivestreams.Publisher;\n-import org.reactivestreams.Subscriber;\n-\n-/**\n- * This is an implementation of {@link org.reactivestreams.Publisher} that read messages from\n- * Kafka and push them downstream to one subscriber.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see io.helidon.config.Config\n- */\n-public class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n-\n-    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n-    private static final String POLL_TIMEOUT = \"poll.timeout\";\n-    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n-    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n-    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n-    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n-\n-    private final Lock taskLock = new ReentrantLock();\n-    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final ScheduledExecutorService scheduler;\n-    private final AtomicLong requests = new AtomicLong();\n-    private final EmittingPublisher<KafkaMessage<K, V>> emitter =\n-            new EmittingPublisher<>(n -> requests.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE));\n-    private final List<String> topics;\n-    private final long periodExecutions;\n-    private final long pollTimeout;\n-    private final boolean autoCommit;\n-    private final long ackTimeout;\n-    private final int limitNoAck;\n-    private final Supplier<Consumer<K, V>> consumerSupplier;\n-\n-    private Consumer<K, V> kafkaConsumer;\n-\n-    private KafkaPublisher(ScheduledExecutorService scheduler, Supplier<Consumer<K, V>> consumerSupplier,\n-            List<String> topics, long pollTimeout, long periodExecutions, boolean autoCommit,\n-            long ackTimeout, int limitNoAck) {\n-        this.scheduler = scheduler;\n-        this.topics = topics;\n-        this.periodExecutions = periodExecutions;\n-        this.pollTimeout = pollTimeout;\n-        this.autoCommit = autoCommit;\n-        this.ackTimeout = ackTimeout;\n-        this.limitNoAck = limitNoAck;\n-        this.consumerSupplier = consumerSupplier;\n-    }\n-\n-    /**\n-     * Starts to consume events from Kafka to send them downstream till\n-     * {@link KafkaPublisher#stop()} is invoked.\n-     * A new KafkaConsumer will be instanced if it was not provided before.\n-     * This execution runs in one thread that is triggered by the scheduler.\n-     */\n-    private void start() {\n-        LOGGER.fine(() -> \"KafkaPublisher starts to consume from Kafka\");\n-        kafkaConsumer = consumerSupplier.get();\n-        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n-        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n-        scheduler.scheduleAtFixedRate(() -> {\n-            try {\n-                // Need to lock to avoid onClose() is executed meanwhile task is running\n-                taskLock.lock();\n-                if (!scheduler.isShutdown() && !emitter.isTerminated()) {\n-                    int currentNoAck = currentNoAck();\n-                    if (currentNoAck < limitNoAck) {\n-                        if (backPressureBuffer.isEmpty()) {\n-                            try {\n-                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n-                            } catch (WakeupException e) {\n-                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n-                            }\n-                        } else {\n-                            long totalToEmit = requests.get();\n-                            // Avoid index out bound exceptions\n-                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n-                            for (long i = 0; i < eventsToEmit; i++) {\n-                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n-                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n-                                if (!autoCommit) {\n-                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n-                                            kafkaMessage.getPayload().partition());\n-                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n-                                } else {\n-                                    kafkaCommit.complete(null);\n-                                }\n-                                requests.decrementAndGet();\n-                                runInNewContext(() ->  emitter.emit(kafkaMessage));\n-                            }\n-                        }\n-                    } else {\n-                        throw new IllegalStateException(\n-                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n-                                        currentNoAck, limitNoAck));\n-                    }\n-                }\n-                // Commit ACKs\n-                processACK();\n-            } catch (Exception e) {\n-                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n-                emitter.fail(e);\n-            } finally {\n-                taskLock.unlock();\n-            }\n-        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n-    }\n-\n-    private int currentNoAck() {\n-        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n-    }\n-\n-    /**\n-     * Process the ACKs only if enable.auto.commit is false.\n-     * This will search ACK events and it will commit them to Kafka.\n-     * Those events that are committed will complete KafkaMessage#ack().\n-     */\n-    private void processACK() {\n-        if (!autoCommit) {\n-            Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n-            List<KafkaMessage<K, V>> messagesToCommit = new LinkedList<>();\n-            // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n-            for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n-                // No need to sort it, offsets are consumed in order\n-                List<KafkaMessage<K, V>> byPartition = entry.getValue();\n-                Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n-                KafkaMessage<K, V> highest = null;\n-                while (iterator.hasNext()) {\n-                    KafkaMessage<K, V> element = iterator.next();\n-                    if (element.isAck()) {\n-                        messagesToCommit.add(element);\n-                        highest = element;\n-                        iterator.remove();\n-                    } else {\n-                        break;\n-                    }\n-                }\n-                if (highest != null) {\n-                    OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n-                    LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n-                    offsets.put(entry.getKey(), offset);\n-                }\n-            }\n-            if (!messagesToCommit.isEmpty()) {\n-                LOGGER.fine(() -> String.format(\"Offsets %s\", offsets));\n-                try {\n-                    kafkaConsumer.commitSync(offsets);\n-                    messagesToCommit.stream().forEach(message -> message.kafkaCommit().complete(null));\n-                } catch (RuntimeException e) {\n-                    LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n-                    messagesToCommit.stream().forEach(message -> message.kafkaCommit().completeExceptionally(e));\n-                }\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Closes the connections to Kafka and stops to process new events.\n-     */\n-    public void stop() {\n-        // Stops pooling\n-        kafkaConsumer.wakeup();\n-        // Wait that current task finishes in case it is still running\n-        try {\n-            taskLock.lock();\n-            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n-            // Terminate waiting ACKs\n-            pendingCommits.values().stream().flatMap(List::stream)\n-            .forEach(message ->\n-            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n-            kafkaConsumer.close();\n-            emitter.complete();\n-        } catch (RuntimeException e) {\n-            emitter.fail(e);\n-        } finally {\n-            taskLock.unlock();\n-        }\n-        LOGGER.fine(() -> \"Closed\");\n-    }\n-\n-    //Move to messaging incoming connector\n-    protected void runInNewContext(Runnable runnable) {\n-        Context.Builder contextBuilder = Context.builder()\n-                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n-        Contexts.context().ifPresent(contextBuilder::parent);\n-        Contexts.runInContext(contextBuilder.build(), runnable);\n-    }\n-\n-    @Override\n-    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-        emitter.subscribe(subscriber);\n-        start();\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * A builder for KafkaPublisher.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @return builder to create a new instance\n-     */\n-    public static <K, V> Builder<K, V> builder() {\n-        return new Builder<>();\n-    }\n-\n-    /**\n-     * Load this builder from a configuration.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @param config configuration to load from\n-     * @return updated builder instance\n-     */\n-    public static <K, V> KafkaPublisher<K, V> create(Config config) {\n-        return (KafkaPublisher<K, V>) builder().config(config).build();\n-    }\n-\n-    /**\n-     * Fluent API builder for {@link KafkaPublisher}.\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     */\n-    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaPublisher<K, V>> {\n-\n-        private long pollTimeout = 50L;\n-        private long periodExecutions = 100L;\n-        private Boolean autoCommit;\n-        private long ackTimeout = Long.MAX_VALUE;\n-        private int limitNoAck = Integer.MAX_VALUE;\n-        private List<String> topics;\n-        private ScheduledExecutorService scheduler;\n-        private Supplier<Consumer<K, V>> consumerSupplier;\n-\n-        private Builder() {\n-        }\n-\n-        /**\n-         * Load this builder from a configuration.\n-         *\n-         * @param config configuration to load from\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> config(Config config) {\n-            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n-            consumerSupplier(() -> new KafkaConsumer<>(kafkaConfig.asMap()));\n-            topics(kafkaConfig.topics());\n-            config.get(POLL_TIMEOUT).asLong().ifPresent(this::pollTimeout);\n-            config.get(PERIOD_EXECUTIONS).asLong().ifPresent(this::periodExecutions);\n-            config.get(ENABLE_AUTOCOMMIT).asBoolean().ifPresent(this::autoCommit);\n-            config.get(ACK_TIMEOUT).asLong().ifPresent(this::ackTimeout);\n-            config.get(LIMIT_NO_ACK).asInt().ifPresent(this::limitNoAck);\n-            return this;\n-        }\n-\n-        /**\n-         * Defines how to instantiate the KafkaConsumer. It will be invoked\n-         * in {@link KafkaPublisher#subscribe(Subscriber)}\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param consumerSupplier\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> consumerSupplier(Supplier<Consumer<K, V>> consumerSupplier) {\n-            this.consumerSupplier = consumerSupplier;\n-            return this;\n-        }\n-\n-        /**\n-         * The list of topics to subscribe to.\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param topics\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> topics(List<String> topics) {\n-            this.topics = topics;\n-            return this;\n-        }\n-\n-        /**\n-         * Specify a scheduler that will read ad process messages coming from Kafka.\n-         * Is it intended that this scheduler is reused for other tasks.\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param scheduler\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> scheduler(ScheduledExecutorService scheduler) {\n-            this.scheduler = scheduler;\n-            return this;\n-        }\n-\n-        /**\n-         * Specifies the period in milliseconds between successive scheduler executions.\n-         * The default value is 100 milliseconds.\n-         *\n-         * @param periodExecutions\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> periodExecutions(long periodExecutions) {\n-            this.periodExecutions = periodExecutions;\n-            return this;\n-        }\n-\n-        /**\n-         * Specifies maximum time in milliseconds to block polling messages from Kafka.\n-         * The default value is 50 milliseconds.\n-         *\n-         * @param pollTimeout\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> pollTimeout(long pollTimeout) {\n-            this.pollTimeout = pollTimeout;\n-            return this;\n-        }\n-\n-        /**\n-         * This flag defines the strategy of committing messages to Kafka.\n-         * When true, the messages are committed in the moment they are polled from Kafka.\n-         * When false, the messages are committed when {@link KafkaMessage#ack()} is invoked.\n-         *\n-         * This value is mandatory to be specified and it must be consistent with the value enable.auto.commit\n-         * in Kafka properties. Failing to do this will result the next scenarios:\n-         * - For autoCommit = true and enable.auto.commit = false, messages will never be committed in Kafka.\n-         * - For autoCommit = false and enable.auto.commit = true, all messages will be committed and\n-         * {@link KafkaMessage#ack()} will have no effect.\n-         *\n-         * @param autoCommit\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> autoCommit(boolean autoCommit) {\n-            this.autoCommit = autoCommit;\n-            return this;\n-        }\n-\n-        /**\n-         * This value applies only when autoCommit is set to false.\n-         * It defines the maximum time in milliseconds that {@link KafkaMessage#ack()} will be waiting\n-         * for the commit in Kafka.\n-         *\n-         * The default value is Long.MAX_VALUE\n-         *\n-         * @param ackTimeout\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> ackTimeout(long ackTimeout) {\n-            this.ackTimeout = ackTimeout;\n-            return this;\n-        }\n-\n-        /**\n-         * This value applies only when autoCommit is set to false.\n-         * It specifies the limit of messages waiting to be committed in Kafka.\n-         * If this value is overflown, the KafkaPublisher will notify a failure.\n-         *\n-         * The intention of this value is to fail gracefully when there are many pending commits,\n-         * instead of failing with OutOfMemoryError.\n-         *\n-         * @param limitNoAck\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> limitNoAck(int limitNoAck) {\n-            this.limitNoAck = limitNoAck;\n-            return this;\n-        }\n-\n-        @Override\n-        public KafkaPublisher<K, V> build() {\n-            if (Objects.isNull(topics) || topics.isEmpty()) {\n-                throw new IllegalArgumentException(\"The topic is a required value\");\n-            }\n-            if (Objects.isNull(autoCommit)) {\n-                String message =\n-                    String.format(\"The autoCommit is a required value and be equals to KafkaProperty %s\", ENABLE_AUTOCOMMIT);\n-                throw new IllegalArgumentException(message);\n-            }\n-            if (Objects.isNull(scheduler)) {\n-                throw new IllegalArgumentException(\"The scheduler is a required value\");\n-            }\n-            if (Objects.isNull(consumerSupplier)) {\n-                throw new IllegalArgumentException(\"The kafkaConsumerSupplier is a required value\");\n-            }\n-            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, consumerSupplier, topics,\n-                    pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n-            return publisher;\n-        }\n-    }\n-\n-}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nnew file mode 100644\nindex 000000000..8e296b4bf\n--- /dev/null\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -0,0 +1,340 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                } else {\n+                                    kafkaCommit.complete(null);\n+                                }\n+                                requests.decrementAndGet();\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search ACK events and it will commit them to Kafka.\n+     * Those events that are committed will complete KafkaMessage#ack().\n+     */\n+    private void processACK() {\n+        if (!autoCommit) {\n+            Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+            List<KafkaMessage<K, V>> messagesToCommit = new LinkedList<>();\n+            // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+            for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+                // No need to sort it, offsets are consumed in order\n+                List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+                Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+                KafkaMessage<K, V> highest = null;\n+                while (iterator.hasNext()) {\n+                    KafkaMessage<K, V> element = iterator.next();\n+                    if (element.isAck()) {\n+                        messagesToCommit.add(element);\n+                        highest = element;\n+                        iterator.remove();\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                if (highest != null) {\n+                    OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                    LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                    offsets.put(entry.getKey(), offset);\n+                }\n+            }\n+            if (!messagesToCommit.isEmpty()) {\n+                Optional<RuntimeException> exception = commitInKafka(offsets);\n+                messagesToCommit.stream().forEach(message -> {\n+                    exception.ifPresentOrElse(\n+                            ex -> message.kafkaCommit().completeExceptionally(ex),\n+                            () -> message.kafkaCommit().complete(null));\n+                });\n+            }\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n+        LOGGER.fine(() -> \"Closed\");\n+    }\n+\n+    //Move to messaging incoming connector\n+    protected void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    static <K, V> KafkaPublisherBuilder<K, V> builder(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics) {\n+        return new KafkaPublisherBuilder<>(scheduler, kafkaConsumer, topics);\n+    }\n+\n+    /**\n+     * Fluent API builder for {@link KafkaPublisher}.\n+     */\n+    static final class KafkaPublisherBuilder<K, V> implements io.helidon.common.Builder<KafkaPublisher<K, V>> {\n+\n+        private long pollTimeout = 50L;\n+        private long periodExecutions = 100L;\n+        private boolean autoCommit = true;\n+        private long ackTimeout = Long.MAX_VALUE;\n+        private int limitNoAck = Integer.MAX_VALUE;\n+        private final List<String> topics;\n+        private final ScheduledExecutorService scheduler;\n+        private final Consumer<K, V> kafkaConsumer;\n+\n+        private KafkaPublisherBuilder(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+                List<String> topics) {\n+            this.scheduler = scheduler;\n+            this.kafkaConsumer = kafkaConsumer;\n+            this.topics = topics;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> config(Config config) {\n+            config.get(POLL_TIMEOUT).asLong().ifPresent(this::pollTimeout);\n+            config.get(PERIOD_EXECUTIONS).asLong().ifPresent(this::periodExecutions);\n+            config.get(ENABLE_AUTOCOMMIT).asBoolean().ifPresent(this::autoCommit);\n+            config.get(ACK_TIMEOUT).asLong().ifPresent(this::ackTimeout);\n+            config.get(LIMIT_NO_ACK).asInt().ifPresent(this::limitNoAck);\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> periodExecutions(long periodExecutions) {\n+            this.periodExecutions = periodExecutions;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> pollTimeout(long pollTimeout) {\n+            this.pollTimeout = pollTimeout;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> autoCommit(boolean autoCommit) {\n+            this.autoCommit = autoCommit;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> ackTimeout(long ackTimeout) {\n+            this.ackTimeout = ackTimeout;\n+            return this;\n+        }\n+\n+        KafkaPublisherBuilder<K, V> limitNoAck(int limitNoAck) {\n+            this.limitNoAck = limitNoAck;\n+            return this;\n+        }\n+\n+        @Override\n+        public KafkaPublisher<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n+                    pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n+            publisher.execute();\n+            return publisher;\n+        }\n+    }\n+\n+}\n", "next_change": {"commit": "7081c593461d66f7eea754455b70d825a6a7cd96", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 8e296b4bf..b99e8425b 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n", "chunk": "@@ -300,39 +345,139 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> periodExecutions(long periodExecutions) {\n+        /**\n+         * Defines how to instantiate the KafkaConsumer. It will be invoked\n+         * in {@link KafkaPublisher#subscribe(Subscriber)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param consumerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> consumerSupplier(Supplier<Consumer<K, V>> consumerSupplier) {\n+            this.consumerSupplier = consumerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * The list of topics to subscribe to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n+\n+        /**\n+         * Specify a scheduler that will read ad process messages coming from Kafka.\n+         * Is it intended that this scheduler is reused for other tasks.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param scheduler\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> scheduler(ScheduledExecutorService scheduler) {\n+            this.scheduler = scheduler;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the period in milliseconds between successive scheduler executions.\n+         * The default value is 100 milliseconds.\n+         *\n+         * @param periodExecutions\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> periodExecutions(long periodExecutions) {\n             this.periodExecutions = periodExecutions;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> pollTimeout(long pollTimeout) {\n+        /**\n+         * Specifies maximum time in milliseconds to block polling messages from Kafka.\n+         * The default value is 50 milliseconds.\n+         *\n+         * @param pollTimeout\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> pollTimeout(long pollTimeout) {\n             this.pollTimeout = pollTimeout;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> autoCommit(boolean autoCommit) {\n+        /**\n+         * This flag defines the strategy of committing messages to Kafka.\n+         * When true, the messages are committed in the moment they are polled from Kafka.\n+         * When false, the messages are committed when {@link KafkaMessage#ack()} is invoked.\n+         *\n+         * This value is mandatory to be specified and it must be consistent with the value enable.auto.commit\n+         * in Kafka properties. Failing to do this will result the next scenarios:\n+         * - For autoCommit = true and enable.auto.commit = false, messages will never be committed in Kafka.\n+         * - For autoCommit = false and enable.auto.commit = true, all messages will be committed and\n+         * {@link KafkaMessage#ack()} will have no effect.\n+         *\n+         * @param autoCommit\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> autoCommit(boolean autoCommit) {\n             this.autoCommit = autoCommit;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> ackTimeout(long ackTimeout) {\n+        /**\n+         * This value applies only when autoCommit is set to false.\n+         * It defines the maximum time in milliseconds that {@link KafkaMessage#ack()} will be waiting\n+         * for the commit in Kafka.\n+         *\n+         * The default value is Long.MAX_VALUE\n+         *\n+         * @param ackTimeout\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> ackTimeout(long ackTimeout) {\n             this.ackTimeout = ackTimeout;\n             return this;\n         }\n \n-        KafkaPublisherBuilder<K, V> limitNoAck(int limitNoAck) {\n+        /**\n+         * This value applies only when autoCommit is set to false.\n+         * It specifies the limit of messages waiting to be committed in Kafka.\n+         * If this value is overflown, the KafkaPublisher will notify a failure.\n+         *\n+         * The intention of this value is to fail gracefully when there are many pending commits,\n+         * instead of failing with OutOfMemoryError.\n+         *\n+         * @param limitNoAck\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> limitNoAck(int limitNoAck) {\n             this.limitNoAck = limitNoAck;\n             return this;\n         }\n \n         @Override\n         public KafkaPublisher<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, kafkaConsumer, topics,\n+            if (Objects.isNull(autoCommit)) {\n+                String message =\n+                    String.format(\"The autoCommit is a required value and be equals to KafkaProperty %s\", ENABLE_AUTOCOMMIT);\n+                throw new IllegalArgumentException(message);\n+            }\n+            if (Objects.isNull(scheduler)) {\n+                throw new IllegalArgumentException(\"The scheduler is a required value\");\n+            }\n+            if (Objects.isNull(consumerSupplier)) {\n+                throw new IllegalArgumentException(\"The kafkaConsumerSupplier is a required value\");\n+            }\n+            KafkaPublisher<K, V> publisher = new KafkaPublisher<>(scheduler, consumerSupplier, topics,\n                     pollTimeout, periodExecutions, autoCommit, ackTimeout, limitNoAck);\n-            publisher.execute();\n             return publisher;\n         }\n     }\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzU1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317556", "body": "The only allows factory method is `create` unless there is a good reason not to use it. I do not see a good reason, so please rename to `create(Config)` (even though this is package local only)", "bodyText": "The only allows factory method is create unless there is a good reason not to use it. I do not see a good reason, so please rename to create(Config) (even though this is package local only)", "bodyHTML": "<p dir=\"auto\">The only allows factory method is <code>create</code> unless there is a good reason not to use it. I do not see a good reason, so please rename to <code>create(Config)</code> (even though this is package local only)</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:28Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -93,19 +117,19 @@ class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n      * @param config With the KafkaSubscriber required parameters\n      * @return A new KafkaSubscriber instance\n      */\n-    static <T> KafkaSubscriber<T> build(Config config) {\n+    static <K, V> KafkaSubscriber<K, V> build(Config config) {\n         Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n         List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n         if (topics.isEmpty()) {\n             throw new IllegalArgumentException(\"The topic is a required configuration value\");\n         }\n         long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, new KafkaProducer<>(kafkaConfig)), backpressure);\n+        return new KafkaSubscriber<K, V>(new KafkaProducer<>(kafkaConfig), topics, backpressure);\n     }\n \n     // For tests\n-    static <T> KafkaSubscriber<T> build(List<String> topics, long backpressure, Producer<?, T> producer) {\n-        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, producer), backpressure);\n+    static <K, V> KafkaSubscriber<K, V> build(List<String> topics, long backpressure, Producer<K, V> producer) {\n+        return new KafkaSubscriber<K, V>(producer, topics, backpressure);\n     }\n \n }\n", "next_change": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 23abff5e0..a6a77fa63 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -109,27 +106,40 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     }\n \n     /**\n-     * Creates a new instance of KafkaSubscriber given the configuration.\n-     * Note: Every new instance of this type opens Kafka resources and it will be opened\n-     * till onComplete() or onError() is invoked.\n-     *\n-     * @param <T> The type to push\n-     * @param config With the KafkaSubscriber required parameters\n-     * @return A new KafkaSubscriber instance\n+     * Fluent API builder for {@link KafkaPublisher}.\n      */\n-    static <K, V> KafkaSubscriber<K, V> build(Config config) {\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private final Producer<K, V> producer;\n+        private final List<String> topics;\n+        private long backpressure = 5L;\n+\n+        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n+            this.producer = producer;\n+            this.topics = topics;\n         }\n-        long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        return new KafkaSubscriber<K, V>(new KafkaProducer<>(kafkaConfig), topics, backpressure);\n-    }\n \n-    // For tests\n-    static <K, V> KafkaSubscriber<K, V> build(List<String> topics, long backpressure, Producer<K, V> producer) {\n-        return new KafkaSubscriber<K, V>(producer, topics, backpressure);\n+        static <K, V> KafkaSubscriberBuilder<K, V> builderSE(Producer<K, V> producer, List<String> topics) {\n+            return new KafkaSubscriberBuilder<>(producer, topics);\n+        }\n+\n+        @Override\n+        public KafkaSubscriber<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producer, topics, backpressure);\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> config(Config config) {\n+            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n+            return this;\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+            this.backpressure = backpressure;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex a6a77fa63..bea7961b0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -96,50 +102,114 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     public void onError(Throwable t) {\n         Objects.requireNonNull(t);\n         LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        producer.close();\n+        kafkaProducer.close();\n     }\n \n     @Override\n     public void onComplete() {\n-        LOGGER.fine(\"Subscriber has finished\");\n-        producer.close();\n+        LOGGER.fine(() -> \"Subscriber has finished\");\n+        kafkaProducer.close();\n     }\n \n     /**\n-     * Fluent API builder for {@link KafkaPublisher}.\n+     * A builder for KafkaSubscriber.\n+     *\n+     * @return builder to create a new instance\n      */\n-    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+    public static <K, V> Builder<K, V> builder() {\n+        return new Builder<>();\n+    }\n \n-        private final Producer<K, V> producer;\n-        private final List<String> topics;\n-        private long backpressure = 5L;\n+    /**\n+     * Load this builder from a configuration.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param config configuration to load from\n+     * @return updated builder instance\n+     */\n+    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n+        return (KafkaSubscriber<K, V>) builder().config(config).build();\n+    }\n \n-        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n-            this.producer = producer;\n-            this.topics = topics;\n-        }\n+    /**\n+     * Fluent API builder for {@link KafkaSubscriber}.\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     */\n+    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private Supplier<Producer<K, V>> producerSupplier;\n+        private List<String> topics;\n+        private long backpressure = 5L;\n \n-        static <K, V> KafkaSubscriberBuilder<K, V> builderSE(Producer<K, V> producer, List<String> topics) {\n-            return new KafkaSubscriberBuilder<>(producer, topics);\n+        private Builder() {\n         }\n \n         @Override\n         public KafkaSubscriber<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            return new KafkaSubscriber<>(producer, topics, backpressure);\n+            if (Objects.isNull(producerSupplier)) {\n+                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n         }\n \n-        KafkaSubscriberBuilder<K, V> config(Config config) {\n+        /**\n+         * Load this builder from a configuration.\n+         *\n+         * @param config configuration to load from\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> config(Config config) {\n+            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n+            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n+            topics(kafkaConfig.topics());\n             config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n             return this;\n         }\n \n-        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+        /**\n+         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n+         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param producerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n+            this.producerSupplier = producerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the number of messages that are requested after processing them.\n+         *\n+         * The default value is 5.\n+         *\n+         * @param backpressure\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> backpressure(long backpressure) {\n             this.backpressure = backpressure;\n             return this;\n         }\n+\n+        /**\n+         * The list of topics the messages should be sent to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\ndeleted file mode 100644\nindex bea7961b0..000000000\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.connectors.kafka;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.function.Supplier;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.Producer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-/**\n- * Reactive streams subscriber implementation.\n- * @param <K> kafka record key type\n- * @param <V> kafka record value type\n- */\n-public class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n-\n-    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n-    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n-\n-    private final long backpressure;\n-    private final Supplier<Producer<K, V>> producerSupplier;\n-    private final List<String> topics;\n-    private final AtomicLong backpressureCounter = new AtomicLong();\n-\n-    private Subscription subscription;\n-    private Producer<K, V> kafkaProducer;\n-\n-    private KafkaSubscriber(Supplier<Producer<K, V>> producerSupplier, List<String> topics, long backpressure){\n-        this.backpressure = backpressure;\n-        this.producerSupplier = producerSupplier;\n-        this.topics = topics;\n-    }\n-\n-    @Override\n-    public void onSubscribe(Subscription subscription) {\n-        if (this.subscription == null) {\n-            this.kafkaProducer = producerSupplier.get();\n-            this.subscription = subscription;\n-            this.subscription.request(backpressure);\n-        } else {\n-            subscription.cancel();\n-        }\n-    }\n-\n-    @Override\n-    public void onNext(Message<V> message) {\n-        Objects.requireNonNull(message);\n-        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n-            futureList.add(completableFuture);\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n-            kafkaProducer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    subscription.cancel();\n-                    completableFuture.completeExceptionally(exception);\n-                } else {\n-                    completableFuture.complete(null);\n-                }\n-            });\n-        }\n-        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n-        .whenComplete((success, exception) -> {\n-            if (exception == null) {\n-                message.ack().whenComplete((a, b) -> {\n-                    if (backpressureCounter.incrementAndGet() == backpressure) {\n-                        backpressureCounter.set(0);\n-                        subscription.request(backpressure);\n-                    }\n-                });\n-            }\n-        });\n-    }\n-\n-    @Override\n-    public void onError(Throwable t) {\n-        Objects.requireNonNull(t);\n-        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        kafkaProducer.close();\n-    }\n-\n-    @Override\n-    public void onComplete() {\n-        LOGGER.fine(() -> \"Subscriber has finished\");\n-        kafkaProducer.close();\n-    }\n-\n-    /**\n-     * A builder for KafkaSubscriber.\n-     *\n-     * @return builder to create a new instance\n-     */\n-    public static <K, V> Builder<K, V> builder() {\n-        return new Builder<>();\n-    }\n-\n-    /**\n-     * Load this builder from a configuration.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @param config configuration to load from\n-     * @return updated builder instance\n-     */\n-    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n-        return (KafkaSubscriber<K, V>) builder().config(config).build();\n-    }\n-\n-    /**\n-     * Fluent API builder for {@link KafkaSubscriber}.\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     */\n-    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n-\n-        private Supplier<Producer<K, V>> producerSupplier;\n-        private List<String> topics;\n-        private long backpressure = 5L;\n-\n-        private Builder() {\n-        }\n-\n-        @Override\n-        public KafkaSubscriber<K, V> build() {\n-            if (Objects.isNull(topics) || topics.isEmpty()) {\n-                throw new IllegalArgumentException(\"The topic is a required value\");\n-            }\n-            if (Objects.isNull(producerSupplier)) {\n-                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n-            }\n-            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n-        }\n-\n-        /**\n-         * Load this builder from a configuration.\n-         *\n-         * @param config configuration to load from\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> config(Config config) {\n-            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n-            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n-            topics(kafkaConfig.topics());\n-            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n-            return this;\n-        }\n-\n-        /**\n-         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n-         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param producerSupplier\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n-            this.producerSupplier = producerSupplier;\n-            return this;\n-        }\n-\n-        /**\n-         * Specifies the number of messages that are requested after processing them.\n-         *\n-         * The default value is 5.\n-         *\n-         * @param backpressure\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> backpressure(long backpressure) {\n-            this.backpressure = backpressure;\n-            return this;\n-        }\n-\n-        /**\n-         * The list of topics the messages should be sent to.\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param topics\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> topics(List<String> topics) {\n-            this.topics = topics;\n-            return this;\n-        }\n-    }\n-\n-}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nnew file mode 100644\nindex 000000000..f43e2d062\n--- /dev/null\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private final long backpressure;\n+    private final Producer<K, V> producer;\n+    private final List<String> topics;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(Producer<K, V> producer, List<String> topics, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+        this.topics = topics;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<V> message) {\n+        Objects.requireNonNull(message);\n+        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n+            futureList.add(completableFuture);\n+            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n+            producer.send(record, (metadata, exception) -> {\n+                if (exception != null) {\n+                    subscription.cancel();\n+                    completableFuture.completeExceptionally(exception);\n+                } else {\n+                    completableFuture.complete(null);\n+                }\n+            });\n+        }\n+        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n+        .whenComplete((success, exception) -> {\n+            if (exception == null) {\n+                message.ack().whenComplete((a, b) -> {\n+                    if (backpressureCounter.incrementAndGet() == backpressure) {\n+                        backpressureCounter.set(0);\n+                        subscription.request(backpressure);\n+                    }\n+                });\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n+        return new KafkaSubscriberBuilder<>(producer, topics);\n+    }\n+\n+    /**\n+     * Fluent API builder for {@link KafkaSubscriber}.\n+     */\n+    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private final Producer<K, V> producer;\n+        private final List<String> topics;\n+        private long backpressure = 5L;\n+\n+        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n+            this.producer = producer;\n+            this.topics = topics;\n+        }\n+\n+        @Override\n+        public KafkaSubscriber<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producer, topics, backpressure);\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> config(Config config) {\n+            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n+            return this;\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+            this.backpressure = backpressure;\n+            return this;\n+        }\n+    }\n+\n+}\n", "next_change": {"commit": "7081c593461d66f7eea754455b70d825a6a7cd96", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex f43e2d062..c49f38778 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -96,50 +107,116 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     public void onError(Throwable t) {\n         Objects.requireNonNull(t);\n         LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        producer.close();\n+        kafkaProducer.close();\n     }\n \n     @Override\n     public void onComplete() {\n-        LOGGER.fine(\"Subscriber has finished\");\n-        producer.close();\n+        LOGGER.fine(() -> \"Subscriber has finished\");\n+        kafkaProducer.close();\n+    }\n+\n+    /**\n+     * A builder for KafkaSubscriber.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @return builder to create a new instance\n+     */\n+    public static <K, V> Builder<K, V> builder() {\n+        return new Builder<>();\n     }\n \n-    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n-        return new KafkaSubscriberBuilder<>(producer, topics);\n+    /**\n+     * Load this builder from a configuration.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param config configuration to load from\n+     * @return updated builder instance\n+     */\n+    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n+        return (KafkaSubscriber<K, V>) builder().config(config).build();\n     }\n \n     /**\n      * Fluent API builder for {@link KafkaSubscriber}.\n+     * @param <K> Key type\n+     * @param <V> Value type\n      */\n-    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n \n-        private final Producer<K, V> producer;\n-        private final List<String> topics;\n+        private Supplier<Producer<K, V>> producerSupplier;\n+        private List<String> topics;\n         private long backpressure = 5L;\n \n-        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n-            this.producer = producer;\n-            this.topics = topics;\n+        private Builder() {\n         }\n \n         @Override\n         public KafkaSubscriber<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            return new KafkaSubscriber<>(producer, topics, backpressure);\n+            if (Objects.isNull(producerSupplier)) {\n+                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n         }\n \n-        KafkaSubscriberBuilder<K, V> config(Config config) {\n+        /**\n+         * Load this builder from a configuration.\n+         *\n+         * @param config configuration to load from\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> config(Config config) {\n+            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n+            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n+            topics(kafkaConfig.topics());\n             config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n             return this;\n         }\n \n-        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+        /**\n+         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n+         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param producerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n+            this.producerSupplier = producerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the number of messages that are requested after processing them.\n+         *\n+         * The default value is 5.\n+         *\n+         * @param backpressure\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> backpressure(long backpressure) {\n             this.backpressure = backpressure;\n             return this;\n         }\n+\n+        /**\n+         * The list of topics the messages should be sent to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317831", "body": "Please do not do this.\r\nUse a builder if you want to specify details and fill them either manually or from config.", "bodyText": "Please do not do this.\nUse a builder if you want to specify details and fill them either manually or from config.", "bodyHTML": "<p dir=\"auto\">Please do not do this.<br>\nUse a builder if you want to specify details and fill them either manually or from config.</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:56Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, new KafkaProducer<>(kafkaConfig)), backpressure);\n+    }\n+\n+    // For tests", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -93,19 +117,19 @@ class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n      * @param config With the KafkaSubscriber required parameters\n      * @return A new KafkaSubscriber instance\n      */\n-    static <T> KafkaSubscriber<T> build(Config config) {\n+    static <K, V> KafkaSubscriber<K, V> build(Config config) {\n         Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n         List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n         if (topics.isEmpty()) {\n             throw new IllegalArgumentException(\"The topic is a required configuration value\");\n         }\n         long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, new KafkaProducer<>(kafkaConfig)), backpressure);\n+        return new KafkaSubscriber<K, V>(new KafkaProducer<>(kafkaConfig), topics, backpressure);\n     }\n \n     // For tests\n-    static <T> KafkaSubscriber<T> build(List<String> topics, long backpressure, Producer<?, T> producer) {\n-        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, producer), backpressure);\n+    static <K, V> KafkaSubscriber<K, V> build(List<String> topics, long backpressure, Producer<K, V> producer) {\n+        return new KafkaSubscriber<K, V>(producer, topics, backpressure);\n     }\n \n }\n", "next_change": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 23abff5e0..a6a77fa63 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -109,27 +106,40 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     }\n \n     /**\n-     * Creates a new instance of KafkaSubscriber given the configuration.\n-     * Note: Every new instance of this type opens Kafka resources and it will be opened\n-     * till onComplete() or onError() is invoked.\n-     *\n-     * @param <T> The type to push\n-     * @param config With the KafkaSubscriber required parameters\n-     * @return A new KafkaSubscriber instance\n+     * Fluent API builder for {@link KafkaPublisher}.\n      */\n-    static <K, V> KafkaSubscriber<K, V> build(Config config) {\n-        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n-        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n-        if (topics.isEmpty()) {\n-            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private final Producer<K, V> producer;\n+        private final List<String> topics;\n+        private long backpressure = 5L;\n+\n+        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n+            this.producer = producer;\n+            this.topics = topics;\n         }\n-        long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n-        return new KafkaSubscriber<K, V>(new KafkaProducer<>(kafkaConfig), topics, backpressure);\n-    }\n \n-    // For tests\n-    static <K, V> KafkaSubscriber<K, V> build(List<String> topics, long backpressure, Producer<K, V> producer) {\n-        return new KafkaSubscriber<K, V>(producer, topics, backpressure);\n+        static <K, V> KafkaSubscriberBuilder<K, V> builderSE(Producer<K, V> producer, List<String> topics) {\n+            return new KafkaSubscriberBuilder<>(producer, topics);\n+        }\n+\n+        @Override\n+        public KafkaSubscriber<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producer, topics, backpressure);\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> config(Config config) {\n+            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n+            return this;\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+            this.backpressure = backpressure;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": {"commit": "907340606f71273dc743136f182a860bdb84d348", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex a6a77fa63..bea7961b0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -96,50 +102,114 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     public void onError(Throwable t) {\n         Objects.requireNonNull(t);\n         LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        producer.close();\n+        kafkaProducer.close();\n     }\n \n     @Override\n     public void onComplete() {\n-        LOGGER.fine(\"Subscriber has finished\");\n-        producer.close();\n+        LOGGER.fine(() -> \"Subscriber has finished\");\n+        kafkaProducer.close();\n     }\n \n     /**\n-     * Fluent API builder for {@link KafkaPublisher}.\n+     * A builder for KafkaSubscriber.\n+     *\n+     * @return builder to create a new instance\n      */\n-    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+    public static <K, V> Builder<K, V> builder() {\n+        return new Builder<>();\n+    }\n \n-        private final Producer<K, V> producer;\n-        private final List<String> topics;\n-        private long backpressure = 5L;\n+    /**\n+     * Load this builder from a configuration.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param config configuration to load from\n+     * @return updated builder instance\n+     */\n+    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n+        return (KafkaSubscriber<K, V>) builder().config(config).build();\n+    }\n \n-        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n-            this.producer = producer;\n-            this.topics = topics;\n-        }\n+    /**\n+     * Fluent API builder for {@link KafkaSubscriber}.\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     */\n+    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private Supplier<Producer<K, V>> producerSupplier;\n+        private List<String> topics;\n+        private long backpressure = 5L;\n \n-        static <K, V> KafkaSubscriberBuilder<K, V> builderSE(Producer<K, V> producer, List<String> topics) {\n-            return new KafkaSubscriberBuilder<>(producer, topics);\n+        private Builder() {\n         }\n \n         @Override\n         public KafkaSubscriber<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            return new KafkaSubscriber<>(producer, topics, backpressure);\n+            if (Objects.isNull(producerSupplier)) {\n+                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n         }\n \n-        KafkaSubscriberBuilder<K, V> config(Config config) {\n+        /**\n+         * Load this builder from a configuration.\n+         *\n+         * @param config configuration to load from\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> config(Config config) {\n+            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n+            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n+            topics(kafkaConfig.topics());\n             config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n             return this;\n         }\n \n-        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+        /**\n+         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n+         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param producerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n+            this.producerSupplier = producerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the number of messages that are requested after processing them.\n+         *\n+         * The default value is 5.\n+         *\n+         * @param backpressure\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> backpressure(long backpressure) {\n             this.backpressure = backpressure;\n             return this;\n         }\n+\n+        /**\n+         * The list of topics the messages should be sent to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\ndeleted file mode 100644\nindex bea7961b0..000000000\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.connectors.kafka;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.function.Supplier;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.config.Config;\n-\n-import org.apache.kafka.clients.producer.KafkaProducer;\n-import org.apache.kafka.clients.producer.Producer;\n-import org.apache.kafka.clients.producer.ProducerRecord;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-/**\n- * Reactive streams subscriber implementation.\n- * @param <K> kafka record key type\n- * @param <V> kafka record value type\n- */\n-public class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n-\n-    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n-    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n-\n-    private final long backpressure;\n-    private final Supplier<Producer<K, V>> producerSupplier;\n-    private final List<String> topics;\n-    private final AtomicLong backpressureCounter = new AtomicLong();\n-\n-    private Subscription subscription;\n-    private Producer<K, V> kafkaProducer;\n-\n-    private KafkaSubscriber(Supplier<Producer<K, V>> producerSupplier, List<String> topics, long backpressure){\n-        this.backpressure = backpressure;\n-        this.producerSupplier = producerSupplier;\n-        this.topics = topics;\n-    }\n-\n-    @Override\n-    public void onSubscribe(Subscription subscription) {\n-        if (this.subscription == null) {\n-            this.kafkaProducer = producerSupplier.get();\n-            this.subscription = subscription;\n-            this.subscription.request(backpressure);\n-        } else {\n-            subscription.cancel();\n-        }\n-    }\n-\n-    @Override\n-    public void onNext(Message<V> message) {\n-        Objects.requireNonNull(message);\n-        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n-        for (String topic : topics) {\n-            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n-            futureList.add(completableFuture);\n-            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n-            kafkaProducer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    subscription.cancel();\n-                    completableFuture.completeExceptionally(exception);\n-                } else {\n-                    completableFuture.complete(null);\n-                }\n-            });\n-        }\n-        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n-        .whenComplete((success, exception) -> {\n-            if (exception == null) {\n-                message.ack().whenComplete((a, b) -> {\n-                    if (backpressureCounter.incrementAndGet() == backpressure) {\n-                        backpressureCounter.set(0);\n-                        subscription.request(backpressure);\n-                    }\n-                });\n-            }\n-        });\n-    }\n-\n-    @Override\n-    public void onError(Throwable t) {\n-        Objects.requireNonNull(t);\n-        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        kafkaProducer.close();\n-    }\n-\n-    @Override\n-    public void onComplete() {\n-        LOGGER.fine(() -> \"Subscriber has finished\");\n-        kafkaProducer.close();\n-    }\n-\n-    /**\n-     * A builder for KafkaSubscriber.\n-     *\n-     * @return builder to create a new instance\n-     */\n-    public static <K, V> Builder<K, V> builder() {\n-        return new Builder<>();\n-    }\n-\n-    /**\n-     * Load this builder from a configuration.\n-     *\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     * @param config configuration to load from\n-     * @return updated builder instance\n-     */\n-    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n-        return (KafkaSubscriber<K, V>) builder().config(config).build();\n-    }\n-\n-    /**\n-     * Fluent API builder for {@link KafkaSubscriber}.\n-     * @param <K> Key type\n-     * @param <V> Value type\n-     */\n-    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n-\n-        private Supplier<Producer<K, V>> producerSupplier;\n-        private List<String> topics;\n-        private long backpressure = 5L;\n-\n-        private Builder() {\n-        }\n-\n-        @Override\n-        public KafkaSubscriber<K, V> build() {\n-            if (Objects.isNull(topics) || topics.isEmpty()) {\n-                throw new IllegalArgumentException(\"The topic is a required value\");\n-            }\n-            if (Objects.isNull(producerSupplier)) {\n-                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n-            }\n-            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n-        }\n-\n-        /**\n-         * Load this builder from a configuration.\n-         *\n-         * @param config configuration to load from\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> config(Config config) {\n-            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n-            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n-            topics(kafkaConfig.topics());\n-            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n-            return this;\n-        }\n-\n-        /**\n-         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n-         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param producerSupplier\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n-            this.producerSupplier = producerSupplier;\n-            return this;\n-        }\n-\n-        /**\n-         * Specifies the number of messages that are requested after processing them.\n-         *\n-         * The default value is 5.\n-         *\n-         * @param backpressure\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> backpressure(long backpressure) {\n-            this.backpressure = backpressure;\n-            return this;\n-        }\n-\n-        /**\n-         * The list of topics the messages should be sent to.\n-         *\n-         * This is a mandatory parameter.\n-         *\n-         * @param topics\n-         * @return updated builder instance\n-         */\n-        public Builder<K, V> topics(List<String> topics) {\n-            this.topics = topics;\n-            return this;\n-        }\n-    }\n-\n-}\n", "next_change": {"commit": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nnew file mode 100644\nindex 000000000..f43e2d062\n--- /dev/null\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private final long backpressure;\n+    private final Producer<K, V> producer;\n+    private final List<String> topics;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(Producer<K, V> producer, List<String> topics, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+        this.topics = topics;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<V> message) {\n+        Objects.requireNonNull(message);\n+        List<CompletableFuture<Void>> futureList = new ArrayList<>(topics.size());\n+        for (String topic : topics) {\n+            CompletableFuture<Void> completableFuture = new CompletableFuture<>();\n+            futureList.add(completableFuture);\n+            ProducerRecord<K, V> record = new ProducerRecord<>(topic, message.getPayload());\n+            producer.send(record, (metadata, exception) -> {\n+                if (exception != null) {\n+                    subscription.cancel();\n+                    completableFuture.completeExceptionally(exception);\n+                } else {\n+                    completableFuture.complete(null);\n+                }\n+            });\n+        }\n+        CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0]))\n+        .whenComplete((success, exception) -> {\n+            if (exception == null) {\n+                message.ack().whenComplete((a, b) -> {\n+                    if (backpressureCounter.incrementAndGet() == backpressure) {\n+                        backpressureCounter.set(0);\n+                        subscription.request(backpressure);\n+                    }\n+                });\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n+        return new KafkaSubscriberBuilder<>(producer, topics);\n+    }\n+\n+    /**\n+     * Fluent API builder for {@link KafkaSubscriber}.\n+     */\n+    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+\n+        private final Producer<K, V> producer;\n+        private final List<String> topics;\n+        private long backpressure = 5L;\n+\n+        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n+            this.producer = producer;\n+            this.topics = topics;\n+        }\n+\n+        @Override\n+        public KafkaSubscriber<K, V> build() {\n+            if (topics.isEmpty()) {\n+                throw new IllegalArgumentException(\"The topic is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producer, topics, backpressure);\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> config(Config config) {\n+            config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n+            return this;\n+        }\n+\n+        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+            this.backpressure = backpressure;\n+            return this;\n+        }\n+    }\n+\n+}\n", "next_change": {"commit": "7081c593461d66f7eea754455b70d825a6a7cd96", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex f43e2d062..c49f38778 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n", "chunk": "@@ -96,50 +107,116 @@ class KafkaSubscriber<K, V> implements Subscriber<Message<V>> {\n     public void onError(Throwable t) {\n         Objects.requireNonNull(t);\n         LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-        producer.close();\n+        kafkaProducer.close();\n     }\n \n     @Override\n     public void onComplete() {\n-        LOGGER.fine(\"Subscriber has finished\");\n-        producer.close();\n+        LOGGER.fine(() -> \"Subscriber has finished\");\n+        kafkaProducer.close();\n+    }\n+\n+    /**\n+     * A builder for KafkaSubscriber.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @return builder to create a new instance\n+     */\n+    public static <K, V> Builder<K, V> builder() {\n+        return new Builder<>();\n     }\n \n-    static <K, V> KafkaSubscriberBuilder<K, V> builder(Producer<K, V> producer, List<String> topics) {\n-        return new KafkaSubscriberBuilder<>(producer, topics);\n+    /**\n+     * Load this builder from a configuration.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param config configuration to load from\n+     * @return updated builder instance\n+     */\n+    public static <K, V> KafkaSubscriber<K, V> create(Config config) {\n+        return (KafkaSubscriber<K, V>) builder().config(config).build();\n     }\n \n     /**\n      * Fluent API builder for {@link KafkaSubscriber}.\n+     * @param <K> Key type\n+     * @param <V> Value type\n      */\n-    static final class KafkaSubscriberBuilder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n+    public static final class Builder<K, V> implements io.helidon.common.Builder<KafkaSubscriber<K, V>> {\n \n-        private final Producer<K, V> producer;\n-        private final List<String> topics;\n+        private Supplier<Producer<K, V>> producerSupplier;\n+        private List<String> topics;\n         private long backpressure = 5L;\n \n-        private KafkaSubscriberBuilder(Producer<K, V> producer, List<String> topics) {\n-            this.producer = producer;\n-            this.topics = topics;\n+        private Builder() {\n         }\n \n         @Override\n         public KafkaSubscriber<K, V> build() {\n-            if (topics.isEmpty()) {\n+            if (Objects.isNull(topics) || topics.isEmpty()) {\n                 throw new IllegalArgumentException(\"The topic is a required value\");\n             }\n-            return new KafkaSubscriber<>(producer, topics, backpressure);\n+            if (Objects.isNull(producerSupplier)) {\n+                throw new IllegalArgumentException(\"The producerSupplier is a required value\");\n+            }\n+            return new KafkaSubscriber<>(producerSupplier, topics, backpressure);\n         }\n \n-        KafkaSubscriberBuilder<K, V> config(Config config) {\n+        /**\n+         * Load this builder from a configuration.\n+         *\n+         * @param config configuration to load from\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> config(Config config) {\n+            KafkaConfig kafkaConfig = KafkaConfig.create(config);\n+            producerSupplier(() -> new KafkaProducer<>(kafkaConfig.asMap()));\n+            topics(kafkaConfig.topics());\n             config.get(BACKPRESSURE_SIZE_KEY).asLong().ifPresent(this::backpressure);\n             return this;\n         }\n \n-        KafkaSubscriberBuilder<K, V> backpressure(long backpressure) {\n+        /**\n+         * Defines how to instantiate the KafkaSubscriber. It will be invoked\n+         * in {@link KafkaSubscriber#onSubscribe(Subscription)}\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param producerSupplier\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> producerSupplier(Supplier<Producer<K, V>> producerSupplier) {\n+            this.producerSupplier = producerSupplier;\n+            return this;\n+        }\n+\n+        /**\n+         * Specifies the number of messages that are requested after processing them.\n+         *\n+         * The default value is 5.\n+         *\n+         * @param backpressure\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> backpressure(long backpressure) {\n             this.backpressure = backpressure;\n             return this;\n         }\n+\n+        /**\n+         * The list of topics the messages should be sent to.\n+         *\n+         * This is a mandatory parameter.\n+         *\n+         * @param topics\n+         * @return updated builder instance\n+         */\n+        public Builder<K, V> topics(List<String> topics) {\n+            this.topics = topics;\n+            return this;\n+        }\n     }\n \n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408318293", "body": "This seems not to do anything useful. Isn't the contract of `ConsumerRebalancerListener` a bit more complicated?", "bodyText": "This seems not to do anything useful. Isn't the contract of ConsumerRebalancerListener a bit more complicated?", "bodyHTML": "<p dir=\"auto\">This seems not to do anything useful. Isn't the contract of <code>ConsumerRebalancerListener</code> a bit more complicated?</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:40:43Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NDQ5NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408594495", "bodyText": "From the point of view of reading new events, we don't care about the partitions because we don't specify them.\nBut this could affects pending commits. This is the current strategy when this happens:\n\nEvent is coming from partition A.\nUser is processing it.\nKafka revoke that partition.\nUser ack, commit is sent.\nKafka should throw CommitFailedException, because that partition doesn't exist.\nThe exception is sent to the ack, so user can decide. Note that publisher doesn't fail in this scenario, but the user can cancel the subscription if he wants to.\n\nOnce partition is up again, Kafka should send the message again (because it was not successfully committed).", "author": "jbescos", "createdAt": "2020-04-15T05:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTA2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319069", "body": "Helidon MP implementations must not be required in an SE module.\r\nIf you want to split into SE/MP, then SE modules can only depend on other SE modules.\r\nSE modules can depend on MP APIs and SPIs of the specifications.", "bodyText": "Helidon MP implementations must not be required in an SE module.\nIf you want to split into SE/MP, then SE modules can only depend on other SE modules.\nSE modules can depend on MP APIs and SPIs of the specifications.", "bodyHTML": "<p dir=\"auto\">Helidon MP implementations must not be required in an SE module.<br>\nIf you want to split into SE/MP, then SE modules can only depend on other SE modules.<br>\nSE modules can depend on MP APIs and SPIs of the specifications.</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:41:55Z", "path": "messaging/connectors/kafka/src/main/java/module-info.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+module io.helidon.microprofile.connectors.kafka {\n+    requires java.logging;\n+\n+    requires static cdi.api;\n+    requires static javax.inject;\n+    requires static java.activation;\n+    requires static kafka.clients;\n+    requires io.helidon.microprofile.config;", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/module-info.java b/messaging/connectors/kafka/src/main/java/module-info.java\nindex 27cb3ee3a..d8f63acd2 100644\n--- a/messaging/connectors/kafka/src/main/java/module-info.java\n+++ b/messaging/connectors/kafka/src/main/java/module-info.java\n", "chunk": "@@ -14,21 +14,20 @@\n  * limitations under the License.\n  */\n \n-module io.helidon.microprofile.connectors.kafka {\n+module io.helidon.messaging.connectors.kafka {\n     requires java.logging;\n \n     requires static cdi.api;\n     requires static javax.inject;\n-    requires static java.activation;\n     requires static kafka.clients;\n-    requires io.helidon.microprofile.config;\n-    requires io.helidon.microprofile.server;\n     requires io.helidon.microprofile.reactive;\n     requires org.reactivestreams;\n     requires transitive io.helidon.config;\n+    // FIXME Remove microprofile\n     requires transitive microprofile.reactive.messaging.api;\n     requires transitive microprofile.reactive.streams.operators.api;\n-    requires io.helidon.microprofile.messaging;\n+    requires io.helidon.common.context;\n+    requires io.helidon.common.configurable;\n \n     exports io.helidon.messaging.connectors.kafka;\n }\n\\ No newline at end of file\n", "next_change": {"commit": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/main/java/module-info.java b/microprofile/connectors/kafka/src/main/java/module-info.java\nsimilarity index 78%\nrename from messaging/connectors/kafka/src/main/java/module-info.java\nrename to microprofile/connectors/kafka/src/main/java/module-info.java\nindex d8f63acd2..8209fc27a 100644\n--- a/messaging/connectors/kafka/src/main/java/module-info.java\n+++ b/microprofile/connectors/kafka/src/main/java/module-info.java\n", "chunk": "@@ -14,20 +14,21 @@\n  * limitations under the License.\n  */\n \n-module io.helidon.messaging.connectors.kafka {\n+module io.helidon.microprofile.connectors.kafka {\n     requires java.logging;\n \n     requires static cdi.api;\n     requires static javax.inject;\n+    requires static java.activation;\n     requires static kafka.clients;\n+    requires io.helidon.microprofile.config;\n+    requires io.helidon.microprofile.server;\n     requires io.helidon.microprofile.reactive;\n     requires org.reactivestreams;\n     requires transitive io.helidon.config;\n-    // FIXME Remove microprofile\n     requires transitive microprofile.reactive.messaging.api;\n     requires transitive microprofile.reactive.streams.operators.api;\n-    requires io.helidon.common.context;\n-    requires io.helidon.common.configurable;\n+    requires io.helidon.microprofile.messaging;\n \n-    exports io.helidon.messaging.connectors.kafka;\n+    exports io.helidon.microprofile.connectors.kafka;\n }\n\\ No newline at end of file\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTMzNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319337", "body": "This is a CDI bean, should not be part of SE impementation at all.", "bodyText": "This is a CDI bean, should not be part of SE impementation at all.", "bodyHTML": "<p dir=\"auto\">This is a CDI bean, should not be part of SE impementation at all.</p>", "author": "tomas-langer", "createdAt": "2020-04-14T17:42:21Z", "path": "messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import io.helidon.messaging.connectors.kafka.KafkaMessage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.Outgoing;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * This class contains the outputs of the tests. In order to avoid that one test mess up in the results\n+ * of other tests (this could happen when some data is produced in one test and it is not committed),\n+ * there are many subclasses of AbstractSampleBean.\n+ */\n+abstract class AbstractSampleBean {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "85dd94f6a91b16c06439440205939007d81c4011", "changed_code": [{"header": "diff --git a/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java b/tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\nsimilarity index 87%\nrename from messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\nrename to tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\nindex 83f6a5257..7b8cec249 100644\n--- a/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\n+++ b/tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\n", "chunk": "@@ -49,19 +52,38 @@ abstract class AbstractSampleBean {\n \n     private static final Logger LOGGER = Logger.getLogger(AbstractSampleBean.class.getName());\n     private final List<String> consumed = Collections.synchronizedList(new ArrayList<>());\n-    private CountDownLatch testChannelLatch = new CountDownLatch(0);\n+    private final AtomicLong requests = new AtomicLong();\n+    private final AtomicLong expectedRequests = new AtomicLong(Long.MAX_VALUE);\n+    private CountDownLatch testChannelLatch = new CountDownLatch(1);\n \n     protected List<String> consumed() {\n         return consumed;\n     }\n \n-    protected void setCountDownLatch(CountDownLatch testChannelLatch) {\n-        this.testChannelLatch = testChannelLatch;\n+    void expectedRequests(long expectedRequests) {\n+        this.expectedRequests.getAndSet(expectedRequests);\n+    }\n+\n+    void await() {\n+        try {\n+            assertTrue(testChannelLatch.await(30, TimeUnit.SECONDS));\n+        } catch (InterruptedException e) {\n+            fail(e);\n+        }\n+    }\n+\n+    void restart() {\n+        requests.getAndSet(0);\n+        expectedRequests.getAndSet(Long.MAX_VALUE);\n+        testChannelLatch = new CountDownLatch(1);\n+        consumed.clear();\n     }\n \n     protected void countDown(String method) {\n         LOGGER.fine(\"Count down triggered by \" + method);\n-        testChannelLatch.countDown();\n+        if (requests.incrementAndGet() >= expectedRequests.get()) {\n+            testChannelLatch.countDown();\n+        }\n     }\n \n     @ApplicationScoped\n", "next_change": null}]}}, {"oid": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "url": "https://github.com/oracle/helidon/commit/a5ee8f4ea70246d599e74c8967656108e723fb6e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T18:42:50Z", "type": "forcePushed"}, {"oid": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "url": "https://github.com/oracle/helidon/commit/b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T06:44:07Z", "type": "forcePushed"}, {"oid": "724b9f8a6ee57542b101efb18ee500be8f96aa7b", "url": "https://github.com/oracle/helidon/commit/724b9f8a6ee57542b101efb18ee500be8f96aa7b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T09:17:11Z", "type": "forcePushed"}, {"oid": "819cb9c1a418302727f9f2d2dfffb59d350c951c", "url": "https://github.com/oracle/helidon/commit/819cb9c1a418302727f9f2d2dfffb59d350c951c", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T10:15:53Z", "type": "forcePushed"}, {"oid": "443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "url": "https://github.com/oracle/helidon/commit/443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T16:30:56Z", "type": "forcePushed"}, {"oid": "127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "url": "https://github.com/oracle/helidon/commit/127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T17:10:53Z", "type": "forcePushed"}, {"oid": "5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "url": "https://github.com/oracle/helidon/commit/5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T05:43:59Z", "type": "forcePushed"}, {"oid": "1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "url": "https://github.com/oracle/helidon/commit/1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T06:57:02Z", "type": "forcePushed"}, {"oid": "ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "url": "https://github.com/oracle/helidon/commit/ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T08:22:05Z", "type": "forcePushed"}, {"oid": "040e496c48dfb131b358d5bc47394fdeeb0ab990", "url": "https://github.com/oracle/helidon/commit/040e496c48dfb131b358d5bc47394fdeeb0ab990", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T09:24:46Z", "type": "forcePushed"}, {"oid": "5164ac2f69b9548de389795d65fc47e06603fee5", "url": "https://github.com/oracle/helidon/commit/5164ac2f69b9548de389795d65fc47e06603fee5", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T10:11:13Z", "type": "forcePushed"}, {"oid": "f92986265ac9529f58a0d10e4e256624f4f6fe1b", "url": "https://github.com/oracle/helidon/commit/f92986265ac9529f58a0d10e4e256624f4f6fe1b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:27:35Z", "type": "forcePushed"}, {"oid": "6ce484aeb96ee5891ce29899845a44cc5d67dc80", "url": "https://github.com/oracle/helidon/commit/6ce484aeb96ee5891ce29899845a44cc5d67dc80", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:31:34Z", "type": "forcePushed"}, {"oid": "5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "url": "https://github.com/oracle/helidon/commit/5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T12:11:27Z", "type": "forcePushed"}, {"oid": "85dd94f6a91b16c06439440205939007d81c4011", "url": "https://github.com/oracle/helidon/commit/85dd94f6a91b16c06439440205939007d81c4011", "message": "Merge branch 'master' into kafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T05:38:12Z", "type": "forcePushed"}, {"oid": "3d22f6ce368e806ceea0047da247afc6460b84f3", "url": "https://github.com/oracle/helidon/commit/3d22f6ce368e806ceea0047da247afc6460b84f3", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:29:55Z", "type": "forcePushed"}, {"oid": "9f76d6cd40937d5622e165bf013edb08cfcf75ef", "url": "https://github.com/oracle/helidon/commit/9f76d6cd40937d5622e165bf013edb08cfcf75ef", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:44:22Z", "type": "forcePushed"}, {"oid": "907340606f71273dc743136f182a860bdb84d348", "url": "https://github.com/oracle/helidon/commit/907340606f71273dc743136f182a860bdb84d348", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:02:46Z", "type": "forcePushed"}, {"oid": "6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "url": "https://github.com/oracle/helidon/commit/6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:09:09Z", "type": "forcePushed"}, {"oid": "c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "url": "https://github.com/oracle/helidon/commit/c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:24:44Z", "type": "forcePushed"}, {"oid": "48eae481f9df3b5c50d586874f8f5610e2af0a3d", "url": "https://github.com/oracle/helidon/commit/48eae481f9df3b5c50d586874f8f5610e2af0a3d", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:01:08Z", "type": "forcePushed"}, {"oid": "c556cc8a85175caf5a60eb795a41cd425acb40b7", "url": "https://github.com/oracle/helidon/commit/c556cc8a85175caf5a60eb795a41cd425acb40b7", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:42:19Z", "type": "forcePushed"}, {"oid": "e17c7ab5db7ddb6861e52dd90796cf77806556ec", "url": "https://github.com/oracle/helidon/commit/e17c7ab5db7ddb6861e52dd90796cf77806556ec", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-21T10:11:04Z", "type": "forcePushed"}, {"oid": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "url": "https://github.com/oracle/helidon/commit/3a896fa19cbc3a64ea69121d1bec080ce30389f8", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:37Z", "type": "commit"}, {"oid": "b5e59d0868995c86edabe0588e078bc4c47e8660", "url": "https://github.com/oracle/helidon/commit/b5e59d0868995c86edabe0588e078bc4c47e8660", "message": "A few small changes.\n\nSigned-off-by: Tomas Langer <tomas.langer@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "0bae61f51a5d949b3b26503945948f198cfa2534", "url": "https://github.com/oracle/helidon/commit/0bae61f51a5d949b3b26503945948f198cfa2534", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "32be94abc930b1516e6eca952fd4e379619563c1", "url": "https://github.com/oracle/helidon/commit/32be94abc930b1516e6eca952fd4e379619563c1", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "url": "https://github.com/oracle/helidon/commit/150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "message": "Move tck tests inside kafka module\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "c4558980f68943d3afb03efc198fa9ba94bf30cb", "url": "https://github.com/oracle/helidon/commit/c4558980f68943d3afb03efc198fa9ba94bf30cb", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "url": "https://github.com/oracle/helidon/commit/430995f595845e9f88bdb7296f3ee72bf8a750c0", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7d68ce582ca13d11dedf930423920617870c62fd", "url": "https://github.com/oracle/helidon/commit/7d68ce582ca13d11dedf930423920617870c62fd", "message": "Improving the tests\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "url": "https://github.com/oracle/helidon/commit/8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7081c593461d66f7eea754455b70d825a6a7cd96", "url": "https://github.com/oracle/helidon/commit/7081c593461d66f7eea754455b70d825a6a7cd96", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:24:13Z", "type": "forcePushed"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "commit"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "forcePushed"}, {"oid": "620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "url": "https://github.com/oracle/helidon/commit/620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "message": "Copyright fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-23T09:07:42Z", "type": "commit"}]}