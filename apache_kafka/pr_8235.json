{"pr_number": 8235, "pr_title": "KAFKA-9176: Do not update limit offset if we are in RESTORE_ACTIVE mode", "pr_author": "guozhangwang", "pr_createdAt": "2020-03-05T22:58:01Z", "pr_url": "https://github.com/apache/kafka/pull/8235", "timeline": [{"oid": "fb50eb8a6fe7fc5956ab3c1b55ce490b294c05c9", "url": "https://github.com/apache/kafka/commit/fb50eb8a6fe7fc5956ab3c1b55ce490b294c05c9", "message": "fix the unnecessary check", "committedDate": "2020-03-05T22:27:05Z", "type": "commit"}, {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf", "url": "https://github.com/apache/kafka/commit/39da56bcae9df1d3640329fd145c31d2d15739bf", "message": "update unit tests", "committedDate": "2020-03-05T22:42:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389866684", "body": "```suggestion\r\n            // when the interval has elapsed we should try to update the limit offset for standbys reading from\r\n            // a source changelog with the new committed offset, unless there are no buffered records since \r\n            // we only need the limit when processing new records\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n          \n          \n            \n                        // we can try to update the limit offset as either committed offset for source changelog partitions;\n          \n          \n            \n                        // when the interval has elapsed we should try to update the limit offset for standbys reading from\n          \n          \n            \n                        // a source changelog with the new committed offset, unless there are no buffered records since \n          \n          \n            \n                        // we only need the limit when processing new records", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">            <span class=\"pl-c\"><span class=\"pl-c\">//</span> for standby changelogs, if the interval has elapsed and there are buffered records not applicable,</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">            <span class=\"pl-c\"><span class=\"pl-c\">//</span> we can try to update the limit offset as either committed offset for source changelog partitions;</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">            <span class=\"pl-c\"><span class=\"pl-c\">//</span> when the interval has elapsed we should try to update the limit offset for standbys reading from</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">            <span class=\"pl-c\"><span class=\"pl-c\">//</span> a source changelog with the new committed offset, unless there are no buffered records since </span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">            <span class=\"pl-c\"><span class=\"pl-c\">//</span> we only need the limit when processing new records</span></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "ableegoldman", "createdAt": "2020-03-09T18:03:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "originalCommit": "39da56bcae9df1d3640329fd145c31d2d15739bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3MTYxMg==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389871612", "bodyText": "Q: if we skip updating the limit offsets because the buffer is empty, and then fetch new records immediately after, we'll still have to wait another commit interval to update the limit offsets. This means the standbys are potentially  lagging behind by up to the commit interval. Do we think that's worth the optimization of skipping the limit offset update?. Note I've seen a number of users report setting the commit interval quite large for various reasons (eg avoid flushing the memtables prematurely).\nMaybe we can find some middle ground by updating the limits before the commit interval if we do get some buffered records and skipped updating previously?", "author": "ableegoldman", "createdAt": "2020-03-09T18:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk3NDg5Nw==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389974897", "bodyText": "We only update lastUpdateOffsetTime = time.milliseconds(); when we've indeed refreshed our committed offsets, so if we did not refresh the committed offsets the timer would not be reset. I think that should be sufficient for covering your concern?", "author": "guozhangwang", "createdAt": "2020-03-09T21:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk4NTU0MA==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389985540", "bodyText": "Cool, yeah that addresses my concern \ud83d\udc4d", "author": "ableegoldman", "createdAt": "2020-03-09T21:59:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkxNDc4Ng==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389914786", "body": "Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.", "bodyText": "Could we just add one more boolean condition into the filter and check whether changelogsWithLimitOffsets is empty or not.", "bodyHTML": "<p dir=\"auto\">Could we just add one more boolean condition into the filter and check whether <code>changelogsWithLimitOffsets</code> is empty or not.</p>", "author": "abbccdda", "createdAt": "2020-03-09T19:33:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;\n+            // for other changelog partitions we do not need to update limit offset at all since we never need to\n+            // check when it completes based on limit offset anyways: the end offset would keep increasing and the\n+            // standby never need to stop\n+            final Set<TopicPartition> changelogsWithLimitOffsets = changelogs.entrySet().stream()\n+                .filter(entry -> entry.getValue().stateManager.taskType() == Task.TaskType.STANDBY &&\n+                    entry.getValue().stateManager.changelogAsSource(entry.getKey()))\n+                .map(Map.Entry::getKey).collect(Collectors.toSet());", "originalCommit": "39da56bcae9df1d3640329fd145c31d2d15739bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk3NjAzOA==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389976038", "bodyText": "If changelogsWithLimitOffsets is empty then the for loop would be a no-op and the updateLimitOffsetsForStandbyChangelogs would not be called.", "author": "guozhangwang", "createdAt": "2020-03-09T21:37:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkxNDc4Ng=="}], "type": "inlineReview"}, {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9", "url": "https://github.com/apache/kafka/commit/00390129ed6c84be7b74018e60f4e6f764efeab9", "message": "Update streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java\n\nCo-Authored-By: A. Sophie Blee-Goldman <ableegoldman@gmail.com>", "committedDate": "2020-03-09T21:35:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjI2NQ==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r390016265", "body": "Is this really sufficient? We did call `restore()` once above with no available records and thus we don't expect that offset limits are updated. Now we call `restore()` again but would potentially update the offset limits at the very end -- hence would we not need one more call to `restore()` that the offset limits did no change?", "bodyText": "Is this really sufficient? We did call restore() once above with no available records and thus we don't expect that offset limits are updated. Now we call restore() again but would potentially update the offset limits at the very end -- hence would we not need one more call to restore() that the offset limits did no change?", "bodyHTML": "<p dir=\"auto\">Is this really sufficient? We did call <code>restore()</code> once above with no available records and thus we don't expect that offset limits are updated. Now we call <code>restore()</code> again but would potentially update the offset limits at the very end -- hence would we not need one more call to <code>restore()</code> that the offset limits did no change?</p>", "author": "mjsax", "createdAt": "2020-03-09T23:29:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -627,6 +627,58 @@ public void shouldOnlyRestoreStandbyChangelogInUpdateStandbyState() {\n         assertTrue(changelogReader.changelogMetadata(tp).bufferedRecords().isEmpty());\n     }\n \n+    @Test\n+    public void shouldNotUpdateLimitForNonSourceStandbyChangelog() {\n+        EasyMock.expect(standbyStateManager.changelogAsSource(tp)).andReturn(false).anyTimes();\n+        EasyMock.replay(standbyStateManager, storeMetadata, store);\n+\n+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {\n+            @Override\n+            public Map<TopicPartition, OffsetAndMetadata> committed(final Set<TopicPartition> partitions) {\n+                throw new AssertionError(\"Should not try to fetch committed offsets\");\n+            }\n+        };\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);\n+        final StreamsConfig config = new StreamsConfig(StreamsTestUtils.getStreamsConfig(\"test-reader\", properties));\n+        final StoreChangelogReader changelogReader = new StoreChangelogReader(time, config, logContext, consumer, callback);\n+        changelogReader.setMainConsumer(consumer);\n+        changelogReader.transitToUpdateStandby();\n+\n+        consumer.updateBeginningOffsets(Collections.singletonMap(tp, 5L));\n+        changelogReader.register(tp, standbyStateManager);\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        // if there's no records fetchable, nothings gets restored\n+        changelogReader.restore();\n+        assertNull(callback.restoreTopicPartition);\n+        assertNull(callback.storeNameCalledStates.get(RESTORE_START));\n+        assertEquals(StoreChangelogReader.ChangelogState.RESTORING, changelogReader.changelogMetadata(tp).state());\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 5L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 6L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 7L, \"key\".getBytes(), \"value\".getBytes()));\n+        // null key should be ignored\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 8L, null, \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 9L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 10L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 11L, \"key\".getBytes(), \"value\".getBytes()));\n+\n+        // we should be able to restore to the log end offsets since there's no limit\n+        changelogReader.restore();", "originalCommit": "00390129ed6c84be7b74018e60f4e6f764efeab9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjU1MA==", "url": "https://github.com/apache/kafka/pull/8235#discussion_r390016550", "bodyText": "Or is this covered by the mockconsumer that would throw?", "author": "mjsax", "createdAt": "2020-03-09T23:30:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjI2NQ=="}], "type": "inlineReview"}]}