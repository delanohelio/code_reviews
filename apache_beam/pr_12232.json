{"pr_number": 12232, "pr_title": "[Beam-9543] Support Match Recognition in Beam SQL", "pr_createdAt": "2020-07-12T09:38:54Z", "pr_url": "https://github.com/apache/beam/pull/12232", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453294680", "bodyText": "In my last PR (#12073), Rui suggested there is a row comparator available for use. I found it a private class in BeamSortRel. I just wonder if I could copy the code from there (maybe also and a reference?). What is the correct way of doing it?", "author": "Mark-Zeng", "createdAt": "2020-07-12T09:52:17Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk3OTgwMA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453979800", "bodyText": "Hi you can update that class to public to use it.", "author": "amaliujia", "createdAt": "2020-07-13T22:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTgzNA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989834", "bodyText": "In fact what you are doing is ok. This is minor.", "author": "amaliujia", "createdAt": "2020-07-13T22:45:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMjY2OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r454912669", "bodyText": "I just updated the implementation using the comparator in BeamSortRel. You will see it in my next commit!", "author": "Mark-Zeng", "createdAt": "2020-07-15T09:19:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4Nzc4OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453987789", "bodyText": "This seems a unused class?", "author": "amaliujia", "createdAt": "2020-07-13T22:40:15Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java", "diffHunk": "@@ -0,0 +1,20 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+public class CEPOperand {}", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMTc2OA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r454911768", "bodyText": "This is indeed redundant.", "author": "Mark-Zeng", "createdAt": "2020-07-15T09:18:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4Nzc4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java b/sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/package-info.java\nsimilarity index 90%\nrename from sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java\nrename to sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/package-info.java\nindex ef451aa93e..642ebf1460 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java\n+++ b/sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/package-info.java\n", "chunk": "@@ -15,6 +15,6 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.impl.cep;\n \n-public class CEPOperand {}\n+/** Snowflake IO data types. */\n+package org.apache.beam.sdk.io.snowflake.data;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989202", "bodyText": "To make sure I understand this example.\nDoes PATTERN(A B C) means it should produce rows, in which each three rows are a set, and in each set, names should be a, b, c and also in this order?", "author": "amaliujia", "createdAt": "2020-07-13T22:44:14Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkyNTQwMQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r454925401", "bodyText": "Yes, if by ''set'' you mean partition. There are 2 output modes: ALL ROWS PER MATCH and ONE ROW PER MATCH (default) which I have not implemented. I just output all rows from a match because I want to check if the pattern-match part works.", "author": "Mark-Zeng", "createdAt": "2020-07-15T09:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkyODAwOA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r454928008", "bodyText": "In Flink CEP, it only supports ONE ROW PER MATCH and the output columns are determined by the PARTITION BY and the MEASURES (not implemented yet) clauses.", "author": "Mark-Zeng", "createdAt": "2020-07-15T09:45:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5MjE0OA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455392148", "bodyText": "I see. Thanks for clarification.", "author": "amaliujia", "createdAt": "2020-07-15T22:05:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\ndeleted file mode 100644\nindex a6657685e0..0000000000\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ /dev/null\n", "chunk": "@@ -1,75 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.sdk.extensions.sql.impl.rel;\n-\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n-\n-import org.apache.beam.sdk.extensions.sql.TestUtils;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n-import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.Row;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-public class BeamMatchRelTest {\n-\n-  @Rule public final TestPipeline pipeline = TestPipeline.create();\n-\n-  @Test\n-  public void matchLogicalPlanTest() {\n-    Schema schemaType =\n-        Schema.builder()\n-            .addInt32Field(\"id\")\n-            .addStringField(\"name\")\n-            .addInt32Field(\"proctime\")\n-            .build();\n-\n-    registerTable(\n-        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n-\n-    String sql =\n-        \"SELECT * \"\n-            + \"FROM TestTable \"\n-            + \"MATCH_RECOGNIZE (\"\n-            + \"PARTITION BY id \"\n-            + \"ORDER BY proctime \"\n-            + \"PATTERN (A B C) \"\n-            + \"DEFINE \"\n-            + \"A AS name = 'a', \"\n-            + \"B AS name = 'b', \"\n-            + \"C AS name = 'c' \"\n-            + \") AS T\";\n-\n-    PCollection<Row> result = compilePipeline(sql, pipeline);\n-\n-    PAssert.that(result)\n-        .containsInAnyOrder(\n-            TestUtils.RowsBuilder.of(\n-                    Schema.FieldType.INT32, \"id\",\n-                    Schema.FieldType.STRING, \"name\",\n-                    Schema.FieldType.INT32, \"proctime\")\n-                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n-                .getRows());\n-\n-    pipeline.run().waitUntilFinish();\n-  }\n-}\n", "next_change": {"commit": "b2b189dfdea88baf34849a17b203058f29212b00", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nnew file mode 100644\nindex 0000000000..4d73a15eb0\n--- /dev/null\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -0,0 +1,80 @@\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Test;\n+import org.apache.beam.sdk.schemas.Schema;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+public class BeamMatchRelTest {\n+\n+  public static final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void MatchLogicalPlanTest() {\n+    Schema schemaType = Schema.builder()\n+        .addInt32Field(\"id\")\n+        .addStringField(\"name\")\n+        .addInt32Field(\"proctime\")\n+        .build();\n+\n+    registerTable(\n+            \"TestTable\",\n+            TestBoundedTable.of(\n+                    schemaType)\n+                    .addRows(\n+                            1, \"a\", 1,\n+                            1, \"b\", 2,\n+                            1, \"c\", 3\n+                    ));\n+\n+//\n+//    PCollection<Row> input =\n+//        pipeline.apply(\n+//            Create.of(\n+//                Row.withSchema(schemaType).addValues(\n+//                        1, \"a\", 1,\n+//                        1, \"b\", 2,\n+//                        1, \"c\", 3\n+//                ).build())\n+//                .withRowSchema(schemaType));\n+\n+    String sql = \"SELECT T.aid, T.bid, T.cid \" +\n+        \"FROM TestTable \" +\n+        \"MATCH_RECOGNIZE (\" +\n+        \"PARTITION BY id \" +\n+        \"ORDER BY proctime \" +\n+        \"MEASURES \" +\n+        \"A.id AS aid, \" +\n+        \"B.id AS bid, \" +\n+        \"C.id AS cid \" +\n+        \"PATTERN (A B C) \" +\n+        \"DEFINE \" +\n+        \"A AS name = 'a', \" +\n+        \"B AS name = 'b', \" +\n+        \"C AS name = 'c' \" +\n+        \") AS T\";\n+\n+//    PCollection<Row> result = input.apply(SqlTransform.query(sql));\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+            .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+            .getRows()\n+        );\n+\n+    pipeline.run().waitUntilFinish();\n+\n+  }\n+}\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex 4d73a15eb0..a6657685e0 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -1,80 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.rel;\n \n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n import org.apache.beam.sdk.extensions.sql.TestUtils;\n import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n import org.junit.Test;\n-import org.apache.beam.sdk.schemas.Schema;\n-\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n \n public class BeamMatchRelTest {\n \n-  public static final TestPipeline pipeline = TestPipeline.create();\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n \n   @Test\n-  public void MatchLogicalPlanTest() {\n-    Schema schemaType = Schema.builder()\n-        .addInt32Field(\"id\")\n-        .addStringField(\"name\")\n-        .addInt32Field(\"proctime\")\n-        .build();\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n \n     registerTable(\n-            \"TestTable\",\n-            TestBoundedTable.of(\n-                    schemaType)\n-                    .addRows(\n-                            1, \"a\", 1,\n-                            1, \"b\", 2,\n-                            1, \"c\", 3\n-                    ));\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n \n-//\n-//    PCollection<Row> input =\n-//        pipeline.apply(\n-//            Create.of(\n-//                Row.withSchema(schemaType).addValues(\n-//                        1, \"a\", 1,\n-//                        1, \"b\", 2,\n-//                        1, \"c\", 3\n-//                ).build())\n-//                .withRowSchema(schemaType));\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n \n-    String sql = \"SELECT T.aid, T.bid, T.cid \" +\n-        \"FROM TestTable \" +\n-        \"MATCH_RECOGNIZE (\" +\n-        \"PARTITION BY id \" +\n-        \"ORDER BY proctime \" +\n-        \"MEASURES \" +\n-        \"A.id AS aid, \" +\n-        \"B.id AS bid, \" +\n-        \"C.id AS cid \" +\n-        \"PATTERN (A B C) \" +\n-        \"DEFINE \" +\n-        \"A AS name = 'a', \" +\n-        \"B AS name = 'b', \" +\n-        \"C AS name = 'c' \" +\n-        \") AS T\";\n-\n-//    PCollection<Row> result = input.apply(SqlTransform.query(sql));\n     PCollection<Row> result = compilePipeline(sql, pipeline);\n \n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n-            .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n-            .getRows()\n-        );\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n \n     pipeline.run().waitUntilFinish();\n-\n   }\n }\n", "next_change": {"commit": "f529b876a2c2e43d012c71b3a83ebd55eb16f4ff", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex a6657685e0..eb6d9372ad 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -72,4 +72,43 @@ public class BeamMatchRelTest {\n \n     pipeline.run().waitUntilFinish();\n   }\n+\n+  @Test\n+  public void matchQuantifierTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n }\n", "next_change": {"commit": "0bf24db5e75c0db715c6954b11afac357c49d7f6", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex eb6d9372ad..b936263e1d 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -103,9 +105,9 @@ public class BeamMatchRelTest {\n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n                 .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n                 .getRows());\n \n", "next_change": {"commit": "adc2354752ef48237020f3fa84d00ab65c2ead74", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex b936263e1d..46f18f13cb 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -113,4 +115,130 @@ public class BeamMatchRelTest {\n \n     pipeline.run().waitUntilFinish();\n   }\n+\n+  @Test\n+  public void matchMeasuresTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType).addRows(\n+            1, \"a\", 1,\n+            1, \"a\", 2,\n+            1, \"b\", 3,\n+            1, \"c\", 4,\n+            1, \"b\", 8,\n+            1, \"a\", 7,\n+            1, \"c\", 9,\n+            2, \"a\", 6,\n+            2, \"b\", 10,\n+            2, \"c\", 11,\n+            5, \"a\", 0));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"MEASURES \"\n+            + \"LAST (A.id) AS aid, \"\n+            + \"B.id AS bid, \"\n+            + \"C.id AS cid \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"T.aid\",\n+                Schema.FieldType.INT32, \"T.bid\",\n+                Schema.FieldType.INT32, \"T.cid\")\n+                .addRows(2, 3, 4,\n+                    7, 8, 9,\n+                    6, 10, 11\n+                    )\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  /*\n+  @Test\n+  public void matchNFATest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addStringField(\"Symbol\")\n+            .addDateTimeField(\"TradeDay\")\n+            .addInt32Field(\"Price\")\n+            .build();\n+\n+    registerTable(\n+        \"Ticker\", TestBoundedTable.of(schemaType).addRows(\n+            \"a\", \"2020-07-01\", 32, // 1st A\n+            \"a\", \"2020-06-01\", 34,\n+            \"a\", \"2020-07-02\", 31, // B\n+            \"a\", \"2020-08-30\", 30, // B\n+            \"a\", \"2020-08-31\", 35, // C\n+            \"a\", \"2020-10-01\", 28,\n+            \"a\", \"2020-10-15\", 30, // 2nd A\n+            \"a\", \"2020-11-01\", 22, // B\n+            \"a\", \"2020-11-08\", 29, // C\n+            \"a\", \"2020-12-10\", 30, // C\n+            \"b\", \"2020-12-01\", 22,\n+            \"c\", \"2020-05-16\", 27, // A\n+            \"c\", \"2020-09-14\", 26, // B\n+            \"c\", \"2020-10-13\", 30)); // C\n+\n+    // match `V` shapes in prices\n+    String sql =\n+        \"SELECT M.Symbol,\"\n+            + \" M.Matchno,\"\n+            + \" M.Startp,\"\n+            + \" M.Bottomp,\"\n+            + \" M.Endp,\"\n+            + \" M.Avgp\"\n+            + \"FROM Ticker \"\n+            + \"MATCH_RECOGNIZE (\"\n+              + \"PARTITION BY Symbol \"\n+              + \"ORDER BY Tradeday \"\n+              + \"MEASURES \"\n+              + \"MATCH_NUMBER() AS Matchno, \"\n+              + \"A.price AS Startp, \"\n+              + \"LAST (B.Price) AS Bottomp, \"\n+              + \"LAST (C.Price) AS ENDp, \"\n+              + \"AVG (U.Price) AS Avgp \"\n+              + \"AFTER MATCH SKIP PAST LAST ROW \"\n+              + \"PATTERN (A B+ C+) \"\n+              + \"SUBSET U = (A, B, C) \"\n+              + \"DEFINE \"\n+              + \"B AS B.Price < PREV (B.Price), \"\n+              + \"C AS C.Price > PREV (C.Price) \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+  */\n }\n", "next_change": {"commit": "799491bbe96fdcefd6ca1cc0f974c202159c8a91", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex 46f18f13cb..097ca190bc 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -232,13 +253,12 @@ public class BeamMatchRelTest {\n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n                 .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n                 .getRows());\n \n     pipeline.run().waitUntilFinish();\n   }\n-  */\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTM4OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989389", "bodyText": "For all new classes, please add javadoc to explain these classes (i.e. /** */). Adding comments are usual good idea to improve your code's readability.", "author": "amaliujia", "createdAt": "2020-07-13T22:44:44Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rule;\n+\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamLogicalConvention;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamMatchRel;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.Convention;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.convert.ConverterRule;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n+\n+public class BeamMatchRule extends ConverterRule {", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex efdd0da649..6b2decaf63 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -1,24 +1,7 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n package org.apache.beam.sdk.extensions.sql.impl.rule;\n \n-import org.apache.beam.sdk.extensions.sql.impl.rel.BeamLogicalConvention;\n import org.apache.beam.sdk.extensions.sql.impl.rel.BeamMatchRel;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamLogicalConvention;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.Convention;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.convert.ConverterRule;\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex 6b2decaf63..efdd0da649 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -1,7 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.rule;\n \n-import org.apache.beam.sdk.extensions.sql.impl.rel.BeamMatchRel;\n import org.apache.beam.sdk.extensions.sql.impl.rel.BeamLogicalConvention;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamMatchRel;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.Convention;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.convert.ConverterRule;\n", "next_change": {"commit": "0bf24db5e75c0db715c6954b11afac357c49d7f6", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex efdd0da649..6441c79840 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -25,6 +25,7 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.convert.Con\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n \n+/** {@code ConverterRule} to replace {@code Match} with {@code BeamMatchRel}. */\n public class BeamMatchRule extends ConverterRule {\n   public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n \n", "next_change": null}]}}, {"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex 6b2decaf63..efdd0da649 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -9,35 +26,31 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n \n public class BeamMatchRule extends ConverterRule {\n-    public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n+  public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n \n-    private BeamMatchRule() {\n-        super(LogicalMatch.class,\n-                Convention.NONE,\n-                BeamLogicalConvention.INSTANCE,\n-                \"BeamMatchRule\");\n-    }\n+  private BeamMatchRule() {\n+    super(LogicalMatch.class, Convention.NONE, BeamLogicalConvention.INSTANCE, \"BeamMatchRule\");\n+  }\n \n-    @Override\n-    public RelNode convert(RelNode rel) {\n-        Match match = (Match) rel;\n-        final RelNode input = match.getInput();\n-        return new BeamMatchRel(\n-            match.getCluster(),\n-            match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n-            convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n-            match.getRowType(),\n-            match.getPattern(),\n-            match.isStrictStart(),\n-            match.isStrictEnd(),\n-            match.getPatternDefinitions(),\n-            match.getMeasures(),\n-            match.getAfter(),\n-            match.getSubsets(),\n-            match.isAllRows(),\n-            match.getPartitionKeys(),\n-            match.getOrderKeys(),\n-            match.getInterval()\n-        );\n-    }\n+  @Override\n+  public RelNode convert(RelNode rel) {\n+    Match match = (Match) rel;\n+    final RelNode input = match.getInput();\n+    return new BeamMatchRel(\n+        match.getCluster(),\n+        match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n+        convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n+        match.getRowType(),\n+        match.getPattern(),\n+        match.isStrictStart(),\n+        match.isStrictEnd(),\n+        match.getPatternDefinitions(),\n+        match.getMeasures(),\n+        match.getAfter(),\n+        match.getSubsets(),\n+        match.isAllRows(),\n+        match.getPartitionKeys(),\n+        match.getOrderKeys(),\n+        match.getInterval());\n+  }\n }\n", "next_change": null}]}}, {"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex efdd0da649..6b2decaf63 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -26,31 +9,35 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n \n public class BeamMatchRule extends ConverterRule {\n-  public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n+    public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n \n-  private BeamMatchRule() {\n-    super(LogicalMatch.class, Convention.NONE, BeamLogicalConvention.INSTANCE, \"BeamMatchRule\");\n-  }\n+    private BeamMatchRule() {\n+        super(LogicalMatch.class,\n+                Convention.NONE,\n+                BeamLogicalConvention.INSTANCE,\n+                \"BeamMatchRule\");\n+    }\n \n-  @Override\n-  public RelNode convert(RelNode rel) {\n-    Match match = (Match) rel;\n-    final RelNode input = match.getInput();\n-    return new BeamMatchRel(\n-        match.getCluster(),\n-        match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n-        convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n-        match.getRowType(),\n-        match.getPattern(),\n-        match.isStrictStart(),\n-        match.isStrictEnd(),\n-        match.getPatternDefinitions(),\n-        match.getMeasures(),\n-        match.getAfter(),\n-        match.getSubsets(),\n-        match.isAllRows(),\n-        match.getPartitionKeys(),\n-        match.getOrderKeys(),\n-        match.getInterval());\n-  }\n+    @Override\n+    public RelNode convert(RelNode rel) {\n+        Match match = (Match) rel;\n+        final RelNode input = match.getInput();\n+        return new BeamMatchRel(\n+            match.getCluster(),\n+            match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n+            convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n+            match.getRowType(),\n+            match.getPattern(),\n+            match.isStrictStart(),\n+            match.isStrictEnd(),\n+            match.getPatternDefinitions(),\n+            match.getMeasures(),\n+            match.getAfter(),\n+            match.getSubsets(),\n+            match.isAllRows(),\n+            match.getPartitionKeys(),\n+            match.getOrderKeys(),\n+            match.getInterval()\n+        );\n+    }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\nindex 6b2decaf63..efdd0da649 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java\n", "chunk": "@@ -9,35 +26,31 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n \n public class BeamMatchRule extends ConverterRule {\n-    public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n+  public static final BeamMatchRule INSTANCE = new BeamMatchRule();\n \n-    private BeamMatchRule() {\n-        super(LogicalMatch.class,\n-                Convention.NONE,\n-                BeamLogicalConvention.INSTANCE,\n-                \"BeamMatchRule\");\n-    }\n+  private BeamMatchRule() {\n+    super(LogicalMatch.class, Convention.NONE, BeamLogicalConvention.INSTANCE, \"BeamMatchRule\");\n+  }\n \n-    @Override\n-    public RelNode convert(RelNode rel) {\n-        Match match = (Match) rel;\n-        final RelNode input = match.getInput();\n-        return new BeamMatchRel(\n-            match.getCluster(),\n-            match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n-            convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n-            match.getRowType(),\n-            match.getPattern(),\n-            match.isStrictStart(),\n-            match.isStrictEnd(),\n-            match.getPatternDefinitions(),\n-            match.getMeasures(),\n-            match.getAfter(),\n-            match.getSubsets(),\n-            match.isAllRows(),\n-            match.getPartitionKeys(),\n-            match.getOrderKeys(),\n-            match.getInterval()\n-        );\n-    }\n+  @Override\n+  public RelNode convert(RelNode rel) {\n+    Match match = (Match) rel;\n+    final RelNode input = match.getInput();\n+    return new BeamMatchRel(\n+        match.getCluster(),\n+        match.getTraitSet().replace(BeamLogicalConvention.INSTANCE),\n+        convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)),\n+        match.getRowType(),\n+        match.getPattern(),\n+        match.isStrictStart(),\n+        match.isStrictEnd(),\n+        match.getPatternDefinitions(),\n+        match.getMeasures(),\n+        match.getAfter(),\n+        match.getSubsets(),\n+        match.isAllRows(),\n+        match.getPartitionKeys(),\n+        match.getOrderKeys(),\n+        match.getInterval());\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MTEzMQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453991131", "bodyText": "Nit: upstreamSchema might be a better variable name.", "author": "amaliujia", "createdAt": "2020-07-13T22:49:42Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxOTg2MA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r454919860", "bodyText": "Done.", "author": "Mark-Zeng", "createdAt": "2020-07-15T09:32:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MTEzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453992257", "bodyText": "Ah so is collectionSchema's field name the same as varNode's name (including that $)?\nSee Schema.getName API: https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L1270", "author": "amaliujia", "createdAt": "2020-07-13T22:53:05Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5NzgzNg==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455197836", "bodyText": "The varNode is an instance of RexVariable. I wanted to get the index or column name from it; the getName method returns a string like '$9' which is the field index preceded by a dollar sign. This is a very awkward way of extracting the information, but I could not think of a better one.", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:01:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgxNDUwNQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r458814505", "bodyText": "I just spotted that I used the wrong class. The subclass RexInputRef of RexVariable has the information of both the column index and field (column) name. This part will change in my later commits.", "author": "Mark-Zeng", "createdAt": "2020-07-22T14:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjQyNA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453992424", "bodyText": "Nit: name it PartitionKeySchema might be more readable.", "author": "amaliujia", "createdAt": "2020-07-13T22:53:36Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5ODkzMg==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455198932", "bodyText": "Agree. Still need practice on naming variables : )", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:03:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjQyNA=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5Mzc5OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453993799", "bodyText": "In fact, there is also a NullDirection to consider (Null first/Null last): https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/RelFieldCollation.java#L185\nIt is ok to not handle it for now, but please leave a TODO comment (i.e. // TODO: handle NullDirection)", "author": "amaliujia", "createdAt": "2020-07-13T22:57:36Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwMTU2MA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455201560", "bodyText": "I have just added the implementation for it.", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:07:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5Mzc5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NDcxNA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453994714", "bodyText": "I think you got to make Pattern p as a variable to compile once?", "author": "amaliujia", "createdAt": "2020-07-13T23:00:14Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwNDc5MQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455204791", "bodyText": "I am very new to the regex library of Java. This my first time of using it. I just followed the Oracle doc on regex and the example in it.", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NDcxNA=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453995446", "bodyText": "Is it possible to reuse https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L413?", "author": "amaliujia", "createdAt": "2020-07-13T23:02:28Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.io.Serializable;\n+\n+public enum CEPTypeName implements Serializable {", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NjI0OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453996249", "bodyText": "It is ok though if you still want to use a separate enum for CEP types, since it is a standalone library.\nJust curious, is there a type that is not covered by https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L413?", "author": "amaliujia", "createdAt": "2020-07-13T23:04:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzk0OQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455213949", "bodyText": "I think it is possible; reusing the schema types seems natural also. I will update it in my next commit.", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:26:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxODUyMg==", "url": "https://github.com/apache/beam/pull/12232#discussion_r455218522", "bodyText": "There are some differences. RexLiteral class seems to have a wider support for time interval. And I just found in the page that the value of the Double type in a literal node is BigDecimal? weird.", "author": "Mark-Zeng", "createdAt": "2020-07-15T17:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java b/sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/SnowflakeDataType.java\nsimilarity index 80%\nrename from sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java\nrename to sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/SnowflakeDataType.java\nindex ede10e431f..4b8caf5748 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java\n+++ b/sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/data/SnowflakeDataType.java\n", "chunk": "@@ -15,19 +15,11 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.impl.cep;\n+package org.apache.beam.sdk.io.snowflake.data;\n \n import java.io.Serializable;\n \n-public enum CEPTypeName implements Serializable {\n-  BYTE,\n-  INT16,\n-  INT32,\n-  INT64,\n-  DECIMAL,\n-  FLOAT,\n-  DOUBLE,\n-  STRING,\n-  DATETIME,\n-  BOOLEAN\n+/** Interface for data types to provide SQLs for themselves. */\n+public interface SnowflakeDataType extends Serializable {\n+  String sql();\n }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NjgwMw==", "url": "https://github.com/apache/beam/pull/12232#discussion_r453996803", "bodyText": "This will rely on an assumption that Fusion will fuse operators here so the sorted result will be preserved for the next match transform. In most of the runners (if not all) this should be true.", "author": "amaliujia", "createdAt": "2020-07-13T23:06:30Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));", "originalCommit": "1727e170ef88ed8150a7fd30f6f9254ef1031548", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex b948ca791b..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -48,333 +13,127 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-\n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n-\n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema collectionSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(collectionSchema.getField(index));\n-      }\n-      Schema mySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output the all rows of each pattern matched (for testing purpose)\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.toString();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n-        this.cSchema = cSchema;\n-\n-        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-        Collections.reverse(revOrderKeys);\n-        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-        for (RelFieldCollation i : revOrderKeys) {\n-          int fIndex = i.getFieldIndex();\n-          RelFieldCollation.Direction dir = i.getDirection();\n-          if (dir == RelFieldCollation.Direction.ASCENDING) {\n-            revOrderKeysList.add(new OrderKey(fIndex, false));\n-          } else {\n-            revOrderKeysList.add(new OrderKey(fIndex, true));\n-          }\n-        }\n-\n-        this.orderKeys = revOrderKeysList;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-        for (OrderKey i : orderKeys) {\n-          int fIndex = i.getIndex();\n-          boolean dir = i.getDir();\n-          rows.sort(new SortComparator(fIndex, dir));\n-        }\n-        // TODO: Change the comparator to the row comparator:\n-        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n-\n-      private class SortComparator implements Comparator<Row> {\n-\n-        private final int fIndex;\n-        private final int inv;\n-\n-        public SortComparator(int fIndex, boolean inverse) {\n-          this.fIndex = fIndex;\n-          this.inv = inverse ? -1 : 1;\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        @Override\n-        public int compare(Row o1, Row o2) {\n-          Schema.Field fd = cSchema.getField(fIndex);\n-          Schema.FieldType dtype = fd.getType();\n-          switch (dtype.getTypeName()) {\n-            case BYTE:\n-              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n-            case INT16:\n-              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n-            case INT32:\n-              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n-            case INT64:\n-              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n-            case DECIMAL:\n-              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n-            case FLOAT:\n-              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n-            case DOUBLE:\n-              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n-            case STRING:\n-              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n-            case DATETIME:\n-              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n-            case BOOLEAN:\n-              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n-            default:\n-              throw new SqlConversionException(\"Order not supported for specified column\");\n-          }\n-        }\n-      }\n+        return null;\n     }\n-  }\n \n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    private final Schema mySchema;\n-\n-    public MapKeys(Schema mySchema) {\n-      this.mySchema = mySchema;\n-    }\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n-\n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : mySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2Nzc1MA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r457867750", "bodyText": "I realized this should be a while loop. And for regex implementation, the default after match strategy is \"skip past last row\". I will change it to a while loop in my next commit.", "author": "Mark-Zeng", "createdAt": "2020-07-21T06:34:19Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil.makeOrderKeysFromCollation;\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@code BeamRelNode} to replace a {@code Match} node.\n+ *\n+ * <p>The {@code BeamMatchRel} is the Beam implementation of {@code MATCH_RECOGNIZE} in SQL.\n+ *\n+ * <p>For now, the underline implementation is based on java.util.regex.\n+ */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema upstreamSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(upstreamSchema.getField(index));\n+      }\n+      Schema partitionKeySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream =\n+          upstream.apply(ParDo.of(new MapKeys(partitionKeySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(partitionKeySchema), RowCoder.of(upstreamSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      ArrayList<OrderKey> orderKeyList = makeOrderKeysFromCollation(orderKeys);\n+      // This will rely on an assumption that Fusion will fuse\n+      // operators here so the sorted result will be preserved\n+      // for the next match transform.\n+      // In most of the runners (if not all) this should be true.\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(upstreamSchema, orderKeyList)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(upstreamSchema, pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern(pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output all rows of each pattern matched (for testing purpose)\n+      // TODO: add ONE ROW PER MATCH and MEASURES implementation.\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(upstreamSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.getPatternVar();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }", "originalCommit": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex 380af0528f..c20c4b189b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -55,284 +22,118 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-/**\n- * {@code BeamRelNode} to replace a {@code Match} node.\n- *\n- * <p>The {@code BeamMatchRel} is the Beam implementation of {@code MATCH_RECOGNIZE} in SQL.\n- *\n- * <p>For now, the underline implementation is based on java.util.regex.\n- */\n-public class BeamMatchRel extends Match implements BeamRelNode {\n-\n-  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n-\n-  public BeamMatchRel(\n-      RelOptCluster cluster,\n-      RelTraitSet traitSet,\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n-\n-    super(\n-        cluster,\n-        traitSet,\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n-\n-  @Override\n-  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n-  }\n-\n-  @Override\n-  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-    // a simple way of getting some estimate data\n-    // to be examined further\n-    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-    double numRows = inputEstimate.getRowCount();\n-    double winSize = inputEstimate.getWindow();\n-    double rate = inputEstimate.getRate();\n-\n-    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n-  }\n-\n-  @Override\n-  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-\n-    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n-  }\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n \n-  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private final List<RexNode> parKeys;\n-    private final RelCollation orderKeys;\n-    private final RexNode pattern;\n-    private final Map<String, RexNode> patternDefs;\n+    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public MatchTransform(\n-        List<RexNode> parKeys,\n-        RelCollation orderKeys,\n+    public BeamMatchRel(\n+        RelOptCluster cluster,\n+        RelTraitSet traitSet,\n+        RelNode input,\n+        RelDataType rowType,\n         RexNode pattern,\n-        Map<String, RexNode> patternDefs) {\n-      this.parKeys = parKeys;\n-      this.orderKeys = orderKeys;\n-      this.pattern = pattern;\n-      this.patternDefs = patternDefs;\n-    }\n-\n-    @Override\n-    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n-      checkArgument(\n-          pinput.size() == 1,\n-          \"Wrong number of inputs for %s: %s\",\n-          BeamMatchRel.class.getSimpleName(),\n-          pinput);\n-      PCollection<Row> upstream = pinput.get(0);\n-\n-      Schema upstreamSchema = upstream.getSchema();\n-\n-      Schema.Builder schemaBuilder = new Schema.Builder();\n-      for (RexNode i : parKeys) {\n-        RexVariable varNode = (RexVariable) i;\n-        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n-        schemaBuilder.addField(upstreamSchema.getField(index));\n-      }\n-      Schema partitionKeySchema = schemaBuilder.build();\n-\n-      // partition according to the partition keys\n-      PCollection<KV<Row, Row>> keyedUpstream =\n-          upstream.apply(ParDo.of(new MapKeys(partitionKeySchema)));\n-\n-      // group by keys\n-      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n-          keyedUpstream\n-              .setCoder(KvCoder.of(RowCoder.of(partitionKeySchema), RowCoder.of(upstreamSchema)))\n-              .apply(GroupByKey.create());\n-\n-      // sort within each keyed partition\n-      ArrayList<OrderKey> orderKeyList = makeOrderKeysFromCollation(orderKeys);\n-      // This will rely on an assumption that Fusion will fuse\n-      // operators here so the sorted result will be preserved\n-      // for the next match transform.\n-      // In most of the runners (if not all) this should be true.\n-      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n-          groupedUpstream.apply(ParDo.of(new SortPerKey(upstreamSchema, orderKeyList)));\n-\n-      // apply the pattern match in each partition\n-      ArrayList<CEPPattern> cepPattern =\n-          CEPUtil.getCEPPatternFromPattern(upstreamSchema, pattern, patternDefs);\n-      String regexPattern = CEPUtil.getRegexFromPattern(pattern);\n-      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n-          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n-\n-      // apply the ParDo for the measures clause\n-      // for now, output all rows of each pattern matched (for testing purpose)\n-      // TODO: add ONE ROW PER MATCH and MEASURES implementation.\n-      PCollection<Row> outStream =\n-          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(upstreamSchema);\n+        boolean strictStart,\n+        boolean strictEnd,\n+        Map<String, RexNode> patternDefinitions,\n+        Map<String, RexNode> measures,\n+        RexNode after,\n+        Map<String, ? extends SortedSet<String>> subsets,\n+        boolean allRows,\n+        List<RexNode> partitionKeys,\n+        RelCollation orderKeys,\n+        RexNode interval) {\n+\n+        super(cluster,\n+            traitSet,\n+            input,\n+            rowType,\n+            pattern,\n+            strictStart,\n+            strictEnd,\n+            patternDefinitions,\n+            measures,\n+            after,\n+            subsets,\n+            allRows,\n+            partitionKeys,\n+            orderKeys,\n+            interval);\n \n-      return outStream;\n     }\n \n-    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n-\n-      @ProcessElement\n-      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n-        for (Row i : keyRows.getValue()) {\n-          out.output(i);\n-        }\n-      }\n+    @Override\n+    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n     }\n \n-    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n-    // support only one row per match for now.\n-    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final ArrayList<CEPPattern> pattern;\n-      private final String regexPattern;\n-\n-      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n-        this.pattern = pattern;\n-        this.regexPattern = regexPattern;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<>();\n-        StringBuilder patternString = new StringBuilder();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-          // check pattern of row i\n-          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n-          for (int j = 0; j < pattern.size(); ++j) {\n-            CEPPattern tryPattern = pattern.get(j);\n-            if (tryPattern.evalRow(i)) {\n-              patternOfRow = tryPattern.getPatternVar();\n-            }\n-          }\n-          patternString.append(patternOfRow);\n-        }\n-\n-        Pattern p = Pattern.compile(regexPattern);\n-        Matcher m = p.matcher(patternString.toString());\n-        // if the pattern is (A B+ C),\n-        // it should return a List three rows matching A B C respectively\n-        if (m.matches()) {\n-          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n-        }\n-      }\n+    @Override\n+    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+        // a simple way of getting some estimate data\n+        // to be examined further\n+        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+        double numRows = inputEstimate.getRowCount();\n+        double winSize = inputEstimate.getWindow();\n+        double rate = inputEstimate.getRate();\n+\n+        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n     }\n \n-    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n-\n-      private final Schema cSchema;\n-      private final ArrayList<OrderKey> orderKeys;\n-\n-      public SortPerKey(Schema cSchema, ArrayList<OrderKey> orderKeys) {\n-        this.cSchema = cSchema;\n-        this.orderKeys = orderKeys;\n-      }\n-\n-      @ProcessElement\n-      public void processElement(\n-          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n-        ArrayList<Row> rows = new ArrayList<Row>();\n-        for (Row i : keyRows.getValue()) {\n-          rows.add(i);\n-        }\n-\n-        ArrayList<Integer> fIndexList = new ArrayList<>();\n-        ArrayList<Boolean> dirList = new ArrayList<>();\n-        ArrayList<Boolean> nullDirList = new ArrayList<>();\n-        for (OrderKey i : orderKeys) {\n-          fIndexList.add(i.getIndex());\n-          dirList.add(i.getDir());\n-          nullDirList.add(i.getNullFirst());\n+    @Override\n+    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+        // get the partition columns\n+        for(RexNode i : this.partitionKeys) {\n+            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n         }\n \n-        rows.sort(new BeamSortRel.BeamSqlRowComparator(fIndexList, dirList, nullDirList));\n-\n-        out.output(KV.of(keyRows.getKey(), rows));\n-      }\n+        return null;\n     }\n-  }\n-\n-  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n-\n-    private final Schema partitionKeySchema;\n \n-    public MapKeys(Schema partitionKeySchema) {\n-      this.partitionKeySchema = partitionKeySchema;\n-    }\n+//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+//        public matchTransform()\n+//    }\n \n-    @ProcessElement\n-    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n-      Row.Builder newRowBuilder = Row.withSchema(partitionKeySchema);\n+//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n+//        private final Schema keySchema;\n+//        public mapKeys(Schema keySchema) {\n+//            this.keySchema = keySchema;\n+//        }\n+//    }\n \n-      // no partition specified would result in empty row as keys for rows\n-      for (Schema.Field i : partitionKeySchema.getFields()) {\n-        String fieldName = i.getName();\n-        newRowBuilder.addValue(eleRow.getValue(fieldName));\n-      }\n-      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n-      out.output(kvPair);\n+    @Override\n+    public Match copy(RelNode input,\n+          RelDataType rowType,\n+          RexNode pattern,\n+          boolean strictStart,\n+          boolean strictEnd,\n+          Map<String, RexNode> patternDefinitions,\n+          Map<String, RexNode> measures,\n+          RexNode after,\n+          Map<String, ? extends SortedSet<String>> subsets,\n+          boolean allRows,\n+          List<RexNode> partitionKeys,\n+          RelCollation orderKeys,\n+          RexNode interval) {\n+\n+        return new BeamMatchRel(getCluster(),\n+                getTraitSet(),\n+                input,\n+                rowType,\n+                pattern,\n+                strictStart,\n+                strictEnd,\n+                patternDefinitions,\n+                measures,\n+                after,\n+                subsets,\n+                allRows,\n+                partitionKeys,\n+                orderKeys,\n+                interval);\n     }\n-  }\n-\n-  @Override\n-  public Match copy(\n-      RelNode input,\n-      RelDataType rowType,\n-      RexNode pattern,\n-      boolean strictStart,\n-      boolean strictEnd,\n-      Map<String, RexNode> patternDefinitions,\n-      Map<String, RexNode> measures,\n-      RexNode after,\n-      Map<String, ? extends SortedSet<String>> subsets,\n-      boolean allRows,\n-      List<RexNode> partitionKeys,\n-      RelCollation orderKeys,\n-      RexNode interval) {\n \n-    return new BeamMatchRel(\n-        getCluster(),\n-        getTraitSet(),\n-        input,\n-        rowType,\n-        pattern,\n-        strictStart,\n-        strictEnd,\n-        patternDefinitions,\n-        measures,\n-        after,\n-        subsets,\n-        allRows,\n-        partitionKeys,\n-        orderKeys,\n-        interval);\n-  }\n }\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\nindex c20c4b189b..b948ca791b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java\n", "chunk": "@@ -13,127 +48,333 @@ import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptClus\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-\n /** {@link BeamRelNode} to replace a {@link Match} node. */\n public class BeamMatchRel extends Match implements BeamRelNode {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n \n-    public BeamMatchRel(\n-        RelOptCluster cluster,\n-        RelTraitSet traitSet,\n-        RelNode input,\n-        RelDataType rowType,\n-        RexNode pattern,\n-        boolean strictStart,\n-        boolean strictEnd,\n-        Map<String, RexNode> patternDefinitions,\n-        Map<String, RexNode> measures,\n-        RexNode after,\n-        Map<String, ? extends SortedSet<String>> subsets,\n-        boolean allRows,\n-        List<RexNode> partitionKeys,\n-        RelCollation orderKeys,\n-        RexNode interval) {\n-\n-        super(cluster,\n-            traitSet,\n-            input,\n-            rowType,\n-            pattern,\n-            strictStart,\n-            strictEnd,\n-            patternDefinitions,\n-            measures,\n-            after,\n-            subsets,\n-            allRows,\n-            partitionKeys,\n-            orderKeys,\n-            interval);\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n \n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n     }\n \n     @Override\n-    public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n-        return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n     }\n \n-    @Override\n-    public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n-        // a simple way of getting some estimate data\n-        // to be examined further\n-        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n-        double numRows = inputEstimate.getRowCount();\n-        double winSize = inputEstimate.getWindow();\n-        double rate = inputEstimate.getRate();\n-\n-        return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n     }\n \n-    @Override\n-    public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n-        // get the partition columns\n-        for(RexNode i : this.partitionKeys) {\n-            LOG.info(((RexVariable) i).getName() + \" \" + i.getType());\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n         }\n \n-        return null;\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n     }\n \n-//    private static class matchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n-//        public matchTransform()\n-//    }\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n \n-//    private class mapKeys extends DoFn<Row, KV<Row, Row>> {\n-//        private final Schema keySchema;\n-//        public mapKeys(Schema keySchema) {\n-//            this.keySchema = keySchema;\n-//        }\n-//    }\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n \n-    @Override\n-    public Match copy(RelNode input,\n-          RelDataType rowType,\n-          RexNode pattern,\n-          boolean strictStart,\n-          boolean strictEnd,\n-          Map<String, RexNode> patternDefinitions,\n-          Map<String, RexNode> measures,\n-          RexNode after,\n-          Map<String, ? extends SortedSet<String>> subsets,\n-          boolean allRows,\n-          List<RexNode> partitionKeys,\n-          RelCollation orderKeys,\n-          RexNode interval) {\n-\n-        return new BeamMatchRel(getCluster(),\n-                getTraitSet(),\n-                input,\n-                rowType,\n-                pattern,\n-                strictStart,\n-                strictEnd,\n-                patternDefinitions,\n-                measures,\n-                after,\n-                subsets,\n-                allRows,\n-                partitionKeys,\n-                orderKeys,\n-                interval);\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+\n+        public SortComparator(int fIndex, boolean inverse) {\n+          this.fIndex = fIndex;\n+          this.inv = inverse ? -1 : 1;\n+        }\n+\n+        @Override\n+        public int compare(Row o1, Row o2) {\n+          Schema.Field fd = cSchema.getField(fIndex);\n+          Schema.FieldType dtype = fd.getType();\n+          switch (dtype.getTypeName()) {\n+            case BYTE:\n+              return o1.getByte(fIndex).compareTo(o2.getByte(fIndex)) * inv;\n+            case INT16:\n+              return o1.getInt16(fIndex).compareTo(o2.getInt16(fIndex)) * inv;\n+            case INT32:\n+              return o1.getInt32(fIndex).compareTo(o2.getInt32(fIndex)) * inv;\n+            case INT64:\n+              return o1.getInt64(fIndex).compareTo(o2.getInt64(fIndex)) * inv;\n+            case DECIMAL:\n+              return o1.getDecimal(fIndex).compareTo(o2.getDecimal(fIndex)) * inv;\n+            case FLOAT:\n+              return o1.getFloat(fIndex).compareTo(o2.getFloat(fIndex)) * inv;\n+            case DOUBLE:\n+              return o1.getDouble(fIndex).compareTo(o2.getDouble(fIndex)) * inv;\n+            case STRING:\n+              return o1.getString(fIndex).compareTo(o2.getString(fIndex)) * inv;\n+            case DATETIME:\n+              return o1.getDateTime(fIndex).compareTo(o2.getDateTime(fIndex)) * inv;\n+            case BOOLEAN:\n+              return o1.getBoolean(fIndex).compareTo(o2.getBoolean(fIndex)) * inv;\n+            default:\n+              throw new SqlConversionException(\"Order not supported for specified column\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static class MapKeys extends DoFn<Row, KV<Row, Row>> {\n+\n+    private final Schema mySchema;\n+\n+    public MapKeys(Schema mySchema) {\n+      this.mySchema = mySchema;\n     }\n \n+    @ProcessElement\n+    public void processElement(@Element Row eleRow, OutputReceiver<KV<Row, Row>> out) {\n+      Row.Builder newRowBuilder = Row.withSchema(mySchema);\n+\n+      // no partition specified would result in empty row as keys for rows\n+      for (Schema.Field i : mySchema.getFields()) {\n+        String fieldName = i.getName();\n+        newRowBuilder.addValue(eleRow.getValue(fieldName));\n+      }\n+      KV kvPair = KV.of(newRowBuilder.build(), eleRow);\n+      out.output(kvPair);\n+    }\n+  }\n+\n+  @Override\n+  public Match copy(\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    return new BeamMatchRel(\n+        getCluster(),\n+        getTraitSet(),\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r460341711", "bodyText": "Is it correct to not handle other classes?\nIf so can you add an exception in the last else?", "author": "amaliujia", "createdAt": "2020-07-25T00:42:35Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n+ * operands. It has the similar functionality as Calcite's {@code RexCall}.\n+ */\n+public class CEPCall extends CEPOperation {\n+\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n+\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n+\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n+\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n+\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n+\n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }", "originalCommit": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjU1Mg==", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966552", "bodyText": "will do.", "author": "Mark-Zeng", "createdAt": "2020-08-01T14:21:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjY3NA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r464116674", "bodyText": "Please print the RexNode so people can see which RexNode is not supported.\n \"the RexNode is not recognized: \" + i", "author": "amaliujia", "createdAt": "2020-08-02T19:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNzI2NA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r464337264", "bodyText": "Ok", "author": "Mark-Zeng", "createdAt": "2020-08-03T10:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\ndeleted file mode 100644\nindex b4978035f2..0000000000\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n+++ /dev/null\n", "chunk": "@@ -1,72 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.sdk.extensions.sql.impl.cep;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n-\n-/**\n- * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n- * operands. It has the similar functionality as Calcite's {@code RexCall}.\n- */\n-public class CEPCall extends CEPOperation {\n-\n-  private final CEPOperator operator;\n-  private final List<CEPOperation> operands;\n-\n-  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n-    this.operator = operator;\n-    this.operands = operands;\n-  }\n-\n-  public CEPOperator getOperator() {\n-    return operator;\n-  }\n-\n-  public List<CEPOperation> getOperands() {\n-    return operands;\n-  }\n-\n-  public static CEPCall of(RexCall operation) {\n-    SqlOperator call = operation.getOperator();\n-    CEPOperator myOp = CEPOperator.of(call);\n-\n-    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n-    for (RexNode i : operation.getOperands()) {\n-      if (i.getClass() == RexCall.class) {\n-        CEPCall callToAdd = CEPCall.of((RexCall) i);\n-        operandsList.add(callToAdd);\n-      } else if (i.getClass() == RexLiteral.class) {\n-        RexLiteral lit = (RexLiteral) i;\n-        CEPLiteral litToAdd = CEPLiteral.of(lit);\n-        operandsList.add(litToAdd);\n-      } else if (i.getClass() == RexPatternFieldRef.class) {\n-        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n-        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n-        operandsList.add(fieldRefToAdd);\n-      }\n-    }\n-\n-    return new CEPCall(myOp, operandsList);\n-  }\n-}\n", "next_change": {"commit": "ec7c929c340ba38615145908249be24778ffc436", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\nnew file mode 100644\nindex 0000000000..bb34dd9cb3\n--- /dev/null\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n", "chunk": "@@ -0,0 +1,54 @@\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class CEPCall extends CEPOperation {\n+\n+    private final CEPOperator operator;\n+    private final List<CEPOperation> operands;\n+\n+    private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+        this.operator = operator;\n+        this.operands = operands;\n+    }\n+\n+    public CEPOperator getOperator() {\n+        return operator;\n+    }\n+\n+    public List<CEPOperation> getOperands() {\n+        return operands;\n+    }\n+\n+    public static CEPCall of(RexCall operation) {\n+        SqlOperator call = operation.getOperator();\n+        CEPOperator myOp = CEPOperator.of(call);\n+\n+        ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+        for(RexNode i : operation.getOperands()) {\n+            if(i.getClass() == RexCall.class) {\n+                CEPCall callToAdd = CEPCall.of((RexCall) i);\n+                operandsList.add(callToAdd);\n+            } else if(i.getClass() == RexLiteral.class) {\n+                RexLiteral lit = (RexLiteral) i;\n+                CEPLiteral litToAdd = CEPLiteral.of(lit);\n+                operandsList.add(litToAdd);\n+            } else if(i.getClass() == RexPatternFieldRef.class) {\n+                RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+                CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+                operandsList.add(fieldRefToAdd);\n+            }\n+        }\n+\n+        return new CEPCall(myOp, operandsList);\n+\n+    }\n+\n+}\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\nindex bb34dd9cb3..221d6972ba 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n", "chunk": "@@ -1,54 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.cep;\n \n+import java.util.ArrayList;\n+import java.util.List;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n \n-import java.util.ArrayList;\n-import java.util.List;\n-\n public class CEPCall extends CEPOperation {\n \n-    private final CEPOperator operator;\n-    private final List<CEPOperation> operands;\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n \n-    private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n-        this.operator = operator;\n-        this.operands = operands;\n-    }\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n \n-    public CEPOperator getOperator() {\n-        return operator;\n-    }\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n \n-    public List<CEPOperation> getOperands() {\n-        return operands;\n-    }\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n \n-    public static CEPCall of(RexCall operation) {\n-        SqlOperator call = operation.getOperator();\n-        CEPOperator myOp = CEPOperator.of(call);\n-\n-        ArrayList<CEPOperation> operandsList = new ArrayList<>();\n-        for(RexNode i : operation.getOperands()) {\n-            if(i.getClass() == RexCall.class) {\n-                CEPCall callToAdd = CEPCall.of((RexCall) i);\n-                operandsList.add(callToAdd);\n-            } else if(i.getClass() == RexLiteral.class) {\n-                RexLiteral lit = (RexLiteral) i;\n-                CEPLiteral litToAdd = CEPLiteral.of(lit);\n-                operandsList.add(litToAdd);\n-            } else if(i.getClass() == RexPatternFieldRef.class) {\n-                RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n-                CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n-                operandsList.add(fieldRefToAdd);\n-            }\n-        }\n-\n-        return new CEPCall(myOp, operandsList);\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n \n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }\n     }\n \n+    return new CEPCall(myOp, operandsList);\n+  }\n }\n", "next_change": {"commit": "ebc41a263dc07b305c190f0ded1ec86e90099ee7", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\nindex 221d6972ba..5a3a6771a1 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java\n", "chunk": "@@ -65,4 +69,13 @@ public class CEPCall extends CEPOperation {\n \n     return new CEPCall(myOp, operandsList);\n   }\n+\n+  @Override\n+  public String toString() {\n+    ArrayList<String> operandStrings = new ArrayList<>();\n+    for (CEPOperation i : operands) {\n+      operandStrings.add(i.toString());\n+    }\n+    return operator.toString() + \"(\" + String.join(\", \", operandStrings) + \")\";\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTg2OA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r460341868", "bodyText": "nit: no need add literal here.", "author": "amaliujia", "createdAt": "2020-07-25T00:43:45Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.math.BigDecimal;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.joda.time.ReadableDateTime;\n+\n+/**\n+ * {@code CEPLiteral} represents a literal node. It corresponds to {@code RexLiteral} in Calcite.\n+ */\n+public class CEPLiteral extends CEPOperation {\n+\n+  private final Schema.TypeName typeName;\n+\n+  private CEPLiteral(Schema.TypeName typeName) {\n+    this.typeName = typeName;\n+  }\n+\n+  // TODO: deal with other types (byte, short...)\n+  public static CEPLiteral of(RexLiteral lit) {\n+    switch (lit.getTypeName()) {\n+      case INTEGER:\n+        return of(lit.getValueAs(Integer.class));\n+      case BIGINT:\n+        return of(lit.getValueAs(Long.class));\n+      case DECIMAL:\n+        return of(lit.getValueAs(BigDecimal.class));\n+      case FLOAT:\n+        return of(lit.getValueAs(Float.class));\n+      case DOUBLE:\n+        return of(lit.getValueAs(Double.class));\n+      case BOOLEAN:\n+        return of(lit.getValueAs(Boolean.class));\n+      case DATE:\n+        return of(lit.getValueAs(ReadableDateTime.class));\n+      case CHAR:\n+      case VARCHAR:\n+        return of(lit.getValueAs(String.class));\n+      default:\n+        throw new SqlConversionException(\n+            \"sql literal type not supported: \" + lit.getTypeName().toString());", "originalCommit": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjU1NQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966555", "bodyText": "ok.", "author": "Mark-Zeng", "createdAt": "2020-08-01T14:21:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTg2OA=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\ndeleted file mode 100644\nindex 19505b35fd..0000000000\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n+++ /dev/null\n", "chunk": "@@ -1,196 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.sdk.extensions.sql.impl.cep;\n-\n-import java.math.BigDecimal;\n-import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n-import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n-import org.joda.time.ReadableDateTime;\n-\n-/**\n- * {@code CEPLiteral} represents a literal node. It corresponds to {@code RexLiteral} in Calcite.\n- */\n-public class CEPLiteral extends CEPOperation {\n-\n-  private final Schema.TypeName typeName;\n-\n-  private CEPLiteral(Schema.TypeName typeName) {\n-    this.typeName = typeName;\n-  }\n-\n-  // TODO: deal with other types (byte, short...)\n-  public static CEPLiteral of(RexLiteral lit) {\n-    switch (lit.getTypeName()) {\n-      case INTEGER:\n-        return of(lit.getValueAs(Integer.class));\n-      case BIGINT:\n-        return of(lit.getValueAs(Long.class));\n-      case DECIMAL:\n-        return of(lit.getValueAs(BigDecimal.class));\n-      case FLOAT:\n-        return of(lit.getValueAs(Float.class));\n-      case DOUBLE:\n-        return of(lit.getValueAs(Double.class));\n-      case BOOLEAN:\n-        return of(lit.getValueAs(Boolean.class));\n-      case DATE:\n-        return of(lit.getValueAs(ReadableDateTime.class));\n-      case CHAR:\n-      case VARCHAR:\n-        return of(lit.getValueAs(String.class));\n-      default:\n-        throw new SqlConversionException(\n-            \"sql literal type not supported: \" + lit.getTypeName().toString());\n-    }\n-  }\n-\n-  public static CEPLiteral of(Byte myByte) {\n-    return new CEPLiteral(Schema.TypeName.BYTE) {\n-      @Override\n-      public Byte getByte() {\n-        return myByte;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Short myShort) {\n-    return new CEPLiteral(Schema.TypeName.INT16) {\n-      @Override\n-      public Short getInt16() {\n-        return myShort;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Integer myInt) {\n-    return new CEPLiteral(Schema.TypeName.INT32) {\n-      @Override\n-      public Integer getInt32() {\n-        return myInt;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Long myLong) {\n-    return new CEPLiteral(Schema.TypeName.INT64) {\n-      @Override\n-      public Long getInt64() {\n-        return myLong;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(BigDecimal myDecimal) {\n-    return new CEPLiteral(Schema.TypeName.DECIMAL) {\n-      @Override\n-      public BigDecimal getDecimal() {\n-        return myDecimal;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Float myFloat) {\n-    return new CEPLiteral(Schema.TypeName.FLOAT) {\n-      @Override\n-      public Float getFloat() {\n-        return myFloat;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Double myDouble) {\n-    return new CEPLiteral(Schema.TypeName.DOUBLE) {\n-      @Override\n-      public Double getDouble() {\n-        return myDouble;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(ReadableDateTime myDateTime) {\n-    return new CEPLiteral(Schema.TypeName.DATETIME) {\n-      @Override\n-      public ReadableDateTime getDateTime() {\n-        return myDateTime;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(Boolean myBoolean) {\n-    return new CEPLiteral(Schema.TypeName.BOOLEAN) {\n-      @Override\n-      public Boolean getBoolean() {\n-        return myBoolean;\n-      }\n-    };\n-  }\n-\n-  public static CEPLiteral of(String myString) {\n-    return new CEPLiteral(Schema.TypeName.STRING) {\n-      @Override\n-      public String getString() {\n-        return myString;\n-      }\n-    };\n-  }\n-\n-  public Byte getByte() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Short getInt16() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Integer getInt32() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Long getInt64() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public BigDecimal getDecimal() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Float getFloat() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Double getDouble() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public ReadableDateTime getDateTime() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Boolean getBoolean() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public String getString() {\n-    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-  }\n-\n-  public Schema.TypeName getTypeName() {\n-    return typeName;\n-  }\n-}\n", "next_change": {"commit": "ec7c929c340ba38615145908249be24778ffc436", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\nnew file mode 100644\nindex 0000000000..0e42011df3\n--- /dev/null\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n", "chunk": "@@ -0,0 +1,176 @@\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.joda.time.ReadableDateTime;\n+\n+import java.math.BigDecimal;\n+\n+public class CEPLiteral extends CEPOperation {\n+\n+    private final CEPTypeName typeName;\n+\n+    private CEPLiteral(CEPTypeName typeName) {\n+        this.typeName = typeName;\n+    }\n+\n+    // TODO: deal with other types (byte, short...)\n+    public static CEPLiteral of(RexLiteral lit) {\n+        switch(lit.getTypeName()) {\n+            case INTEGER:\n+                return of(lit.getValueAs(Integer.class));\n+            case BIGINT:\n+                return of(lit.getValueAs(Long.class));\n+            case DECIMAL:\n+                return of(lit.getValueAs(BigDecimal.class));\n+            case FLOAT:\n+                return of(lit.getValueAs(Float.class));\n+            case DOUBLE:\n+                return of(lit.getValueAs(Double.class));\n+            case BOOLEAN:\n+                return of(lit.getValueAs(Boolean.class));\n+            case DATE:\n+                return of(lit.getValueAs(ReadableDateTime.class));\n+            case CHAR: case VARCHAR:\n+                return of(lit.getValueAs(String.class));\n+            default:\n+                throw new SqlConversionException(\"sql literal type not supported: \"\n+                        + lit.getTypeName().toString());\n+        }\n+    }\n+\n+    public static CEPLiteral of(Byte myByte) {\n+        return new CEPLiteral(CEPTypeName.BYTE) {\n+            @Override\n+            public Byte getByte() {\n+                return myByte;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Short myShort) {\n+        return new CEPLiteral(CEPTypeName.INT16) {\n+            @Override\n+            public Short getInt16() {\n+                return myShort;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Integer myInt) {\n+        return new CEPLiteral(CEPTypeName.INT32) {\n+            @Override\n+            public Integer getInt32() {\n+                return myInt;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Long myLong) {\n+        return new CEPLiteral(CEPTypeName.INT64) {\n+            @Override\n+            public Long getInt64() {\n+                return myLong;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(BigDecimal myDecimal) {\n+        return new CEPLiteral(CEPTypeName.DECIMAL) {\n+            @Override\n+            public BigDecimal getDecimal() {\n+                return myDecimal;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Float myFloat) {\n+        return new CEPLiteral(CEPTypeName.FLOAT) {\n+            @Override\n+            public Float getFloat() {\n+                return myFloat;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Double myDouble) {\n+        return new CEPLiteral(CEPTypeName.DOUBLE) {\n+            @Override\n+            public Double getDouble() {\n+                return myDouble;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(ReadableDateTime myDateTime) {\n+        return new CEPLiteral(CEPTypeName.DATETIME) {\n+            @Override\n+            public ReadableDateTime getDateTime() {\n+                return myDateTime;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(Boolean myBoolean) {\n+        return new CEPLiteral(CEPTypeName.BOOLEAN) {\n+            @Override\n+            public Boolean getBoolean() {\n+                return myBoolean;\n+            }\n+        };\n+    }\n+\n+    public static CEPLiteral of(String myString) {\n+        return new CEPLiteral(CEPTypeName.STRING) {\n+            @Override\n+            public String getString() {\n+                return myString;\n+            }\n+        };\n+    }\n+\n+    public Byte getByte() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Short getInt16() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Integer getInt32() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Long getInt64() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public BigDecimal getDecimal() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Float getFloat() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Double getDouble() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public ReadableDateTime getDateTime() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public Boolean getBoolean() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public String getString() {\n+        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+    }\n+\n+    public CEPTypeName getTypeName() {\n+        return typeName;\n+    }\n+\n+}\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\nindex 0e42011df3..1fe04c2382 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n", "chunk": "@@ -1,176 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.cep;\n \n+import java.math.BigDecimal;\n import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n import org.joda.time.ReadableDateTime;\n \n-import java.math.BigDecimal;\n-\n public class CEPLiteral extends CEPOperation {\n \n-    private final CEPTypeName typeName;\n-\n-    private CEPLiteral(CEPTypeName typeName) {\n-        this.typeName = typeName;\n-    }\n-\n-    // TODO: deal with other types (byte, short...)\n-    public static CEPLiteral of(RexLiteral lit) {\n-        switch(lit.getTypeName()) {\n-            case INTEGER:\n-                return of(lit.getValueAs(Integer.class));\n-            case BIGINT:\n-                return of(lit.getValueAs(Long.class));\n-            case DECIMAL:\n-                return of(lit.getValueAs(BigDecimal.class));\n-            case FLOAT:\n-                return of(lit.getValueAs(Float.class));\n-            case DOUBLE:\n-                return of(lit.getValueAs(Double.class));\n-            case BOOLEAN:\n-                return of(lit.getValueAs(Boolean.class));\n-            case DATE:\n-                return of(lit.getValueAs(ReadableDateTime.class));\n-            case CHAR: case VARCHAR:\n-                return of(lit.getValueAs(String.class));\n-            default:\n-                throw new SqlConversionException(\"sql literal type not supported: \"\n-                        + lit.getTypeName().toString());\n-        }\n-    }\n-\n-    public static CEPLiteral of(Byte myByte) {\n-        return new CEPLiteral(CEPTypeName.BYTE) {\n-            @Override\n-            public Byte getByte() {\n-                return myByte;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Short myShort) {\n-        return new CEPLiteral(CEPTypeName.INT16) {\n-            @Override\n-            public Short getInt16() {\n-                return myShort;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Integer myInt) {\n-        return new CEPLiteral(CEPTypeName.INT32) {\n-            @Override\n-            public Integer getInt32() {\n-                return myInt;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Long myLong) {\n-        return new CEPLiteral(CEPTypeName.INT64) {\n-            @Override\n-            public Long getInt64() {\n-                return myLong;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(BigDecimal myDecimal) {\n-        return new CEPLiteral(CEPTypeName.DECIMAL) {\n-            @Override\n-            public BigDecimal getDecimal() {\n-                return myDecimal;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Float myFloat) {\n-        return new CEPLiteral(CEPTypeName.FLOAT) {\n-            @Override\n-            public Float getFloat() {\n-                return myFloat;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Double myDouble) {\n-        return new CEPLiteral(CEPTypeName.DOUBLE) {\n-            @Override\n-            public Double getDouble() {\n-                return myDouble;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(ReadableDateTime myDateTime) {\n-        return new CEPLiteral(CEPTypeName.DATETIME) {\n-            @Override\n-            public ReadableDateTime getDateTime() {\n-                return myDateTime;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(Boolean myBoolean) {\n-        return new CEPLiteral(CEPTypeName.BOOLEAN) {\n-            @Override\n-            public Boolean getBoolean() {\n-                return myBoolean;\n-            }\n-        };\n-    }\n-\n-    public static CEPLiteral of(String myString) {\n-        return new CEPLiteral(CEPTypeName.STRING) {\n-            @Override\n-            public String getString() {\n-                return myString;\n-            }\n-        };\n-    }\n-\n-    public Byte getByte() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Short getInt16() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Integer getInt32() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Long getInt64() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public BigDecimal getDecimal() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Float getFloat() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Double getDouble() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public ReadableDateTime getDateTime() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public Boolean getBoolean() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public String getString() {\n-        throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n-    }\n-\n-    public CEPTypeName getTypeName() {\n-        return typeName;\n-    }\n-\n+  private final CEPTypeName typeName;\n+\n+  private CEPLiteral(CEPTypeName typeName) {\n+    this.typeName = typeName;\n+  }\n+\n+  // TODO: deal with other types (byte, short...)\n+  public static CEPLiteral of(RexLiteral lit) {\n+    switch (lit.getTypeName()) {\n+      case INTEGER:\n+        return of(lit.getValueAs(Integer.class));\n+      case BIGINT:\n+        return of(lit.getValueAs(Long.class));\n+      case DECIMAL:\n+        return of(lit.getValueAs(BigDecimal.class));\n+      case FLOAT:\n+        return of(lit.getValueAs(Float.class));\n+      case DOUBLE:\n+        return of(lit.getValueAs(Double.class));\n+      case BOOLEAN:\n+        return of(lit.getValueAs(Boolean.class));\n+      case DATE:\n+        return of(lit.getValueAs(ReadableDateTime.class));\n+      case CHAR:\n+      case VARCHAR:\n+        return of(lit.getValueAs(String.class));\n+      default:\n+        throw new SqlConversionException(\n+            \"sql literal type not supported: \" + lit.getTypeName().toString());\n+    }\n+  }\n+\n+  public static CEPLiteral of(Byte myByte) {\n+    return new CEPLiteral(CEPTypeName.BYTE) {\n+      @Override\n+      public Byte getByte() {\n+        return myByte;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Short myShort) {\n+    return new CEPLiteral(CEPTypeName.INT16) {\n+      @Override\n+      public Short getInt16() {\n+        return myShort;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Integer myInt) {\n+    return new CEPLiteral(CEPTypeName.INT32) {\n+      @Override\n+      public Integer getInt32() {\n+        return myInt;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Long myLong) {\n+    return new CEPLiteral(CEPTypeName.INT64) {\n+      @Override\n+      public Long getInt64() {\n+        return myLong;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(BigDecimal myDecimal) {\n+    return new CEPLiteral(CEPTypeName.DECIMAL) {\n+      @Override\n+      public BigDecimal getDecimal() {\n+        return myDecimal;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Float myFloat) {\n+    return new CEPLiteral(CEPTypeName.FLOAT) {\n+      @Override\n+      public Float getFloat() {\n+        return myFloat;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Double myDouble) {\n+    return new CEPLiteral(CEPTypeName.DOUBLE) {\n+      @Override\n+      public Double getDouble() {\n+        return myDouble;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(ReadableDateTime myDateTime) {\n+    return new CEPLiteral(CEPTypeName.DATETIME) {\n+      @Override\n+      public ReadableDateTime getDateTime() {\n+        return myDateTime;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(Boolean myBoolean) {\n+    return new CEPLiteral(CEPTypeName.BOOLEAN) {\n+      @Override\n+      public Boolean getBoolean() {\n+        return myBoolean;\n+      }\n+    };\n+  }\n+\n+  public static CEPLiteral of(String myString) {\n+    return new CEPLiteral(CEPTypeName.STRING) {\n+      @Override\n+      public String getString() {\n+        return myString;\n+      }\n+    };\n+  }\n+\n+  public Byte getByte() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Short getInt16() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Integer getInt32() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Long getInt64() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public BigDecimal getDecimal() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Float getFloat() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Double getDouble() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public ReadableDateTime getDateTime() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public Boolean getBoolean() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public String getString() {\n+    throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n+  }\n+\n+  public CEPTypeName getTypeName() {\n+    return typeName;\n+  }\n }\n", "next_change": {"commit": "422cbe2b87a6f69b8efda0f6ec88baa973bd26c4", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\nindex 1fe04c2382..19505b35fd 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java\n", "chunk": "@@ -186,7 +190,7 @@ public class CEPLiteral extends CEPOperation {\n     throw new SqlConversionException(\"the class must be subclassed properly to get the value\");\n   }\n \n-  public CEPTypeName getTypeName() {\n+  public Schema.TypeName getTypeName() {\n     return typeName;\n   }\n }\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r460343374", "bodyText": "This reverse seems not useful.", "author": "amaliujia", "createdAt": "2020-07-25T00:54:12Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlKind;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * Some utility methods for transforming Calcite's constructs into our own Beam constructs (for\n+ * serialization purpose).\n+ */\n+public class CEPUtil {\n+\n+  private static Quantifier getQuantifier(int start, int end, boolean isReluctant) {\n+    Quantifier quantToAdd;\n+    if (!isReluctant) {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }\");\n+          }\n+        }\n+      }\n+    } else {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK_RELUCTANT;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS_RELUCTANT;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK_RELUCTANT;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }?\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }?\");\n+          }\n+        }\n+      }\n+    }\n+\n+    return quantToAdd;\n+  }\n+\n+  /** Construct a list of {@code CEPPattern}s from a {@code RexNode}. */\n+  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n+      Schema upStreamSchema, RexNode call, Map<String, RexNode> patternDefs) {\n+    ArrayList<CEPPattern> patternList = new ArrayList<>();\n+    if (call.getClass() == RexLiteral.class) {\n+      String p = ((RexLiteral) call).getValueAs(String.class);\n+      RexNode pd = patternDefs.get(p);\n+      patternList.add(CEPPattern.of(upStreamSchema, p, (RexCall) pd, Quantifier.NONE));\n+    } else {\n+      RexCall patCall = (RexCall) call;\n+      SqlOperator operator = patCall.getOperator();\n+      List<RexNode> operands = patCall.getOperands();\n+\n+      // check if if the node has quantifier\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        RexNode pd = patternDefs.get(p);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+\n+        patternList.add(\n+            CEPPattern.of(upStreamSchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n+      } else {\n+        for (RexNode i : operands) {\n+          patternList.addAll(getCEPPatternFromPattern(upStreamSchema, i, patternDefs));\n+        }\n+      }\n+    }\n+    return patternList;\n+  }\n+\n+  /** Recursively construct a regular expression from a {@code RexNode}. */\n+  public static String getRegexFromPattern(RexNode call) {\n+    if (call.getClass() == RexLiteral.class) {\n+      return ((RexLiteral) call).getValueAs(String.class);\n+    } else {\n+      RexCall opr = (RexCall) call;\n+      SqlOperator operator = opr.getOperator();\n+      List<RexNode> operands = opr.getOperands();\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n+        return p + quantifier.toString();\n+      }\n+      return getRegexFromPattern(opr.getOperands().get(0))\n+          + getRegexFromPattern(opr.getOperands().get(1));\n+    }\n+  }\n+\n+  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n+  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n+    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+    Collections.reverse(revOrderKeys);", "originalCommit": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjgxMQ==", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966811", "bodyText": "The thing is, for the order clause, the leftmost (the beginning) key is the most significant. I think the right way to sort should be starting from the least significant key to the most significant key. That is why I wanted to reverse the array.", "author": "Mark-Zeng", "createdAt": "2020-08-01T14:24:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODcwMA==", "url": "https://github.com/apache/beam/pull/12232#discussion_r464338700", "bodyText": "I have changed this part as well. I think reversing the order could be confusing. I moved it into the SortKey transform.", "author": "Mark-Zeng", "createdAt": "2020-08-03T10:51:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\ndeleted file mode 100644\nindex 9d073c897c..0000000000\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ /dev/null\n", "chunk": "@@ -1,155 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.sdk.extensions.sql.impl.cep;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlKind;\n-import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n-\n-/**\n- * Some utility methods for transforming Calcite's constructs into our own Beam constructs (for\n- * serialization purpose).\n- */\n-public class CEPUtil {\n-\n-  private static Quantifier getQuantifier(int start, int end, boolean isReluctant) {\n-    Quantifier quantToAdd;\n-    if (!isReluctant) {\n-      if (start == end) {\n-        quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n-      } else {\n-        if (end == -1) {\n-          if (start == 0) {\n-            quantToAdd = Quantifier.ASTERISK;\n-          } else if (start == 1) {\n-            quantToAdd = Quantifier.PLUS;\n-          } else {\n-            quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n-          }\n-        } else {\n-          if (start == 0 && end == 1) {\n-            quantToAdd = Quantifier.QMARK;\n-          } else if (start == -1) {\n-            quantToAdd = new Quantifier(\"{ , \" + end + \" }\");\n-          } else {\n-            quantToAdd = new Quantifier(\"{ \" + start + \" , }\");\n-          }\n-        }\n-      }\n-    } else {\n-      if (start == end) {\n-        quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n-      } else {\n-        if (end == -1) {\n-          if (start == 0) {\n-            quantToAdd = Quantifier.ASTERISK_RELUCTANT;\n-          } else if (start == 1) {\n-            quantToAdd = Quantifier.PLUS_RELUCTANT;\n-          } else {\n-            quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n-          }\n-        } else {\n-          if (start == 0 && end == 1) {\n-            quantToAdd = Quantifier.QMARK_RELUCTANT;\n-          } else if (start == -1) {\n-            quantToAdd = new Quantifier(\"{ , \" + end + \" }?\");\n-          } else {\n-            quantToAdd = new Quantifier(\"{ \" + start + \" , }?\");\n-          }\n-        }\n-      }\n-    }\n-\n-    return quantToAdd;\n-  }\n-\n-  /** Construct a list of {@code CEPPattern}s from a {@code RexNode}. */\n-  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n-      Schema upStreamSchema, RexNode call, Map<String, RexNode> patternDefs) {\n-    ArrayList<CEPPattern> patternList = new ArrayList<>();\n-    if (call.getClass() == RexLiteral.class) {\n-      String p = ((RexLiteral) call).getValueAs(String.class);\n-      RexNode pd = patternDefs.get(p);\n-      patternList.add(CEPPattern.of(upStreamSchema, p, (RexCall) pd, Quantifier.NONE));\n-    } else {\n-      RexCall patCall = (RexCall) call;\n-      SqlOperator operator = patCall.getOperator();\n-      List<RexNode> operands = patCall.getOperands();\n-\n-      // check if if the node has quantifier\n-      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n-        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n-        RexNode pd = patternDefs.get(p);\n-        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n-        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n-        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n-\n-        patternList.add(\n-            CEPPattern.of(upStreamSchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n-      } else {\n-        for (RexNode i : operands) {\n-          patternList.addAll(getCEPPatternFromPattern(upStreamSchema, i, patternDefs));\n-        }\n-      }\n-    }\n-    return patternList;\n-  }\n-\n-  /** Recursively construct a regular expression from a {@code RexNode}. */\n-  public static String getRegexFromPattern(RexNode call) {\n-    if (call.getClass() == RexLiteral.class) {\n-      return ((RexLiteral) call).getValueAs(String.class);\n-    } else {\n-      RexCall opr = (RexCall) call;\n-      SqlOperator operator = opr.getOperator();\n-      List<RexNode> operands = opr.getOperands();\n-      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n-        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n-        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n-        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n-        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n-        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n-        return p + quantifier.toString();\n-      }\n-      return getRegexFromPattern(opr.getOperands().get(0))\n-          + getRegexFromPattern(opr.getOperands().get(1));\n-    }\n-  }\n-\n-  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n-  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n-    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n-    Collections.reverse(revOrderKeys);\n-\n-    ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n-    for (RelFieldCollation i : revOrderKeys) {\n-      revOrderKeysList.add(OrderKey.of(i));\n-    }\n-\n-    return revOrderKeysList;\n-  }\n-}\n", "next_change": {"commit": "03a33c6ea20b4bde3541d3acba742903cc03b24e", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nnew file mode 100644\nindex 0000000000..b46f3101bc\n--- /dev/null\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -0,0 +1,44 @@\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+\n+import java.util.ArrayList;\n+import java.util.Map;\n+\n+public class CEPUtil {\n+    // construct a list of ceppatterns from the rexnode\n+    public static ArrayList<CEPPattern> getCEPPatternFromPattern(Schema mySchema,\n+                                                                 RexNode call,\n+                                                                 Map<String, RexNode> patternDefs) {\n+        ArrayList<CEPPattern> patternList = new ArrayList<>();\n+        if(call.getClass() == RexLiteral.class) {\n+            String p = ((RexLiteral) call).getValueAs(String.class);\n+            RexNode pd = patternDefs.get(p);\n+            patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd));\n+        } else {\n+            RexCall oprs = (RexCall) call;\n+            patternList.addAll(getCEPPatternFromPattern(mySchema,\n+                    oprs.getOperands().get(0),\n+                    patternDefs));\n+            patternList.addAll(getCEPPatternFromPattern(mySchema,\n+                    oprs.getOperands().get(1),\n+                    patternDefs));\n+        }\n+        return patternList;\n+    }\n+\n+    // recursively change a RexNode into a regular expr\n+    // TODO: support quantifiers: PATTERN_QUANTIFIER('A', 1, -1, false) false?\n+    public static String getRegexFromPattern(RexNode call) {\n+        if(call.getClass() == RexLiteral.class) {\n+            return ((RexLiteral) call).getValueAs(String.class);\n+        } else {\n+            RexCall oprs = (RexCall) call;\n+            return getRegexFromPattern(oprs.getOperands().get(0)) +\n+                    getRegexFromPattern(oprs.getOperands().get(1));\n+        }\n+    }\n+}\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex b46f3101bc..436ee9f47d 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -1,44 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.cep;\n \n+import java.util.ArrayList;\n+import java.util.Map;\n import org.apache.beam.sdk.schemas.Schema;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n \n-import java.util.ArrayList;\n-import java.util.Map;\n-\n public class CEPUtil {\n-    // construct a list of ceppatterns from the rexnode\n-    public static ArrayList<CEPPattern> getCEPPatternFromPattern(Schema mySchema,\n-                                                                 RexNode call,\n-                                                                 Map<String, RexNode> patternDefs) {\n-        ArrayList<CEPPattern> patternList = new ArrayList<>();\n-        if(call.getClass() == RexLiteral.class) {\n-            String p = ((RexLiteral) call).getValueAs(String.class);\n-            RexNode pd = patternDefs.get(p);\n-            patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd));\n-        } else {\n-            RexCall oprs = (RexCall) call;\n-            patternList.addAll(getCEPPatternFromPattern(mySchema,\n-                    oprs.getOperands().get(0),\n-                    patternDefs));\n-            patternList.addAll(getCEPPatternFromPattern(mySchema,\n-                    oprs.getOperands().get(1),\n-                    patternDefs));\n-        }\n-        return patternList;\n+  // construct a list of ceppatterns from the rexnode\n+  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n+      Schema mySchema, RexNode call, Map<String, RexNode> patternDefs) {\n+    ArrayList<CEPPattern> patternList = new ArrayList<>();\n+    if (call.getClass() == RexLiteral.class) {\n+      String p = ((RexLiteral) call).getValueAs(String.class);\n+      RexNode pd = patternDefs.get(p);\n+      patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd));\n+    } else {\n+      RexCall oprs = (RexCall) call;\n+      patternList.addAll(\n+          getCEPPatternFromPattern(mySchema, oprs.getOperands().get(0), patternDefs));\n+      patternList.addAll(\n+          getCEPPatternFromPattern(mySchema, oprs.getOperands().get(1), patternDefs));\n     }\n+    return patternList;\n+  }\n \n-    // recursively change a RexNode into a regular expr\n-    // TODO: support quantifiers: PATTERN_QUANTIFIER('A', 1, -1, false) false?\n-    public static String getRegexFromPattern(RexNode call) {\n-        if(call.getClass() == RexLiteral.class) {\n-            return ((RexLiteral) call).getValueAs(String.class);\n-        } else {\n-            RexCall oprs = (RexCall) call;\n-            return getRegexFromPattern(oprs.getOperands().get(0)) +\n-                    getRegexFromPattern(oprs.getOperands().get(1));\n-        }\n+  // recursively change a RexNode into a regular expr\n+  // TODO: support quantifiers: PATTERN_QUANTIFIER('A', 1, -1, false) false?\n+  public static String getRegexFromPattern(RexNode call) {\n+    if (call.getClass() == RexLiteral.class) {\n+      return ((RexLiteral) call).getValueAs(String.class);\n+    } else {\n+      RexCall oprs = (RexCall) call;\n+      return getRegexFromPattern(oprs.getOperands().get(0))\n+          + getRegexFromPattern(oprs.getOperands().get(1));\n     }\n+  }\n }\n", "next_change": {"commit": "f529b876a2c2e43d012c71b3a83ebd55eb16f4ff", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex 436ee9f47d..022539a2dc 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -32,26 +87,50 @@ public class CEPUtil {\n     if (call.getClass() == RexLiteral.class) {\n       String p = ((RexLiteral) call).getValueAs(String.class);\n       RexNode pd = patternDefs.get(p);\n-      patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd));\n+      patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd, Quantifier.NONE));\n     } else {\n-      RexCall oprs = (RexCall) call;\n-      patternList.addAll(\n-          getCEPPatternFromPattern(mySchema, oprs.getOperands().get(0), patternDefs));\n-      patternList.addAll(\n-          getCEPPatternFromPattern(mySchema, oprs.getOperands().get(1), patternDefs));\n+      RexCall patCall = (RexCall) call;\n+      SqlOperator operator = patCall.getOperator();\n+      List<RexNode> operands = patCall.getOperands();\n+\n+      // check if if the node has quantifier\n+      if(operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        RexNode pd = patternDefs.get(p);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+\n+        patternList.add(CEPPattern.of(mySchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n+      } else {\n+        for(RexNode i : operands) {\n+          patternList.addAll(\n+              getCEPPatternFromPattern(mySchema, i, patternDefs));\n+        }\n+      }\n     }\n     return patternList;\n   }\n \n   // recursively change a RexNode into a regular expr\n-  // TODO: support quantifiers: PATTERN_QUANTIFIER('A', 1, -1, false) false?\n+  // TODO: support quantifiers: PATTERN_QUANTIFIER('A', 1, -1, false)\n   public static String getRegexFromPattern(RexNode call) {\n     if (call.getClass() == RexLiteral.class) {\n       return ((RexLiteral) call).getValueAs(String.class);\n     } else {\n-      RexCall oprs = (RexCall) call;\n-      return getRegexFromPattern(oprs.getOperands().get(0))\n-          + getRegexFromPattern(oprs.getOperands().get(1));\n+      RexCall opr = (RexCall) call;\n+      SqlOperator operator = opr.getOperator();\n+      List<RexNode> operands = opr.getOperands();\n+      if(operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n+        return p + quantifier.toString();\n+      }\n+      return getRegexFromPattern(opr.getOperands().get(0))\n+          + getRegexFromPattern(opr.getOperands().get(1));\n     }\n   }\n }\n", "next_change": {"commit": "0bf24db5e75c0db715c6954b11afac357c49d7f6", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex 022539a2dc..af7aed46b2 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -133,4 +139,17 @@ public class CEPUtil {\n           + getRegexFromPattern(opr.getOperands().get(1));\n     }\n   }\n+\n+  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n+  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n+    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+    Collections.reverse(revOrderKeys);\n+\n+    ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+    for (RelFieldCollation i : revOrderKeys) {\n+      revOrderKeysList.add(OrderKey.of(i));\n+    }\n+\n+    return revOrderKeysList;\n+  }\n }\n", "next_change": {"commit": "adc2354752ef48237020f3fa84d00ab65c2ead74", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex af7aed46b2..bc55f2944b 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -152,4 +155,87 @@ public class CEPUtil {\n \n     return revOrderKeysList;\n   }\n+\n+  /** Transform the Measures clause Map into CEPOperations.\n+   * For now, we only support FINAL. The transformation leaves out any\n+   * FINAL/RUNNING keywords */\n+  public static Map<String, CEPOperation> getCEPOperationFromMeasures(Map<String, RexNode> measures) {\n+    ImmutableMap.Builder<String, CEPOperation> cepMeasures = new ImmutableMap.Builder<>();\n+    for(Map.Entry<String, RexNode> i : measures.entrySet()) {\n+      cepMeasures.put(i.getKey(), CEPOperation.of(i.getValue()));\n+    }\n+    return cepMeasures.build();\n+  }\n+\n+  /** Transform the partition columns into serializable CEPFieldRef. */\n+  public static List<CEPFieldRef> getCEPFieldRefFromParKeys(List<RexNode> parKeys) {\n+    ArrayList<CEPFieldRef> fieldList = new ArrayList<>();\n+    for(RexNode i : parKeys) {\n+      RexInputRef parKey = (RexInputRef) i;\n+      fieldList.add(new CEPFieldRef(parKey.getName(), parKey.getIndex()));\n+    }\n+    return fieldList;\n+  }\n+\n+  /** a function that finds a pattern reference recursively */\n+  public static CEPFieldRef getFieldRef(CEPOperation opr) {\n+    if(opr.getClass() == CEPFieldRef.class) {\n+      CEPFieldRef field = (CEPFieldRef) opr;\n+      return field;\n+    } else if(opr.getClass() == CEPCall.class) {\n+      CEPCall call = (CEPCall) opr;\n+      CEPFieldRef field;\n+\n+      for(CEPOperation i : call.getOperands()) {\n+        field = getFieldRef(i);\n+        if(field != null) {\n+          return field;\n+        }\n+      }\n+      return null;\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  public static Schema.FieldType getFieldType(Schema streamSchema, CEPOperation measureOperation) {\n+\n+    if(measureOperation.getClass() == CEPFieldRef.class) {\n+      CEPFieldRef field =  (CEPFieldRef) measureOperation;\n+      return streamSchema.getField(field.getIndex()).getType();\n+    } else if(measureOperation.getClass() == CEPCall.class) {\n+\n+      CEPCall call = (CEPCall) measureOperation;\n+      CEPKind oprKind = call.getOperator().getCepKind();\n+\n+      if(oprKind == CEPKind.SUM ||\n+          oprKind == CEPKind.COUNT) {\n+        return Schema.FieldType.INT32;\n+      } else if(oprKind == CEPKind.AVG) {\n+        return Schema.FieldType.DOUBLE;\n+      }\n+      CEPFieldRef refOpt;\n+      for(CEPOperation i : call.getOperands()) {\n+        refOpt = getFieldRef(i);\n+        if(refOpt != null) {\n+          return streamSchema.getField(refOpt.getIndex()).getType();\n+        }\n+      }\n+      throw new UnsupportedOperationException(\"the function in Measures is not recognized.\");\n+    } else {\n+      throw new UnsupportedOperationException(\"the function in Measures is not recognized.\");\n+    }\n+  }\n+\n+  /** Get CEPFieldRef from the Map of Measures */\n+  public static Map<String, CEPFieldRef> getFieldRefFromMeasures(Schema streamSchema, Map<String, CEPOperation> measures) {\n+    ImmutableMap.Builder<String, CEPFieldRef> fieldRefMeasures = new ImmutableMap.Builder<>();\n+    for(Map.Entry<String, CEPOperation> i : measures.entrySet()) {\n+      CEPFieldRef refOpt = getFieldRef(i.getValue());\n+      if(refOpt != null) {\n+        fieldRefMeasures.put(i.getKey(), refOpt);\n+      }\n+    }\n+    return fieldRefMeasures.build();\n+  }\n }\n", "next_change": {"commit": "ebc41a263dc07b305c190f0ded1ec86e90099ee7", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex bc55f2944b..25c94792de 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -227,15 +213,39 @@ public class CEPUtil {\n     }\n   }\n \n-  /** Get CEPFieldRef from the Map of Measures */\n-  public static Map<String, CEPFieldRef> getFieldRefFromMeasures(Schema streamSchema, Map<String, CEPOperation> measures) {\n-    ImmutableMap.Builder<String, CEPFieldRef> fieldRefMeasures = new ImmutableMap.Builder<>();\n-    for(Map.Entry<String, CEPOperation> i : measures.entrySet()) {\n-      CEPFieldRef refOpt = getFieldRef(i.getValue());\n-      if(refOpt != null) {\n-        fieldRefMeasures.put(i.getKey(), refOpt);\n-      }\n+  public static Schema decideSchema(\n+      List<CEPMeasure> measures,\n+      boolean allRows,\n+      List<CEPFieldRef> parKeys,\n+      Schema upstreamSchema) {\n+    // if the measures clause does not present\n+    // then output the schema from the pattern and the partition columns\n+    if (measures.isEmpty() && !allRows) {\n+      throw new UnsupportedOperationException(\n+          \"The Measures clause cannot be empty for ONE ROW PER MATCH\");\n+    }\n+\n+    // TODO: implement ALL ROWS PER MATCH\n+    // for now, return all rows as they were (return the origin schema)\n+    if (allRows) {\n+      return upstreamSchema;\n     }\n-    return fieldRefMeasures.build();\n+\n+    Schema.Builder outTableSchemaBuilder = new Schema.Builder();\n+\n+    // take the partition keys first\n+    for (CEPFieldRef i : parKeys) {\n+      outTableSchemaBuilder.addField(upstreamSchema.getField(i.getIndex()));\n+    }\n+\n+    // add the fields in the Measures clause\n+    for (CEPMeasure i : measures) {\n+      Schema.Field fieldToAdd = Schema.Field.of(i.getName(), i.getType());\n+      outTableSchemaBuilder.addField(fieldToAdd);\n+    }\n+\n+    // TODO: optionally add any columns left for ALL ROWS PER MATCH\n+\n+    return outTableSchemaBuilder.build();\n   }\n }\n", "next_change": {"commit": "799491bbe96fdcefd6ca1cc0f974c202159c8a91", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\nindex 25c94792de..d501abd4bd 100644\n--- a/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n+++ b/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java\n", "chunk": "@@ -244,7 +242,7 @@ public class CEPUtil {\n       outTableSchemaBuilder.addField(fieldToAdd);\n     }\n \n-    // TODO: optionally add any columns left for ALL ROWS PER MATCH\n+    // TODO: add any columns left for ALL ROWS PER MATCH\n \n     return outTableSchemaBuilder.build();\n   }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjM4Nw==", "url": "https://github.com/apache/beam/pull/12232#discussion_r464116387", "bodyText": "You can either remove this commented test, or leave it with @ignore(\"the reason to ignore this test\").", "author": "amaliujia", "createdAt": "2020-08-02T19:47:34Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+/** Test for {@code BeamMatchRel}. */\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchQuantifierTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchMeasuresTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType)\n+            .addRows(\n+                1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4, 1, \"b\", 8, 1, \"a\", 7, 1, \"c\", 9, 2, \"a\",\n+                6, 2, \"b\", 10, 2, \"c\", 11, 5, \"a\", 0));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"MEASURES \"\n+            + \"LAST (A.proctime) AS atime, \"\n+            + \"B.proctime AS btime, \"\n+            + \"C.proctime AS ctime \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.INT32, \"T.atime\",\n+                    Schema.FieldType.INT32, \"T.btime\",\n+                    Schema.FieldType.INT32, \"T.ctime\")\n+                .addRows(1, 2, 3, 4, 1, 7, 8, 9, 2, 6, 10, 11)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  /*", "originalCommit": "f79922b3b4c1d3b2a88cea091ebb6257806b23aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzOTEyNw==", "url": "https://github.com/apache/beam/pull/12232#discussion_r464339127", "bodyText": "I will ignore it for now. This test is wrote for testing the NFA.", "author": "Mark-Zeng", "createdAt": "2020-08-03T10:51:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjM4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "4e56953a135e40bbb3415d05ec6d14bbab947927", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\ndeleted file mode 100644\nindex 2df29eb4d4..0000000000\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ /dev/null\n", "chunk": "@@ -1,234 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.sdk.extensions.sql.impl.rel;\n-\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n-\n-import org.apache.beam.sdk.extensions.sql.TestUtils;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n-import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.Row;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-/** Test for {@code BeamMatchRel}. */\n-public class BeamMatchRelTest {\n-\n-  @Rule public final TestPipeline pipeline = TestPipeline.create();\n-\n-  @Test\n-  public void matchLogicalPlanTest() {\n-    Schema schemaType =\n-        Schema.builder()\n-            .addInt32Field(\"id\")\n-            .addStringField(\"name\")\n-            .addInt32Field(\"proctime\")\n-            .build();\n-\n-    registerTable(\n-        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n-\n-    String sql =\n-        \"SELECT * \"\n-            + \"FROM TestTable \"\n-            + \"MATCH_RECOGNIZE (\"\n-            + \"PARTITION BY id \"\n-            + \"ORDER BY proctime \"\n-            + \"ALL ROWS PER MATCH \"\n-            + \"PATTERN (A B C) \"\n-            + \"DEFINE \"\n-            + \"A AS name = 'a', \"\n-            + \"B AS name = 'b', \"\n-            + \"C AS name = 'c' \"\n-            + \") AS T\";\n-\n-    PCollection<Row> result = compilePipeline(sql, pipeline);\n-\n-    PAssert.that(result)\n-        .containsInAnyOrder(\n-            TestUtils.RowsBuilder.of(\n-                    Schema.FieldType.INT32, \"id\",\n-                    Schema.FieldType.STRING, \"name\",\n-                    Schema.FieldType.INT32, \"proctime\")\n-                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n-                .getRows());\n-\n-    pipeline.run().waitUntilFinish();\n-  }\n-\n-  @Test\n-  public void matchQuantifierTest() {\n-    Schema schemaType =\n-        Schema.builder()\n-            .addInt32Field(\"id\")\n-            .addStringField(\"name\")\n-            .addInt32Field(\"proctime\")\n-            .build();\n-\n-    registerTable(\n-        \"TestTable\",\n-        TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n-\n-    String sql =\n-        \"SELECT * \"\n-            + \"FROM TestTable \"\n-            + \"MATCH_RECOGNIZE (\"\n-            + \"PARTITION BY id \"\n-            + \"ORDER BY proctime \"\n-            + \"ALL ROWS PER MATCH \"\n-            + \"PATTERN (A+ B C) \"\n-            + \"DEFINE \"\n-            + \"A AS name = 'a', \"\n-            + \"B AS name = 'b', \"\n-            + \"C AS name = 'c' \"\n-            + \") AS T\";\n-\n-    PCollection<Row> result = compilePipeline(sql, pipeline);\n-\n-    PAssert.that(result)\n-        .containsInAnyOrder(\n-            TestUtils.RowsBuilder.of(\n-                    Schema.FieldType.INT32, \"id\",\n-                    Schema.FieldType.STRING, \"name\",\n-                    Schema.FieldType.INT32, \"proctime\")\n-                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n-                .getRows());\n-\n-    pipeline.run().waitUntilFinish();\n-  }\n-\n-  @Test\n-  public void matchMeasuresTest() {\n-    Schema schemaType =\n-        Schema.builder()\n-            .addInt32Field(\"id\")\n-            .addStringField(\"name\")\n-            .addInt32Field(\"proctime\")\n-            .build();\n-\n-    registerTable(\n-        \"TestTable\",\n-        TestBoundedTable.of(schemaType)\n-            .addRows(\n-                1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4, 1, \"b\", 8, 1, \"a\", 7, 1, \"c\", 9, 2, \"a\",\n-                6, 2, \"b\", 10, 2, \"c\", 11, 5, \"a\", 0));\n-\n-    String sql =\n-        \"SELECT * \"\n-            + \"FROM TestTable \"\n-            + \"MATCH_RECOGNIZE (\"\n-            + \"PARTITION BY id \"\n-            + \"ORDER BY proctime \"\n-            + \"MEASURES \"\n-            + \"LAST (A.proctime) AS atime, \"\n-            + \"B.proctime AS btime, \"\n-            + \"C.proctime AS ctime \"\n-            + \"PATTERN (A+ B C) \"\n-            + \"DEFINE \"\n-            + \"A AS name = 'a', \"\n-            + \"B AS name = 'b', \"\n-            + \"C AS name = 'c' \"\n-            + \") AS T\";\n-\n-    PCollection<Row> result = compilePipeline(sql, pipeline);\n-\n-    PAssert.that(result)\n-        .containsInAnyOrder(\n-            TestUtils.RowsBuilder.of(\n-                    Schema.FieldType.INT32, \"id\",\n-                    Schema.FieldType.INT32, \"T.atime\",\n-                    Schema.FieldType.INT32, \"T.btime\",\n-                    Schema.FieldType.INT32, \"T.ctime\")\n-                .addRows(1, 2, 3, 4, 1, 7, 8, 9, 2, 6, 10, 11)\n-                .getRows());\n-\n-    pipeline.run().waitUntilFinish();\n-  }\n-\n-  /*\n-  @Test\n-  public void matchNFATest() {\n-    Schema schemaType =\n-        Schema.builder()\n-            .addStringField(\"Symbol\")\n-            .addDateTimeField(\"TradeDay\")\n-            .addInt32Field(\"Price\")\n-            .build();\n-\n-    registerTable(\n-        \"Ticker\", TestBoundedTable.of(schemaType).addRows(\n-            \"a\", \"2020-07-01\", 32, // 1st A\n-            \"a\", \"2020-06-01\", 34,\n-            \"a\", \"2020-07-02\", 31, // B\n-            \"a\", \"2020-08-30\", 30, // B\n-            \"a\", \"2020-08-31\", 35, // C\n-            \"a\", \"2020-10-01\", 28,\n-            \"a\", \"2020-10-15\", 30, // 2nd A\n-            \"a\", \"2020-11-01\", 22, // B\n-            \"a\", \"2020-11-08\", 29, // C\n-            \"a\", \"2020-12-10\", 30, // C\n-            \"b\", \"2020-12-01\", 22,\n-            \"c\", \"2020-05-16\", 27, // A\n-            \"c\", \"2020-09-14\", 26, // B\n-            \"c\", \"2020-10-13\", 30)); // C\n-\n-    // match `V` shapes in prices\n-    String sql =\n-        \"SELECT M.Symbol,\"\n-            + \" M.Matchno,\"\n-            + \" M.Startp,\"\n-            + \" M.Bottomp,\"\n-            + \" M.Endp,\"\n-            + \" M.Avgp\"\n-            + \"FROM Ticker \"\n-            + \"MATCH_RECOGNIZE (\"\n-              + \"PARTITION BY Symbol \"\n-              + \"ORDER BY Tradeday \"\n-              + \"MEASURES \"\n-              + \"MATCH_NUMBER() AS Matchno, \"\n-              + \"A.price AS Startp, \"\n-              + \"LAST (B.Price) AS Bottomp, \"\n-              + \"LAST (C.Price) AS ENDp, \"\n-              + \"AVG (U.Price) AS Avgp \"\n-              + \"AFTER MATCH SKIP PAST LAST ROW \"\n-              + \"PATTERN (A B+ C+) \"\n-              + \"SUBSET U = (A, B, C) \"\n-              + \"DEFINE \"\n-              + \"B AS B.Price < PREV (B.Price), \"\n-              + \"C AS C.Price > PREV (C.Price) \"\n-            + \") AS T\";\n-\n-    PCollection<Row> result = compilePipeline(sql, pipeline);\n-\n-    PAssert.that(result)\n-        .containsInAnyOrder(\n-            TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n-                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n-                .getRows());\n-\n-    pipeline.run().waitUntilFinish();\n-  }\n-  */\n-}\n", "next_change": {"commit": "b2b189dfdea88baf34849a17b203058f29212b00", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nnew file mode 100644\nindex 0000000000..4d73a15eb0\n--- /dev/null\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -0,0 +1,80 @@\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Test;\n+import org.apache.beam.sdk.schemas.Schema;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+public class BeamMatchRelTest {\n+\n+  public static final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void MatchLogicalPlanTest() {\n+    Schema schemaType = Schema.builder()\n+        .addInt32Field(\"id\")\n+        .addStringField(\"name\")\n+        .addInt32Field(\"proctime\")\n+        .build();\n+\n+    registerTable(\n+            \"TestTable\",\n+            TestBoundedTable.of(\n+                    schemaType)\n+                    .addRows(\n+                            1, \"a\", 1,\n+                            1, \"b\", 2,\n+                            1, \"c\", 3\n+                    ));\n+\n+//\n+//    PCollection<Row> input =\n+//        pipeline.apply(\n+//            Create.of(\n+//                Row.withSchema(schemaType).addValues(\n+//                        1, \"a\", 1,\n+//                        1, \"b\", 2,\n+//                        1, \"c\", 3\n+//                ).build())\n+//                .withRowSchema(schemaType));\n+\n+    String sql = \"SELECT T.aid, T.bid, T.cid \" +\n+        \"FROM TestTable \" +\n+        \"MATCH_RECOGNIZE (\" +\n+        \"PARTITION BY id \" +\n+        \"ORDER BY proctime \" +\n+        \"MEASURES \" +\n+        \"A.id AS aid, \" +\n+        \"B.id AS bid, \" +\n+        \"C.id AS cid \" +\n+        \"PATTERN (A B C) \" +\n+        \"DEFINE \" +\n+        \"A AS name = 'a', \" +\n+        \"B AS name = 'b', \" +\n+        \"C AS name = 'c' \" +\n+        \") AS T\";\n+\n+//    PCollection<Row> result = input.apply(SqlTransform.query(sql));\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+            .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+            .getRows()\n+        );\n+\n+    pipeline.run().waitUntilFinish();\n+\n+  }\n+}\n", "next_change": {"commit": "a7d111f896f5f8e14f6211d01811a618b905ec32", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex 4d73a15eb0..a6657685e0 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -1,80 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.beam.sdk.extensions.sql.impl.rel;\n \n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n import org.apache.beam.sdk.extensions.sql.TestUtils;\n import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n import org.junit.Test;\n-import org.apache.beam.sdk.schemas.Schema;\n-\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n-import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n \n public class BeamMatchRelTest {\n \n-  public static final TestPipeline pipeline = TestPipeline.create();\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n \n   @Test\n-  public void MatchLogicalPlanTest() {\n-    Schema schemaType = Schema.builder()\n-        .addInt32Field(\"id\")\n-        .addStringField(\"name\")\n-        .addInt32Field(\"proctime\")\n-        .build();\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n \n     registerTable(\n-            \"TestTable\",\n-            TestBoundedTable.of(\n-                    schemaType)\n-                    .addRows(\n-                            1, \"a\", 1,\n-                            1, \"b\", 2,\n-                            1, \"c\", 3\n-                    ));\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n \n-//\n-//    PCollection<Row> input =\n-//        pipeline.apply(\n-//            Create.of(\n-//                Row.withSchema(schemaType).addValues(\n-//                        1, \"a\", 1,\n-//                        1, \"b\", 2,\n-//                        1, \"c\", 3\n-//                ).build())\n-//                .withRowSchema(schemaType));\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n \n-    String sql = \"SELECT T.aid, T.bid, T.cid \" +\n-        \"FROM TestTable \" +\n-        \"MATCH_RECOGNIZE (\" +\n-        \"PARTITION BY id \" +\n-        \"ORDER BY proctime \" +\n-        \"MEASURES \" +\n-        \"A.id AS aid, \" +\n-        \"B.id AS bid, \" +\n-        \"C.id AS cid \" +\n-        \"PATTERN (A B C) \" +\n-        \"DEFINE \" +\n-        \"A AS name = 'a', \" +\n-        \"B AS name = 'b', \" +\n-        \"C AS name = 'c' \" +\n-        \") AS T\";\n-\n-//    PCollection<Row> result = input.apply(SqlTransform.query(sql));\n     PCollection<Row> result = compilePipeline(sql, pipeline);\n \n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n-            .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n-            .getRows()\n-        );\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n \n     pipeline.run().waitUntilFinish();\n-\n   }\n }\n", "next_change": {"commit": "f529b876a2c2e43d012c71b3a83ebd55eb16f4ff", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex a6657685e0..eb6d9372ad 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -72,4 +72,43 @@ public class BeamMatchRelTest {\n \n     pipeline.run().waitUntilFinish();\n   }\n+\n+  @Test\n+  public void matchQuantifierTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n }\n", "next_change": {"commit": "0bf24db5e75c0db715c6954b11afac357c49d7f6", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex eb6d9372ad..b936263e1d 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -103,9 +105,9 @@ public class BeamMatchRelTest {\n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n                 .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n                 .getRows());\n \n", "next_change": {"commit": "adc2354752ef48237020f3fa84d00ab65c2ead74", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex b936263e1d..46f18f13cb 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -113,4 +115,130 @@ public class BeamMatchRelTest {\n \n     pipeline.run().waitUntilFinish();\n   }\n+\n+  @Test\n+  public void matchMeasuresTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType).addRows(\n+            1, \"a\", 1,\n+            1, \"a\", 2,\n+            1, \"b\", 3,\n+            1, \"c\", 4,\n+            1, \"b\", 8,\n+            1, \"a\", 7,\n+            1, \"c\", 9,\n+            2, \"a\", 6,\n+            2, \"b\", 10,\n+            2, \"c\", 11,\n+            5, \"a\", 0));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"MEASURES \"\n+            + \"LAST (A.id) AS aid, \"\n+            + \"B.id AS bid, \"\n+            + \"C.id AS cid \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"T.aid\",\n+                Schema.FieldType.INT32, \"T.bid\",\n+                Schema.FieldType.INT32, \"T.cid\")\n+                .addRows(2, 3, 4,\n+                    7, 8, 9,\n+                    6, 10, 11\n+                    )\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  /*\n+  @Test\n+  public void matchNFATest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addStringField(\"Symbol\")\n+            .addDateTimeField(\"TradeDay\")\n+            .addInt32Field(\"Price\")\n+            .build();\n+\n+    registerTable(\n+        \"Ticker\", TestBoundedTable.of(schemaType).addRows(\n+            \"a\", \"2020-07-01\", 32, // 1st A\n+            \"a\", \"2020-06-01\", 34,\n+            \"a\", \"2020-07-02\", 31, // B\n+            \"a\", \"2020-08-30\", 30, // B\n+            \"a\", \"2020-08-31\", 35, // C\n+            \"a\", \"2020-10-01\", 28,\n+            \"a\", \"2020-10-15\", 30, // 2nd A\n+            \"a\", \"2020-11-01\", 22, // B\n+            \"a\", \"2020-11-08\", 29, // C\n+            \"a\", \"2020-12-10\", 30, // C\n+            \"b\", \"2020-12-01\", 22,\n+            \"c\", \"2020-05-16\", 27, // A\n+            \"c\", \"2020-09-14\", 26, // B\n+            \"c\", \"2020-10-13\", 30)); // C\n+\n+    // match `V` shapes in prices\n+    String sql =\n+        \"SELECT M.Symbol,\"\n+            + \" M.Matchno,\"\n+            + \" M.Startp,\"\n+            + \" M.Bottomp,\"\n+            + \" M.Endp,\"\n+            + \" M.Avgp\"\n+            + \"FROM Ticker \"\n+            + \"MATCH_RECOGNIZE (\"\n+              + \"PARTITION BY Symbol \"\n+              + \"ORDER BY Tradeday \"\n+              + \"MEASURES \"\n+              + \"MATCH_NUMBER() AS Matchno, \"\n+              + \"A.price AS Startp, \"\n+              + \"LAST (B.Price) AS Bottomp, \"\n+              + \"LAST (C.Price) AS ENDp, \"\n+              + \"AVG (U.Price) AS Avgp \"\n+              + \"AFTER MATCH SKIP PAST LAST ROW \"\n+              + \"PATTERN (A B+ C+) \"\n+              + \"SUBSET U = (A, B, C) \"\n+              + \"DEFINE \"\n+              + \"B AS B.Price < PREV (B.Price), \"\n+              + \"C AS C.Price > PREV (C.Price) \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                Schema.FieldType.INT32, \"id\",\n+                Schema.FieldType.STRING, \"name\",\n+                Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+  */\n }\n", "next_change": {"commit": "799491bbe96fdcefd6ca1cc0f974c202159c8a91", "changed_code": [{"header": "diff --git a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\nindex 46f18f13cb..097ca190bc 100644\n--- a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n+++ b/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java\n", "chunk": "@@ -232,13 +253,12 @@ public class BeamMatchRelTest {\n     PAssert.that(result)\n         .containsInAnyOrder(\n             TestUtils.RowsBuilder.of(\n-                Schema.FieldType.INT32, \"id\",\n-                Schema.FieldType.STRING, \"name\",\n-                Schema.FieldType.INT32, \"proctime\")\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n                 .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n                 .getRows());\n \n     pipeline.run().waitUntilFinish();\n   }\n-  */\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}, {"oid": "4e56953a135e40bbb3415d05ec6d14bbab947927", "url": "https://github.com/apache/beam/commit/4e56953a135e40bbb3415d05ec6d14bbab947927", "message": "[BEAM-9543] built the basis for Match_Recog", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "72232fcdf157ab0a09d72d57603d1f348774a116", "url": "https://github.com/apache/beam/commit/72232fcdf157ab0a09d72d57603d1f348774a116", "message": "[BEAM-9543] built the basis for Match_Recog", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "064ada7257970bcb1d35530be1b88cb3830f242b", "url": "https://github.com/apache/beam/commit/064ada7257970bcb1d35530be1b88cb3830f242b", "message": "[BEAM-9543] implemented `partition by`", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "9cd1a82bec7b2f7c44aacfbd72f5f775bb58b650", "url": "https://github.com/apache/beam/commit/9cd1a82bec7b2f7c44aacfbd72f5f775bb58b650", "message": "[BEAM-9543] implemented `order by`", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "c07b8a89e6c54f699590d5b9e8242cb92de3c505", "url": "https://github.com/apache/beam/commit/c07b8a89e6c54f699590d5b9e8242cb92de3c505", "message": "[BEAM-9543] fixed `order by` coder issue", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "cdb7e9f120a21506a800f6f6840fb315c4b6524b", "url": "https://github.com/apache/beam/commit/cdb7e9f120a21506a800f6f6840fb315c4b6524b", "message": "[BEAM-9543] fixed `order by` coder issue", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "cc63e557faf36656c330f305b3924016fdad2151", "url": "https://github.com/apache/beam/commit/cc63e557faf36656c330f305b3924016fdad2151", "message": "[BEAM-9543] applied regex pattern match", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "b2b189dfdea88baf34849a17b203058f29212b00", "url": "https://github.com/apache/beam/commit/b2b189dfdea88baf34849a17b203058f29212b00", "message": "[BEAM-9543] applied regex pattern match", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "08abbab35e1ee71fe3c9b2b92aa049b945a92763", "url": "https://github.com/apache/beam/commit/08abbab35e1ee71fe3c9b2b92aa049b945a92763", "message": "[BEAM-9543] fixed sortKey serialization problem", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "03a33c6ea20b4bde3541d3acba742903cc03b24e", "url": "https://github.com/apache/beam/commit/03a33c6ea20b4bde3541d3acba742903cc03b24e", "message": "[BEAM-9543] fixed sortKey serialization problem", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "ec7c929c340ba38615145908249be24778ffc436", "url": "https://github.com/apache/beam/commit/ec7c929c340ba38615145908249be24778ffc436", "message": "[BEAM-9543] fixed serialization problem", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "f52d96fc33382fc9e46d3249a2600e2af9f67326", "url": "https://github.com/apache/beam/commit/f52d96fc33382fc9e46d3249a2600e2af9f67326", "message": "[BEAM-9543] recognized simple pattern", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "8d6ffcc213e30999fc495c119b68da4f62fad258", "url": "https://github.com/apache/beam/commit/8d6ffcc213e30999fc495c119b68da4f62fad258", "message": "[BEAM-9543] recognized simple pattern", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "a7d111f896f5f8e14f6211d01811a618b905ec32", "url": "https://github.com/apache/beam/commit/a7d111f896f5f8e14f6211d01811a618b905ec32", "message": "[BEAM-9543] fixed code style", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "f529b876a2c2e43d012c71b3a83ebd55eb16f4ff", "url": "https://github.com/apache/beam/commit/f529b876a2c2e43d012c71b3a83ebd55eb16f4ff", "message": "[BEAM-9543] supported regex quantifier", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "0bf24db5e75c0db715c6954b11afac357c49d7f6", "url": "https://github.com/apache/beam/commit/0bf24db5e75c0db715c6954b11afac357c49d7f6", "message": "[BEAM-9543] added javadoc", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "422cbe2b87a6f69b8efda0f6ec88baa973bd26c4", "url": "https://github.com/apache/beam/commit/422cbe2b87a6f69b8efda0f6ec88baa973bd26c4", "message": "[BEAM-9543] removed CEPTypeName.java", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "adc2354752ef48237020f3fa84d00ab65c2ead74", "url": "https://github.com/apache/beam/commit/adc2354752ef48237020f3fa84d00ab65c2ead74", "message": "[BEAM-9543] added Measures implementation (unfinished)", "committedDate": "2020-08-03T10:40:12Z", "type": "commit"}, {"oid": "ebc41a263dc07b305c190f0ded1ec86e90099ee7", "url": "https://github.com/apache/beam/commit/ebc41a263dc07b305c190f0ded1ec86e90099ee7", "message": "[BEAM-9543] added Measures implementation", "committedDate": "2020-08-03T10:40:13Z", "type": "commit"}, {"oid": "87935746647611aa139d664ebed10c8e638bb024", "url": "https://github.com/apache/beam/commit/87935746647611aa139d664ebed10c8e638bb024", "message": "[BEAM-9543] added Measures implementation", "committedDate": "2020-08-03T10:40:13Z", "type": "commit"}, {"oid": "040d1f41db568a00b1fef898bddfa690be21015e", "url": "https://github.com/apache/beam/commit/040d1f41db568a00b1fef898bddfa690be21015e", "message": "[BEAM-9543] fixed minor issues", "committedDate": "2020-08-03T10:40:13Z", "type": "commit"}, {"oid": "799491bbe96fdcefd6ca1cc0f974c202159c8a91", "url": "https://github.com/apache/beam/commit/799491bbe96fdcefd6ca1cc0f974c202159c8a91", "message": "[BEAM-9543] fixed minor style issues", "committedDate": "2020-08-03T10:40:13Z", "type": "commit"}, {"oid": "799491bbe96fdcefd6ca1cc0f974c202159c8a91", "url": "https://github.com/apache/beam/commit/799491bbe96fdcefd6ca1cc0f974c202159c8a91", "message": "[BEAM-9543] fixed minor style issues", "committedDate": "2020-08-03T10:40:13Z", "type": "forcePushed"}]}