{"pr_number": 530, "pr_title": "KOGITO-3763 - Improved numeric feature handling", "pr_author": "tteofili", "pr_createdAt": "2020-11-09T09:06:13Z", "pr_url": "https://github.com/kiegroup/kogito-apps/pull/530", "timeline": [{"oid": "25b6ab01d9ef0f19869a5f0890252cba8c852d84", "url": "https://github.com/kiegroup/kogito-apps/commit/25b6ab01d9ef0f19869a5f0890252cba8c852d84", "message": "KOGITO-3769 - Make LimeExplainer optionally generate more diverse samples", "committedDate": "2020-11-09T09:00:11Z", "type": "commit"}, {"oid": "d83ae430183c0a39b631680f4bc919cc1754dacf", "url": "https://github.com/kiegroup/kogito-apps/commit/d83ae430183c0a39b631680f4bc919cc1754dacf", "message": "KOGITO-3763 - improved numeric feature encoding", "committedDate": "2020-11-09T09:01:27Z", "type": "commit"}, {"oid": "9fa407fac9f898d2b1d07152f1dae9e55282c309", "url": "https://github.com/kiegroup/kogito-apps/commit/9fa407fac9f898d2b1d07152f1dae9e55282c309", "message": "KOGITO-3763 - improved numeric feature encoding", "committedDate": "2020-11-09T09:01:58Z", "type": "commit"}, {"oid": "d35eed92e8cb7c7c4c1c771a531bfd794fef05a2", "url": "https://github.com/kiegroup/kogito-apps/commit/d35eed92e8cb7c7c4c1c771a531bfd794fef05a2", "message": "KOGITO-3763 - do not integrate KOGITO-3769 changes in here", "committedDate": "2020-11-09T09:24:02Z", "type": "commit"}, {"oid": "72baf99f09b97389a72cf3eea8bf0b4099545143", "url": "https://github.com/kiegroup/kogito-apps/commit/72baf99f09b97389a72cf3eea8bf0b4099545143", "message": "KOGITO-3763 - minor tweaks to tests, reduced stdDev in numeric feature gen", "committedDate": "2020-11-09T14:59:45Z", "type": "commit"}, {"oid": "112de8a1330b52012a4d5c415b396855bc2f16af", "url": "https://github.com/kiegroup/kogito-apps/commit/112de8a1330b52012a4d5c415b396855bc2f16af", "message": "KOGITO-3763 - fixing dmn and pmml tests", "committedDate": "2020-11-10T10:11:44Z", "type": "commit"}, {"oid": "4f84ff071b86089deb226af075d1bbddc5150596", "url": "https://github.com/kiegroup/kogito-apps/commit/4f84ff071b86089deb226af075d1bbddc5150596", "message": "KOGITO-3763 - reduced kernel size for weighting samples, fixed tests", "committedDate": "2020-11-11T15:35:13Z", "type": "commit"}, {"oid": "85160944f1e5d484bc5220ae97847ad52393c73d", "url": "https://github.com/kiegroup/kogito-apps/commit/85160944f1e5d484bc5220ae97847ad52393c73d", "message": "KOGITO-3763 - improved stability metric API: no need to predict", "committedDate": "2020-11-11T15:56:28Z", "type": "commit"}, {"oid": "f82a27d12710a5450bc169c00b7d88291e8624ff", "url": "https://github.com/kiegroup/kogito-apps/commit/f82a27d12710a5450bc169c00b7d88291e8624ff", "message": "KOGITO-3763 - removed useless commented code in DatasetEncoder", "committedDate": "2020-11-11T16:01:34Z", "type": "commit"}, {"oid": "346374af6a7c33f16b191a3246f80fd82fa7f911", "url": "https://github.com/kiegroup/kogito-apps/commit/346374af6a7c33f16b191a3246f80fd82fa7f911", "message": "KOGITO-3763 - resolved bugs reported by sonarcloud", "committedDate": "2020-11-12T09:04:19Z", "type": "commit"}, {"oid": "123b5a88866e17cfc986d1239146a9ea3900713b", "url": "https://github.com/kiegroup/kogito-apps/commit/123b5a88866e17cfc986d1239146a9ea3900713b", "message": "KOGITO-3763 - reduced cognitive complexity, reported by sonarcloud", "committedDate": "2020-11-12T09:08:06Z", "type": "commit"}, {"oid": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "url": "https://github.com/kiegroup/kogito-apps/commit/7db39c4c33c84eca961e21a8c1a0a24628d379e2", "message": "KOGITO-3763 - removed useless comment", "committedDate": "2020-11-12T09:20:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYyNDQ2NQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528624465", "body": "do we need `finalK`?", "bodyText": "do we need finalK?", "bodyHTML": "<p dir=\"auto\">do we need <code>finalK</code>?</p>", "author": "r00ta", "createdAt": "2020-11-23T11:07:05Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java", "diffHunk": "@@ -132,4 +139,109 @@ public static double classificationFidelity(List<Pair<Saliency, Prediction>> pai\n         }\n         return evals == 0 ? 0 : acc / evals;\n     }\n+\n+    /**\n+     * Evaluate stability of a local explainer generating {@code Saliencies}.\n+     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n+     * positive/negative features always the same for a single prediction?\".\n+     *\n+     * @param model                  a model to explain\n+     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param saliencyLocalExplainer a local saliency explainer\n+     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n+     */\n+    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n+                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+                                                                   int topK, int runs)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n+\n+        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n+        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n+        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n+            for (int k = 1; k <= topK; k++) {\n+                String decision = entry.getKey();\n+                List<Saliency> perDecisionSaliencies = entry.getValue();\n+\n+                int finalK = k;", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcyOTEyNA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528729124", "bodyText": "it seems so, as we use k in the stream, but k value changes across iterations for (k=0; k < ...", "author": "tteofili", "createdAt": "2020-11-23T14:10:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYyNDQ2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc5OTc1OA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528799758", "bodyText": "Sorry, I did not see it was used in the lambda", "author": "r00ta", "createdAt": "2020-11-23T15:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYyNDQ2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "e8e67abb1b0fc9e60f928aecd4fe3dc2b4dd9425", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex 5e1b65312..07a49063c 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -145,25 +151,25 @@ public class ExplainabilityMetrics {\n      * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n      * positive/negative features always the same for a single prediction?\".\n      *\n-     * @param model                  a model to explain\n-     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param model a model to explain\n+     * @param prediction the prediction on which explanation stability will be evaluated\n      * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @param topK no. of top k positive/negative features for which stability report will be generated\n+     * @param runs no. of times the saliency for each prediction needs to be generated\n      * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n      */\n     public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-                                                                   int topK, int runs)\n+            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+            int topK, int runs)\n             throws InterruptedException, ExecutionException, TimeoutException {\n         Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n \n         LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n         // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n         for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n+            String decision = entry.getKey();\n+            List<Saliency> perDecisionSaliencies = entry.getValue();\n             for (int k = 1; k <= topK; k++) {\n-                String decision = entry.getKey();\n-                List<Saliency> perDecisionSaliencies = entry.getValue();\n-\n                 int finalK = k;\n                 // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n                 Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\ndeleted file mode 100644\nindex 07a49063c..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ /dev/null\n", "chunk": "@@ -1,440 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.LocalExplainer;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureImportance;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * Utility class providing different methods to evaluate explainability.\n- */\n-public class ExplainabilityMetrics {\n-\n-    private static final Logger LOGGER = LoggerFactory.getLogger(ExplainabilityMetrics.class);\n-\n-    /**\n-     * Drop in confidence score threshold for impact score calculation.\n-     * Confidence scores below {@code originalScore * CONFIDENCE_DROP_RATIO} are considered impactful for a model.\n-     */\n-    private static final double CONFIDENCE_DROP_RATIO = 0.2d;\n-\n-    private ExplainabilityMetrics() {\n-    }\n-\n-    /**\n-     * Measure the explainability of an explanation.\n-     * See paper: \"Towards Quantification of Explainability in Explainable Artificial Intelligence Methods\" by Islam et al.\n-     *\n-     * @param inputCognitiveChunks the no. of cognitive chunks (pieces of information) required to generate the\n-     *        explanation (e.g. the no. of explanation inputs)\n-     * @param outputCognitiveChunks the no. of cognitive chunks generated within the explanation itself\n-     * @param interactionRatio the ratio of interaction (between 0 and 1) required by the explanation\n-     * @return the quantitative explainability measure\n-     */\n-    public static double quantifyExplainability(int inputCognitiveChunks, int outputCognitiveChunks, double interactionRatio) {\n-        return inputCognitiveChunks + outputCognitiveChunks > 0 ? 0.333 / (double) inputCognitiveChunks\n-                + 0.333 / (double) outputCognitiveChunks + 0.333 * (1d - interactionRatio) : 0;\n-    }\n-\n-    /**\n-     * Calculate the impact of dropping the most important features (given by {@link Saliency#getTopFeatures(int)} from the input.\n-     * Highly important features would have rather high impact.\n-     * See paper: Qiu Lin, Zhong, et al. \"Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the\n-     * Performance of Explainability Algorithms.\" 2019.\n-     *\n-     * @param model the model to be explained\n-     * @param prediction a prediction\n-     * @param topFeatures the list of important features that should be dropped\n-     * @return the saliency impact\n-     */\n-    public static double impactScore(PredictionProvider model, Prediction prediction, List<FeatureImportance> topFeatures) throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Feature> copy = List.copyOf(prediction.getInput().getFeatures());\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            copy = DataUtils.dropFeature(copy, featureImportance.getFeature());\n-        }\n-\n-        PredictionInput predictionInput = new PredictionInput(copy);\n-        List<PredictionOutput> predictionOutputs;\n-        try {\n-            predictionOutputs = model.predictAsync(List.of(predictionInput))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        } catch (ExecutionException | TimeoutException e) {\n-            LOGGER.error(\"Impossible to obtain prediction {}\", e.getMessage());\n-            throw new IllegalStateException(\"Impossible to obtain prediction\", e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new IllegalStateException(\"Impossible to obtain prediction (Thread interrupted)\", e);\n-        }\n-        double impact = 0d;\n-        for (PredictionOutput predictionOutput : predictionOutputs) {\n-            double size = predictionOutput.getOutputs().size();\n-            for (int i = 0; i < size; i++) {\n-                Output original = prediction.getOutput().getOutputs().get(i);\n-                Output modified = predictionOutput.getOutputs().get(i);\n-                impact += (!original.getValue().asString().equals(modified.getValue().asString())\n-                        || modified.getScore() < original.getScore() * CONFIDENCE_DROP_RATIO) ? 1d / size : 0d;\n-            }\n-        }\n-        return impact;\n-    }\n-\n-    /**\n-     * Calculate fidelity (accuracy) of boolean classification outputs using saliency predictor function = sign(sum(saliency.scores))\n-     * See papers:\n-     * - Guidotti Riccardo, et al. \"A survey of methods for explaining black box models.\" ACM computing surveys (2018).\n-     * - Bodria, Francesco, et al. \"Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis (Discussion Paper).\"\n-     *\n-     * @param pairs pairs composed by the saliency and the related prediction\n-     * @return the fidelity accuracy\n-     */\n-    public static double classificationFidelity(List<Pair<Saliency, Prediction>> pairs) {\n-        double acc = 0;\n-        double evals = 0;\n-        for (Pair<Saliency, Prediction> pair : pairs) {\n-            Saliency saliency = pair.getLeft();\n-            Prediction prediction = pair.getRight();\n-            for (Output output : prediction.getOutput().getOutputs()) {\n-                Type type = output.getType();\n-                if (Type.BOOLEAN.equals(type)) {\n-                    double predictorOutput = saliency.getPerFeatureImportance().stream().map(FeatureImportance::getScore).mapToDouble(d -> d).sum();\n-                    double v = output.getValue().asNumber();\n-                    if ((v >= 0 && predictorOutput >= 0) || (v < 0 && predictorOutput < 0)) {\n-                        acc++;\n-                    }\n-                    evals++;\n-                }\n-            }\n-        }\n-        return evals == 0 ? 0 : acc / evals;\n-    }\n-\n-    /**\n-     * Evaluate stability of a local explainer generating {@code Saliencies}.\n-     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n-     * positive/negative features always the same for a single prediction?\".\n-     *\n-     * @param model a model to explain\n-     * @param prediction the prediction on which explanation stability will be evaluated\n-     * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK no. of top k positive/negative features for which stability report will be generated\n-     * @param runs no. of times the saliency for each prediction needs to be generated\n-     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n-     */\n-    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int topK, int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n-\n-        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n-        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n-        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n-            String decision = entry.getKey();\n-            List<Saliency> perDecisionSaliencies = entry.getValue();\n-            for (int k = 1; k <= topK; k++) {\n-                int finalK = k;\n-                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n-                // get the most frequent list of positive features\n-                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n-                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n-                // get the most frequent list of negative features\n-                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n-                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // decision stability at k\n-                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n-                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n-                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n-            }\n-        }\n-        return saliencyStability;\n-    }\n-\n-    /**\n-     * Get multiple saliencies, aggregated by decision name.\n-     *\n-     * @param model the model used to perform predictions\n-     * @param prediction the prediction to explain\n-     * @param saliencyLocalExplainer a local explainer that generates saliences\n-     * @param runs the no. of explanations to be generated\n-     * @return the generated saliencies, aggregated by decision name, across the different runs\n-     */\n-    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = new HashMap<>();\n-        int skipped = 0;\n-        for (int i = 0; i < runs; i++) {\n-            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n-                // aggregate saliencies by output name\n-                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n-                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n-                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n-                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n-                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n-                        updatedSaliencies.add(saliencyEntry.getValue());\n-                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n-                    } else {\n-                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n-                    }\n-                } else {\n-                    LOGGER.debug(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n-                    skipped++;\n-                }\n-            }\n-        }\n-        LOGGER.debug(\"skipped {} useless saliencies\", skipped);\n-        return saliencies;\n-    }\n-\n-    private static Map<List<String>, Long> getTopKFeaturesFrequency(List<Saliency> saliencies, Function<Saliency, List<FeatureImportance>> saliencyListFunction) {\n-        return saliencies.stream().map(saliencyListFunction)\n-                .map(l -> l.stream().map(f -> f.getFeature().getName())\n-                        .collect(Collectors.toList()))\n-                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));\n-    }\n-\n-    private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n-        Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n-        return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n-    }\n-\n-    /**\n-     * Evaluate the recall of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the highest score for the given decision and pair them with predictions\n-     * whose outputs have the lowest score for the same decision.\n-     * Get the top k (most important) features (according to the saliency) for the most important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with low score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output on the masked input is equals to the output for the\n-     * input the mask features were take from, that's considered a true positive, otherwise it's a false positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency recall\n-     */\n-    public static double getLocalSaliencyRecall(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-\n-        // get all samples from the data distribution\n-        List<Prediction> sorted = DataUtils.getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falseNegatives = 0;\n-        int currentChunk = 0;\n-        // for each of the top scored predictions, get the top influencing features and copy them over a low scored\n-        // input, then feed the model with this masked input and check the output is equals to the top scored one.\n-        for (Prediction prediction : topChunk) {\n-            Optional<Output> optionalOutput = prediction.getOutput().getByName(outputName);\n-            if (optionalOutput.isPresent()) {\n-                Output output = optionalOutput.get();\n-                Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (stringSaliencyMap.containsKey(outputName)) {\n-                    Saliency saliency = stringSaliencyMap.get(outputName);\n-                    List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                            .sorted((f1, f2) -> Double.compare(f2.getScore(), f1.getScore())).limit(k).collect(Collectors.toList());\n-\n-                    PredictionInput input = bottomChunk.get(currentChunk).getInput();\n-                    PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                    List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                            .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                    if (!predictionOutputList.isEmpty()) {\n-                        PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                        Optional<Output> optionalNewOutput = predictionOutput.getByName(outputName);\n-                        if (optionalNewOutput.isPresent()) {\n-                            Output newOutput = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falseNegatives++;\n-                            }\n-                        }\n-                    }\n-                    currentChunk++;\n-                }\n-            }\n-        }\n-        if ((truePositives + falseNegatives) > 0) {\n-            return truePositives / (truePositives + falseNegatives);\n-        } else {\n-            // if topChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput maskInput(List<FeatureImportance> topFeatures, PredictionInput input) {\n-        List<Feature> importantFeatures = new ArrayList<>();\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            importantFeatures.add(featureImportance.getFeature());\n-        }\n-\n-        return replaceAllFeatures(importantFeatures, input);\n-    }\n-\n-    /**\n-     * Evaluate the precision of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the lowest score for the given decision and pair them with predictions\n-     * whose outputs have the highest score for the same decision.\n-     * Get the bottom k (less important) features (according to the saliency) for the less important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with high score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output changes that's considered a false negative, otherwise\n-     * it's a true positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency precision\n-     */\n-    public static double getLocalSaliencyPrecision(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Prediction> sorted = DataUtils.getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falsePositives = 0;\n-        int currentChunk = 0;\n-\n-        for (Prediction prediction : bottomChunk) {\n-            Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                    .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-            if (stringSaliencyMap.containsKey(outputName)) {\n-                Saliency saliency = stringSaliencyMap.get(outputName);\n-                List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                        .sorted(Comparator.comparingDouble(FeatureImportance::getScore)).limit(k).collect(Collectors.toList());\n-\n-                Prediction topPrediction = topChunk.get(currentChunk);\n-                PredictionInput input = topPrediction.getInput();\n-                PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (!predictionOutputList.isEmpty()) {\n-                    PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                    Optional<Output> newOptionalOutput = predictionOutput.getByName(outputName);\n-                    if (newOptionalOutput.isPresent()) {\n-                        Output newOutput = newOptionalOutput.get();\n-                        Optional<Output> optionalOutput = topPrediction.getOutput().getByName(outputName);\n-                        if (optionalOutput.isPresent()) {\n-                            Output output = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falsePositives++;\n-                            }\n-                        }\n-                    }\n-                }\n-                currentChunk++;\n-            }\n-        }\n-        if ((truePositives + falsePositives) > 0) {\n-            return truePositives / (truePositives + falsePositives);\n-        } else {\n-            // if bottomChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    /**\n-     * Get local saliency F1 score.\n-     *\n-     * see <a href=\"https://en.wikipedia.org/wiki/F-score\"/>\n-     * See {@link #getLocalSaliencyPrecision(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     * See {@link #getLocalSaliencyRecall(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency F1\n-     */\n-    public static double getLocalSaliencyF1(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        double precision = getLocalSaliencyPrecision(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        double recall = getLocalSaliencyRecall(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        if (Double.isFinite(precision + recall) && (precision + recall) > 0) {\n-            return 2 * precision * recall / (precision + recall);\n-        } else {\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput replaceAllFeatures(List<Feature> importantFeatures, PredictionInput input) {\n-        List<Feature> features = List.copyOf(input.getFeatures());\n-        for (Feature f : importantFeatures) {\n-            features = DataUtils.replaceFeatures(f, features);\n-        }\n-        return new PredictionInput(features);\n-    }\n-\n-}\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzMDAyMg==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528630022", "body": "If I've got it properly this can be replaced by something like\r\n```\r\nMap.Entry maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\r\n```", "bodyText": "If I've got it properly this can be replaced by something like\nMap.Entry maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());", "bodyHTML": "<p dir=\"auto\">If I've got it properly this can be replaced by something like</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"Map.Entry maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\"><pre><code>Map.Entry maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n</code></pre></div>", "author": "r00ta", "createdAt": "2020-11-23T11:17:25Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java", "diffHunk": "@@ -132,4 +139,109 @@ public static double classificationFidelity(List<Pair<Saliency, Prediction>> pai\n         }\n         return evals == 0 ? 0 : acc / evals;\n     }\n+\n+    /**\n+     * Evaluate stability of a local explainer generating {@code Saliencies}.\n+     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n+     * positive/negative features always the same for a single prediction?\".\n+     *\n+     * @param model                  a model to explain\n+     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param saliencyLocalExplainer a local saliency explainer\n+     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n+     */\n+    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n+                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+                                                                   int topK, int runs)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n+\n+        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n+        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n+        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n+            for (int k = 1; k <= topK; k++) {\n+                String decision = entry.getKey();\n+                List<Saliency> perDecisionSaliencies = entry.getValue();\n+\n+                int finalK = k;\n+                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n+                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n+                // get the most frequent list of positive features\n+                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n+                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n+\n+                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n+                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n+                // get the most frequent list of negative features\n+                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n+                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n+\n+                // decision stability at k\n+                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n+                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n+                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n+            }\n+        }\n+        return saliencyStability;\n+    }\n+\n+    /**\n+     * Get multiple saliencies, aggregated by decision name.\n+     *\n+     * @param model                  the model used to perform predictions\n+     * @param prediction             the prediction to explain\n+     * @param saliencyLocalExplainer a local explainer that generates saliences\n+     * @param runs                   the no. of explanations to be generated\n+     * @return the generated saliencies, aggregated by decision name, across the different runs\n+     */\n+    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n+                                                                     LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+                                                                     int runs)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<String, List<Saliency>> saliencies = new HashMap<>();\n+        int skipped = 0;\n+        for (int i = 0; i < runs; i++) {\n+            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n+                // aggregate saliencies by output name\n+                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n+                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n+                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n+                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n+                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n+                        updatedSaliencies.add(saliencyEntry.getValue());\n+                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n+                    } else {\n+                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n+                    }\n+                } else {\n+                    LOGGER.warn(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n+                    skipped++;\n+                }\n+            }\n+        }\n+        LOGGER.debug(\"skipped {} useless saliencies\", skipped);\n+        return saliencies;\n+    }\n+\n+    private static Map<List<String>, Long> getTopKFeaturesFrequency(List<Saliency> saliencies, Function<Saliency, List<FeatureImportance>> saliencyListFunction) {\n+        return saliencies.stream().map(saliencyListFunction)\n+                .map(l -> l.stream().map(f -> f.getFeature().getName())\n+                        .collect(Collectors.toList()))\n+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));\n+    }\n+\n+    private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n+        long max = 0L;\n+        Pair<List<String>, Long> topK = Pair.of(Collections.emptyList(), 0L);\n+        for (Map.Entry<List<String>, Long> entry : collect.entrySet()) {\n+            if (entry.getValue() >= max) {\n+                topK = Pair.of(entry.getKey(), entry.getValue());\n+                max = entry.getValue();\n+            }\n+        }\n+        return topK;\n+    }", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc0MzM5NA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528743394", "bodyText": "much better, thanks!", "author": "tteofili", "createdAt": "2020-11-23T14:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzMDAyMg=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex 5e1b65312..d935aa669 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -234,14 +234,7 @@ public class ExplainabilityMetrics {\n     }\n \n     private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n-        long max = 0L;\n-        Pair<List<String>, Long> topK = Pair.of(Collections.emptyList(), 0L);\n-        for (Map.Entry<List<String>, Long> entry : collect.entrySet()) {\n-            if (entry.getValue() >= max) {\n-                topK = Pair.of(entry.getKey(), entry.getValue());\n-                max = entry.getValue();\n-            }\n-        }\n-        return topK;\n+        Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n+        return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n     }\n }\n", "next_change": {"commit": "c0612b33d89568c8550170ac5264c1317e62ca80", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex d935aa669..cb55aec02 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -237,4 +243,220 @@ public class ExplainabilityMetrics {\n         Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n         return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n     }\n+\n+    /**\n+     * Evaluate the recall of a local saliency explainer on a given model.\n+     * Get the predictions having outputs with the highest score for the given decision and pair them with predictions\n+     * whose outputs have the lowest score for the same decision.\n+     * Get the top k (most important) features (according to the saliency) for the most important outputs and\n+     * \"paste\" them on each paired input corresponding to an output with low score (for the target decision).\n+     * Perform prediction on the \"masked\" input, if the output on the masked input is equals to the output for the\n+     * input the mask features were take from, that's considered a true positive, otherwise it's a false positive.\n+     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n+     *\n+     * @param outputName decision to evaluate recall for\n+     * @param predictionProvider the prediction provider to test\n+     * @param localExplainer the explainer to evaluate\n+     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n+     * @param k the no. of features to extract\n+     * @param chunkSize the size of the chunk of predictions to use for evaluation\n+     * @return the saliency recall\n+     */\n+    public static double getLocalSaliencyRecall(String outputName, PredictionProvider predictionProvider,\n+            LocalExplainer<Map<String, Saliency>> localExplainer,\n+            DataDistribution dataDistribution, int k, int chunkSize)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+\n+        // get all samples from the data distribution\n+        List<Prediction> sorted = getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n+\n+        // get the top and bottom 'chunkSize' predictions\n+        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n+        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n+\n+        double truePositives = 0;\n+        double falseNegatives = 0;\n+        int currentChunk = 0;\n+        // for each of the top scored predictions, get the top influencing features and copy them over a low scored\n+        // input, then feed the model with this masked input and check the output is equals to the top scored one.\n+        for (Prediction prediction : topChunk) {\n+            Optional<Output> optionalOutput = prediction.getOutput().getByName(outputName);\n+            if (optionalOutput.isPresent()) {\n+                Output output = optionalOutput.get();\n+                Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n+                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n+                if (stringSaliencyMap.containsKey(outputName)) {\n+                    Saliency saliency = stringSaliencyMap.get(outputName);\n+                    List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n+                            .sorted((f1, f2) -> Double.compare(f2.getScore(), f1.getScore())).limit(k).collect(Collectors.toList());\n+\n+                    PredictionInput input = bottomChunk.get(currentChunk).getInput();\n+                    PredictionInput maskedInput = maskInput(topFeatures, input);\n+\n+                    List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n+                            .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n+                    if (!predictionOutputList.isEmpty()) {\n+                        PredictionOutput predictionOutput = predictionOutputList.get(0);\n+                        Optional<Output> optionalNewOutput = predictionOutput.getByName(outputName);\n+                        if (optionalNewOutput.isPresent()) {\n+                            Output newOutput = optionalOutput.get();\n+                            if (output.getValue().equals(newOutput.getValue())) {\n+                                truePositives++;\n+                            } else {\n+                                falseNegatives++;\n+                            }\n+                        }\n+                    }\n+                    currentChunk++;\n+                }\n+            }\n+        }\n+        if ((truePositives + falseNegatives) > 0) {\n+            return truePositives / (truePositives + falseNegatives);\n+        } else {\n+            // if topChunk is empty or the target output (by name) is not an output of the model.\n+            return Double.NaN;\n+        }\n+    }\n+\n+    private static PredictionInput maskInput(List<FeatureImportance> topFeatures, PredictionInput input) {\n+        List<Feature> importantFeatures = new ArrayList<>();\n+        for (FeatureImportance featureImportance : topFeatures) {\n+            importantFeatures.add(featureImportance.getFeature());\n+        }\n+\n+        return replaceAllFeatures(importantFeatures, input);\n+    }\n+\n+    private static List<Prediction> getScoreSortedPredictions(String outputName, PredictionProvider predictionProvider,\n+            DataDistribution dataDistribution)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        List<PredictionInput> inputs = dataDistribution.getAllSamples();\n+        List<PredictionOutput> predictionOutputs = predictionProvider.predictAsync(inputs)\n+                .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n+        List<Prediction> predictions = DataUtils.getPredictions(inputs, predictionOutputs);\n+\n+        // sort the predictions by Output#getScore, in descending order\n+        return predictions.stream().sorted((p1, p2) -> {\n+            Optional<Output> optionalOutput1 = p1.getOutput().getByName(outputName);\n+            Optional<Output> optionalOutput2 = p2.getOutput().getByName(outputName);\n+            if (optionalOutput1.isPresent() && optionalOutput2.isPresent()) {\n+                Output o1 = optionalOutput1.get();\n+                Output o2 = optionalOutput2.get();\n+                return Double.compare(o2.getScore(), o1.getScore());\n+            } else {\n+                return 0;\n+            }\n+        }).collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * Evaluate the precision of a local saliency explainer on a given model.\n+     * Get the predictions having outputs with the lowest score for the given decision and pair them with predictions\n+     * whose outputs have the highest score for the same decision.\n+     * Get the bottom k (less important) features (according to the saliency) for the less important outputs and\n+     * \"paste\" them on each paired input corresponding to an output with high score (for the target decision).\n+     * Perform prediction on the \"masked\" input, if the output changes that's considered a false negative, otherwise\n+     * it's a true positive.\n+     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n+     *\n+     * @param outputName decision to evaluate recall for\n+     * @param predictionProvider the prediction provider to test\n+     * @param localExplainer the explainer to evaluate\n+     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n+     * @param k the no. of features to extract\n+     * @param chunkSize the size of the chunk of predictions to use for evaluation\n+     * @return the saliency precision\n+     */\n+    public static double getLocalSaliencyPrecision(String outputName, PredictionProvider predictionProvider,\n+            LocalExplainer<Map<String, Saliency>> localExplainer,\n+            DataDistribution dataDistribution, int k, int chunkSize)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        List<Prediction> sorted = getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n+\n+        // get the top and bottom 'chunkSize' predictions\n+        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n+        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n+\n+        double truePositives = 0;\n+        double falsePositives = 0;\n+        int currentChunk = 0;\n+\n+        for (Prediction prediction : bottomChunk) {\n+            Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n+                    .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n+            if (stringSaliencyMap.containsKey(outputName)) {\n+                Saliency saliency = stringSaliencyMap.get(outputName);\n+                List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n+                        .sorted(Comparator.comparingDouble(FeatureImportance::getScore)).limit(k).collect(Collectors.toList());\n+\n+                Prediction topPrediction = topChunk.get(currentChunk);\n+                PredictionInput input = topPrediction.getInput();\n+                PredictionInput maskedInput = maskInput(topFeatures, input);\n+\n+                List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n+                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n+                if (!predictionOutputList.isEmpty()) {\n+                    PredictionOutput predictionOutput = predictionOutputList.get(0);\n+                    Optional<Output> newOptionalOutput = predictionOutput.getByName(outputName);\n+                    if (newOptionalOutput.isPresent()) {\n+                        Output newOutput = newOptionalOutput.get();\n+                        Optional<Output> optionalOutput = topPrediction.getOutput().getByName(outputName);\n+                        if (optionalOutput.isPresent()) {\n+                            Output output = optionalOutput.get();\n+                            if (output.getValue().equals(newOutput.getValue())) {\n+                                truePositives++;\n+                            } else {\n+                                falsePositives++;\n+                            }\n+                        }\n+                    }\n+                }\n+                currentChunk++;\n+            }\n+        }\n+        if ((truePositives + falsePositives) > 0) {\n+            return truePositives / (truePositives + falsePositives);\n+        } else {\n+            // if bottomChunk is empty or the target output (by name) is not an output of the model.\n+            return Double.NaN;\n+        }\n+    }\n+\n+    /**\n+     * Get local saliency F1 score.\n+     *\n+     * see <a href=\"https://en.wikipedia.org/wiki/F-score\"/>\n+     * See {@link #getLocalSaliencyPrecision(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n+     * See {@link #getLocalSaliencyRecall(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n+     *\n+     * @param outputName decision to evaluate recall for\n+     * @param predictionProvider the prediction provider to test\n+     * @param localExplainer the explainer to evaluate\n+     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n+     * @param k the no. of features to extract\n+     * @param chunkSize the size of the chunk of predictions to use for evaluation\n+     * @return the saliency F1\n+     */\n+    public static double getLocalSaliencyF1(String outputName, PredictionProvider predictionProvider,\n+            LocalExplainer<Map<String, Saliency>> localExplainer,\n+            DataDistribution dataDistribution, int k, int chunkSize)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        double precision = getLocalSaliencyPrecision(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n+        double recall = getLocalSaliencyRecall(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n+        if (Double.isFinite(precision + recall) && (precision + recall) > 0) {\n+            return 2 * precision * recall / (precision + recall);\n+        } else {\n+            return Double.NaN;\n+        }\n+    }\n+\n+    private static PredictionInput replaceAllFeatures(List<Feature> importantFeatures, PredictionInput input) {\n+        List<Feature> features = List.copyOf(input.getFeatures());\n+        for (Feature f : importantFeatures) {\n+            features = DataUtils.replaceFeatures(f, features);\n+        }\n+        return new PredictionInput(features);\n+    }\n+\n }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\ndeleted file mode 100644\nindex cb55aec02..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ /dev/null\n", "chunk": "@@ -1,462 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.LocalExplainer;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureImportance;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * Utility class providing different methods to evaluate explainability.\n- */\n-public class ExplainabilityMetrics {\n-\n-    private static final Logger LOGGER = LoggerFactory.getLogger(ExplainabilityMetrics.class);\n-\n-    /**\n-     * Drop in confidence score threshold for impact score calculation.\n-     * Confidence scores below {@code originalScore * CONFIDENCE_DROP_RATIO} are considered impactful for a model.\n-     */\n-    private static final double CONFIDENCE_DROP_RATIO = 0.2d;\n-\n-    private ExplainabilityMetrics() {\n-    }\n-\n-    /**\n-     * Measure the explainability of an explanation.\n-     * See paper: \"Towards Quantification of Explainability in Explainable Artificial Intelligence Methods\" by Islam et al.\n-     *\n-     * @param inputCognitiveChunks the no. of cognitive chunks (pieces of information) required to generate the\n-     *        explanation (e.g. the no. of explanation inputs)\n-     * @param outputCognitiveChunks the no. of cognitive chunks generated within the explanation itself\n-     * @param interactionRatio the ratio of interaction (between 0 and 1) required by the explanation\n-     * @return the quantitative explainability measure\n-     */\n-    public static double quantifyExplainability(int inputCognitiveChunks, int outputCognitiveChunks, double interactionRatio) {\n-        return inputCognitiveChunks + outputCognitiveChunks > 0 ? 0.333 / (double) inputCognitiveChunks\n-                + 0.333 / (double) outputCognitiveChunks + 0.333 * (1d - interactionRatio) : 0;\n-    }\n-\n-    /**\n-     * Calculate the impact of dropping the most important features (given by {@link Saliency#getTopFeatures(int)} from the input.\n-     * Highly important features would have rather high impact.\n-     * See paper: Qiu Lin, Zhong, et al. \"Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the\n-     * Performance of Explainability Algorithms.\" 2019.\n-     *\n-     * @param model the model to be explained\n-     * @param prediction a prediction\n-     * @param topFeatures the list of important features that should be dropped\n-     * @return the saliency impact\n-     */\n-    public static double impactScore(PredictionProvider model, Prediction prediction, List<FeatureImportance> topFeatures) throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Feature> copy = List.copyOf(prediction.getInput().getFeatures());\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            copy = DataUtils.dropFeature(copy, featureImportance.getFeature());\n-        }\n-\n-        PredictionInput predictionInput = new PredictionInput(copy);\n-        List<PredictionOutput> predictionOutputs;\n-        try {\n-            predictionOutputs = model.predictAsync(List.of(predictionInput))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        } catch (ExecutionException | TimeoutException e) {\n-            LOGGER.error(\"Impossible to obtain prediction {}\", e.getMessage());\n-            throw new IllegalStateException(\"Impossible to obtain prediction\", e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new IllegalStateException(\"Impossible to obtain prediction (Thread interrupted)\", e);\n-        }\n-        double impact = 0d;\n-        for (PredictionOutput predictionOutput : predictionOutputs) {\n-            double size = predictionOutput.getOutputs().size();\n-            for (int i = 0; i < size; i++) {\n-                Output original = prediction.getOutput().getOutputs().get(i);\n-                Output modified = predictionOutput.getOutputs().get(i);\n-                impact += (!original.getValue().asString().equals(modified.getValue().asString())\n-                        || modified.getScore() < original.getScore() * CONFIDENCE_DROP_RATIO) ? 1d / size : 0d;\n-            }\n-        }\n-        return impact;\n-    }\n-\n-    /**\n-     * Calculate fidelity (accuracy) of boolean classification outputs using saliency predictor function = sign(sum(saliency.scores))\n-     * See papers:\n-     * - Guidotti Riccardo, et al. \"A survey of methods for explaining black box models.\" ACM computing surveys (2018).\n-     * - Bodria, Francesco, et al. \"Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis (Discussion Paper).\"\n-     *\n-     * @param pairs pairs composed by the saliency and the related prediction\n-     * @return the fidelity accuracy\n-     */\n-    public static double classificationFidelity(List<Pair<Saliency, Prediction>> pairs) {\n-        double acc = 0;\n-        double evals = 0;\n-        for (Pair<Saliency, Prediction> pair : pairs) {\n-            Saliency saliency = pair.getLeft();\n-            Prediction prediction = pair.getRight();\n-            for (Output output : prediction.getOutput().getOutputs()) {\n-                Type type = output.getType();\n-                if (Type.BOOLEAN.equals(type)) {\n-                    double predictorOutput = saliency.getPerFeatureImportance().stream().map(FeatureImportance::getScore).mapToDouble(d -> d).sum();\n-                    double v = output.getValue().asNumber();\n-                    if ((v >= 0 && predictorOutput >= 0) || (v < 0 && predictorOutput < 0)) {\n-                        acc++;\n-                    }\n-                    evals++;\n-                }\n-            }\n-        }\n-        return evals == 0 ? 0 : acc / evals;\n-    }\n-\n-    /**\n-     * Evaluate stability of a local explainer generating {@code Saliencies}.\n-     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n-     * positive/negative features always the same for a single prediction?\".\n-     *\n-     * @param model a model to explain\n-     * @param prediction the prediction on which explanation stability will be evaluated\n-     * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK no. of top k positive/negative features for which stability report will be generated\n-     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n-     */\n-    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int topK, int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n-\n-        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n-        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n-        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n-            for (int k = 1; k <= topK; k++) {\n-                String decision = entry.getKey();\n-                List<Saliency> perDecisionSaliencies = entry.getValue();\n-\n-                int finalK = k;\n-                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n-                // get the most frequent list of positive features\n-                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n-                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n-                // get the most frequent list of negative features\n-                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n-                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // decision stability at k\n-                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n-                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n-                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n-            }\n-        }\n-        return saliencyStability;\n-    }\n-\n-    /**\n-     * Get multiple saliencies, aggregated by decision name.\n-     *\n-     * @param model the model used to perform predictions\n-     * @param prediction the prediction to explain\n-     * @param saliencyLocalExplainer a local explainer that generates saliences\n-     * @param runs the no. of explanations to be generated\n-     * @return the generated saliencies, aggregated by decision name, across the different runs\n-     */\n-    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = new HashMap<>();\n-        int skipped = 0;\n-        for (int i = 0; i < runs; i++) {\n-            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n-                // aggregate saliencies by output name\n-                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n-                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n-                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n-                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n-                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n-                        updatedSaliencies.add(saliencyEntry.getValue());\n-                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n-                    } else {\n-                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n-                    }\n-                } else {\n-                    LOGGER.debug(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n-                    skipped++;\n-                }\n-            }\n-        }\n-        LOGGER.debug(\"skipped {} useless saliencies\", skipped);\n-        return saliencies;\n-    }\n-\n-    private static Map<List<String>, Long> getTopKFeaturesFrequency(List<Saliency> saliencies, Function<Saliency, List<FeatureImportance>> saliencyListFunction) {\n-        return saliencies.stream().map(saliencyListFunction)\n-                .map(l -> l.stream().map(f -> f.getFeature().getName())\n-                        .collect(Collectors.toList()))\n-                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));\n-    }\n-\n-    private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n-        Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n-        return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n-    }\n-\n-    /**\n-     * Evaluate the recall of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the highest score for the given decision and pair them with predictions\n-     * whose outputs have the lowest score for the same decision.\n-     * Get the top k (most important) features (according to the saliency) for the most important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with low score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output on the masked input is equals to the output for the\n-     * input the mask features were take from, that's considered a true positive, otherwise it's a false positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency recall\n-     */\n-    public static double getLocalSaliencyRecall(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-\n-        // get all samples from the data distribution\n-        List<Prediction> sorted = getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falseNegatives = 0;\n-        int currentChunk = 0;\n-        // for each of the top scored predictions, get the top influencing features and copy them over a low scored\n-        // input, then feed the model with this masked input and check the output is equals to the top scored one.\n-        for (Prediction prediction : topChunk) {\n-            Optional<Output> optionalOutput = prediction.getOutput().getByName(outputName);\n-            if (optionalOutput.isPresent()) {\n-                Output output = optionalOutput.get();\n-                Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (stringSaliencyMap.containsKey(outputName)) {\n-                    Saliency saliency = stringSaliencyMap.get(outputName);\n-                    List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                            .sorted((f1, f2) -> Double.compare(f2.getScore(), f1.getScore())).limit(k).collect(Collectors.toList());\n-\n-                    PredictionInput input = bottomChunk.get(currentChunk).getInput();\n-                    PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                    List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                            .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                    if (!predictionOutputList.isEmpty()) {\n-                        PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                        Optional<Output> optionalNewOutput = predictionOutput.getByName(outputName);\n-                        if (optionalNewOutput.isPresent()) {\n-                            Output newOutput = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falseNegatives++;\n-                            }\n-                        }\n-                    }\n-                    currentChunk++;\n-                }\n-            }\n-        }\n-        if ((truePositives + falseNegatives) > 0) {\n-            return truePositives / (truePositives + falseNegatives);\n-        } else {\n-            // if topChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput maskInput(List<FeatureImportance> topFeatures, PredictionInput input) {\n-        List<Feature> importantFeatures = new ArrayList<>();\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            importantFeatures.add(featureImportance.getFeature());\n-        }\n-\n-        return replaceAllFeatures(importantFeatures, input);\n-    }\n-\n-    private static List<Prediction> getScoreSortedPredictions(String outputName, PredictionProvider predictionProvider,\n-            DataDistribution dataDistribution)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        List<PredictionInput> inputs = dataDistribution.getAllSamples();\n-        List<PredictionOutput> predictionOutputs = predictionProvider.predictAsync(inputs)\n-                .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-        List<Prediction> predictions = DataUtils.getPredictions(inputs, predictionOutputs);\n-\n-        // sort the predictions by Output#getScore, in descending order\n-        return predictions.stream().sorted((p1, p2) -> {\n-            Optional<Output> optionalOutput1 = p1.getOutput().getByName(outputName);\n-            Optional<Output> optionalOutput2 = p2.getOutput().getByName(outputName);\n-            if (optionalOutput1.isPresent() && optionalOutput2.isPresent()) {\n-                Output o1 = optionalOutput1.get();\n-                Output o2 = optionalOutput2.get();\n-                return Double.compare(o2.getScore(), o1.getScore());\n-            } else {\n-                return 0;\n-            }\n-        }).collect(Collectors.toList());\n-    }\n-\n-    /**\n-     * Evaluate the precision of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the lowest score for the given decision and pair them with predictions\n-     * whose outputs have the highest score for the same decision.\n-     * Get the bottom k (less important) features (according to the saliency) for the less important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with high score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output changes that's considered a false negative, otherwise\n-     * it's a true positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency precision\n-     */\n-    public static double getLocalSaliencyPrecision(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Prediction> sorted = getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falsePositives = 0;\n-        int currentChunk = 0;\n-\n-        for (Prediction prediction : bottomChunk) {\n-            Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                    .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-            if (stringSaliencyMap.containsKey(outputName)) {\n-                Saliency saliency = stringSaliencyMap.get(outputName);\n-                List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                        .sorted(Comparator.comparingDouble(FeatureImportance::getScore)).limit(k).collect(Collectors.toList());\n-\n-                Prediction topPrediction = topChunk.get(currentChunk);\n-                PredictionInput input = topPrediction.getInput();\n-                PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (!predictionOutputList.isEmpty()) {\n-                    PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                    Optional<Output> newOptionalOutput = predictionOutput.getByName(outputName);\n-                    if (newOptionalOutput.isPresent()) {\n-                        Output newOutput = newOptionalOutput.get();\n-                        Optional<Output> optionalOutput = topPrediction.getOutput().getByName(outputName);\n-                        if (optionalOutput.isPresent()) {\n-                            Output output = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falsePositives++;\n-                            }\n-                        }\n-                    }\n-                }\n-                currentChunk++;\n-            }\n-        }\n-        if ((truePositives + falsePositives) > 0) {\n-            return truePositives / (truePositives + falsePositives);\n-        } else {\n-            // if bottomChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    /**\n-     * Get local saliency F1 score.\n-     *\n-     * see <a href=\"https://en.wikipedia.org/wiki/F-score\"/>\n-     * See {@link #getLocalSaliencyPrecision(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     * See {@link #getLocalSaliencyRecall(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency F1\n-     */\n-    public static double getLocalSaliencyF1(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        double precision = getLocalSaliencyPrecision(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        double recall = getLocalSaliencyRecall(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        if (Double.isFinite(precision + recall) && (precision + recall) > 0) {\n-            return 2 * precision * recall / (precision + recall);\n-        } else {\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput replaceAllFeatures(List<Feature> importantFeatures, PredictionInput input) {\n-        List<Feature> features = List.copyOf(input.getFeatures());\n-        for (Feature f : importantFeatures) {\n-            features = DataUtils.replaceFeatures(f, features);\n-        }\n-        return new PredictionInput(features);\n-    }\n-\n-}\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzMjc0Ng==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528632746", "body": "Do we need `public` or can we keep it internal?", "bodyText": "Do we need public or can we keep it internal?", "bodyHTML": "<p dir=\"auto\">Do we need <code>public</code> or can we keep it internal?</p>", "author": "r00ta", "createdAt": "2020-11-23T11:22:33Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/LocalSaliencyStability.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.kogito.explainability.utils;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Local {@code Saliency} stability evaluation result.\n+ */\n+public class LocalSaliencyStability {", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1MTUxNw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r529251517", "bodyText": "same reasoning as ExplainabilityMetrics#getLocalSaliencyStability, being this the result of the evaluation, it should be public for evaluators to consume it.", "author": "tteofili", "createdAt": "2020-11-24T07:22:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzMjc0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/LocalSaliencyStability.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/LocalSaliencyStability.java\ndeleted file mode 100644\nindex 864167c52..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/LocalSaliencyStability.java\n+++ /dev/null\n", "chunk": "@@ -1,110 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-/**\n- * Local {@code Saliency} stability evaluation result.\n- */\n-public class LocalSaliencyStability {\n-\n-    private final Map<String, Map<Integer, SaliencyFrequencyMetadata>> map;\n-\n-    public LocalSaliencyStability(Set<String> decisions) {\n-        map = new HashMap<>();\n-        for (String k : decisions) {\n-            map.put(k, new HashMap<>());\n-        }\n-    }\n-\n-    public Collection<String> getDecisions() {\n-        return map.keySet();\n-    }\n-\n-    public List<String> getMostFrequentPositive(String decision, int k) {\n-        return map.get(decision).get(k).getPositiveFeatureNames();\n-    }\n-\n-    public List<String> getMostFrequentNegative(String decision, int k) {\n-        return map.get(decision).get(k).getNegativeFeatureNames();\n-    }\n-\n-    public double getPositiveStabilityScore(String decision, int k) {\n-        return map.get(decision).get(k).getPositiveFrequencyRate();\n-    }\n-\n-    public double getNegativeStabilityScore(String decision, int k) {\n-        return map.get(decision).get(k).getNegativeFrequencyRate();\n-    }\n-\n-    /**\n-     * Record stability data about a given decision, on top k features.\n-     * @param decision the decision\n-     * @param k the no. of top features considered\n-     * @param positiveFeatureNames the names of top positive features\n-     * @param positiveFrequencyRate the frequency rate of the top positive features\n-     * @param negativeFeatureNames the names of top negative features\n-     * @param negativeFrequencyRate the frequency rate of the top negative features\n-     */\n-    public void add(String decision, int k, List<String> positiveFeatureNames, double positiveFrequencyRate,\n-                    List<String> negativeFeatureNames, double negativeFrequencyRate) {\n-        if (map.containsKey(decision)) {\n-            Map<Integer, SaliencyFrequencyMetadata> integerMap = map.get(decision);\n-            integerMap.put(k, new SaliencyFrequencyMetadata(positiveFeatureNames, positiveFrequencyRate,\n-                                                            negativeFeatureNames, negativeFrequencyRate));\n-        }\n-    }\n-\n-    /**\n-     * Internal utility class to record stability evaluations for a single decision on top k features.\n-     */\n-    private static class SaliencyFrequencyMetadata {\n-\n-        private final List<String> positiveFeatureNames;\n-        private final double positiveFrequencyRate;\n-        private final List<String> negativeFeatureNames;\n-        private final double negativeFrequencyRate;\n-\n-        private SaliencyFrequencyMetadata(List<String> positiveFeatureNames, double positiveFrequencyRate,\n-                                          List<String> negativeFeatureNames, double negativeFrequencyRate) {\n-            this.positiveFeatureNames = positiveFeatureNames;\n-            this.positiveFrequencyRate = positiveFrequencyRate;\n-            this.negativeFeatureNames = negativeFeatureNames;\n-            this.negativeFrequencyRate = negativeFrequencyRate;\n-        }\n-\n-        double getNegativeFrequencyRate() {\n-            return negativeFrequencyRate;\n-        }\n-\n-        double getPositiveFrequencyRate() {\n-            return positiveFrequencyRate;\n-        }\n-\n-        List<String> getNegativeFeatureNames() {\n-            return negativeFeatureNames;\n-        }\n-\n-        List<String> getPositiveFeatureNames() {\n-            return positiveFeatureNames;\n-        }\n-    }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzOTk0OA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528639948", "body": "I have some questions around the `getPositiveStabilityScore` and `getNegativeStabilityScore`, what do they represent? Since they are rates how can they be both greater than `0.5`? ", "bodyText": "I have some questions around the getPositiveStabilityScore and getNegativeStabilityScore, what do they represent? Since they are rates how can they be both greater than 0.5?", "bodyHTML": "<p dir=\"auto\">I have some questions around the <code>getPositiveStabilityScore</code> and <code>getNegativeStabilityScore</code>, what do they represent? Since they are rates how can they be both greater than <code>0.5</code>?</p>", "author": "r00ta", "createdAt": "2020-11-23T11:36:03Z", "path": "explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java", "diffHunk": "@@ -86,17 +89,30 @@ void testPMMLRegression() throws Exception {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertNotNull(predictionOutputs);\n+            assertFalse(predictionOutputs.isEmpty());\n+            PredictionOutput output = predictionOutputs.get(0);\n+            assertNotNull(output);\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n                 assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n                 assertEquals(1d, v);\n             }\n+            int topK = 1;\n+            LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n+            for (int i = 1; i <= topK; i++) {\n+                for (String decision : stability.getDecisions()) {\n+                    double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n+                    double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n+                    assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n+                    assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1NjE1NQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r529256155", "bodyText": "data about positive and negative stability are kept separate because we want to separately check that the top k negative features are always the same as well as the top k positive features are always the same; it can happen that the negatives are stable whereas this is not true for the positive features.\nHowever I think rate is wrong here, ratio is more accurate (it's basically the number of times the most frequent positive/negative features appear divided by the number of explanations drawn for that decision).", "author": "tteofili", "createdAt": "2020-11-24T07:32:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODYzOTk0OA=="}], "type": "inlineReview", "revised_code": {"commit": "f30e10482c63ec0e028d6d7b3167cddcc1059535", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex df7983a8a..7f6be8749 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -89,30 +91,17 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertNotNull(predictionOutputs);\n-            assertFalse(predictionOutputs.isEmpty());\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertNotNull(output);\n+            PredictionOutput output = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n+                    .get(0);\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n                 assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n                 assertEquals(1d, v);\n             }\n-            int topK = 1;\n-            LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n-            for (int i = 1; i <= topK; i++) {\n-                for (String decision : stability.getDecisions()) {\n-                    double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n-                    double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n-                    assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                    assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                }\n-            }\n         }\n     }\n \n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex 7f6be8749..555059238 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -91,17 +93,22 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertThat(predictionOutputs).isNotNull();\n+            assertThat(predictionOutputs).isNotEmpty();\n+            PredictionOutput output = predictionOutputs.get(0);\n+            assertThat(output).isNotNull();\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n-                assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n-                assertEquals(1d, v);\n+                assertThat(saliency).isNotNull();\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                assertThat(v).isEqualTo(1d);\n             }\n+            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n+                                                                                    0.5, 0.5));\n         }\n     }\n \n", "next_change": {"commit": "cd5adebe96ccc6779dae7fce94d0e9abb8230e69", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\ndeleted file mode 100644\nindex 555059238..000000000\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ /dev/null\n", "chunk": "@@ -1,259 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.explainability.integrationtests.pmml;\n-\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.CompletableFuture;\n-\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Test;\n-import org.kie.api.pmml.PMML4Result;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.lime.LimeConfig;\n-import org.kie.kogito.explainability.local.lime.LimeExplainer;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n-import org.kie.kogito.explainability.utils.ValidationUtils;\n-import org.kie.pmml.api.runtime.PMMLRuntime;\n-import org.kie.kogito.explainability.utils.LocalSaliencyStability;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.kie.kogito.explainability.explainability.integrationtests.pmml.AbstractPMMLTest.getPMMLRuntime;\n-import static org.kie.test.util.filesystem.FileUtils.getFile;\n-\n-class PmmlLimeExplainerTest {\n-\n-    private static PMMLRuntime logisticRegressionIrisRuntime;\n-    private static PMMLRuntime categoricalVariableRegressionRuntime;\n-    private static PMMLRuntime scorecardCategoricalRuntime;\n-    private static PMMLRuntime compoundScoreCardRuntime;\n-\n-    @BeforeAll\n-    static void setUpBefore() {\n-        logisticRegressionIrisRuntime = getPMMLRuntime(getFile(\"logisticRegressionIrisData.pmml\"));\n-        categoricalVariableRegressionRuntime = getPMMLRuntime(getFile(\"categoricalVariablesRegression.pmml\"));\n-        scorecardCategoricalRuntime = getPMMLRuntime(getFile(\"SimpleScorecardCategorical.pmml\"));\n-        compoundScoreCardRuntime = getPMMLRuntime(getFile(\"CompoundNestedPredicateScorecard.pmml\"));\n-    }\n-\n-    @Test\n-    void testPMMLRegression() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(1000)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalLength\", 6.9));\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalWidth\", 3.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalLength\", 5.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalWidth\", 2.3));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    LogisticRegressionIrisDataExecutor pmmlModel = new LogisticRegressionIrisDataExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asNumber(),\n-                            features1.get(2).getValue().asNumber(), features1.get(3).getValue().asNumber());\n-                    PMML4Result result = pmmlModel.execute(logisticRegressionIrisRuntime);\n-                    String species = result.getResultVariables().get(\"Species\").toString();\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"species\", Type.TEXT, new Value<>(species), 1d)));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-\n-    @Test\n-    void testPMMLRegressionCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapX\", \"red\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapY\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(500)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                CategoricalVariablesRegressionExecutor pmmlModel = new CategoricalVariablesRegressionExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(categoricalVariableRegressionRuntime);\n-                String score = result.getResultVariables().get(\"result\").toString();\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"result\", Type.NUMBER, new Value<>(score), 1d)));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isEqualTo(1d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLScorecardCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"input1\", \"classA\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"input2\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(300)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                SimpleScorecardCategoricalExecutor pmmlModel = new SimpleScorecardCategoricalExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(scorecardCategoricalRuntime);\n-                String score = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.TARGET_FIELD);\n-                String reason1 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE1_FIELD);\n-                String reason2 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE2_FIELD);\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                        new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                        new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d),\n-                        new Output(\"reason2\", Type.TEXT, new Value<>(reason2), 1d)\n-                ));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isGreaterThan(0d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLCompoundScorecard() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(300)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"input1\", -50));\n-            features.add(FeatureFactory.newTextFeature(\"input2\", \"classB\"));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    CompoundNestedPredicateScorecardExecutor pmmlModel = new CompoundNestedPredicateScorecardExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asString());\n-                    PMML4Result result = pmmlModel.execute(compoundScoreCardRuntime);\n-                    String score = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.TARGET_FIELD);\n-                    String reason1 = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.REASON_CODE1_FIELD);\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                            new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                            new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d)\n-                    ));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODY0MTAzNQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528641035", "body": "Do we need `public` or can we keep it internal?", "bodyText": "Do we need public or can we keep it internal?", "bodyHTML": "<p dir=\"auto\">Do we need <code>public</code> or can we keep it internal?</p>", "author": "r00ta", "createdAt": "2020-11-23T11:38:15Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java", "diffHunk": "@@ -132,4 +139,109 @@ public static double classificationFidelity(List<Pair<Saliency, Prediction>> pai\n         }\n         return evals == 0 ? 0 : acc / evals;\n     }\n+\n+    /**\n+     * Evaluate stability of a local explainer generating {@code Saliencies}.\n+     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n+     * positive/negative features always the same for a single prediction?\".\n+     *\n+     * @param model                  a model to explain\n+     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param saliencyLocalExplainer a local saliency explainer\n+     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n+     */\n+    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc0NDc3NA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528744774", "bodyText": "being a metric, I supposed it would have made sense to keep it public, however at this stage it's not \"used\" by other components, so we can do both.", "author": "tteofili", "createdAt": "2020-11-23T14:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODY0MTAzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODgwNTg1MQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528805851", "bodyText": "np, if it was intented to be a public api let's keep it as it is. Just wanted to be sure it was not just for our internal testing", "author": "r00ta", "createdAt": "2020-11-23T15:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODY0MTAzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex 5e1b65312..88f3342eb 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -145,15 +145,15 @@ public class ExplainabilityMetrics {\n      * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n      * positive/negative features always the same for a single prediction?\".\n      *\n-     * @param model                  a model to explain\n-     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param model a model to explain\n+     * @param prediction the prediction on which explanation stability will be evaluated\n      * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @param topK no. of top k positive/negative features for which stability report will be generated\n      * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n      */\n     public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-                                                                   int topK, int runs)\n+            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+            int topK, int runs)\n             throws InterruptedException, ExecutionException, TimeoutException {\n         Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n \n", "next_change": {"commit": "e8e67abb1b0fc9e60f928aecd4fe3dc2b4dd9425", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex 88f3342eb..07a49063c 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -160,10 +167,9 @@ public class ExplainabilityMetrics {\n         LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n         // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n         for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n+            String decision = entry.getKey();\n+            List<Saliency> perDecisionSaliencies = entry.getValue();\n             for (int k = 1; k <= topK; k++) {\n-                String decision = entry.getKey();\n-                List<Saliency> perDecisionSaliencies = entry.getValue();\n-\n                 int finalK = k;\n                 // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n                 Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\ndeleted file mode 100644\nindex 07a49063c..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ /dev/null\n", "chunk": "@@ -1,440 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.LocalExplainer;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureImportance;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * Utility class providing different methods to evaluate explainability.\n- */\n-public class ExplainabilityMetrics {\n-\n-    private static final Logger LOGGER = LoggerFactory.getLogger(ExplainabilityMetrics.class);\n-\n-    /**\n-     * Drop in confidence score threshold for impact score calculation.\n-     * Confidence scores below {@code originalScore * CONFIDENCE_DROP_RATIO} are considered impactful for a model.\n-     */\n-    private static final double CONFIDENCE_DROP_RATIO = 0.2d;\n-\n-    private ExplainabilityMetrics() {\n-    }\n-\n-    /**\n-     * Measure the explainability of an explanation.\n-     * See paper: \"Towards Quantification of Explainability in Explainable Artificial Intelligence Methods\" by Islam et al.\n-     *\n-     * @param inputCognitiveChunks the no. of cognitive chunks (pieces of information) required to generate the\n-     *        explanation (e.g. the no. of explanation inputs)\n-     * @param outputCognitiveChunks the no. of cognitive chunks generated within the explanation itself\n-     * @param interactionRatio the ratio of interaction (between 0 and 1) required by the explanation\n-     * @return the quantitative explainability measure\n-     */\n-    public static double quantifyExplainability(int inputCognitiveChunks, int outputCognitiveChunks, double interactionRatio) {\n-        return inputCognitiveChunks + outputCognitiveChunks > 0 ? 0.333 / (double) inputCognitiveChunks\n-                + 0.333 / (double) outputCognitiveChunks + 0.333 * (1d - interactionRatio) : 0;\n-    }\n-\n-    /**\n-     * Calculate the impact of dropping the most important features (given by {@link Saliency#getTopFeatures(int)} from the input.\n-     * Highly important features would have rather high impact.\n-     * See paper: Qiu Lin, Zhong, et al. \"Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the\n-     * Performance of Explainability Algorithms.\" 2019.\n-     *\n-     * @param model the model to be explained\n-     * @param prediction a prediction\n-     * @param topFeatures the list of important features that should be dropped\n-     * @return the saliency impact\n-     */\n-    public static double impactScore(PredictionProvider model, Prediction prediction, List<FeatureImportance> topFeatures) throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Feature> copy = List.copyOf(prediction.getInput().getFeatures());\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            copy = DataUtils.dropFeature(copy, featureImportance.getFeature());\n-        }\n-\n-        PredictionInput predictionInput = new PredictionInput(copy);\n-        List<PredictionOutput> predictionOutputs;\n-        try {\n-            predictionOutputs = model.predictAsync(List.of(predictionInput))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        } catch (ExecutionException | TimeoutException e) {\n-            LOGGER.error(\"Impossible to obtain prediction {}\", e.getMessage());\n-            throw new IllegalStateException(\"Impossible to obtain prediction\", e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new IllegalStateException(\"Impossible to obtain prediction (Thread interrupted)\", e);\n-        }\n-        double impact = 0d;\n-        for (PredictionOutput predictionOutput : predictionOutputs) {\n-            double size = predictionOutput.getOutputs().size();\n-            for (int i = 0; i < size; i++) {\n-                Output original = prediction.getOutput().getOutputs().get(i);\n-                Output modified = predictionOutput.getOutputs().get(i);\n-                impact += (!original.getValue().asString().equals(modified.getValue().asString())\n-                        || modified.getScore() < original.getScore() * CONFIDENCE_DROP_RATIO) ? 1d / size : 0d;\n-            }\n-        }\n-        return impact;\n-    }\n-\n-    /**\n-     * Calculate fidelity (accuracy) of boolean classification outputs using saliency predictor function = sign(sum(saliency.scores))\n-     * See papers:\n-     * - Guidotti Riccardo, et al. \"A survey of methods for explaining black box models.\" ACM computing surveys (2018).\n-     * - Bodria, Francesco, et al. \"Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis (Discussion Paper).\"\n-     *\n-     * @param pairs pairs composed by the saliency and the related prediction\n-     * @return the fidelity accuracy\n-     */\n-    public static double classificationFidelity(List<Pair<Saliency, Prediction>> pairs) {\n-        double acc = 0;\n-        double evals = 0;\n-        for (Pair<Saliency, Prediction> pair : pairs) {\n-            Saliency saliency = pair.getLeft();\n-            Prediction prediction = pair.getRight();\n-            for (Output output : prediction.getOutput().getOutputs()) {\n-                Type type = output.getType();\n-                if (Type.BOOLEAN.equals(type)) {\n-                    double predictorOutput = saliency.getPerFeatureImportance().stream().map(FeatureImportance::getScore).mapToDouble(d -> d).sum();\n-                    double v = output.getValue().asNumber();\n-                    if ((v >= 0 && predictorOutput >= 0) || (v < 0 && predictorOutput < 0)) {\n-                        acc++;\n-                    }\n-                    evals++;\n-                }\n-            }\n-        }\n-        return evals == 0 ? 0 : acc / evals;\n-    }\n-\n-    /**\n-     * Evaluate stability of a local explainer generating {@code Saliencies}.\n-     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n-     * positive/negative features always the same for a single prediction?\".\n-     *\n-     * @param model a model to explain\n-     * @param prediction the prediction on which explanation stability will be evaluated\n-     * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK no. of top k positive/negative features for which stability report will be generated\n-     * @param runs no. of times the saliency for each prediction needs to be generated\n-     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n-     */\n-    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int topK, int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n-\n-        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n-        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n-        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n-            String decision = entry.getKey();\n-            List<Saliency> perDecisionSaliencies = entry.getValue();\n-            for (int k = 1; k <= topK; k++) {\n-                int finalK = k;\n-                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n-                // get the most frequent list of positive features\n-                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n-                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n-                // get the most frequent list of negative features\n-                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n-                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // decision stability at k\n-                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n-                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n-                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n-            }\n-        }\n-        return saliencyStability;\n-    }\n-\n-    /**\n-     * Get multiple saliencies, aggregated by decision name.\n-     *\n-     * @param model the model used to perform predictions\n-     * @param prediction the prediction to explain\n-     * @param saliencyLocalExplainer a local explainer that generates saliences\n-     * @param runs the no. of explanations to be generated\n-     * @return the generated saliencies, aggregated by decision name, across the different runs\n-     */\n-    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n-            LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-            int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = new HashMap<>();\n-        int skipped = 0;\n-        for (int i = 0; i < runs; i++) {\n-            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n-                // aggregate saliencies by output name\n-                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n-                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n-                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n-                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n-                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n-                        updatedSaliencies.add(saliencyEntry.getValue());\n-                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n-                    } else {\n-                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n-                    }\n-                } else {\n-                    LOGGER.debug(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n-                    skipped++;\n-                }\n-            }\n-        }\n-        LOGGER.debug(\"skipped {} useless saliencies\", skipped);\n-        return saliencies;\n-    }\n-\n-    private static Map<List<String>, Long> getTopKFeaturesFrequency(List<Saliency> saliencies, Function<Saliency, List<FeatureImportance>> saliencyListFunction) {\n-        return saliencies.stream().map(saliencyListFunction)\n-                .map(l -> l.stream().map(f -> f.getFeature().getName())\n-                        .collect(Collectors.toList()))\n-                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));\n-    }\n-\n-    private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n-        Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n-        return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n-    }\n-\n-    /**\n-     * Evaluate the recall of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the highest score for the given decision and pair them with predictions\n-     * whose outputs have the lowest score for the same decision.\n-     * Get the top k (most important) features (according to the saliency) for the most important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with low score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output on the masked input is equals to the output for the\n-     * input the mask features were take from, that's considered a true positive, otherwise it's a false positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency recall\n-     */\n-    public static double getLocalSaliencyRecall(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-\n-        // get all samples from the data distribution\n-        List<Prediction> sorted = DataUtils.getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falseNegatives = 0;\n-        int currentChunk = 0;\n-        // for each of the top scored predictions, get the top influencing features and copy them over a low scored\n-        // input, then feed the model with this masked input and check the output is equals to the top scored one.\n-        for (Prediction prediction : topChunk) {\n-            Optional<Output> optionalOutput = prediction.getOutput().getByName(outputName);\n-            if (optionalOutput.isPresent()) {\n-                Output output = optionalOutput.get();\n-                Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (stringSaliencyMap.containsKey(outputName)) {\n-                    Saliency saliency = stringSaliencyMap.get(outputName);\n-                    List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                            .sorted((f1, f2) -> Double.compare(f2.getScore(), f1.getScore())).limit(k).collect(Collectors.toList());\n-\n-                    PredictionInput input = bottomChunk.get(currentChunk).getInput();\n-                    PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                    List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                            .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                    if (!predictionOutputList.isEmpty()) {\n-                        PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                        Optional<Output> optionalNewOutput = predictionOutput.getByName(outputName);\n-                        if (optionalNewOutput.isPresent()) {\n-                            Output newOutput = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falseNegatives++;\n-                            }\n-                        }\n-                    }\n-                    currentChunk++;\n-                }\n-            }\n-        }\n-        if ((truePositives + falseNegatives) > 0) {\n-            return truePositives / (truePositives + falseNegatives);\n-        } else {\n-            // if topChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput maskInput(List<FeatureImportance> topFeatures, PredictionInput input) {\n-        List<Feature> importantFeatures = new ArrayList<>();\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            importantFeatures.add(featureImportance.getFeature());\n-        }\n-\n-        return replaceAllFeatures(importantFeatures, input);\n-    }\n-\n-    /**\n-     * Evaluate the precision of a local saliency explainer on a given model.\n-     * Get the predictions having outputs with the lowest score for the given decision and pair them with predictions\n-     * whose outputs have the highest score for the same decision.\n-     * Get the bottom k (less important) features (according to the saliency) for the less important outputs and\n-     * \"paste\" them on each paired input corresponding to an output with high score (for the target decision).\n-     * Perform prediction on the \"masked\" input, if the output changes that's considered a false negative, otherwise\n-     * it's a true positive.\n-     * see Section 3.2.1 of https://openreview.net/attachment?id=B1xBAA4FwH&name=original_pdf\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency precision\n-     */\n-    public static double getLocalSaliencyPrecision(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Prediction> sorted = DataUtils.getScoreSortedPredictions(outputName, predictionProvider, dataDistribution);\n-\n-        // get the top and bottom 'chunkSize' predictions\n-        List<Prediction> topChunk = new ArrayList<>(sorted.subList(0, chunkSize));\n-        List<Prediction> bottomChunk = new ArrayList<>(sorted.subList(sorted.size() - chunkSize, sorted.size()));\n-\n-        double truePositives = 0;\n-        double falsePositives = 0;\n-        int currentChunk = 0;\n-\n-        for (Prediction prediction : bottomChunk) {\n-            Map<String, Saliency> stringSaliencyMap = localExplainer.explainAsync(prediction, predictionProvider)\n-                    .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-            if (stringSaliencyMap.containsKey(outputName)) {\n-                Saliency saliency = stringSaliencyMap.get(outputName);\n-                List<FeatureImportance> topFeatures = saliency.getPerFeatureImportance().stream()\n-                        .sorted(Comparator.comparingDouble(FeatureImportance::getScore)).limit(k).collect(Collectors.toList());\n-\n-                Prediction topPrediction = topChunk.get(currentChunk);\n-                PredictionInput input = topPrediction.getInput();\n-                PredictionInput maskedInput = maskInput(topFeatures, input);\n-\n-                List<PredictionOutput> predictionOutputList = predictionProvider.predictAsync(List.of(maskedInput))\n-                        .get(Config.DEFAULT_ASYNC_TIMEOUT, Config.DEFAULT_ASYNC_TIMEUNIT);\n-                if (!predictionOutputList.isEmpty()) {\n-                    PredictionOutput predictionOutput = predictionOutputList.get(0);\n-                    Optional<Output> newOptionalOutput = predictionOutput.getByName(outputName);\n-                    if (newOptionalOutput.isPresent()) {\n-                        Output newOutput = newOptionalOutput.get();\n-                        Optional<Output> optionalOutput = topPrediction.getOutput().getByName(outputName);\n-                        if (optionalOutput.isPresent()) {\n-                            Output output = optionalOutput.get();\n-                            if (output.getValue().equals(newOutput.getValue())) {\n-                                truePositives++;\n-                            } else {\n-                                falsePositives++;\n-                            }\n-                        }\n-                    }\n-                }\n-                currentChunk++;\n-            }\n-        }\n-        if ((truePositives + falsePositives) > 0) {\n-            return truePositives / (truePositives + falsePositives);\n-        } else {\n-            // if bottomChunk is empty or the target output (by name) is not an output of the model.\n-            return Double.NaN;\n-        }\n-    }\n-\n-    /**\n-     * Get local saliency F1 score.\n-     *\n-     * see <a href=\"https://en.wikipedia.org/wiki/F-score\"/>\n-     * See {@link #getLocalSaliencyPrecision(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     * See {@link #getLocalSaliencyRecall(String, PredictionProvider, LocalExplainer, DataDistribution, int, int)}\n-     *\n-     * @param outputName decision to evaluate recall for\n-     * @param predictionProvider the prediction provider to test\n-     * @param localExplainer the explainer to evaluate\n-     * @param dataDistribution the data distribution used to obtain inputs for evaluation\n-     * @param k the no. of features to extract\n-     * @param chunkSize the size of the chunk of predictions to use for evaluation\n-     * @return the saliency F1\n-     */\n-    public static double getLocalSaliencyF1(String outputName, PredictionProvider predictionProvider,\n-            LocalExplainer<Map<String, Saliency>> localExplainer,\n-            DataDistribution dataDistribution, int k, int chunkSize)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        double precision = getLocalSaliencyPrecision(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        double recall = getLocalSaliencyRecall(outputName, predictionProvider, localExplainer, dataDistribution, k, chunkSize);\n-        if (Double.isFinite(precision + recall) && (precision + recall) > 0) {\n-            return 2 * precision * recall / (precision + recall);\n-        } else {\n-            return Double.NaN;\n-        }\n-    }\n-\n-    private static PredictionInput replaceAllFeatures(List<Feature> importantFeatures, PredictionInput input) {\n-        List<Feature> features = List.copyOf(input.getFeatures());\n-        for (Feature f : importantFeatures) {\n-            features = DataUtils.replaceFeatures(f, features);\n-        }\n-        return new PredictionInput(features);\n-    }\n-\n-}\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4Njk4OQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528786989", "body": "I think it would be good to use assertj also here. It provides much better messages when test fails.", "bodyText": "I think it would be good to use assertj also here. It provides much better messages when test fails.", "bodyHTML": "<p dir=\"auto\">I think it would be good to use assertj also here. It provides much better messages when test fails.</p>", "author": "jiripetrlik", "createdAt": "2020-11-23T15:29:29Z", "path": "explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java", "diffHunk": "@@ -86,17 +89,30 @@ void testPMMLRegression() throws Exception {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertNotNull(predictionOutputs);", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f30e10482c63ec0e028d6d7b3167cddcc1059535", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex df7983a8a..7f6be8749 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -89,30 +91,17 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertNotNull(predictionOutputs);\n-            assertFalse(predictionOutputs.isEmpty());\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertNotNull(output);\n+            PredictionOutput output = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n+                    .get(0);\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n                 assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n                 assertEquals(1d, v);\n             }\n-            int topK = 1;\n-            LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n-            for (int i = 1; i <= topK; i++) {\n-                for (String decision : stability.getDecisions()) {\n-                    double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n-                    double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n-                    assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                    assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                }\n-            }\n         }\n     }\n \n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex 7f6be8749..555059238 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -91,17 +93,22 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertThat(predictionOutputs).isNotNull();\n+            assertThat(predictionOutputs).isNotEmpty();\n+            PredictionOutput output = predictionOutputs.get(0);\n+            assertThat(output).isNotNull();\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n-                assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n-                assertEquals(1d, v);\n+                assertThat(saliency).isNotNull();\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                assertThat(v).isEqualTo(1d);\n             }\n+            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n+                                                                                    0.5, 0.5));\n         }\n     }\n \n", "next_change": {"commit": "cd5adebe96ccc6779dae7fce94d0e9abb8230e69", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\ndeleted file mode 100644\nindex 555059238..000000000\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ /dev/null\n", "chunk": "@@ -1,259 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.explainability.integrationtests.pmml;\n-\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.CompletableFuture;\n-\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Test;\n-import org.kie.api.pmml.PMML4Result;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.lime.LimeConfig;\n-import org.kie.kogito.explainability.local.lime.LimeExplainer;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n-import org.kie.kogito.explainability.utils.ValidationUtils;\n-import org.kie.pmml.api.runtime.PMMLRuntime;\n-import org.kie.kogito.explainability.utils.LocalSaliencyStability;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.kie.kogito.explainability.explainability.integrationtests.pmml.AbstractPMMLTest.getPMMLRuntime;\n-import static org.kie.test.util.filesystem.FileUtils.getFile;\n-\n-class PmmlLimeExplainerTest {\n-\n-    private static PMMLRuntime logisticRegressionIrisRuntime;\n-    private static PMMLRuntime categoricalVariableRegressionRuntime;\n-    private static PMMLRuntime scorecardCategoricalRuntime;\n-    private static PMMLRuntime compoundScoreCardRuntime;\n-\n-    @BeforeAll\n-    static void setUpBefore() {\n-        logisticRegressionIrisRuntime = getPMMLRuntime(getFile(\"logisticRegressionIrisData.pmml\"));\n-        categoricalVariableRegressionRuntime = getPMMLRuntime(getFile(\"categoricalVariablesRegression.pmml\"));\n-        scorecardCategoricalRuntime = getPMMLRuntime(getFile(\"SimpleScorecardCategorical.pmml\"));\n-        compoundScoreCardRuntime = getPMMLRuntime(getFile(\"CompoundNestedPredicateScorecard.pmml\"));\n-    }\n-\n-    @Test\n-    void testPMMLRegression() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(1000)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalLength\", 6.9));\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalWidth\", 3.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalLength\", 5.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalWidth\", 2.3));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    LogisticRegressionIrisDataExecutor pmmlModel = new LogisticRegressionIrisDataExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asNumber(),\n-                            features1.get(2).getValue().asNumber(), features1.get(3).getValue().asNumber());\n-                    PMML4Result result = pmmlModel.execute(logisticRegressionIrisRuntime);\n-                    String species = result.getResultVariables().get(\"Species\").toString();\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"species\", Type.TEXT, new Value<>(species), 1d)));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-\n-    @Test\n-    void testPMMLRegressionCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapX\", \"red\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapY\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(500)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                CategoricalVariablesRegressionExecutor pmmlModel = new CategoricalVariablesRegressionExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(categoricalVariableRegressionRuntime);\n-                String score = result.getResultVariables().get(\"result\").toString();\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"result\", Type.NUMBER, new Value<>(score), 1d)));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isEqualTo(1d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLScorecardCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"input1\", \"classA\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"input2\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(300)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                SimpleScorecardCategoricalExecutor pmmlModel = new SimpleScorecardCategoricalExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(scorecardCategoricalRuntime);\n-                String score = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.TARGET_FIELD);\n-                String reason1 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE1_FIELD);\n-                String reason2 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE2_FIELD);\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                        new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                        new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d),\n-                        new Output(\"reason2\", Type.TEXT, new Value<>(reason2), 1d)\n-                ));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isGreaterThan(0d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLCompoundScorecard() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(300)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"input1\", -50));\n-            features.add(FeatureFactory.newTextFeature(\"input2\", \"classB\"));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    CompoundNestedPredicateScorecardExecutor pmmlModel = new CompoundNestedPredicateScorecardExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asString());\n-                    PMML4Result result = pmmlModel.execute(compoundScoreCardRuntime);\n-                    String score = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.TARGET_FIELD);\n-                    String reason1 = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.REASON_CODE1_FIELD);\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                            new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                            new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d)\n-                    ));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4NzI4MQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528787281", "body": "Please use assertj.", "bodyText": "Please use assertj.", "bodyHTML": "<p dir=\"auto\">Please use assertj.</p>", "author": "jiripetrlik", "createdAt": "2020-11-23T15:29:47Z", "path": "explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java", "diffHunk": "@@ -86,17 +89,30 @@ void testPMMLRegression() throws Exception {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertNotNull(predictionOutputs);\n+            assertFalse(predictionOutputs.isEmpty());\n+            PredictionOutput output = predictionOutputs.get(0);\n+            assertNotNull(output);\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n                 assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n                 assertEquals(1d, v);", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f30e10482c63ec0e028d6d7b3167cddcc1059535", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex df7983a8a..7f6be8749 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -89,30 +91,17 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertNotNull(predictionOutputs);\n-            assertFalse(predictionOutputs.isEmpty());\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertNotNull(output);\n+            PredictionOutput output = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n+                    .get(0);\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n                 assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n                 assertEquals(1d, v);\n             }\n-            int topK = 1;\n-            LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n-            for (int i = 1; i <= topK; i++) {\n-                for (String decision : stability.getDecisions()) {\n-                    double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n-                    double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n-                    assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                    assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                }\n-            }\n         }\n     }\n \n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex 7f6be8749..555059238 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -91,17 +93,22 @@ class PmmlLimeExplainerTest {\n                 }\n                 return outputs;\n             });\n-            PredictionOutput output = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                    .get(0);\n+            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            assertThat(predictionOutputs).isNotNull();\n+            assertThat(predictionOutputs).isNotEmpty();\n+            PredictionOutput output = predictionOutputs.get(0);\n+            assertThat(output).isNotNull();\n             Prediction prediction = new Prediction(input, output);\n             Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                     .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n             for (Saliency saliency : saliencyMap.values()) {\n-                assertNotNull(saliency);\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getPositiveFeatures(2));\n-                assertEquals(1d, v);\n+                assertThat(saliency).isNotNull();\n+                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+                assertThat(v).isEqualTo(1d);\n             }\n+            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n+                                                                                    0.5, 0.5));\n         }\n     }\n \n", "next_change": {"commit": "cd5adebe96ccc6779dae7fce94d0e9abb8230e69", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\ndeleted file mode 100644\nindex 555059238..000000000\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ /dev/null\n", "chunk": "@@ -1,259 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.explainability.integrationtests.pmml;\n-\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.CompletableFuture;\n-\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Test;\n-import org.kie.api.pmml.PMML4Result;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.lime.LimeConfig;\n-import org.kie.kogito.explainability.local.lime.LimeExplainer;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n-import org.kie.kogito.explainability.utils.ValidationUtils;\n-import org.kie.pmml.api.runtime.PMMLRuntime;\n-import org.kie.kogito.explainability.utils.LocalSaliencyStability;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.kie.kogito.explainability.explainability.integrationtests.pmml.AbstractPMMLTest.getPMMLRuntime;\n-import static org.kie.test.util.filesystem.FileUtils.getFile;\n-\n-class PmmlLimeExplainerTest {\n-\n-    private static PMMLRuntime logisticRegressionIrisRuntime;\n-    private static PMMLRuntime categoricalVariableRegressionRuntime;\n-    private static PMMLRuntime scorecardCategoricalRuntime;\n-    private static PMMLRuntime compoundScoreCardRuntime;\n-\n-    @BeforeAll\n-    static void setUpBefore() {\n-        logisticRegressionIrisRuntime = getPMMLRuntime(getFile(\"logisticRegressionIrisData.pmml\"));\n-        categoricalVariableRegressionRuntime = getPMMLRuntime(getFile(\"categoricalVariablesRegression.pmml\"));\n-        scorecardCategoricalRuntime = getPMMLRuntime(getFile(\"SimpleScorecardCategorical.pmml\"));\n-        compoundScoreCardRuntime = getPMMLRuntime(getFile(\"CompoundNestedPredicateScorecard.pmml\"));\n-    }\n-\n-    @Test\n-    void testPMMLRegression() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(1000)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalLength\", 6.9));\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalWidth\", 3.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalLength\", 5.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalWidth\", 2.3));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    LogisticRegressionIrisDataExecutor pmmlModel = new LogisticRegressionIrisDataExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asNumber(),\n-                            features1.get(2).getValue().asNumber(), features1.get(3).getValue().asNumber());\n-                    PMML4Result result = pmmlModel.execute(logisticRegressionIrisRuntime);\n-                    String species = result.getResultVariables().get(\"Species\").toString();\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"species\", Type.TEXT, new Value<>(species), 1d)));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-\n-    @Test\n-    void testPMMLRegressionCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapX\", \"red\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapY\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(500)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                CategoricalVariablesRegressionExecutor pmmlModel = new CategoricalVariablesRegressionExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(categoricalVariableRegressionRuntime);\n-                String score = result.getResultVariables().get(\"result\").toString();\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"result\", Type.NUMBER, new Value<>(score), 1d)));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isEqualTo(1d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLScorecardCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"input1\", \"classA\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"input2\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(300)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                SimpleScorecardCategoricalExecutor pmmlModel = new SimpleScorecardCategoricalExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(scorecardCategoricalRuntime);\n-                String score = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.TARGET_FIELD);\n-                String reason1 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE1_FIELD);\n-                String reason2 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE2_FIELD);\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                        new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                        new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d),\n-                        new Output(\"reason2\", Type.TEXT, new Value<>(reason2), 1d)\n-                ));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isGreaterThan(0d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLCompoundScorecard() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(300)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"input1\", -50));\n-            features.add(FeatureFactory.newTextFeature(\"input2\", \"classB\"));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    CompoundNestedPredicateScorecardExecutor pmmlModel = new CompoundNestedPredicateScorecardExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asString());\n-                    PMML4Result result = pmmlModel.execute(compoundScoreCardRuntime);\n-                    String score = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.TARGET_FIELD);\n-                    String reason1 = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.REASON_CODE1_FIELD);\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                            new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                            new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d)\n-                    ));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4NzcxNw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r528787717", "body": "Please use assertj also here.", "bodyText": "Please use assertj also here.", "bodyHTML": "<p dir=\"auto\">Please use assertj also here.</p>", "author": "jiripetrlik", "createdAt": "2020-11-23T15:30:18Z", "path": "explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java", "diffHunk": "@@ -294,6 +294,27 @@ void testEncode(Type type) {\n         }\n     }\n \n+    @Test\n+    void testEncodeNumericSymmetric() {\n+        for (int seed = 0; seed < 5; seed++) {\n+            Random random = new Random();\n+            random.setSeed(seed);\n+            PerturbationContext perturbationContext = new PerturbationContext(random, random.nextInt());\n+            Value<?> target = Type.NUMBER.randomValue(perturbationContext);\n+            Value<?>[] values = new Value<?>[6];\n+            for (int i = 0; i < values.length / 2; i++) {\n+                values[i] = new Value<>(target.asNumber() + target.asNumber() * (1 + i) / 100d);\n+                values[values.length - 1 - i] = new Value<>(target.asNumber() - target.asNumber() * (1 + i) / 100d);\n+            }\n+            List<double[]> vectors = Type.NUMBER.encode(target, values);\n+            assertNotNull(vectors);\n+            assertEquals(values.length, vectors.size());\n+            for (int i = 0; i < vectors.size() / 2; i++) {\n+                assertEquals(vectors.get(i)[0], vectors.get(vectors.size() - 1 - i)[0]);", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "09e70298a40832bb8dfdbdb9df72d1b21edb0eb6", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex 12a890fb6..da1be83d6 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -294,27 +309,6 @@ class TypeTest {\n         }\n     }\n \n-    @Test\n-    void testEncodeNumericSymmetric() {\n-        for (int seed = 0; seed < 5; seed++) {\n-            Random random = new Random();\n-            random.setSeed(seed);\n-            PerturbationContext perturbationContext = new PerturbationContext(random, random.nextInt());\n-            Value<?> target = Type.NUMBER.randomValue(perturbationContext);\n-            Value<?>[] values = new Value<?>[6];\n-            for (int i = 0; i < values.length / 2; i++) {\n-                values[i] = new Value<>(target.asNumber() + target.asNumber() * (1 + i) / 100d);\n-                values[values.length - 1 - i] = new Value<>(target.asNumber() - target.asNumber() * (1 + i) / 100d);\n-            }\n-            List<double[]> vectors = Type.NUMBER.encode(target, values);\n-            assertNotNull(vectors);\n-            assertEquals(values.length, vectors.size());\n-            for (int i = 0; i < vectors.size() / 2; i++) {\n-                assertEquals(vectors.get(i)[0], vectors.get(vectors.size() - 1 - i)[0]);\n-            }\n-        }\n-    }\n-\n     @ParameterizedTest\n     @EnumSource\n     void testRandomValue(Type type) {\n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex da1be83d6..b564a70e4 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -309,6 +309,43 @@ class TypeTest {\n         }\n     }\n \n+    @Test\n+    void testEncodeNumericSymmetric() {\n+        for (int seed = 0; seed < 5; seed++) {\n+            Random random = new Random();\n+            random.setSeed(seed);\n+            PerturbationContext perturbationContext = new PerturbationContext(random, random.nextInt());\n+            Value<?> target = Type.NUMBER.randomValue(perturbationContext);\n+            Value<?>[] values = new Value<?>[6];\n+            for (int i = 0; i < values.length / 2; i++) {\n+                values[i] = new Value<>(target.asNumber() + target.asNumber() * (1 + i) / 100d);\n+                values[values.length - 1 - i] = new Value<>(target.asNumber() - target.asNumber() * (1 + i) / 100d);\n+            }\n+            List<double[]> vectors = Type.NUMBER.encode(target, values);\n+            assertNotNull(vectors);\n+            assertEquals(values.length, vectors.size());\n+            for (int i = 0; i < vectors.size() / 2; i++) {\n+                assertThat(vectors.get(i)[0]).isEqualTo(vectors.get(vectors.size() - 1 - i)[0]);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    void testEncodeNaN() {\n+        Random random = new Random();\n+        random.setSeed(4);\n+        PerturbationContext perturbationContext = new PerturbationContext(random, 1);\n+        Value<?> target = Type.NUMBER.randomValue(perturbationContext);\n+        Value<?>[] values = new Value<?>[6];\n+        for (int i = 0; i < values.length - 1; i++) {\n+            values[i] = Type.NUMBER.randomValue(perturbationContext);\n+        }\n+        values[5] = new Value<>(Double.NaN);\n+        List<double[]> vectors = Type.NUMBER.encode(target, values);\n+        assertThat(vectors).isNotEmpty();\n+        assertThat(vectors).doesNotContain(new double[]{Double.NaN});\n+    }\n+\n     @ParameterizedTest\n     @EnumSource\n     void testRandomValue(Type type) {\n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex b564a70e4..897b7274b 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -341,7 +345,7 @@ class TypeTest {\n             values[i] = Type.NUMBER.randomValue(perturbationContext);\n         }\n         values[5] = new Value<>(Double.NaN);\n-        List<double[]> vectors = Type.NUMBER.encode(target, values);\n+        List<double[]> vectors = Type.NUMBER.encode(params, target, values);\n         assertThat(vectors).isNotEmpty();\n         assertThat(vectors).doesNotContain(new double[]{Double.NaN});\n     }\n", "next_change": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex 897b7274b..07ad0b410 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -347,7 +345,7 @@ class TypeTest {\n         values[5] = new Value<>(Double.NaN);\n         List<double[]> vectors = Type.NUMBER.encode(params, target, values);\n         assertThat(vectors).isNotEmpty();\n-        assertThat(vectors).doesNotContain(new double[]{Double.NaN});\n+        assertThat(vectors).doesNotContain(new double[] { Double.NaN });\n     }\n \n     @ParameterizedTest\n", "next_change": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex 07ad0b410..f5f7092ac 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -337,15 +320,16 @@ class TypeTest {\n         Random random = new Random();\n         random.setSeed(4);\n         PerturbationContext perturbationContext = new PerturbationContext(random, 1);\n-        Value<?> target = Type.NUMBER.randomValue(perturbationContext);\n-        Value<?>[] values = new Value<?>[6];\n+        Value target = Type.NUMBER.randomValue(perturbationContext);\n+        Value[] values = new Value[6];\n         for (int i = 0; i < values.length - 1; i++) {\n             values[i] = Type.NUMBER.randomValue(perturbationContext);\n         }\n-        values[5] = new Value<>(Double.NaN);\n+        values[5] = new Value(Double.NaN);\n         List<double[]> vectors = Type.NUMBER.encode(params, target, values);\n-        assertThat(vectors).isNotEmpty();\n-        assertThat(vectors).doesNotContain(new double[] { Double.NaN });\n+        assertThat(vectors)\n+                .isNotEmpty()\n+                .doesNotContain(new double[] { Double.NaN });\n     }\n \n     @ParameterizedTest\n", "next_change": {"commit": "c9090e26255fe85aded7cc9f4bbd6eb293f02605", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\nindex f5f7092ac..73278ee03 100644\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n", "chunk": "@@ -335,10 +330,9 @@ class TypeTest {\n     @ParameterizedTest\n     @EnumSource\n     void testRandomValue(Type type) {\n-        for (int seed = 0; seed < 5; seed++) {\n+        for (long seed = 0; seed < 5; seed++) {\n             Random random = new Random();\n-            random.setSeed(seed);\n-            PerturbationContext perturbationContext = new PerturbationContext(random, random.nextInt());\n+            PerturbationContext perturbationContext = new PerturbationContext(seed, random, random.nextInt());\n             Value value = type.randomValue(perturbationContext);\n             assertNotNull(value);\n             assertDoesNotThrow(() -> type.drop(value));\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java b/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\ndeleted file mode 100644\nindex 73278ee03..000000000\n--- a/explainability/explainability-core/src/test/java/org/kie/kogito/explainability/model/TypeTest.java\n+++ /dev/null\n", "chunk": "@@ -1,342 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.Charset;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.temporal.ChronoUnit;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Locale;\n-import java.util.Random;\n-\n-import org.junit.jupiter.api.Test;\n-import org.junit.jupiter.params.ParameterizedTest;\n-import org.junit.jupiter.params.provider.EnumSource;\n-import org.junit.jupiter.params.provider.ValueSource;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertNotEquals;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-\n-class TypeTest {\n-\n-    @Test\n-    void testPerturbNumericDouble() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(0.1);\n-        Feature f = new Feature(\"name\", Type.NUMBER, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbNumericInteger() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(1);\n-        Feature f = new Feature(\"name\", Type.NUMBER, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbSingleTermString() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(\"foo\");\n-        Feature f = new Feature(\"name\", Type.TEXT, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbMultiTermString() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(\"foo bar\");\n-        Feature f = new Feature(\"name\", Type.TEXT, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbCategorical() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(\"1\");\n-        Feature f = new Feature(\"name\", Type.CATEGORICAL, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbBinary() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        ByteBuffer bytes = ByteBuffer.allocate(16);\n-        bytes.put(\"foo\".getBytes(Charset.defaultCharset()));\n-        Value value = new Value(bytes);\n-        Feature f = new Feature(\"name\", Type.BINARY, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbBoolean() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(false);\n-        Feature f = new Feature(\"name\", Type.BOOLEAN, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbDuration() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(Duration.ofDays(10));\n-        Feature f = new Feature(\"name\", Type.DURATION, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbTime() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(LocalTime.of(10, 10));\n-        Feature f = new Feature(\"name\", Type.TIME, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = { \"http://localhost:8080\", \"http://128.0.0.1:8081\", \"http://localhost:8080/path#paragraph1\" })\n-    void testPerturbURI(String param) {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Value value = new Value(param);\n-        Feature f = new Feature(\"name\", Type.URI, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbVector() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        double[] doubles = new double[3];\n-        Arrays.fill(doubles, 1d);\n-        Value value = new Value(doubles);\n-        Feature f = new Feature(\"name\", Type.VECTOR, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbNestedFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Feature feature = new Feature(\"name\", Type.NUMBER, new Value(1d));\n-        Value value = new Value(feature);\n-        Feature f = new Feature(\"name\", Type.UNDEFINED, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbByteBufferFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        byte[] bytes = new byte[1024];\n-        perturbationContext.getRandom().nextBytes(bytes);\n-        ByteBuffer byteBuffer = ByteBuffer.allocate(1024);\n-        byteBuffer.put(bytes);\n-        Feature feature = new Feature(\"name\", Type.BINARY, new Value(byteBuffer));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbURIFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        URI uri = URI.create(\"https://www.redhat.com/en/technologies/jboss-middleware/process-automation-manager\");\n-        Feature feature = new Feature(\"name\", Type.URI, new Value(uri));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbTimeFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        LocalTime time = LocalTime.now();\n-        Feature feature = new Feature(\"name\", Type.TIME, new Value(time));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbDurationFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Duration time = Duration.of(2, ChronoUnit.DAYS);\n-        Feature feature = new Feature(\"name\", Type.DURATION, new Value(time));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testZeroCategory() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        String category = \"0\";\n-        Feature feature = new Feature(\"name\", Type.CATEGORICAL, new Value(category));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testNonZeroCategory() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        String category = \"1\";\n-        Feature feature = new Feature(\"name\", Type.CATEGORICAL, new Value(category));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbCurrencyFeature() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1);\n-        Currency currency = Currency.getInstance(Locale.ITALY);\n-        Feature feature = new Feature(\"name\", Type.CURRENCY, new Value(currency));\n-        Value perturbedValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-        assertNotEquals(feature.getValue(), perturbedValue);\n-    }\n-\n-    @ParameterizedTest\n-    @ValueSource(longs = { 0, 1, 2, 3, 4 })\n-    void testPerturbCompositeFeature(long seed) {\n-        Random random = new Random();\n-        PerturbationContext perturbationContext = new PerturbationContext(seed, random, 2);\n-        List<Feature> features = new LinkedList<>();\n-        features.add(new Feature(\"f1\", Type.TEXT, new Value(\"foo bar\")));\n-        features.add(new Feature(\"f2\", Type.NUMBER, new Value(1d)));\n-        features.add(new Feature(\"f3\", Type.BOOLEAN, new Value(true)));\n-        Value value = new Value(features);\n-        Feature f = new Feature(\"name\", Type.COMPOSITE, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @Test\n-    void testPerturbCompositeFeatureTooManyPerturbations() {\n-        PerturbationContext perturbationContext = new PerturbationContext(new Random(), 1000);\n-        List<Feature> features = new LinkedList<>();\n-        features.add(new Feature(\"f1\", Type.TEXT, new Value(\"foo bar\")));\n-        features.add(new Feature(\"f2\", Type.NUMBER, new Value(1d)));\n-        Value value = new Value(features);\n-        Feature f = new Feature(\"name\", Type.COMPOSITE, value);\n-        Value perturbedValue = f.getType().perturb(f.getValue(), perturbationContext);\n-        assertNotEquals(value, perturbedValue);\n-    }\n-\n-    @ParameterizedTest\n-    @EnumSource\n-    void testDrop(Type type) {\n-        Value v = new Value(1.0);\n-        Value dropped = type.drop(v);\n-        assertNotEquals(v, dropped);\n-    }\n-\n-    @ParameterizedTest\n-    @EnumSource\n-    void testPerturb(Type type) {\n-        for (long seed = 0; seed < 5; seed++) {\n-            Value v = new Value(1.0);\n-            Random random = new Random();\n-            PerturbationContext perturbationContext = new PerturbationContext(seed, random, 1);\n-            Value perturbed = type.perturb(v, perturbationContext);\n-            assertNotEquals(v, perturbed, type.name());\n-        }\n-    }\n-\n-    @ParameterizedTest\n-    @EnumSource\n-    void testEncode(Type type) {\n-        EncodingParams params = new EncodingParams(1, 0.1);\n-        for (long seed = 0; seed < 5; seed++) {\n-            Random random = new Random();\n-            PerturbationContext perturbationContext = new PerturbationContext(seed, random, random.nextInt());\n-            Value target = type.randomValue(perturbationContext);\n-            Value[] values = new Value[random.nextInt(10)];\n-            for (int i = 0; i < values.length; i++) {\n-                values[i] = type.randomValue(perturbationContext);\n-            }\n-            List<double[]> vectors = type.encode(params, target, values);\n-            assertNotNull(vectors);\n-            assertEquals(values.length, vectors.size());\n-            for (double[] vector : vectors) {\n-                assertThat(Arrays.stream(vector).min().orElse(-2)).isGreaterThanOrEqualTo(-1);\n-                assertThat(Arrays.stream(vector).max().orElse(2)).isLessThanOrEqualTo(1);\n-            }\n-        }\n-    }\n-\n-    @ParameterizedTest\n-    @ValueSource(longs = { 0, 1, 2, 3, 4 })\n-    void testEncodeNumericSymmetric(long seed) {\n-        Random random = new Random();\n-        EncodingParams params = new EncodingParams(1, 0.1);\n-        PerturbationContext perturbationContext = new PerturbationContext(seed, random, random.nextInt());\n-        Value target = Type.NUMBER.randomValue(perturbationContext);\n-        Value[] values = new Value[6];\n-        for (int i = 0; i < values.length / 2; i++) {\n-            values[i] = new Value(target.asNumber() + target.asNumber() * (1 + i) / 100d);\n-            values[values.length - 1 - i] = new Value(target.asNumber() - target.asNumber() * (1 + i) / 100d);\n-        }\n-        List<double[]> vectors = Type.NUMBER.encode(params, target, values);\n-        assertNotNull(vectors);\n-        assertEquals(values.length, vectors.size());\n-        for (int i = 0; i < vectors.size() / 2; i++) {\n-            assertThat(vectors.get(i)[0]).isEqualTo(vectors.get(vectors.size() - 1 - i)[0]);\n-        }\n-    }\n-\n-    @Test\n-    void testEncodeNaN() {\n-        EncodingParams params = new EncodingParams(1, 0.1);\n-        Random random = new Random();\n-        PerturbationContext perturbationContext = new PerturbationContext(4L, random, 1);\n-        Value target = Type.NUMBER.randomValue(perturbationContext);\n-        Value[] values = new Value[6];\n-        for (int i = 0; i < values.length - 1; i++) {\n-            values[i] = Type.NUMBER.randomValue(perturbationContext);\n-        }\n-        values[5] = new Value(Double.NaN);\n-        List<double[]> vectors = Type.NUMBER.encode(params, target, values);\n-        assertThat(vectors)\n-                .isNotEmpty()\n-                .doesNotContain(new double[] { Double.NaN });\n-    }\n-\n-    @ParameterizedTest\n-    @EnumSource\n-    void testRandomValue(Type type) {\n-        for (long seed = 0; seed < 5; seed++) {\n-            Random random = new Random();\n-            PerturbationContext perturbationContext = new PerturbationContext(seed, random, random.nextInt());\n-            Value value = type.randomValue(perturbationContext);\n-            assertNotNull(value);\n-            assertDoesNotThrow(() -> type.drop(value));\n-            assertDoesNotThrow(() -> type.perturb(value, perturbationContext));\n-        }\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}, {"oid": "77ce1f4010b6feb315a66ae99173d54472d3819a", "url": "https://github.com/kiegroup/kogito-apps/commit/77ce1f4010b6feb315a66ae99173d54472d3819a", "message": "KOGITO-3763 - simplified EM#getMostFrequent", "committedDate": "2020-11-24T07:20:18Z", "type": "commit"}, {"oid": "44d81236cdf89672b9823098bbd812620311a7e3", "url": "https://github.com/kiegroup/kogito-apps/commit/44d81236cdf89672b9823098bbd812620311a7e3", "message": "KOGITO-3763 - using assertJ in PmmlLimeExplainerTest", "committedDate": "2020-11-24T08:36:56Z", "type": "commit"}, {"oid": "f02ace0c1d64abb65a2299dfb0483e4ba34bae87", "url": "https://github.com/kiegroup/kogito-apps/commit/f02ace0c1d64abb65a2299dfb0483e4ba34bae87", "message": "KOGITO-3602 - using assertJ in TypeTest", "committedDate": "2020-11-24T08:38:30Z", "type": "commit"}, {"oid": "920cf37dd04455741bd1be6ab4c449a7cdfce38a", "url": "https://github.com/kiegroup/kogito-apps/commit/920cf37dd04455741bd1be6ab4c449a7cdfce38a", "message": "KOGITO-3763 - using assertJ in PmmlLimeExplainerTest", "committedDate": "2020-11-24T15:50:10Z", "type": "commit"}, {"oid": "9f24519754bc0dda092095eeeea00f49da91b75f", "url": "https://github.com/kiegroup/kogito-apps/commit/9f24519754bc0dda092095eeeea00f49da91b75f", "message": "Merge branch 'master' of github.com:kiegroup/kogito-apps into KOGITO-3763", "committedDate": "2020-12-02T15:42:52Z", "type": "commit"}, {"oid": "3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "url": "https://github.com/kiegroup/kogito-apps/commit/3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "message": "Merge branch 'master' of github.com:kiegroup/kogito-apps into KOGITO-3763", "committedDate": "2020-12-02T15:45:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDE1MQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r522444151", "body": "Is this based on some euristic/rule?", "bodyText": "Is this based on some euristic/rule?", "bodyHTML": "<p dir=\"auto\">Is this based on some euristic/rule?</p>", "author": "danielezonca", "createdAt": "2020-11-12T21:42:31Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java", "diffHunk": "@@ -27,7 +27,7 @@\n  */\n class SampleWeighter {\n \n-    private static final double SIGMA = 0.75;\n+    private static final double SIGMA = 0.675;", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4Mjc4MA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540982780", "bodyText": "this softens the proximity function \"strictness\", some of the changes in this PR are somewhat correlated as we have improved the locality of the generated samples (in Type.NUMBER), so that the algo benefits from having less strict proximity function (here the width controls how strict the similarity between two vectors is) and similarity threshold (the CLUSTER_THRESHOLD in Type.NUMBER).", "author": "tteofili", "createdAt": "2020-12-11T14:24:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDE1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "f30e10482c63ec0e028d6d7b3167cddcc1059535", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\nindex 61c416dcf..5691fd69b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n", "chunk": "@@ -17,9 +17,10 @@ package org.kie.kogito.explainability.local.lime;\n \n import java.util.Arrays;\n import java.util.Collection;\n+import java.util.List;\n \n import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.model.PredictionInput;\n+import org.kie.kogito.explainability.model.Feature;\n import org.kie.kogito.explainability.utils.DataUtils;\n \n /**\n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\nindex 5691fd69b..3d799f39e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n", "chunk": "@@ -28,7 +28,7 @@ import org.kie.kogito.explainability.utils.DataUtils;\n  */\n class SampleWeighter {\n \n-    private static final double SIGMA = 0.75;\n+    private static final double SIGMA = 0.675;\n \n     /**\n      * Obtain sample weights for a training set, given a list of target input features to compare with.\n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\nindex 3d799f39e..3cdbbc11a 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n", "chunk": "@@ -28,22 +28,22 @@ import org.kie.kogito.explainability.utils.DataUtils;\n  */\n class SampleWeighter {\n \n-    private static final double SIGMA = 0.675;\n-\n     /**\n      * Obtain sample weights for a training set, given a list of target input features to compare with.\n      * @param targetInputFeatures target input features\n      * @param training the (sparse) training set\n-     * @return a eeight for each sample in the training set\n+     * @param kernelWidth the width of the kernel used to calculate the proximity\n+     * @return a weight for each sample in the training set\n      */\n-    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training) {\n+    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training,\n+                                     double kernelWidth) {\n         int noOfFeatures = targetInputFeatures.size();\n         double[] x = new double[noOfFeatures];\n         Arrays.fill(x, 1);\n \n         return training.stream().map(Pair::getLeft)\n                 .map(d -> DataUtils.euclideanDistance(x, d)) // calculate euclidean distance between target and sample points\n-                .map(d -> DataUtils.exponentialSmoothingKernel(d, SIGMA * Math.sqrt(noOfFeatures))) // transform distance into proximity using an exponential smoothing kernel\n+                .map(d -> DataUtils.exponentialSmoothingKernel(d, kernelWidth)) // transform distance into proximity using an exponential smoothing kernel\n                 .mapToDouble(Double::doubleValue).toArray(); // output to an array\n     }\n }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\ndeleted file mode 100644\nindex 3cdbbc11a..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ /dev/null\n", "chunk": "@@ -1,49 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.local.lime;\n-\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.List;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Utility class to generate weights for the LIME encoded training set, given a prediction.\n- */\n-class SampleWeighter {\n-\n-    /**\n-     * Obtain sample weights for a training set, given a list of target input features to compare with.\n-     * @param targetInputFeatures target input features\n-     * @param training the (sparse) training set\n-     * @param kernelWidth the width of the kernel used to calculate the proximity\n-     * @return a weight for each sample in the training set\n-     */\n-    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training,\n-                                     double kernelWidth) {\n-        int noOfFeatures = targetInputFeatures.size();\n-        double[] x = new double[noOfFeatures];\n-        Arrays.fill(x, 1);\n-\n-        return training.stream().map(Pair::getLeft)\n-                .map(d -> DataUtils.euclideanDistance(x, d)) // calculate euclidean distance between target and sample points\n-                .map(d -> DataUtils.exponentialSmoothingKernel(d, kernelWidth)) // transform distance into proximity using an exponential smoothing kernel\n-                .mapToDouble(Double::doubleValue).toArray(); // output to an array\n-    }\n-}\n", "next_change": null}]}}]}}]}}, {"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\nindex 61c416dcf..5691fd69b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n", "chunk": "@@ -27,10 +28,16 @@ import org.kie.kogito.explainability.utils.DataUtils;\n  */\n class SampleWeighter {\n \n-    private static final double SIGMA = 0.675;\n+    private static final double SIGMA = 0.75;\n \n-    static double[] getSampleWeights(PredictionInput targetInput, Collection<Pair<double[], Double>> training) {\n-        int noOfFeatures = targetInput.getFeatures().size();\n+    /**\n+     * Obtain sample weights for a training set, given a list of target input features to compare with.\n+     * @param targetInputFeatures target input features\n+     * @param training the (sparse) training set\n+     * @return a eeight for each sample in the training set\n+     */\n+    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training) {\n+        int noOfFeatures = targetInputFeatures.size();\n         double[] x = new double[noOfFeatures];\n         Arrays.fill(x, 1);\n \n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\nindex 5691fd69b..3cdbbc11a 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n", "chunk": "@@ -28,22 +28,22 @@ import org.kie.kogito.explainability.utils.DataUtils;\n  */\n class SampleWeighter {\n \n-    private static final double SIGMA = 0.75;\n-\n     /**\n      * Obtain sample weights for a training set, given a list of target input features to compare with.\n      * @param targetInputFeatures target input features\n      * @param training the (sparse) training set\n-     * @return a eeight for each sample in the training set\n+     * @param kernelWidth the width of the kernel used to calculate the proximity\n+     * @return a weight for each sample in the training set\n      */\n-    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training) {\n+    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training,\n+                                     double kernelWidth) {\n         int noOfFeatures = targetInputFeatures.size();\n         double[] x = new double[noOfFeatures];\n         Arrays.fill(x, 1);\n \n         return training.stream().map(Pair::getLeft)\n                 .map(d -> DataUtils.euclideanDistance(x, d)) // calculate euclidean distance between target and sample points\n-                .map(d -> DataUtils.exponentialSmoothingKernel(d, SIGMA * Math.sqrt(noOfFeatures))) // transform distance into proximity using an exponential smoothing kernel\n+                .map(d -> DataUtils.exponentialSmoothingKernel(d, kernelWidth)) // transform distance into proximity using an exponential smoothing kernel\n                 .mapToDouble(Double::doubleValue).toArray(); // output to an array\n     }\n }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\ndeleted file mode 100644\nindex 3cdbbc11a..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/local/lime/SampleWeighter.java\n+++ /dev/null\n", "chunk": "@@ -1,49 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.local.lime;\n-\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.List;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Utility class to generate weights for the LIME encoded training set, given a prediction.\n- */\n-class SampleWeighter {\n-\n-    /**\n-     * Obtain sample weights for a training set, given a list of target input features to compare with.\n-     * @param targetInputFeatures target input features\n-     * @param training the (sparse) training set\n-     * @param kernelWidth the width of the kernel used to calculate the proximity\n-     * @return a weight for each sample in the training set\n-     */\n-    static double[] getSampleWeights(List<Feature> targetInputFeatures, Collection<Pair<double[], Double>> training,\n-                                     double kernelWidth) {\n-        int noOfFeatures = targetInputFeatures.size();\n-        double[] x = new double[noOfFeatures];\n-        Arrays.fill(x, 1);\n-\n-        return training.stream().map(Pair::getLeft)\n-                .map(d -> DataUtils.euclideanDistance(x, d)) // calculate euclidean distance between target and sample points\n-                .map(d -> DataUtils.exponentialSmoothingKernel(d, kernelWidth)) // transform distance into proximity using an exponential smoothing kernel\n-                .mapToDouble(Double::doubleValue).toArray(); // output to an array\n-    }\n-}\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM3MTYyMA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r525371620", "body": "Can you add a test to cover this branch?", "bodyText": "Can you add a test to cover this branch?", "bodyHTML": "<p dir=\"auto\">Can you add a test to cover this branch?</p>", "author": "danielezonca", "createdAt": "2020-11-17T17:59:06Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java", "diffHunk": "@@ -162,7 +162,7 @@ static Feature doubleToFeature(double d) {\n                 perturbationSize = lowerBound;\n             }\n             else if (upperBound > lowerBound) {\n-                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, upperBound).findFirst().orElse(1);\n+                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk1MTQ2MA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540951460", "bodyText": "ok", "author": "tteofili", "createdAt": "2020-12-11T13:36:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM3MTYyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAxNDc3Mw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r541014773", "bodyText": "actually this is already covered in DataUtils#testPerturbDropNumericTwo and DataUtils#testPerturbDropNumericThree.\nwe can't control the actual number being generated unless we induce the generation with a custom Random test-impl (e.g. the FakeRandom class we had).", "author": "tteofili", "createdAt": "2020-12-11T15:10:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM3MTYyMA=="}], "type": "inlineReview", "revised_code": {"commit": "b2e8228db5759f7e30ca6dea4555035a2db0cd72", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 1cd0206b4..40ec57cf9 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -162,7 +168,7 @@ public class DataUtils {\n                 perturbationSize = lowerBound;\n             }\n             else if (upperBound > lowerBound) {\n-                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);\n+                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, upperBound).findFirst().orElse(1);\n             }\n             if (perturbationSize > 0) {\n                 int[] indexesToBePerturbed = perturbationContext.getRandom().ints(0, newFeatures.size())\n", "next_change": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 40ec57cf9..86e751bdd 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -168,7 +168,7 @@ public class DataUtils {\n                 perturbationSize = lowerBound;\n             }\n             else if (upperBound > lowerBound) {\n-                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, upperBound).findFirst().orElse(1);\n+                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);\n             }\n             if (perturbationSize > 0) {\n                 int[] indexesToBePerturbed = perturbationContext.getRandom().ints(0, newFeatures.size())\n", "next_change": {"commit": "209385abf479e38b41a64d4ce5aed88e0bf0d6e7", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 86e751bdd..88b3fda95 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -166,8 +172,7 @@ public class DataUtils {\n             int perturbationSize = 0;\n             if (lowerBound == upperBound) {\n                 perturbationSize = lowerBound;\n-            }\n-            else if (upperBound > lowerBound) {\n+            } else if (upperBound > lowerBound) {\n                 perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);\n             }\n             if (perturbationSize > 0) {\n", "next_change": {"commit": "f79c6963e64d6d15a16c14145d7782d36c3a2402", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 88b3fda95..0c43e2b21 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -180,8 +203,14 @@ public class DataUtils {\n                         .distinct().limit(perturbationSize).toArray();\n                 for (int index : indexesToBePerturbed) {\n                     Feature feature = newFeatures.get(index);\n+                    Value newValue;\n+                    if (featureDistributionsMap.containsKey(feature.getName())) {\n+                        newValue = featureDistributionsMap.get(feature.getName()).sample();\n+                    } else {\n+                        newValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n+                    }\n                     Feature perturbedFeature =\n-                            FeatureFactory.copyOf(feature, feature.getType().perturb(feature.getValue(), perturbationContext));\n+                            FeatureFactory.copyOf(feature, newValue);\n                     newFeatures.set(index, perturbedFeature);\n                 }\n             }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\ndeleted file mode 100644\nindex 0c43e2b21..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ /dev/null\n", "chunk": "@@ -1,598 +0,0 @@\n-/*\n- * Copyright 2021 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.io.BufferedReader;\n-import java.io.IOException;\n-import java.io.Writer;\n-import java.nio.charset.MalformedInputException;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-import java.util.stream.IntStream;\n-\n-import org.apache.commons.csv.CSVFormat;\n-import org.apache.commons.csv.CSVPrinter;\n-import org.apache.commons.csv.CSVRecord;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureDistribution;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.IndependentFeaturesDataDistribution;\n-import org.kie.kogito.explainability.model.NumericFeatureDistribution;\n-import org.kie.kogito.explainability.model.PartialDependenceGraph;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionInputsDataDistribution;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-\n-/**\n- * Utility methods to handle and manipulate data.\n- */\n-public class DataUtils {\n-\n-    private DataUtils() {\n-    }\n-\n-    /**\n-     * Generate a dataset of a certain size, sampled from a normal distribution, given mean and standard deviation.\n-     * Samples are generated from a normal distribution, multiplied by {@code stdDeviation} and summed to {@code mean},\n-     * actual mean {@code m} and standard deviation {@code d} are calculated.\n-     * Then all numbers are multiplied by the same number so that the standard deviation also gets\n-     * multiplied by the same number, hence we multiply each random number by {@code stdDeviation / d}.\n-     * The resultant set has standard deviation {@code stdDeviation} and mean {@code m1=m*stdDeviation/d}.\n-     * If a same number is added to all values the mean also changes by the same number so we add {@code mean - m1} to\n-     * all numbers.\n-     *\n-     * @param mean desired mean\n-     * @param stdDeviation desired standard deviation\n-     * @param size size of the array\n-     * @return the generated data\n-     */\n-    public static double[] generateData(double mean, double stdDeviation, int size, Random random) {\n-\n-        // generate random data from a normal (gaussian) distribution\n-        double[] data = new double[size];\n-        for (int i = 0; i < size; i++) {\n-            data[i] = random.nextGaussian() * stdDeviation + mean;\n-        }\n-\n-        double generatedDataMean = getMean(data);\n-        double generatedDataStdDev = getStdDev(data, generatedDataMean);\n-\n-        // force desired standard deviation\n-        double newStdDeviation = generatedDataStdDev != 0 ? stdDeviation / generatedDataStdDev : stdDeviation; // avoid division by zero\n-        for (int i = 0; i < size; i++) {\n-            data[i] *= newStdDeviation;\n-        }\n-\n-        // get the new mean\n-        double newMean = generatedDataStdDev != 0 ? generatedDataMean * stdDeviation / generatedDataStdDev\n-                : generatedDataMean * stdDeviation;\n-\n-        // force desired mean\n-        for (int i = 0; i < size; i++) {\n-            data[i] += mean - newMean;\n-        }\n-\n-        return data;\n-    }\n-\n-    public static double getMean(double[] data) {\n-        double m = 0;\n-        for (double datum : data) {\n-            m += datum;\n-        }\n-        m = m / data.length;\n-        return m;\n-    }\n-\n-    public static double getStdDev(double[] data, double mean) {\n-        double d = 0;\n-        for (double datum : data) {\n-            d += Math.pow(datum - mean, 2);\n-        }\n-        d /= data.length;\n-        d = Math.sqrt(d);\n-        return d;\n-    }\n-\n-    /**\n-     * Generate equally {@code size} sampled values between {@code min} and {@code max}.\n-     *\n-     * @param min minimum value\n-     * @param max maximum value\n-     * @param size dataset size\n-     * @return the generated data\n-     */\n-    public static double[] generateSamples(double min, double max, int size) {\n-        double[] data = new double[size];\n-        double val = min;\n-        double sum = max / size;\n-        for (int i = 0; i < size; i++) {\n-            data[i] = val;\n-            val += sum;\n-        }\n-        return data;\n-    }\n-\n-    /**\n-     * Transform an array of double into a list of numerical features.\n-     *\n-     * @param inputs an array of double numbers\n-     * @return a list of numerical features\n-     */\n-    public static List<Feature> doublesToFeatures(double[] inputs) {\n-        return DoubleStream.of(inputs).mapToObj(DataUtils::doubleToFeature).collect(Collectors.toList());\n-    }\n-\n-    /**\n-     * Transform a double into a numerical feature.\n-     *\n-     * @param d the double value\n-     * @return a numerical feature\n-     */\n-    static Feature doubleToFeature(double d) {\n-        return FeatureFactory.newNumericalFeature(String.valueOf(d), d);\n-    }\n-\n-    /**\n-     * Perform perturbations on a fixed number of features in the given input.\n-     * Which feature will be perturbed is non deterministic.\n-     *\n-     * @param originalFeatures the input features that need to be perturbed\n-     * @param perturbationContext the perturbation context\n-     * @return a perturbed copy of the input features\n-     */\n-    public static List<Feature> perturbFeatures(List<Feature> originalFeatures, PerturbationContext perturbationContext) {\n-        return perturbFeatures(originalFeatures, perturbationContext, Collections.emptyMap());\n-    }\n-\n-    /**\n-     * Perform perturbations on a fixed number of features in the given input.\n-     * A map of feature distributions to draw (all, none or some of them) is given.\n-     * Which feature will be perturbed is non deterministic.\n-     *\n-     * @param originalFeatures the input features that need to be perturbed\n-     * @param perturbationContext the perturbation context\n-     * @param featureDistributionsMap the map of feature distributions\n-     * @return a perturbed copy of the input features\n-     */\n-    public static List<Feature> perturbFeatures(List<Feature> originalFeatures, PerturbationContext perturbationContext,\n-            Map<String, FeatureDistribution> featureDistributionsMap) {\n-        List<Feature> newFeatures = new ArrayList<>(originalFeatures);\n-        if (!newFeatures.isEmpty()) {\n-            // perturb at most in the range [|features|/2), noOfPerturbations]\n-            int lowerBound = (int) Math.min(perturbationContext.getNoOfPerturbations(), 0.5d * newFeatures.size());\n-            int upperBound = (int) Math.max(perturbationContext.getNoOfPerturbations(), 0.5d * newFeatures.size());\n-            upperBound = Math.min(upperBound, newFeatures.size());\n-            lowerBound = Math.max(1, lowerBound); // lower bound should always be greater than zero (not ok to not perturb)\n-            int perturbationSize = 0;\n-            if (lowerBound == upperBound) {\n-                perturbationSize = lowerBound;\n-            } else if (upperBound > lowerBound) {\n-                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);\n-            }\n-            if (perturbationSize > 0) {\n-                int[] indexesToBePerturbed = perturbationContext.getRandom().ints(0, newFeatures.size())\n-                        .distinct().limit(perturbationSize).toArray();\n-                for (int index : indexesToBePerturbed) {\n-                    Feature feature = newFeatures.get(index);\n-                    Value newValue;\n-                    if (featureDistributionsMap.containsKey(feature.getName())) {\n-                        newValue = featureDistributionsMap.get(feature.getName()).sample();\n-                    } else {\n-                        newValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-                    }\n-                    Feature perturbedFeature =\n-                            FeatureFactory.copyOf(feature, newValue);\n-                    newFeatures.set(index, perturbedFeature);\n-                }\n-            }\n-        }\n-        return newFeatures;\n-    }\n-\n-    /**\n-     * Drop a given feature from a list of existing features.\n-     *\n-     * @param features the existing features\n-     * @param target the feature to drop\n-     * @return a new list of features having the target feature dropped\n-     */\n-    public static List<Feature> dropFeature(List<Feature> features, Feature target) {\n-        List<Feature> newList = new ArrayList<>(features.size());\n-        for (Feature sourceFeature : features) {\n-            String sourceFeatureName = sourceFeature.getName();\n-            Type sourceFeatureType = sourceFeature.getType();\n-            Value sourceFeatureValue = sourceFeature.getValue();\n-            Feature f;\n-            if (target.getName().equals(sourceFeatureName)) {\n-                if (target.getType().equals(sourceFeatureType) && target.getValue().equals(sourceFeatureValue)) {\n-                    Value droppedValue = sourceFeatureType.drop(sourceFeatureValue);\n-                    f = FeatureFactory.copyOf(sourceFeature, droppedValue);\n-                } else {\n-                    f = dropOnLinearizedFeatures(target, sourceFeature);\n-                }\n-            } else if (Type.COMPOSITE.equals(sourceFeatureType)) {\n-                List<Feature> nestedFeatures = (List<Feature>) sourceFeatureValue.getUnderlyingObject();\n-                f = FeatureFactory.newCompositeFeature(sourceFeatureName, dropFeature(nestedFeatures, target));\n-            } else {\n-                // not found\n-                f = FeatureFactory.copyOf(sourceFeature, sourceFeatureValue);\n-            }\n-            newList.add(f);\n-        }\n-\n-        return newList;\n-    }\n-\n-    /**\n-     * Drop a target feature from a \"linearized\" version of a source feature.\n-     * Any of such linearized features are eventually dropped if they match on associated name, type and value.\n-     *\n-     * @param target the target feature\n-     * @param sourceFeature the source feature\n-     * @return the source feature having one of its underlying \"linearized\" values eventually dropped\n-     */\n-    protected static Feature dropOnLinearizedFeatures(Feature target, Feature sourceFeature) {\n-        Feature f = null;\n-        List<Feature> linearizedFeatures = DataUtils.getLinearizedFeatures(List.of(sourceFeature));\n-        int i = 0;\n-        for (Feature linearizedFeature : linearizedFeatures) {\n-            if (target.getValue().equals(linearizedFeature.getValue())) {\n-                linearizedFeatures.set(i,\n-                        FeatureFactory.copyOf(linearizedFeature, linearizedFeature.getType().drop(target.getValue())));\n-                f = FeatureFactory.newCompositeFeature(target.getName(), linearizedFeatures);\n-                break;\n-            } else {\n-                i++;\n-            }\n-        }\n-        // not found\n-        if (f == null) {\n-            f = FeatureFactory.copyOf(sourceFeature, sourceFeature.getValue());\n-        }\n-        return f;\n-    }\n-\n-    /**\n-     * Calculate the Hamming distance between two points.\n-     * <p>\n-     * see https://en.wikipedia.org/wiki/Hamming_distance\n-     *\n-     * @param x first point\n-     * @param y second point\n-     * @return the Hamming distance\n-     */\n-    public static double hammingDistance(double[] x, double[] y) {\n-        if (x.length != y.length) {\n-            return Double.NaN;\n-        } else {\n-            double h = 0d;\n-            for (int i = 0; i < x.length; i++) {\n-                if (x[i] != y[i]) {\n-                    h++;\n-                }\n-            }\n-            return h;\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Hamming distance between two text strings.\n-     * <p>\n-     * see https://en.wikipedia.org/wiki/Hamming_distance\n-     *\n-     * @param x first string\n-     * @param y second string\n-     * @return the Hamming distance\n-     */\n-    public static double hammingDistance(String x, String y) {\n-        if (x.length() != y.length()) {\n-            return Double.NaN;\n-        } else {\n-            double h = 0;\n-            for (int i = 0; i < x.length(); i++) {\n-                if (x.charAt(i) != y.charAt(i)) {\n-                    h++;\n-                }\n-            }\n-            return h;\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Euclidean distance between two points.\n-     *\n-     * @param x first point\n-     * @param y second point\n-     * @return the Euclidean distance\n-     */\n-    public static double euclideanDistance(double[] x, double[] y) {\n-        if (x.length != y.length) {\n-            return Double.NaN;\n-        } else {\n-            double e = 0;\n-            for (int i = 0; i < x.length; i++) {\n-                e += Math.pow(x[i] - y[i], 2);\n-            }\n-            return Math.sqrt(e);\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Gaussian kernel of a given value.\n-     *\n-     * @param x Gaussian kernel input value\n-     * @param mu mean\n-     * @param sigma variance\n-     * @return the Gaussian filtered value\n-     */\n-    public static double gaussianKernel(double x, double mu, double sigma) {\n-        return Math.exp(-Math.pow((x - mu) / sigma, 2) / 2) / (sigma * Math.sqrt(2d * Math.PI));\n-    }\n-\n-    /**\n-     * Calculate exponentially smoothed kernel of a given value (e.g. distance between two points).\n-     *\n-     * @param x value to smooth\n-     * @param width kernel width\n-     * @return the exponentially smoothed value\n-     */\n-    public static double exponentialSmoothingKernel(double x, double width) {\n-        return Math.sqrt(Math.exp(-(Math.pow(x, 2)) / Math.pow(width, 2)));\n-    }\n-\n-    /**\n-     * Generate a random data distribution.\n-     *\n-     * @param noOfFeatures number of features\n-     * @param distributionSize number of samples for each feature\n-     * @return a data distribution\n-     */\n-    public static DataDistribution generateRandomDataDistribution(int noOfFeatures, int distributionSize, Random random) {\n-        List<FeatureDistribution> featureDistributions = new LinkedList<>();\n-        for (int i = 0; i < noOfFeatures; i++) {\n-            double[] doubles = generateData(random.nextDouble(), random.nextDouble(), distributionSize, random);\n-            Feature feature = FeatureFactory.newNumericalFeature(\"f_\" + i, Double.NaN);\n-            FeatureDistribution featureDistribution = new NumericFeatureDistribution(feature, doubles);\n-            featureDistributions.add(featureDistribution);\n-        }\n-        return new IndependentFeaturesDataDistribution(featureDistributions);\n-    }\n-\n-    /**\n-     * Transform a list of prediction inputs into another list of the same prediction inputs but having linearized features.\n-     *\n-     * @param predictionInputs a list of prediction inputs\n-     * @return a list of prediction inputs with linearized features\n-     */\n-    public static List<PredictionInput> linearizeInputs(List<PredictionInput> predictionInputs) {\n-        List<PredictionInput> newInputs = new LinkedList<>();\n-        for (PredictionInput predictionInput : predictionInputs) {\n-            List<Feature> originalFeatures = predictionInput.getFeatures();\n-            List<Feature> flattenedFeatures = getLinearizedFeatures(originalFeatures);\n-            newInputs.add(new PredictionInput(flattenedFeatures));\n-        }\n-        return newInputs;\n-    }\n-\n-    /**\n-     * Transform a list of eventually composite / nested features into a flat list of non composite / non nested features.\n-     *\n-     * @param originalFeatures a list of features\n-     * @return a flat list of features\n-     */\n-    public static List<Feature> getLinearizedFeatures(List<Feature> originalFeatures) {\n-        List<Feature> flattenedFeatures = new LinkedList<>();\n-        for (Feature f : originalFeatures) {\n-            linearizeFeature(flattenedFeatures, f);\n-        }\n-        return flattenedFeatures;\n-    }\n-\n-    private static void linearizeFeature(List<Feature> flattenedFeatures, Feature f) {\n-        if (Type.UNDEFINED.equals(f.getType())) {\n-            if (f.getValue().getUnderlyingObject() instanceof Feature) {\n-                linearizeFeature(flattenedFeatures, (Feature) f.getValue().getUnderlyingObject());\n-            } else {\n-                flattenedFeatures.add(f);\n-            }\n-        } else if (Type.COMPOSITE.equals(f.getType())) {\n-            if (f.getValue().getUnderlyingObject() instanceof List) {\n-                List<Feature> features = (List<Feature>) f.getValue().getUnderlyingObject();\n-                for (Feature feature : features) {\n-                    linearizeFeature(flattenedFeatures, feature);\n-                }\n-            } else {\n-                flattenedFeatures.add(f);\n-            }\n-        } else {\n-            flattenedFeatures.add(f);\n-        }\n-    }\n-\n-    /**\n-     * Build Predictions from PredictionInputs and PredictionOutputs.\n-     *\n-     * @param inputs prediction inputs\n-     * @param os prediction outputs\n-     * @return a list of predictions\n-     */\n-    public static List<Prediction> getPredictions(List<PredictionInput> inputs, List<PredictionOutput> os) {\n-        return IntStream.range(0, os.size())\n-                .mapToObj(i -> new Prediction(inputs.get(i), os.get(i))).collect(Collectors.toList());\n-    }\n-\n-    /**\n-     * Sample (with replacement) from a list of values.\n-     *\n-     * @param values the list to sample from\n-     * @param sampleSize the no. of samples to draw\n-     * @param random a random instance\n-     * @param <T> the type of values to sample\n-     * @return a list of sampled values\n-     */\n-    public static <T> List<T> sampleWithReplacement(List<T> values, int sampleSize, Random random) {\n-        if (sampleSize <= 0 || values.isEmpty()) {\n-            return Collections.emptyList();\n-        } else {\n-            return random\n-                    .ints(sampleSize, 0, values.size())\n-                    .mapToObj(values::get)\n-                    .collect(Collectors.toList());\n-        }\n-    }\n-\n-    /**\n-     * Replace an existing feature in a list with another feature.\n-     * The feature to be replaced is the one whose name is equals to the name of the feature to use as replacement.\n-     *\n-     * @param featureToUse feature to use as replacmement\n-     * @param existingFeatures list of features containing the feature to be replaced\n-     * @return a new list of features having the \"replaced\" feature\n-     */\n-    public static List<Feature> replaceFeatures(Feature featureToUse, List<Feature> existingFeatures) {\n-        List<Feature> newFeatures = new ArrayList<>();\n-        for (Feature f : existingFeatures) {\n-            Feature newFeature;\n-            if (f.getName().equals(featureToUse.getName())) {\n-                newFeature = FeatureFactory.copyOf(f, featureToUse.getValue());\n-            } else {\n-                if (Type.COMPOSITE == f.getType()) {\n-                    List<Feature> elements = (List<Feature>) f.getValue().getUnderlyingObject();\n-                    newFeature = FeatureFactory.newCompositeFeature(f.getName(), replaceFeatures(featureToUse, elements));\n-                } else {\n-                    newFeature = FeatureFactory.copyOf(f, f.getValue());\n-                }\n-            }\n-            newFeatures.add(newFeature);\n-        }\n-        return newFeatures;\n-    }\n-\n-    /**\n-     * Persist a {@link PartialDependenceGraph} into a CSV file.\n-     * \n-     * @param partialDependenceGraph the PDP to persist\n-     * @param path the path to the CSV file to be created\n-     * @throws IOException whether any IO error occurs while writing the CSV\n-     */\n-    public static void toCSV(PartialDependenceGraph partialDependenceGraph, Path path) throws IOException {\n-        try (Writer writer = Files.newBufferedWriter(path)) {\n-            List<Value> xAxis = partialDependenceGraph.getX();\n-            List<Value> yAxis = partialDependenceGraph.getY();\n-            CSVFormat format = CSVFormat.DEFAULT.withHeader(\n-                    partialDependenceGraph.getFeature().getName(), partialDependenceGraph.getOutput().getName());\n-            CSVPrinter printer = new CSVPrinter(writer, format);\n-            for (int i = 0; i < xAxis.size(); i++) {\n-                printer.printRecord(xAxis.get(i).asString(), yAxis.get(i).asString());\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Read a CSV file into a {@link DataDistribution} object.\n-     *\n-     * @param file the path to the CSV file\n-     * @param schema an ordered list of {@link Type}s as the 'schema', used to determine\n-     *        the {@link Type} of each feature / column\n-     * @return the parsed CSV as a {@link DataDistribution}\n-     * @throws IOException when failing at reading the CSV file\n-     * @throws MalformedInputException if any record in CSV has different size with respect to the specified schema\n-     */\n-    public static DataDistribution readCSV(Path file, List<Type> schema) throws IOException {\n-        List<PredictionInput> inputs = new ArrayList<>();\n-        try (BufferedReader reader = Files.newBufferedReader(file)) {\n-            Iterable<CSVRecord> records = CSVFormat.RFC4180.withFirstRecordAsHeader().parse(reader);\n-            for (CSVRecord record : records) {\n-                int size = record.size();\n-                if (schema.size() == size) {\n-                    List<Feature> features = new ArrayList<>();\n-                    for (int i = 0; i < size; i++) {\n-                        String s = record.get(i);\n-                        Type type = schema.get(i);\n-                        features.add(new Feature(record.getParser().getHeaderNames().get(i), type, new Value(s)));\n-                    }\n-                    inputs.add(new PredictionInput(features));\n-                } else {\n-                    throw new MalformedInputException(size);\n-                }\n-            }\n-        }\n-        return new PredictionInputsDataDistribution(inputs);\n-    }\n-\n-    /**\n-     * Generate feature distributions from an existing (evantually small) {@link DataDistribution} for each {@link Feature}.\n-     * Each feature intervals (min, max) and density information (mean, stdDev) are generated using bootstrap, then\n-     * data points are sampled from a normal distribution (see {@link #generateData(double, double, int, Random)}).\n-     *\n-     * @param dataDistribution data distribution to take feature values from\n-     * @param perturbationContext perturbation context\n-     * @param featureDistributionSize desired size of generated feature distributions\n-     * @param draws number of times sampling from feature values is performed\n-     * @param sampleSize size of each sample draw\n-     * @return a map feature name -> generated feature distribution\n-     */\n-    public static Map<String, FeatureDistribution> boostrapFeatureDistributions(DataDistribution dataDistribution,\n-            PerturbationContext perturbationContext, int featureDistributionSize, int draws, int sampleSize) {\n-        Map<String, FeatureDistribution> featureDistributions = new HashMap<>();\n-        for (FeatureDistribution featureDistribution : dataDistribution.asFeatureDistributions()) {\n-            Feature feature = featureDistribution.getFeature();\n-            if (Type.NUMBER.equals(feature.getType())) {\n-                List<Value> values = featureDistribution.getAllSamples();\n-                double[] means = new double[draws];\n-                double[] stdDevs = new double[draws];\n-                double[] mins = new double[draws];\n-                double[] maxs = new double[draws];\n-                for (int i = 0; i < draws; i++) {\n-                    List<Value> sampledValues = DataUtils.sampleWithReplacement(values, sampleSize, perturbationContext.getRandom());\n-                    double mean = DataUtils.getMean(sampledValues.stream().mapToDouble(Value::asNumber).toArray());\n-                    double stdDev = Math.pow(DataUtils.getStdDev(sampledValues.stream().mapToDouble(Value::asNumber).toArray(), mean), 2);\n-                    double min = sampledValues.stream().mapToDouble(Value::asNumber).min().orElse(Double.MIN_VALUE);\n-                    double max = sampledValues.stream().mapToDouble(Value::asNumber).max().orElse(Double.MAX_VALUE);\n-                    means[i] = mean;\n-                    stdDevs[i] = stdDev;\n-                    mins[i] = min;\n-                    maxs[i] = max;\n-                }\n-                double finalMean = DataUtils.getMean(means);\n-                double finalStdDev = Math.sqrt(DataUtils.getMean(stdDevs));\n-                double finalMin = DataUtils.getMean(mins);\n-                double finalMax = DataUtils.getMean(maxs);\n-                double[] doubles = DataUtils.generateData(finalMean, finalStdDev, featureDistributionSize, perturbationContext.getRandom());\n-                double[] boundedData = Arrays.stream(doubles).map(d -> Math.min(Math.max(d, finalMin), finalMax)).toArray();\n-                NumericFeatureDistribution numericFeatureDistribution = new NumericFeatureDistribution(feature, boundedData);\n-                featureDistributions.put(feature.getName(), numericFeatureDistribution);\n-            }\n-        }\n-        return featureDistributions;\n-    }\n-}\n", "next_change": null}]}}]}}, {"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 86e751bdd..88b3fda95 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -175,7 +180,8 @@ public class DataUtils {\n                         .distinct().limit(perturbationSize).toArray();\n                 for (int index : indexesToBePerturbed) {\n                     Feature feature = newFeatures.get(index);\n-                    Feature perturbedFeature = FeatureFactory.copyOf(feature, feature.getType().perturb(feature.getValue(), perturbationContext));\n+                    Feature perturbedFeature =\n+                            FeatureFactory.copyOf(feature, feature.getType().perturb(feature.getValue(), perturbationContext));\n                     newFeatures.set(index, perturbedFeature);\n                 }\n             }\n", "next_change": {"commit": "f79c6963e64d6d15a16c14145d7782d36c3a2402", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\nindex 88b3fda95..0c43e2b21 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n", "chunk": "@@ -180,8 +203,14 @@ public class DataUtils {\n                         .distinct().limit(perturbationSize).toArray();\n                 for (int index : indexesToBePerturbed) {\n                     Feature feature = newFeatures.get(index);\n+                    Value newValue;\n+                    if (featureDistributionsMap.containsKey(feature.getName())) {\n+                        newValue = featureDistributionsMap.get(feature.getName()).sample();\n+                    } else {\n+                        newValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n+                    }\n                     Feature perturbedFeature =\n-                            FeatureFactory.copyOf(feature, feature.getType().perturb(feature.getValue(), perturbationContext));\n+                            FeatureFactory.copyOf(feature, newValue);\n                     newFeatures.set(index, perturbedFeature);\n                 }\n             }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\ndeleted file mode 100644\nindex 0c43e2b21..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/DataUtils.java\n+++ /dev/null\n", "chunk": "@@ -1,598 +0,0 @@\n-/*\n- * Copyright 2021 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import java.io.BufferedReader;\n-import java.io.IOException;\n-import java.io.Writer;\n-import java.nio.charset.MalformedInputException;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-import java.util.stream.IntStream;\n-\n-import org.apache.commons.csv.CSVFormat;\n-import org.apache.commons.csv.CSVPrinter;\n-import org.apache.commons.csv.CSVRecord;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureDistribution;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.IndependentFeaturesDataDistribution;\n-import org.kie.kogito.explainability.model.NumericFeatureDistribution;\n-import org.kie.kogito.explainability.model.PartialDependenceGraph;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionInputsDataDistribution;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-\n-/**\n- * Utility methods to handle and manipulate data.\n- */\n-public class DataUtils {\n-\n-    private DataUtils() {\n-    }\n-\n-    /**\n-     * Generate a dataset of a certain size, sampled from a normal distribution, given mean and standard deviation.\n-     * Samples are generated from a normal distribution, multiplied by {@code stdDeviation} and summed to {@code mean},\n-     * actual mean {@code m} and standard deviation {@code d} are calculated.\n-     * Then all numbers are multiplied by the same number so that the standard deviation also gets\n-     * multiplied by the same number, hence we multiply each random number by {@code stdDeviation / d}.\n-     * The resultant set has standard deviation {@code stdDeviation} and mean {@code m1=m*stdDeviation/d}.\n-     * If a same number is added to all values the mean also changes by the same number so we add {@code mean - m1} to\n-     * all numbers.\n-     *\n-     * @param mean desired mean\n-     * @param stdDeviation desired standard deviation\n-     * @param size size of the array\n-     * @return the generated data\n-     */\n-    public static double[] generateData(double mean, double stdDeviation, int size, Random random) {\n-\n-        // generate random data from a normal (gaussian) distribution\n-        double[] data = new double[size];\n-        for (int i = 0; i < size; i++) {\n-            data[i] = random.nextGaussian() * stdDeviation + mean;\n-        }\n-\n-        double generatedDataMean = getMean(data);\n-        double generatedDataStdDev = getStdDev(data, generatedDataMean);\n-\n-        // force desired standard deviation\n-        double newStdDeviation = generatedDataStdDev != 0 ? stdDeviation / generatedDataStdDev : stdDeviation; // avoid division by zero\n-        for (int i = 0; i < size; i++) {\n-            data[i] *= newStdDeviation;\n-        }\n-\n-        // get the new mean\n-        double newMean = generatedDataStdDev != 0 ? generatedDataMean * stdDeviation / generatedDataStdDev\n-                : generatedDataMean * stdDeviation;\n-\n-        // force desired mean\n-        for (int i = 0; i < size; i++) {\n-            data[i] += mean - newMean;\n-        }\n-\n-        return data;\n-    }\n-\n-    public static double getMean(double[] data) {\n-        double m = 0;\n-        for (double datum : data) {\n-            m += datum;\n-        }\n-        m = m / data.length;\n-        return m;\n-    }\n-\n-    public static double getStdDev(double[] data, double mean) {\n-        double d = 0;\n-        for (double datum : data) {\n-            d += Math.pow(datum - mean, 2);\n-        }\n-        d /= data.length;\n-        d = Math.sqrt(d);\n-        return d;\n-    }\n-\n-    /**\n-     * Generate equally {@code size} sampled values between {@code min} and {@code max}.\n-     *\n-     * @param min minimum value\n-     * @param max maximum value\n-     * @param size dataset size\n-     * @return the generated data\n-     */\n-    public static double[] generateSamples(double min, double max, int size) {\n-        double[] data = new double[size];\n-        double val = min;\n-        double sum = max / size;\n-        for (int i = 0; i < size; i++) {\n-            data[i] = val;\n-            val += sum;\n-        }\n-        return data;\n-    }\n-\n-    /**\n-     * Transform an array of double into a list of numerical features.\n-     *\n-     * @param inputs an array of double numbers\n-     * @return a list of numerical features\n-     */\n-    public static List<Feature> doublesToFeatures(double[] inputs) {\n-        return DoubleStream.of(inputs).mapToObj(DataUtils::doubleToFeature).collect(Collectors.toList());\n-    }\n-\n-    /**\n-     * Transform a double into a numerical feature.\n-     *\n-     * @param d the double value\n-     * @return a numerical feature\n-     */\n-    static Feature doubleToFeature(double d) {\n-        return FeatureFactory.newNumericalFeature(String.valueOf(d), d);\n-    }\n-\n-    /**\n-     * Perform perturbations on a fixed number of features in the given input.\n-     * Which feature will be perturbed is non deterministic.\n-     *\n-     * @param originalFeatures the input features that need to be perturbed\n-     * @param perturbationContext the perturbation context\n-     * @return a perturbed copy of the input features\n-     */\n-    public static List<Feature> perturbFeatures(List<Feature> originalFeatures, PerturbationContext perturbationContext) {\n-        return perturbFeatures(originalFeatures, perturbationContext, Collections.emptyMap());\n-    }\n-\n-    /**\n-     * Perform perturbations on a fixed number of features in the given input.\n-     * A map of feature distributions to draw (all, none or some of them) is given.\n-     * Which feature will be perturbed is non deterministic.\n-     *\n-     * @param originalFeatures the input features that need to be perturbed\n-     * @param perturbationContext the perturbation context\n-     * @param featureDistributionsMap the map of feature distributions\n-     * @return a perturbed copy of the input features\n-     */\n-    public static List<Feature> perturbFeatures(List<Feature> originalFeatures, PerturbationContext perturbationContext,\n-            Map<String, FeatureDistribution> featureDistributionsMap) {\n-        List<Feature> newFeatures = new ArrayList<>(originalFeatures);\n-        if (!newFeatures.isEmpty()) {\n-            // perturb at most in the range [|features|/2), noOfPerturbations]\n-            int lowerBound = (int) Math.min(perturbationContext.getNoOfPerturbations(), 0.5d * newFeatures.size());\n-            int upperBound = (int) Math.max(perturbationContext.getNoOfPerturbations(), 0.5d * newFeatures.size());\n-            upperBound = Math.min(upperBound, newFeatures.size());\n-            lowerBound = Math.max(1, lowerBound); // lower bound should always be greater than zero (not ok to not perturb)\n-            int perturbationSize = 0;\n-            if (lowerBound == upperBound) {\n-                perturbationSize = lowerBound;\n-            } else if (upperBound > lowerBound) {\n-                perturbationSize = perturbationContext.getRandom().ints(1, lowerBound, 1 + upperBound).findFirst().orElse(1);\n-            }\n-            if (perturbationSize > 0) {\n-                int[] indexesToBePerturbed = perturbationContext.getRandom().ints(0, newFeatures.size())\n-                        .distinct().limit(perturbationSize).toArray();\n-                for (int index : indexesToBePerturbed) {\n-                    Feature feature = newFeatures.get(index);\n-                    Value newValue;\n-                    if (featureDistributionsMap.containsKey(feature.getName())) {\n-                        newValue = featureDistributionsMap.get(feature.getName()).sample();\n-                    } else {\n-                        newValue = feature.getType().perturb(feature.getValue(), perturbationContext);\n-                    }\n-                    Feature perturbedFeature =\n-                            FeatureFactory.copyOf(feature, newValue);\n-                    newFeatures.set(index, perturbedFeature);\n-                }\n-            }\n-        }\n-        return newFeatures;\n-    }\n-\n-    /**\n-     * Drop a given feature from a list of existing features.\n-     *\n-     * @param features the existing features\n-     * @param target the feature to drop\n-     * @return a new list of features having the target feature dropped\n-     */\n-    public static List<Feature> dropFeature(List<Feature> features, Feature target) {\n-        List<Feature> newList = new ArrayList<>(features.size());\n-        for (Feature sourceFeature : features) {\n-            String sourceFeatureName = sourceFeature.getName();\n-            Type sourceFeatureType = sourceFeature.getType();\n-            Value sourceFeatureValue = sourceFeature.getValue();\n-            Feature f;\n-            if (target.getName().equals(sourceFeatureName)) {\n-                if (target.getType().equals(sourceFeatureType) && target.getValue().equals(sourceFeatureValue)) {\n-                    Value droppedValue = sourceFeatureType.drop(sourceFeatureValue);\n-                    f = FeatureFactory.copyOf(sourceFeature, droppedValue);\n-                } else {\n-                    f = dropOnLinearizedFeatures(target, sourceFeature);\n-                }\n-            } else if (Type.COMPOSITE.equals(sourceFeatureType)) {\n-                List<Feature> nestedFeatures = (List<Feature>) sourceFeatureValue.getUnderlyingObject();\n-                f = FeatureFactory.newCompositeFeature(sourceFeatureName, dropFeature(nestedFeatures, target));\n-            } else {\n-                // not found\n-                f = FeatureFactory.copyOf(sourceFeature, sourceFeatureValue);\n-            }\n-            newList.add(f);\n-        }\n-\n-        return newList;\n-    }\n-\n-    /**\n-     * Drop a target feature from a \"linearized\" version of a source feature.\n-     * Any of such linearized features are eventually dropped if they match on associated name, type and value.\n-     *\n-     * @param target the target feature\n-     * @param sourceFeature the source feature\n-     * @return the source feature having one of its underlying \"linearized\" values eventually dropped\n-     */\n-    protected static Feature dropOnLinearizedFeatures(Feature target, Feature sourceFeature) {\n-        Feature f = null;\n-        List<Feature> linearizedFeatures = DataUtils.getLinearizedFeatures(List.of(sourceFeature));\n-        int i = 0;\n-        for (Feature linearizedFeature : linearizedFeatures) {\n-            if (target.getValue().equals(linearizedFeature.getValue())) {\n-                linearizedFeatures.set(i,\n-                        FeatureFactory.copyOf(linearizedFeature, linearizedFeature.getType().drop(target.getValue())));\n-                f = FeatureFactory.newCompositeFeature(target.getName(), linearizedFeatures);\n-                break;\n-            } else {\n-                i++;\n-            }\n-        }\n-        // not found\n-        if (f == null) {\n-            f = FeatureFactory.copyOf(sourceFeature, sourceFeature.getValue());\n-        }\n-        return f;\n-    }\n-\n-    /**\n-     * Calculate the Hamming distance between two points.\n-     * <p>\n-     * see https://en.wikipedia.org/wiki/Hamming_distance\n-     *\n-     * @param x first point\n-     * @param y second point\n-     * @return the Hamming distance\n-     */\n-    public static double hammingDistance(double[] x, double[] y) {\n-        if (x.length != y.length) {\n-            return Double.NaN;\n-        } else {\n-            double h = 0d;\n-            for (int i = 0; i < x.length; i++) {\n-                if (x[i] != y[i]) {\n-                    h++;\n-                }\n-            }\n-            return h;\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Hamming distance between two text strings.\n-     * <p>\n-     * see https://en.wikipedia.org/wiki/Hamming_distance\n-     *\n-     * @param x first string\n-     * @param y second string\n-     * @return the Hamming distance\n-     */\n-    public static double hammingDistance(String x, String y) {\n-        if (x.length() != y.length()) {\n-            return Double.NaN;\n-        } else {\n-            double h = 0;\n-            for (int i = 0; i < x.length(); i++) {\n-                if (x.charAt(i) != y.charAt(i)) {\n-                    h++;\n-                }\n-            }\n-            return h;\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Euclidean distance between two points.\n-     *\n-     * @param x first point\n-     * @param y second point\n-     * @return the Euclidean distance\n-     */\n-    public static double euclideanDistance(double[] x, double[] y) {\n-        if (x.length != y.length) {\n-            return Double.NaN;\n-        } else {\n-            double e = 0;\n-            for (int i = 0; i < x.length; i++) {\n-                e += Math.pow(x[i] - y[i], 2);\n-            }\n-            return Math.sqrt(e);\n-        }\n-    }\n-\n-    /**\n-     * Calculate the Gaussian kernel of a given value.\n-     *\n-     * @param x Gaussian kernel input value\n-     * @param mu mean\n-     * @param sigma variance\n-     * @return the Gaussian filtered value\n-     */\n-    public static double gaussianKernel(double x, double mu, double sigma) {\n-        return Math.exp(-Math.pow((x - mu) / sigma, 2) / 2) / (sigma * Math.sqrt(2d * Math.PI));\n-    }\n-\n-    /**\n-     * Calculate exponentially smoothed kernel of a given value (e.g. distance between two points).\n-     *\n-     * @param x value to smooth\n-     * @param width kernel width\n-     * @return the exponentially smoothed value\n-     */\n-    public static double exponentialSmoothingKernel(double x, double width) {\n-        return Math.sqrt(Math.exp(-(Math.pow(x, 2)) / Math.pow(width, 2)));\n-    }\n-\n-    /**\n-     * Generate a random data distribution.\n-     *\n-     * @param noOfFeatures number of features\n-     * @param distributionSize number of samples for each feature\n-     * @return a data distribution\n-     */\n-    public static DataDistribution generateRandomDataDistribution(int noOfFeatures, int distributionSize, Random random) {\n-        List<FeatureDistribution> featureDistributions = new LinkedList<>();\n-        for (int i = 0; i < noOfFeatures; i++) {\n-            double[] doubles = generateData(random.nextDouble(), random.nextDouble(), distributionSize, random);\n-            Feature feature = FeatureFactory.newNumericalFeature(\"f_\" + i, Double.NaN);\n-            FeatureDistribution featureDistribution = new NumericFeatureDistribution(feature, doubles);\n-            featureDistributions.add(featureDistribution);\n-        }\n-        return new IndependentFeaturesDataDistribution(featureDistributions);\n-    }\n-\n-    /**\n-     * Transform a list of prediction inputs into another list of the same prediction inputs but having linearized features.\n-     *\n-     * @param predictionInputs a list of prediction inputs\n-     * @return a list of prediction inputs with linearized features\n-     */\n-    public static List<PredictionInput> linearizeInputs(List<PredictionInput> predictionInputs) {\n-        List<PredictionInput> newInputs = new LinkedList<>();\n-        for (PredictionInput predictionInput : predictionInputs) {\n-            List<Feature> originalFeatures = predictionInput.getFeatures();\n-            List<Feature> flattenedFeatures = getLinearizedFeatures(originalFeatures);\n-            newInputs.add(new PredictionInput(flattenedFeatures));\n-        }\n-        return newInputs;\n-    }\n-\n-    /**\n-     * Transform a list of eventually composite / nested features into a flat list of non composite / non nested features.\n-     *\n-     * @param originalFeatures a list of features\n-     * @return a flat list of features\n-     */\n-    public static List<Feature> getLinearizedFeatures(List<Feature> originalFeatures) {\n-        List<Feature> flattenedFeatures = new LinkedList<>();\n-        for (Feature f : originalFeatures) {\n-            linearizeFeature(flattenedFeatures, f);\n-        }\n-        return flattenedFeatures;\n-    }\n-\n-    private static void linearizeFeature(List<Feature> flattenedFeatures, Feature f) {\n-        if (Type.UNDEFINED.equals(f.getType())) {\n-            if (f.getValue().getUnderlyingObject() instanceof Feature) {\n-                linearizeFeature(flattenedFeatures, (Feature) f.getValue().getUnderlyingObject());\n-            } else {\n-                flattenedFeatures.add(f);\n-            }\n-        } else if (Type.COMPOSITE.equals(f.getType())) {\n-            if (f.getValue().getUnderlyingObject() instanceof List) {\n-                List<Feature> features = (List<Feature>) f.getValue().getUnderlyingObject();\n-                for (Feature feature : features) {\n-                    linearizeFeature(flattenedFeatures, feature);\n-                }\n-            } else {\n-                flattenedFeatures.add(f);\n-            }\n-        } else {\n-            flattenedFeatures.add(f);\n-        }\n-    }\n-\n-    /**\n-     * Build Predictions from PredictionInputs and PredictionOutputs.\n-     *\n-     * @param inputs prediction inputs\n-     * @param os prediction outputs\n-     * @return a list of predictions\n-     */\n-    public static List<Prediction> getPredictions(List<PredictionInput> inputs, List<PredictionOutput> os) {\n-        return IntStream.range(0, os.size())\n-                .mapToObj(i -> new Prediction(inputs.get(i), os.get(i))).collect(Collectors.toList());\n-    }\n-\n-    /**\n-     * Sample (with replacement) from a list of values.\n-     *\n-     * @param values the list to sample from\n-     * @param sampleSize the no. of samples to draw\n-     * @param random a random instance\n-     * @param <T> the type of values to sample\n-     * @return a list of sampled values\n-     */\n-    public static <T> List<T> sampleWithReplacement(List<T> values, int sampleSize, Random random) {\n-        if (sampleSize <= 0 || values.isEmpty()) {\n-            return Collections.emptyList();\n-        } else {\n-            return random\n-                    .ints(sampleSize, 0, values.size())\n-                    .mapToObj(values::get)\n-                    .collect(Collectors.toList());\n-        }\n-    }\n-\n-    /**\n-     * Replace an existing feature in a list with another feature.\n-     * The feature to be replaced is the one whose name is equals to the name of the feature to use as replacement.\n-     *\n-     * @param featureToUse feature to use as replacmement\n-     * @param existingFeatures list of features containing the feature to be replaced\n-     * @return a new list of features having the \"replaced\" feature\n-     */\n-    public static List<Feature> replaceFeatures(Feature featureToUse, List<Feature> existingFeatures) {\n-        List<Feature> newFeatures = new ArrayList<>();\n-        for (Feature f : existingFeatures) {\n-            Feature newFeature;\n-            if (f.getName().equals(featureToUse.getName())) {\n-                newFeature = FeatureFactory.copyOf(f, featureToUse.getValue());\n-            } else {\n-                if (Type.COMPOSITE == f.getType()) {\n-                    List<Feature> elements = (List<Feature>) f.getValue().getUnderlyingObject();\n-                    newFeature = FeatureFactory.newCompositeFeature(f.getName(), replaceFeatures(featureToUse, elements));\n-                } else {\n-                    newFeature = FeatureFactory.copyOf(f, f.getValue());\n-                }\n-            }\n-            newFeatures.add(newFeature);\n-        }\n-        return newFeatures;\n-    }\n-\n-    /**\n-     * Persist a {@link PartialDependenceGraph} into a CSV file.\n-     * \n-     * @param partialDependenceGraph the PDP to persist\n-     * @param path the path to the CSV file to be created\n-     * @throws IOException whether any IO error occurs while writing the CSV\n-     */\n-    public static void toCSV(PartialDependenceGraph partialDependenceGraph, Path path) throws IOException {\n-        try (Writer writer = Files.newBufferedWriter(path)) {\n-            List<Value> xAxis = partialDependenceGraph.getX();\n-            List<Value> yAxis = partialDependenceGraph.getY();\n-            CSVFormat format = CSVFormat.DEFAULT.withHeader(\n-                    partialDependenceGraph.getFeature().getName(), partialDependenceGraph.getOutput().getName());\n-            CSVPrinter printer = new CSVPrinter(writer, format);\n-            for (int i = 0; i < xAxis.size(); i++) {\n-                printer.printRecord(xAxis.get(i).asString(), yAxis.get(i).asString());\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Read a CSV file into a {@link DataDistribution} object.\n-     *\n-     * @param file the path to the CSV file\n-     * @param schema an ordered list of {@link Type}s as the 'schema', used to determine\n-     *        the {@link Type} of each feature / column\n-     * @return the parsed CSV as a {@link DataDistribution}\n-     * @throws IOException when failing at reading the CSV file\n-     * @throws MalformedInputException if any record in CSV has different size with respect to the specified schema\n-     */\n-    public static DataDistribution readCSV(Path file, List<Type> schema) throws IOException {\n-        List<PredictionInput> inputs = new ArrayList<>();\n-        try (BufferedReader reader = Files.newBufferedReader(file)) {\n-            Iterable<CSVRecord> records = CSVFormat.RFC4180.withFirstRecordAsHeader().parse(reader);\n-            for (CSVRecord record : records) {\n-                int size = record.size();\n-                if (schema.size() == size) {\n-                    List<Feature> features = new ArrayList<>();\n-                    for (int i = 0; i < size; i++) {\n-                        String s = record.get(i);\n-                        Type type = schema.get(i);\n-                        features.add(new Feature(record.getParser().getHeaderNames().get(i), type, new Value(s)));\n-                    }\n-                    inputs.add(new PredictionInput(features));\n-                } else {\n-                    throw new MalformedInputException(size);\n-                }\n-            }\n-        }\n-        return new PredictionInputsDataDistribution(inputs);\n-    }\n-\n-    /**\n-     * Generate feature distributions from an existing (evantually small) {@link DataDistribution} for each {@link Feature}.\n-     * Each feature intervals (min, max) and density information (mean, stdDev) are generated using bootstrap, then\n-     * data points are sampled from a normal distribution (see {@link #generateData(double, double, int, Random)}).\n-     *\n-     * @param dataDistribution data distribution to take feature values from\n-     * @param perturbationContext perturbation context\n-     * @param featureDistributionSize desired size of generated feature distributions\n-     * @param draws number of times sampling from feature values is performed\n-     * @param sampleSize size of each sample draw\n-     * @return a map feature name -> generated feature distribution\n-     */\n-    public static Map<String, FeatureDistribution> boostrapFeatureDistributions(DataDistribution dataDistribution,\n-            PerturbationContext perturbationContext, int featureDistributionSize, int draws, int sampleSize) {\n-        Map<String, FeatureDistribution> featureDistributions = new HashMap<>();\n-        for (FeatureDistribution featureDistribution : dataDistribution.asFeatureDistributions()) {\n-            Feature feature = featureDistribution.getFeature();\n-            if (Type.NUMBER.equals(feature.getType())) {\n-                List<Value> values = featureDistribution.getAllSamples();\n-                double[] means = new double[draws];\n-                double[] stdDevs = new double[draws];\n-                double[] mins = new double[draws];\n-                double[] maxs = new double[draws];\n-                for (int i = 0; i < draws; i++) {\n-                    List<Value> sampledValues = DataUtils.sampleWithReplacement(values, sampleSize, perturbationContext.getRandom());\n-                    double mean = DataUtils.getMean(sampledValues.stream().mapToDouble(Value::asNumber).toArray());\n-                    double stdDev = Math.pow(DataUtils.getStdDev(sampledValues.stream().mapToDouble(Value::asNumber).toArray(), mean), 2);\n-                    double min = sampledValues.stream().mapToDouble(Value::asNumber).min().orElse(Double.MIN_VALUE);\n-                    double max = sampledValues.stream().mapToDouble(Value::asNumber).max().orElse(Double.MAX_VALUE);\n-                    means[i] = mean;\n-                    stdDevs[i] = stdDev;\n-                    mins[i] = min;\n-                    maxs[i] = max;\n-                }\n-                double finalMean = DataUtils.getMean(means);\n-                double finalStdDev = Math.sqrt(DataUtils.getMean(stdDevs));\n-                double finalMin = DataUtils.getMean(mins);\n-                double finalMax = DataUtils.getMean(maxs);\n-                double[] doubles = DataUtils.generateData(finalMean, finalStdDev, featureDistributionSize, perturbationContext.getRandom());\n-                double[] boundedData = Arrays.stream(doubles).map(d -> Math.min(Math.max(d, finalMin), finalMax)).toArray();\n-                NumericFeatureDistribution numericFeatureDistribution = new NumericFeatureDistribution(feature, boundedData);\n-                featureDistributions.put(feature.getName(), numericFeatureDistribution);\n-            }\n-        }\n-        return featureDistributions;\n-    }\n-}\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1NzE3NA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r527057174", "body": "Why are you adding `originalValue` to feature scaling? Can you please clarify?", "bodyText": "Why are you adding originalValue to feature scaling? Can you please clarify?", "bodyHTML": "<p dir=\"auto\">Why are you adding <code>originalValue</code> to feature scaling? Can you please clarify?</p>", "author": "danielezonca", "createdAt": "2020-11-19T17:12:09Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -164,20 +165,28 @@\n         @Override\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n-            double[] doubles = new double[values.length];\n+            double[] doubles = new double[values.length + 1];\n             int i = 0;\n             for (Value<?> v : values) {\n                 doubles[i] = v.asNumber();\n                 i++;\n             }\n             double originalValue = target.asNumber();\n+            doubles[i] = originalValue; // include target number in feature scaling", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk1MTIxMA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540951210", "bodyText": "as per issue description:\n\nincluded the original feature value in min / max scaling (which is performed during sparse encoding), this fixes cases where the original value doesn't fall in the value range of the sampled perturbed values (which is rare but can still happen)", "author": "tteofili", "createdAt": "2020-12-11T13:36:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1NzE3NA=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex d01e09ea6..dadf67e41 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -166,25 +166,25 @@ public enum Type {\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n             double[] doubles = new double[values.length + 1];\n-            int i = 0;\n+            int valueIndex = 0;\n             for (Value<?> v : values) {\n-                doubles[i] = v.asNumber();\n-                i++;\n+                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n+                valueIndex++;\n             }\n-            double originalValue = target.asNumber();\n-            doubles[i] = originalValue; // include target number in feature scaling\n+            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n+            doubles[valueIndex] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n \n             // feature scaling\n             List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(i); // extract the scaled original value (it must not appear in encoded values)\n+            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n             double sigma = 1;\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> Double.isNaN(d) ? 0 : d).map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n+                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n \n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex dadf67e41..3b78c5c2b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -181,12 +181,12 @@ public enum Type {\n             double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n-            double sigma = 1;\n+            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n                     .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n             return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n         }\n", "next_change": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 3b78c5c2b..b9761f367 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -188,7 +188,7 @@ public enum Type {\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n-            return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n+            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n         }\n \n         @Override\n", "next_change": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex b9761f367..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -192,41 +190,41 @@ public enum Type {\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextDouble());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextDouble());\n         }\n     },\n \n     BOOLEAN(\"boolean\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(null);\n+        public Value drop(Value value) {\n+            return new Value(null);\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n-            return new Value<>(!Boolean.parseBoolean(value.asString()));\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n+            return new Value(!Boolean.parseBoolean(value.asString()));\n         }\n \n         @Override\n-        public List<double[]> encode(EncodingParams params, Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextBoolean());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextBoolean());\n         }\n     },\n \n     URI(\"uri\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(java.net.URI.create(\"\"));\n+        public Value drop(Value value) {\n+            return new Value(java.net.URI.create(\"\"));\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             String uriAsString = value.asString();\n             java.net.URI uri = java.net.URI.create(uriAsString);\n             String scheme = uri.getScheme();\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1NzUyMA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r527057520", "body": "```suggestion\r\n            int valueIndex = 0;\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        int i = 0;\n          \n          \n            \n                        int valueIndex = 0;", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">            <span class=\"pl-k\">int</span> <span class=\"x x-first x-last\">i</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>;</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">            <span class=\"pl-k\">int</span> <span class=\"x x-first x-last\">valueIndex</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>;</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "danielezonca", "createdAt": "2020-11-19T17:12:43Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -164,20 +165,28 @@\n         @Override\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n-            double[] doubles = new double[values.length];\n+            double[] doubles = new double[values.length + 1];\n             int i = 0;", "originalCommit": "7db39c4c33c84eca961e21a8c1a0a24628d379e2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex d01e09ea6..dadf67e41 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -166,25 +166,25 @@ public enum Type {\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n             double[] doubles = new double[values.length + 1];\n-            int i = 0;\n+            int valueIndex = 0;\n             for (Value<?> v : values) {\n-                doubles[i] = v.asNumber();\n-                i++;\n+                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n+                valueIndex++;\n             }\n-            double originalValue = target.asNumber();\n-            doubles[i] = originalValue; // include target number in feature scaling\n+            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n+            doubles[valueIndex] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n \n             // feature scaling\n             List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(i); // extract the scaled original value (it must not appear in encoded values)\n+            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n             double sigma = 1;\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> Double.isNaN(d) ? 0 : d).map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n+                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n \n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex dadf67e41..3b78c5c2b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -181,12 +181,12 @@ public enum Type {\n             double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n-            double sigma = 1;\n+            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n                     .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n             return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n         }\n", "next_change": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 3b78c5c2b..b9761f367 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -188,7 +188,7 @@ public enum Type {\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n-            return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n+            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n         }\n \n         @Override\n", "next_change": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex b9761f367..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -192,41 +190,41 @@ public enum Type {\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextDouble());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextDouble());\n         }\n     },\n \n     BOOLEAN(\"boolean\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(null);\n+        public Value drop(Value value) {\n+            return new Value(null);\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n-            return new Value<>(!Boolean.parseBoolean(value.asString()));\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n+            return new Value(!Boolean.parseBoolean(value.asString()));\n         }\n \n         @Override\n-        public List<double[]> encode(EncodingParams params, Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextBoolean());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextBoolean());\n         }\n     },\n \n     URI(\"uri\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(java.net.URI.create(\"\"));\n+        public Value drop(Value value) {\n+            return new Value(java.net.URI.create(\"\"));\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             String uriAsString = value.asString();\n             java.net.URI uri = java.net.URI.create(uriAsString);\n             String scheme = uri.getScheme();\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMwNjA5NQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r535306095", "body": "Magic number?", "bodyText": "Magic number?", "bodyHTML": "<p dir=\"auto\">Magic number?</p>", "author": "danielezonca", "createdAt": "2020-12-03T14:59:14Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -130,7 +130,7 @@\n     },\n \n     NUMBER(\"number\") {\n-        private static final double CLUSTER_THRESHOLD = 1e-3;\n+        private static final double CLUSTER_THRESHOLD = 1e-1;", "originalCommit": "3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4NDgwNw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540984807", "bodyText": "see comment above for SampleWeighter.SIGMA", "author": "tteofili", "createdAt": "2020-12-11T14:27:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMwNjA5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex d01e09ea6..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -109,41 +109,39 @@ public enum Type {\n                 for (int index : indexes) {\n                     copy[index] = 0;\n                 }\n-                return new Value<>(ByteBuffer.wrap(copy));\n+                return new Value(ByteBuffer.wrap(copy));\n             } else {\n                 ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value<>(byteBuffer);\n+                return new Value(byteBuffer);\n             }\n         }\n \n         @Override\n-        public List<double[]> encode(Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n+        public Value randomValue(PerturbationContext perturbationContext) {\n             byte[] bytes = new byte[8];\n             perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value<>(ByteBuffer.wrap(bytes));\n+            return new Value(ByteBuffer.wrap(bytes));\n         }\n     },\n \n     NUMBER(\"number\") {\n-        private static final double CLUSTER_THRESHOLD = 1e-1;\n-\n         @Override\n-        public Value<?> drop(Value<?> value) {\n+        public Value drop(Value value) {\n             if (value.asNumber() == 0) {\n-                value = new Value<>(Double.NaN);\n+                value = new Value(Double.NaN);\n             } else {\n-                value = new Value<>(0d);\n+                value = new Value(0d);\n             }\n             return value;\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             double originalFeatureValue = value.asNumber();\n             boolean intValue = originalFeatureValue % 1 == 0;\n \n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkwNTE4NA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r535905184", "body": "I see the same \"pattern\" in all the tests (not only PMML but all the others too) with only number of samples, topK and positive/negative stability score as difference.\r\nDoes it make sense to move it to a common method?\r\nAre all of them magic numbers? Like why 300/500 samples? Why the last assertion is >= 0.5/0.8?", "bodyText": "I see the same \"pattern\" in all the tests (not only PMML but all the others too) with only number of samples, topK and positive/negative stability score as difference.\nDoes it make sense to move it to a common method?\nAre all of them magic numbers? Like why 300/500 samples? Why the last assertion is >= 0.5/0.8?", "bodyHTML": "<p dir=\"auto\">I see the same \"pattern\" in all the tests (not only PMML but all the others too) with only number of samples, topK and positive/negative stability score as difference.<br>\nDoes it make sense to move it to a common method?<br>\nAre all of them magic numbers? Like why 300/500 samples? Why the last assertion is &gt;= 0.5/0.8?</p>", "author": "danielezonca", "createdAt": "2020-12-04T08:00:34Z", "path": "explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java", "diffHunk": "@@ -131,16 +144,29 @@ void testPMMLRegressionCategorical() throws Exception {\n             }\n             return outputs;\n         });\n-        PredictionOutput output = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit())\n-                .get(0);\n+        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n+                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+        assertThat(predictionOutputs).isNotNull();\n+        assertThat(predictionOutputs).isNotEmpty();\n+        PredictionOutput output = predictionOutputs.get(0);\n+        assertThat(output).isNotNull();\n         Prediction prediction = new Prediction(input, output);\n         Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                 .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n         for (Saliency saliency : saliencyMap.values()) {\n-            assertNotNull(saliency);\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(1));\n-            assertEquals(1d, v);\n+            assertThat(saliency).isNotNull();\n+            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n+            assertThat(v).isEqualTo(1d);\n+        }\n+        int topK = 1;\n+        LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n+        for (int i = 1; i <= topK; i++) {\n+            for (String decision : stability.getDecisions()) {\n+                double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n+                double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n+                assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n+                assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);\n+            }", "originalCommit": "3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk5MjM4NQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540992385", "bodyText": "Sure it makes sense to have a common method in TestUtils, I'll do that.\nFor the specific settings: I've set a minimum expected stability rate to 0.5 for the models having more than 5 features (when linearized) including ones of Type.NUMBER since they're harder.\nI've set a high minimum (0.8) for OpenNLP ITs since the input only consists of 4 words that can only be set to empty String when perturbed, resulting in a low no. of possible perturbed inputs and hence an higher stability.", "author": "tteofili", "createdAt": "2020-12-11T14:37:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkwNTE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAzNTcxOQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r541035719", "bodyText": "a practical problem with such a test method is that we need to inject a test classified version of explainability-core into the IT modules, which I think it's a bit overkill given the fact that we just need a few lines of code to check the expected stability scores.", "author": "tteofili", "createdAt": "2020-12-11T15:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkwNTE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA3NzM5Mw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r541077393", "bodyText": "I've moved the boilerplate to validate local saliency stability in a ValidationUtils#validateLocalSaliencyStability method so that it can be re-used easily in ITs as well.", "author": "tteofili", "createdAt": "2020-12-11T16:39:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkwNTE4NA=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\nindex 25bcc2434..555059238 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n", "chunk": "@@ -158,16 +152,8 @@ class PmmlLimeExplainerTest {\n             double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n             assertThat(v).isEqualTo(1d);\n         }\n-        int topK = 1;\n-        LocalSaliencyStability stability = ExplainabilityMetrics.getLocalSaliencyStability(model, prediction, limeExplainer, topK, 10);\n-        for (int i = 1; i <= topK; i++) {\n-            for (String decision : stability.getDecisions()) {\n-                double positiveStabilityScore = stability.getPositiveStabilityScore(decision, i);\n-                double negativeStabilityScore = stability.getNegativeStabilityScore(decision, i);\n-                assertThat(positiveStabilityScore).isGreaterThanOrEqualTo(0.5);\n-                assertThat(negativeStabilityScore).isGreaterThanOrEqualTo(0.5);\n-            }\n-        }\n+        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n+                                                                                0.5, 0.5));\n     }\n \n     @Test\n", "next_change": {"commit": "cd5adebe96ccc6779dae7fce94d0e9abb8230e69", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\ndeleted file mode 100644\nindex 555059238..000000000\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-pmml/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/pmml/PmmlLimeExplainerTest.java\n+++ /dev/null\n", "chunk": "@@ -1,259 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.explainability.integrationtests.pmml;\n-\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.CompletableFuture;\n-\n-import org.junit.jupiter.api.BeforeAll;\n-import org.junit.jupiter.api.Test;\n-import org.kie.api.pmml.PMML4Result;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.lime.LimeConfig;\n-import org.kie.kogito.explainability.local.lime.LimeExplainer;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.kie.kogito.explainability.model.Value;\n-import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n-import org.kie.kogito.explainability.utils.ValidationUtils;\n-import org.kie.pmml.api.runtime.PMMLRuntime;\n-import org.kie.kogito.explainability.utils.LocalSaliencyStability;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.kie.kogito.explainability.explainability.integrationtests.pmml.AbstractPMMLTest.getPMMLRuntime;\n-import static org.kie.test.util.filesystem.FileUtils.getFile;\n-\n-class PmmlLimeExplainerTest {\n-\n-    private static PMMLRuntime logisticRegressionIrisRuntime;\n-    private static PMMLRuntime categoricalVariableRegressionRuntime;\n-    private static PMMLRuntime scorecardCategoricalRuntime;\n-    private static PMMLRuntime compoundScoreCardRuntime;\n-\n-    @BeforeAll\n-    static void setUpBefore() {\n-        logisticRegressionIrisRuntime = getPMMLRuntime(getFile(\"logisticRegressionIrisData.pmml\"));\n-        categoricalVariableRegressionRuntime = getPMMLRuntime(getFile(\"categoricalVariablesRegression.pmml\"));\n-        scorecardCategoricalRuntime = getPMMLRuntime(getFile(\"SimpleScorecardCategorical.pmml\"));\n-        compoundScoreCardRuntime = getPMMLRuntime(getFile(\"CompoundNestedPredicateScorecard.pmml\"));\n-    }\n-\n-    @Test\n-    void testPMMLRegression() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(1000)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalLength\", 6.9));\n-            features.add(FeatureFactory.newNumericalFeature(\"sepalWidth\", 3.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalLength\", 5.1));\n-            features.add(FeatureFactory.newNumericalFeature(\"petalWidth\", 2.3));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    LogisticRegressionIrisDataExecutor pmmlModel = new LogisticRegressionIrisDataExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asNumber(),\n-                            features1.get(2).getValue().asNumber(), features1.get(3).getValue().asNumber());\n-                    PMML4Result result = pmmlModel.execute(logisticRegressionIrisRuntime);\n-                    String species = result.getResultVariables().get(\"Species\").toString();\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"species\", Type.TEXT, new Value<>(species), 1d)));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-\n-    @Test\n-    void testPMMLRegressionCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapX\", \"red\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"mapY\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(500)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                CategoricalVariablesRegressionExecutor pmmlModel = new CategoricalVariablesRegressionExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(categoricalVariableRegressionRuntime);\n-                String score = result.getResultVariables().get(\"result\").toString();\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(new Output(\"result\", Type.NUMBER, new Value<>(score), 1d)));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isEqualTo(1d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLScorecardCategorical() throws Exception {\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCategoricalFeature(\"input1\", \"classA\"));\n-        features.add(FeatureFactory.newCategoricalFeature(\"input2\", \"classB\"));\n-        PredictionInput input = new PredictionInput(features);\n-\n-        Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(300)\n-                .withPerturbationContext(new PerturbationContext(random, 1));\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-        PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-            List<PredictionOutput> outputs = new LinkedList<>();\n-            for (PredictionInput input1 : inputs) {\n-                List<Feature> features1 = input1.getFeatures();\n-                SimpleScorecardCategoricalExecutor pmmlModel = new SimpleScorecardCategoricalExecutor(\n-                        features1.get(0).getValue().asString(), features1.get(1).getValue().asString());\n-                PMML4Result result = pmmlModel.execute(scorecardCategoricalRuntime);\n-                String score = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.TARGET_FIELD);\n-                String reason1 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE1_FIELD);\n-                String reason2 = \"\" + result.getResultVariables().get(SimpleScorecardCategoricalExecutor.REASON_CODE2_FIELD);\n-                PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                        new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                        new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d),\n-                        new Output(\"reason2\", Type.TEXT, new Value<>(reason2), 1d)\n-                ));\n-                outputs.add(predictionOutput);\n-            }\n-            return outputs;\n-        });\n-\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        assertThat(predictionOutputs).isNotNull();\n-        assertThat(predictionOutputs).isNotEmpty();\n-        PredictionOutput output = predictionOutputs.get(0);\n-        assertThat(output).isNotNull();\n-        Prediction prediction = new Prediction(input, output);\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertThat(saliency).isNotNull();\n-            double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-            assertThat(v).isGreaterThan(0d);\n-        }\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                0.5, 0.5));\n-    }\n-\n-    @Test\n-    void testPMMLCompoundScorecard() throws Exception {\n-        Random random = new Random();\n-        for (int seed = 0; seed < 5; seed++) {\n-            random.setSeed(seed);\n-            LimeConfig limeConfig = new LimeConfig()\n-                    .withSamples(300)\n-                    .withPerturbationContext(new PerturbationContext(random, 1));\n-            LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-            List<Feature> features = new LinkedList<>();\n-            features.add(FeatureFactory.newNumericalFeature(\"input1\", -50));\n-            features.add(FeatureFactory.newTextFeature(\"input2\", \"classB\"));\n-            PredictionInput input = new PredictionInput(features);\n-\n-            PredictionProvider model = inputs -> CompletableFuture.supplyAsync(() -> {\n-                List<PredictionOutput> outputs = new LinkedList<>();\n-                for (PredictionInput input1 : inputs) {\n-                    List<Feature> features1 = input1.getFeatures();\n-                    CompoundNestedPredicateScorecardExecutor pmmlModel = new CompoundNestedPredicateScorecardExecutor(\n-                            features1.get(0).getValue().asNumber(), features1.get(1).getValue().asString());\n-                    PMML4Result result = pmmlModel.execute(compoundScoreCardRuntime);\n-                    String score = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.TARGET_FIELD);\n-                    String reason1 = \"\" + result.getResultVariables().get(CompoundNestedPredicateScorecardExecutor.REASON_CODE1_FIELD);\n-                    PredictionOutput predictionOutput = new PredictionOutput(List.of(\n-                            new Output(\"score\", Type.TEXT, new Value<>(score), 1d),\n-                            new Output(\"reason1\", Type.TEXT, new Value<>(reason1), 1d)\n-                    ));\n-                    outputs.add(predictionOutput);\n-                }\n-                return outputs;\n-            });\n-            List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(input))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            assertThat(predictionOutputs).isNotNull();\n-            assertThat(predictionOutputs).isNotEmpty();\n-            PredictionOutput output = predictionOutputs.get(0);\n-            assertThat(output).isNotNull();\n-            Prediction prediction = new Prediction(input, output);\n-            Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Saliency saliency : saliencyMap.values()) {\n-                assertThat(saliency).isNotNull();\n-                double v = ExplainabilityMetrics.impactScore(model, prediction, saliency.getTopFeatures(2));\n-                assertThat(v).isEqualTo(1d);\n-            }\n-            assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                                                                                    0.5, 0.5));\n-        }\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk0MjM1OA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540942358", "body": "Why fixed value?", "bodyText": "Why fixed value?", "bodyHTML": "<p dir=\"auto\">Why fixed value?</p>", "author": "danielezonca", "createdAt": "2020-12-11T13:21:49Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -164,20 +165,28 @@\n         @Override\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n-            double[] doubles = new double[values.length];\n+            double[] doubles = new double[values.length + 1];\n             int i = 0;\n             for (Value<?> v : values) {\n                 doubles[i] = v.asNumber();\n                 i++;\n             }\n             double originalValue = target.asNumber();\n+            doubles[i] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-            // feature scaling + kernel based clustering\n-            double threshold = DataUtils.gaussianKernel((originalValue - min) / (max - min), 0, 1);\n-            List<Double> encodedValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min))\n-                    .map(d -> Double.isNaN(d) ? 1 : d).boxed().map(d -> DataUtils.gaussianKernel(d, 0, 1))\n-                    .map(d -> (d - threshold < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+\n+            // feature scaling\n+            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n+            double scaledOriginalValue = scaledValues.remove(i); // extract the scaled original value (it must not appear in encoded values)\n+\n+            // kernel based clustering\n+            double sigma = 1;", "originalCommit": "3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk5OTk4OQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540999989", "bodyText": "this has not changed, previously the gaussian filter was centered in 0 with a stdDev of 1, now its centered around the original value with the same stdDev (as per PR description).\nthis way the encoding mechanism sets to 1 those scaled numbers that fall close to the center of the gaussian (= the original feature value).", "author": "tteofili", "createdAt": "2020-12-11T14:48:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk0MjM1OA=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex d01e09ea6..dadf67e41 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -166,25 +166,25 @@ public enum Type {\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n             double[] doubles = new double[values.length + 1];\n-            int i = 0;\n+            int valueIndex = 0;\n             for (Value<?> v : values) {\n-                doubles[i] = v.asNumber();\n-                i++;\n+                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n+                valueIndex++;\n             }\n-            double originalValue = target.asNumber();\n-            doubles[i] = originalValue; // include target number in feature scaling\n+            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n+            doubles[valueIndex] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n \n             // feature scaling\n             List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(i); // extract the scaled original value (it must not appear in encoded values)\n+            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n             double sigma = 1;\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> Double.isNaN(d) ? 0 : d).map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n+                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n \n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex dadf67e41..3b78c5c2b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -181,12 +181,12 @@ public enum Type {\n             double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n-            double sigma = 1;\n+            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n                     .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n             return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n         }\n", "next_change": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 3b78c5c2b..b9761f367 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -188,7 +188,7 @@ public enum Type {\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n-            return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n+            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n         }\n \n         @Override\n", "next_change": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex b9761f367..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -192,41 +190,41 @@ public enum Type {\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextDouble());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextDouble());\n         }\n     },\n \n     BOOLEAN(\"boolean\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(null);\n+        public Value drop(Value value) {\n+            return new Value(null);\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n-            return new Value<>(!Boolean.parseBoolean(value.asString()));\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n+            return new Value(!Boolean.parseBoolean(value.asString()));\n         }\n \n         @Override\n-        public List<double[]> encode(EncodingParams params, Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextBoolean());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextBoolean());\n         }\n     },\n \n     URI(\"uri\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(java.net.URI.create(\"\"));\n+        public Value drop(Value value) {\n+            return new Value(java.net.URI.create(\"\"));\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             String uriAsString = value.asString();\n             java.net.URI uri = java.net.URI.create(uriAsString);\n             String scheme = uri.getScheme();\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk0Mzk2MA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540943960", "body": "What about make both of them `warn` or `debug`?", "bodyText": "What about make both of them warn or debug?", "bodyHTML": "<p dir=\"auto\">What about make both of them <code>warn</code> or <code>debug</code>?</p>", "author": "danielezonca", "createdAt": "2020-12-11T13:24:33Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java", "diffHunk": "@@ -132,4 +139,102 @@ public static double classificationFidelity(List<Pair<Saliency, Prediction>> pai\n         }\n         return evals == 0 ? 0 : acc / evals;\n     }\n+\n+    /**\n+     * Evaluate stability of a local explainer generating {@code Saliencies}.\n+     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n+     * positive/negative features always the same for a single prediction?\".\n+     *\n+     * @param model                  a model to explain\n+     * @param prediction             the prediction on which explanation stability will be evaluated\n+     * @param saliencyLocalExplainer a local saliency explainer\n+     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n+     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n+     */\n+    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n+                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+                                                                   int topK, int runs)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n+\n+        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n+        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n+        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n+            for (int k = 1; k <= topK; k++) {\n+                String decision = entry.getKey();\n+                List<Saliency> perDecisionSaliencies = entry.getValue();\n+\n+                int finalK = k;\n+                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n+                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n+                // get the most frequent list of positive features\n+                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n+                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n+\n+                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n+                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n+                // get the most frequent list of negative features\n+                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n+                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n+\n+                // decision stability at k\n+                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n+                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n+                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n+            }\n+        }\n+        return saliencyStability;\n+    }\n+\n+    /**\n+     * Get multiple saliencies, aggregated by decision name.\n+     *\n+     * @param model                  the model used to perform predictions\n+     * @param prediction             the prediction to explain\n+     * @param saliencyLocalExplainer a local explainer that generates saliences\n+     * @param runs                   the no. of explanations to be generated\n+     * @return the generated saliencies, aggregated by decision name, across the different runs\n+     */\n+    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n+                                                                     LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n+                                                                     int runs)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<String, List<Saliency>> saliencies = new HashMap<>();\n+        int skipped = 0;\n+        for (int i = 0; i < runs; i++) {\n+            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n+                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n+            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n+                // aggregate saliencies by output name\n+                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n+                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n+                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n+                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n+                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n+                        updatedSaliencies.add(saliencyEntry.getValue());\n+                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n+                    } else {\n+                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n+                    }\n+                } else {\n+                    LOGGER.warn(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n+                    skipped++;\n+                }\n+            }\n+        }\n+        LOGGER.debug(\"skipped {} useless saliencies\", skipped);", "originalCommit": "3580d487a5dd0f45d40be5a8886a4ee68a30c10e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk5MjczNQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r540992735", "bodyText": "ok", "author": "tteofili", "createdAt": "2020-12-11T14:37:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk0Mzk2MA=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\nindex d508f860a..d935aa669 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n", "chunk": "@@ -217,7 +217,7 @@ public class ExplainabilityMetrics {\n                         saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n                     }\n                 } else {\n-                    LOGGER.warn(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n+                    LOGGER.debug(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n                     skipped++;\n                 }\n             }\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\ndeleted file mode 100644\nindex d935aa669..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/utils/ExplainabilityMetrics.java\n+++ /dev/null\n", "chunk": "@@ -1,240 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.utils;\n-\n-import org.apache.commons.lang3.tuple.Pair;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.LocalExplainer;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureImportance;\n-import org.kie.kogito.explainability.model.Output;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.Type;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Utility class providing different methods to evaluate explainability.\n- */\n-public class ExplainabilityMetrics {\n-\n-    private static final Logger LOGGER = LoggerFactory.getLogger(ExplainabilityMetrics.class);\n-\n-    /**\n-     * Drop in confidence score threshold for impact score calculation.\n-     * Confidence scores below {@code originalScore * CONFIDENCE_DROP_RATIO} are considered impactful for a model.\n-     */\n-    private static final double CONFIDENCE_DROP_RATIO = 0.2d;\n-\n-    private ExplainabilityMetrics() {\n-    }\n-\n-    /**\n-     * Measure the explainability of an explanation.\n-     * See paper: \"Towards Quantification of Explainability in Explainable Artificial Intelligence Methods\" by Islam et al.\n-     *\n-     * @param inputCognitiveChunks  the no. of cognitive chunks (pieces of information) required to generate the\n-     *                              explanation (e.g. the no. of explanation inputs)\n-     * @param outputCognitiveChunks the no. of cognitive chunks generated within the explanation itself\n-     * @param interactionRatio      the ratio of interaction (between 0 and 1) required by the explanation\n-     * @return the quantitative explainability measure\n-     */\n-    public static double quantifyExplainability(int inputCognitiveChunks, int outputCognitiveChunks, double interactionRatio) {\n-        return inputCognitiveChunks + outputCognitiveChunks > 0 ? 0.333 / (double) inputCognitiveChunks\n-                + 0.333 / (double) outputCognitiveChunks + 0.333 * (1d - interactionRatio) : 0;\n-    }\n-\n-    /**\n-     * Calculate the impact of dropping the most important features (given by {@link Saliency#getTopFeatures(int)} from the input.\n-     * Highly important features would have rather high impact.\n-     * See paper: Qiu Lin, Zhong, et al. \"Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the\n-     * Performance of Explainability Algorithms.\" 2019.\n-     *\n-     * @param model       the model to be explained\n-     * @param prediction  a prediction\n-     * @param topFeatures the list of important features that should be dropped\n-     * @return the saliency impact\n-     */\n-    public static double impactScore(PredictionProvider model, Prediction prediction, List<FeatureImportance> topFeatures) throws InterruptedException, ExecutionException, TimeoutException {\n-        List<Feature> copy = List.copyOf(prediction.getInput().getFeatures());\n-        for (FeatureImportance featureImportance : topFeatures) {\n-            copy = DataUtils.dropFeature(copy, featureImportance.getFeature());\n-        }\n-\n-        PredictionInput predictionInput = new PredictionInput(copy);\n-        List<PredictionOutput> predictionOutputs;\n-        try {\n-            predictionOutputs = model.predictAsync(List.of(predictionInput))\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        } catch (InterruptedException | ExecutionException | TimeoutException e) {\n-            LOGGER.error(\"Impossible to obtain prediction {}\", e.getMessage());\n-            throw e;\n-        }\n-        double impact = 0d;\n-        for (PredictionOutput predictionOutput : predictionOutputs) {\n-            double size = predictionOutput.getOutputs().size();\n-            for (int i = 0; i < size; i++) {\n-                Output original = prediction.getOutput().getOutputs().get(i);\n-                Output modified = predictionOutput.getOutputs().get(i);\n-                impact += (!original.getValue().asString().equals(modified.getValue().asString())\n-                        || modified.getScore() < original.getScore() * CONFIDENCE_DROP_RATIO) ? 1d / size : 0d;\n-            }\n-        }\n-        return impact;\n-    }\n-\n-    /**\n-     * Calculate fidelity (accuracy) of boolean classification outputs using saliency predictor function = sign(sum(saliency.scores))\n-     * See papers:\n-     * - Guidotti Riccardo, et al. \"A survey of methods for explaining black box models.\" ACM computing surveys (2018).\n-     * - Bodria, Francesco, et al. \"Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis (Discussion Paper).\"\n-     *\n-     * @param pairs pairs composed by the saliency and the related prediction\n-     * @return the fidelity accuracy\n-     */\n-    public static double classificationFidelity(List<Pair<Saliency, Prediction>> pairs) {\n-        double acc = 0;\n-        double evals = 0;\n-        for (Pair<Saliency, Prediction> pair : pairs) {\n-            Saliency saliency = pair.getLeft();\n-            Prediction prediction = pair.getRight();\n-            for (Output output : prediction.getOutput().getOutputs()) {\n-                Type type = output.getType();\n-                if (Type.BOOLEAN.equals(type)) {\n-                    double predictorOutput = saliency.getPerFeatureImportance().stream().map(FeatureImportance::getScore).mapToDouble(d -> d).sum();\n-                    double v = output.getValue().asNumber();\n-                    if ((v >= 0 && predictorOutput >= 0) || (v < 0 && predictorOutput < 0)) {\n-                        acc++;\n-                    }\n-                    evals++;\n-                }\n-            }\n-        }\n-        return evals == 0 ? 0 : acc / evals;\n-    }\n-\n-    /**\n-     * Evaluate stability of a local explainer generating {@code Saliencies}.\n-     * Such an evaluation is intended to measure how stable the explanations are in terms of \"are the top k most important\n-     * positive/negative features always the same for a single prediction?\".\n-     *\n-     * @param model                  a model to explain\n-     * @param prediction             the prediction on which explanation stability will be evaluated\n-     * @param saliencyLocalExplainer a local saliency explainer\n-     * @param topK                   no. of top k positive/negative features for which stability report will be generated\n-     * @return a report about stability of all the decisions/predictions (and for each {@code k < topK})\n-     */\n-    public static LocalSaliencyStability getLocalSaliencyStability(PredictionProvider model, Prediction prediction,\n-                                                                   LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-                                                                   int topK, int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = getMultipleSaliencies(model, prediction, saliencyLocalExplainer, runs);\n-\n-        LocalSaliencyStability saliencyStability = new LocalSaliencyStability(saliencies.keySet());\n-        // for each decision, calculate the stability rate for the top k important feature set, for each k < topK\n-        for (Map.Entry<String, List<Saliency>> entry : saliencies.entrySet()) {\n-            for (int k = 1; k <= topK; k++) {\n-                String decision = entry.getKey();\n-                List<Saliency> perDecisionSaliencies = entry.getValue();\n-\n-                int finalK = k;\n-                // get the top k positive features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKPositive = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getPositiveFeatures(finalK));\n-                // get the most frequent list of positive features\n-                Pair<List<String>, Long> positiveMostFrequent = getMostFrequent(topKPositive);\n-                double positiveFrequencyRate = (double) positiveMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // get the top k negative features list from each saliency and count the frequency of each such list across all saliencies\n-                Map<List<String>, Long> topKNegative = getTopKFeaturesFrequency(perDecisionSaliencies, s -> s.getNegativeFeatures(finalK));\n-                // get the most frequent list of negative features\n-                Pair<List<String>, Long> negativeMostFrequent = getMostFrequent(topKNegative);\n-                double negativeFrequencyRate = (double) negativeMostFrequent.getValue() / (double) perDecisionSaliencies.size();\n-\n-                // decision stability at k\n-                List<String> positiveFeatureNames = positiveMostFrequent.getKey();\n-                List<String> negativeFeatureNames = negativeMostFrequent.getKey();\n-                saliencyStability.add(decision, k, positiveFeatureNames, positiveFrequencyRate, negativeFeatureNames, negativeFrequencyRate);\n-            }\n-        }\n-        return saliencyStability;\n-    }\n-\n-    /**\n-     * Get multiple saliencies, aggregated by decision name.\n-     *\n-     * @param model                  the model used to perform predictions\n-     * @param prediction             the prediction to explain\n-     * @param saliencyLocalExplainer a local explainer that generates saliences\n-     * @param runs                   the no. of explanations to be generated\n-     * @return the generated saliencies, aggregated by decision name, across the different runs\n-     */\n-    private static Map<String, List<Saliency>> getMultipleSaliencies(PredictionProvider model, Prediction prediction,\n-                                                                     LocalExplainer<Map<String, Saliency>> saliencyLocalExplainer,\n-                                                                     int runs)\n-            throws InterruptedException, ExecutionException, TimeoutException {\n-        Map<String, List<Saliency>> saliencies = new HashMap<>();\n-        int skipped = 0;\n-        for (int i = 0; i < runs; i++) {\n-            Map<String, Saliency> saliencyMap = saliencyLocalExplainer.explainAsync(prediction, model)\n-                    .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-            for (Map.Entry<String, Saliency> saliencyEntry : saliencyMap.entrySet()) {\n-                // aggregate saliencies by output name\n-                List<FeatureImportance> topFeatures = saliencyEntry.getValue().getTopFeatures(1);\n-                if (!topFeatures.isEmpty() && topFeatures.get(0).getScore() != 0) { // skip empty or 0 valued saliencies\n-                    if (saliencies.containsKey(saliencyEntry.getKey())) {\n-                        List<Saliency> localSaliencies = saliencies.get(saliencyEntry.getKey());\n-                        List<Saliency> updatedSaliencies = new ArrayList<>(localSaliencies);\n-                        updatedSaliencies.add(saliencyEntry.getValue());\n-                        saliencies.put(saliencyEntry.getKey(), updatedSaliencies);\n-                    } else {\n-                        saliencies.put(saliencyEntry.getKey(), List.of(saliencyEntry.getValue()));\n-                    }\n-                } else {\n-                    LOGGER.debug(\"skipping empty / zero saliency for {}\", saliencyEntry.getKey());\n-                    skipped++;\n-                }\n-            }\n-        }\n-        LOGGER.debug(\"skipped {} useless saliencies\", skipped);\n-        return saliencies;\n-    }\n-\n-    private static Map<List<String>, Long> getTopKFeaturesFrequency(List<Saliency> saliencies, Function<Saliency, List<FeatureImportance>> saliencyListFunction) {\n-        return saliencies.stream().map(saliencyListFunction)\n-                .map(l -> l.stream().map(f -> f.getFeature().getName())\n-                        .collect(Collectors.toList()))\n-                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));\n-    }\n-\n-    private static Pair<List<String>, Long> getMostFrequent(Map<List<String>, Long> collect) {\n-        Map.Entry<List<String>, Long> maxEntry = Collections.max(collect.entrySet(), Map.Entry.comparingByValue());\n-        return Pair.of(maxEntry.getKey(), maxEntry.getValue());\n-    }\n-}\n", "next_change": null}]}}]}}, {"oid": "b59a83e1c4e624334d9e8616ecfb9a538d7d8975", "url": "https://github.com/kiegroup/kogito-apps/commit/b59a83e1c4e624334d9e8616ecfb9a538d7d8975", "message": "KOGITO-3763 - consistent debug level in metrics", "committedDate": "2020-12-11T14:42:15Z", "type": "commit"}, {"oid": "53696c9d78ef55cb9ae5a76c354ab4db9311cf8d", "url": "https://github.com/kiegroup/kogito-apps/commit/53696c9d78ef55cb9ae5a76c354ab4db9311cf8d", "message": "KOGITO-3763 - renamed index", "committedDate": "2020-12-11T14:58:56Z", "type": "commit"}, {"oid": "6713f89de9a5f3718465b8e66d2571ff208712b0", "url": "https://github.com/kiegroup/kogito-apps/commit/6713f89de9a5f3718465b8e66d2571ff208712b0", "message": "KOGITO-3763 - checking stability on dummy models", "committedDate": "2020-12-11T15:40:00Z", "type": "commit"}, {"oid": "4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "url": "https://github.com/kiegroup/kogito-apps/commit/4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "message": "KOGITO-3763 - added validation utils to validate local saliency stability", "committedDate": "2020-12-11T16:37:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM2OTE2MQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542369161", "body": "Would it be possible make \"0.01\" a constant and move it somewhere up to be more visible. I think it is better to have constants visible rather than hidden inside mothods etc.", "bodyText": "Would it be possible make \"0.01\" a constant and move it somewhere up to be more visible. I think it is better to have constants visible rather than hidden inside mothods etc.", "bodyHTML": "<p dir=\"auto\">Would it be possible make \"0.01\" a constant and move it somewhere up to be more visible. I think it is better to have constants visible rather than hidden inside mothods etc.</p>", "author": "jiripetrlik", "createdAt": "2020-12-14T13:08:01Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -150,12 +150,13 @@\n             // sample from a standard normal distribution and center around feature value\n             double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n             if (originalFeatureValue != 0d) {\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + originalFeatureValue;\n+                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value", "originalCommit": "4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQzMDgwMA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542430800", "bodyText": "I am thinking of placing all those parameters inside LimeConfig so that they are centralized, configurable and visible frome one place. I will make a new PR for that.", "author": "tteofili", "createdAt": "2020-12-14T14:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM2OTE2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQzMzE1MQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542433151", "bodyText": "here's the Jira issue: https://issues.redhat.com/browse/FAI-342", "author": "tteofili", "createdAt": "2020-12-14T14:37:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM2OTE2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 1cf2c5f0f..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -109,41 +109,39 @@ public enum Type {\n                 for (int index : indexes) {\n                     copy[index] = 0;\n                 }\n-                return new Value<>(ByteBuffer.wrap(copy));\n+                return new Value(ByteBuffer.wrap(copy));\n             } else {\n                 ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value<>(byteBuffer);\n+                return new Value(byteBuffer);\n             }\n         }\n \n         @Override\n-        public List<double[]> encode(Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n+        public Value randomValue(PerturbationContext perturbationContext) {\n             byte[] bytes = new byte[8];\n             perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value<>(ByteBuffer.wrap(bytes));\n+            return new Value(ByteBuffer.wrap(bytes));\n         }\n     },\n \n     NUMBER(\"number\") {\n-        private static final double CLUSTER_THRESHOLD = 1e-1;\n-\n         @Override\n-        public Value<?> drop(Value<?> value) {\n+        public Value drop(Value value) {\n             if (value.asNumber() == 0) {\n-                value = new Value<>(Double.NaN);\n+                value = new Value(Double.NaN);\n             } else {\n-                value = new Value<>(0d);\n+                value = new Value(0d);\n             }\n             return value;\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             double originalFeatureValue = value.asNumber();\n             boolean intValue = originalFeatureValue % 1 == 0;\n \n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3MDczNw==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542370737", "body": "Same here. Can we make \"1d\" a constant and move it to more visible place?", "bodyText": "Same here. Can we make \"1d\" a constant and move it to more visible place?", "bodyHTML": "<p dir=\"auto\">Same here. Can we make \"1d\" a constant and move it to more visible place?</p>", "author": "jiripetrlik", "createdAt": "2020-12-14T13:10:32Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -150,12 +150,13 @@\n             // sample from a standard normal distribution and center around feature value\n             double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n             if (originalFeatureValue != 0d) {\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + originalFeatureValue;\n+                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n+                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n             }\n             if (intValue) {\n                 normalDistributionSample = (int) normalDistributionSample;\n                 if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample * 10d;\n+                    normalDistributionSample = (int) normalDistributionSample + 1d;", "originalCommit": "4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQzMzQ4MA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542433480", "bodyText": "I'd resolve this also as part of https://issues.redhat.com/browse/FAI-342", "author": "tteofili", "createdAt": "2020-12-14T14:37:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3MDczNw=="}], "type": "inlineReview", "revised_code": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 1cf2c5f0f..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -159,19 +157,19 @@ public enum Type {\n                     normalDistributionSample = (int) normalDistributionSample + 1d;\n                 }\n             }\n-            return new Value<>(normalDistributionSample);\n+            return new Value(normalDistributionSample);\n         }\n \n         @Override\n-        public List<double[]> encode(Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             // find maximum and minimum values\n             double[] doubles = new double[values.length + 1];\n             int valueIndex = 0;\n-            for (Value<?> v : values) {\n-                doubles[valueIndex] = v.asNumber();\n+            for (Value v : values) {\n+                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n                 valueIndex++;\n             }\n-            double originalValue = target.asNumber();\n+            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n             doubles[valueIndex] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n", "next_change": {"commit": "6f8c7aba45fa7dcbc868966517200a16243bbbfa", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex ce469e18e..b2d4fd595 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -171,11 +171,12 @@ public enum Type {\n             }\n             double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n             doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n \n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n+            double mean = DataUtils.getMean(doubles);\n+            double stdDev = DataUtils.getStdDev(doubles, mean);\n+\n+            // feature standardization\n+            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - mean) / stdDev).boxed().collect(Collectors.toList());\n             double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex b2d4fd595..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,581 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-\n-            double mean = DataUtils.getMean(doubles);\n-            double stdDev = DataUtils.getStdDev(doubles, mean);\n-\n-            // feature standardization\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - mean) / stdDev).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ2ODkyNA==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542468924", "body": "just wondering.. if `Double.isNan(d)` is true the previous call at line 180\r\n```\r\n            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\r\n```\r\nshould have crashed right? ", "bodyText": "just wondering.. if Double.isNan(d) is true the previous call at line 180\n            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n\nshould have crashed right?", "bodyHTML": "<p dir=\"auto\">just wondering.. if <code>Double.isNan(d)</code> is true the previous call at line 180</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"            List&lt;Double&gt; scaledValues = DoubleStream.of(doubles).map(d -&gt; (d - min) / (max - min)).boxed().collect(Collectors.toList());\"><pre><code>            List&lt;Double&gt; scaledValues = DoubleStream.of(doubles).map(d -&gt; (d - min) / (max - min)).boxed().collect(Collectors.toList());\n</code></pre></div>\n<p dir=\"auto\">should have crashed right?</p>", "author": "r00ta", "createdAt": "2020-12-14T15:22:33Z", "path": "explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java", "diffHunk": "@@ -164,20 +165,28 @@\n         @Override\n         public List<double[]> encode(Value<?> target, Value<?>... values) {\n             // find maximum and minimum values\n-            double[] doubles = new double[values.length];\n-            int i = 0;\n+            double[] doubles = new double[values.length + 1];\n+            int valueIndex = 0;\n             for (Value<?> v : values) {\n-                doubles[i] = v.asNumber();\n-                i++;\n+                doubles[valueIndex] = v.asNumber();\n+                valueIndex++;\n             }\n             double originalValue = target.asNumber();\n+            doubles[valueIndex] = originalValue; // include target number in feature scaling\n             double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n             double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-            // feature scaling + kernel based clustering\n-            double threshold = DataUtils.gaussianKernel((originalValue - min) / (max - min), 0, 1);\n-            List<Double> encodedValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min))\n-                    .map(d -> Double.isNaN(d) ? 1 : d).boxed().map(d -> DataUtils.gaussianKernel(d, 0, 1))\n-                    .map(d -> (d - threshold < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+\n+            // feature scaling\n+            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n+            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n+\n+            // kernel based clustering\n+            double sigma = 1;\n+            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n+            List<Double> clusteredValues = scaledValues.stream()\n+                    .map(d -> Double.isNaN(d) ? 0 : d).map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());", "originalCommit": "4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjU0MTUzMg==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542541532", "bodyText": "good catch, it doesn't actually catch but the fact that there's a NaN makes all the values become NaN too in the feature scaling.\nI've fixed it and added a test to cover this case.", "author": "tteofili", "createdAt": "2020-12-14T16:52:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ2ODkyNA=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 1cf2c5f0f..dadf67e41 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -184,7 +184,7 @@ public enum Type {\n             double sigma = 1;\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> Double.isNaN(d) ? 0 : d).map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n+                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n \n", "next_change": {"commit": "1c1b5896acf08f4e83e326b09425c1d7a6a008ae", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex dadf67e41..3b78c5c2b 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -181,12 +181,12 @@ public enum Type {\n             double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n \n             // kernel based clustering\n-            double sigma = 1;\n+            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n             double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n             List<Double> clusteredValues = scaledValues.stream()\n                     .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n             List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < CLUSTER_THRESHOLD) ? 1d : 0d).collect(Collectors.toList());\n+                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n             return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n         }\n", "next_change": {"commit": "8e04ea41dcc99df454fa8dcc958ee64618f8d51d", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex 3b78c5c2b..b9761f367 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -188,7 +188,7 @@ public enum Type {\n             List<Double> encodedValues = clusteredValues.stream()\n                     .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n \n-            return encodedValues.stream().map(d -> new double[]{d}).collect(Collectors.toList());\n+            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n         }\n \n         @Override\n", "next_change": {"commit": "1aa10f7b448297891963cfa722cc027d2318e499", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\nindex b9761f367..ce469e18e 100644\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n", "chunk": "@@ -192,41 +190,41 @@ public enum Type {\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextDouble());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextDouble());\n         }\n     },\n \n     BOOLEAN(\"boolean\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(null);\n+        public Value drop(Value value) {\n+            return new Value(null);\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n-            return new Value<>(!Boolean.parseBoolean(value.asString()));\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n+            return new Value(!Boolean.parseBoolean(value.asString()));\n         }\n \n         @Override\n-        public List<double[]> encode(EncodingParams params, Value<?> target, Value<?>... values) {\n+        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n             return encodeEquals(target, values);\n         }\n \n         @Override\n-        public Value<?> randomValue(PerturbationContext perturbationContext) {\n-            return new Value<>(perturbationContext.getRandom().nextBoolean());\n+        public Value randomValue(PerturbationContext perturbationContext) {\n+            return new Value(perturbationContext.getRandom().nextBoolean());\n         }\n     },\n \n     URI(\"uri\") {\n         @Override\n-        public Value<?> drop(Value<?> value) {\n-            return new Value<>(java.net.URI.create(\"\"));\n+        public Value drop(Value value) {\n+            return new Value(java.net.URI.create(\"\"));\n         }\n \n         @Override\n-        public Value<?> perturb(Value<?> value, PerturbationContext perturbationContext) {\n+        public Value perturb(Value value, PerturbationContext perturbationContext) {\n             String uriAsString = value.asString();\n             java.net.URI uri = java.net.URI.create(uriAsString);\n             String scheme = uri.getScheme();\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java b/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\ndeleted file mode 100644\nindex ce469e18e..000000000\n--- a/explainability/explainability-core/src/main/java/org/kie/kogito/explainability/model/Type.java\n+++ /dev/null\n", "chunk": "@@ -1,580 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.model;\n-\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.ByteBuffer;\n-import java.time.DateTimeException;\n-import java.time.Duration;\n-import java.time.LocalTime;\n-import java.time.format.DateTimeParseException;\n-import java.time.temporal.ChronoUnit;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Currency;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.DoubleStream;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-\n-/**\n- * Allowed data types.\n- */\n-public enum Type {\n-\n-    TEXT(\"text\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(randomString(perturbationContext.getRandom()));\n-        }\n-    },\n-\n-    CATEGORICAL(\"categorical\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(\"\");\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String category = value.asString();\n-            if (!\"0\".equals(category)) {\n-                category = \"0\";\n-            } else {\n-                category = \"1\";\n-            }\n-            return new Value(category);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(String.valueOf(perturbationContext.getRandom().nextInt(4)));\n-        }\n-    },\n-\n-    BINARY(\"binary\") {\n-        @Override\n-        public Value drop(Value value) {\n-            ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-            return new Value(byteBuffer);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof ByteBuffer) {\n-                ByteBuffer currentBuffer = (ByteBuffer) value.getUnderlyingObject();\n-                byte[] copy = new byte[currentBuffer.array().length];\n-                int maxPerturbationSize = Math.min(copy.length, Math.max((int) (copy.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                System.arraycopy(currentBuffer.array(), 0, copy, 0, currentBuffer.array().length);\n-                int[] indexes = perturbationContext.getRandom().ints(0, copy.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int index : indexes) {\n-                    copy[index] = 0;\n-                }\n-                return new Value(ByteBuffer.wrap(copy));\n-            } else {\n-                ByteBuffer byteBuffer = ByteBuffer.allocate(0);\n-                return new Value(byteBuffer);\n-            }\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            byte[] bytes = new byte[8];\n-            perturbationContext.getRandom().nextBytes(bytes);\n-            return new Value(ByteBuffer.wrap(bytes));\n-        }\n-    },\n-\n-    NUMBER(\"number\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.asNumber() == 0) {\n-                value = new Value(Double.NaN);\n-            } else {\n-                value = new Value(0d);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            double originalFeatureValue = value.asNumber();\n-            boolean intValue = originalFeatureValue % 1 == 0;\n-\n-            // sample from a standard normal distribution and center around feature value\n-            double normalDistributionSample = perturbationContext.getRandom().nextGaussian();\n-            if (originalFeatureValue != 0d) {\n-                double stDev = originalFeatureValue * 0.01; // set std dev at 1% of feature value\n-                normalDistributionSample = normalDistributionSample * originalFeatureValue + stDev;\n-            }\n-            if (intValue) {\n-                normalDistributionSample = (int) normalDistributionSample;\n-                if (normalDistributionSample == originalFeatureValue) {\n-                    normalDistributionSample = (int) normalDistributionSample + 1d;\n-                }\n-            }\n-            return new Value(normalDistributionSample);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            // find maximum and minimum values\n-            double[] doubles = new double[values.length + 1];\n-            int valueIndex = 0;\n-            for (Value v : values) {\n-                doubles[valueIndex] = Double.isNaN(v.asNumber()) ? 0 : v.asNumber();\n-                valueIndex++;\n-            }\n-            double originalValue = Double.isNaN(target.asNumber()) ? 0 : target.asNumber();\n-            doubles[valueIndex] = originalValue; // include target number in feature scaling\n-            double min = DoubleStream.of(doubles).min().orElse(Double.MIN_VALUE);\n-            double max = DoubleStream.of(doubles).max().orElse(Double.MAX_VALUE);\n-\n-            // feature scaling\n-            List<Double> scaledValues = DoubleStream.of(doubles).map(d -> (d - min) / (max - min)).boxed().collect(Collectors.toList());\n-            double scaledOriginalValue = scaledValues.remove(valueIndex); // extract the scaled original value (it must not appear in encoded values)\n-\n-            // kernel based clustering\n-            double sigma = params.getNumericTypeClusterGaussianFilterWidth();\n-            double threshold = DataUtils.gaussianKernel(scaledOriginalValue, scaledOriginalValue, sigma);\n-            List<Double> clusteredValues = scaledValues.stream()\n-                    .map(d -> DataUtils.gaussianKernel(d, scaledOriginalValue, sigma)).collect(Collectors.toList());\n-            List<Double> encodedValues = clusteredValues.stream()\n-                    .map(d -> (Math.abs(d - threshold) < params.getNumericTypeClusterThreshold()) ? 1d : 0d).collect(Collectors.toList());\n-\n-            return encodedValues.stream().map(d -> new double[] { d }).collect(Collectors.toList());\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextDouble());\n-        }\n-    },\n-\n-    BOOLEAN(\"boolean\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            return new Value(!Boolean.parseBoolean(value.asString()));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(perturbationContext.getRandom().nextBoolean());\n-        }\n-    },\n-\n-    URI(\"uri\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(java.net.URI.create(\"\"));\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            String uriAsString = value.asString();\n-            java.net.URI uri = java.net.URI.create(uriAsString);\n-            String scheme = uri.getScheme();\n-            String host = uri.getHost();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (\"localhost\".equalsIgnoreCase(host)) {\n-                    host = \"0.0.0.0\";\n-                } else {\n-                    host = \"localhost\";\n-                }\n-            }\n-            String path = uri.getPath();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                path = \"\";\n-            }\n-            String fragment = uri.getFragment();\n-            if (perturbationContext.getRandom().nextBoolean()) {\n-                if (fragment != null && fragment.length() > 0) {\n-                    fragment = \"\";\n-                } else { // generate a random string\n-                    fragment = Long.toHexString(Double.doubleToLongBits(perturbationContext.getRandom().nextDouble()));\n-                }\n-            }\n-            java.net.URI newURI;\n-            try {\n-                newURI = new URI(scheme, host, path, fragment);\n-                if (uri.equals(newURI)) { // to avoid \"unfortunate\" cases where no URI parameter has been perturbed\n-                    newURI = java.net.URI.create(\"\");\n-                }\n-            } catch (URISyntaxException e) {\n-                newURI = java.net.URI.create(\"\");\n-            }\n-            return new Value(newURI);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            String uriString = \"http://\" + randomString(perturbationContext.getRandom()) + \".com\";\n-            URI uri = java.net.URI.create(uriString);\n-            return new Value(uri);\n-        }\n-    },\n-\n-    TIME(\"time\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            LocalTime featureValue;\n-            try {\n-                featureValue = LocalTime.parse(value.asString());\n-            } catch (DateTimeException dateTimeException) {\n-                featureValue = LocalTime.now();\n-            }\n-            return new Value(featureValue.minusHours(1L + perturbationContext.getRandom().nextInt(23)));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(LocalTime.of(perturbationContext.getRandom().nextInt(23), perturbationContext.getRandom().nextInt(59)));\n-        }\n-    },\n-\n-    DURATION(\"duration\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            Duration duration;\n-            try {\n-                duration = Duration.parse(value.asString());\n-            } catch (DateTimeParseException parseException) {\n-                duration = Duration.of(0, ChronoUnit.HOURS);\n-            }\n-            duration = duration.plus(1L + perturbationContext.getRandom().nextInt(23), ChronoUnit.HOURS);\n-            return new Value(duration);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(Duration.ofDays(perturbationContext.getRandom().nextInt(30)));\n-        }\n-    },\n-\n-    VECTOR(\"vector\") {\n-        @Override\n-        public Value drop(Value value) {\n-            double[] values = value.asVector();\n-            if (values.length > 0) {\n-                Arrays.fill(values, 0);\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            // set a number of non zero values to zero (or decrease them by 1)\n-            double[] vector = value.asVector();\n-            double[] values = Arrays.copyOf(vector, vector.length);\n-            if (values.length > 1) {\n-                int maxPerturbationSize = Math.min(vector.length, Math.max((int) (vector.length * 0.5), perturbationContext.getNoOfPerturbations()));\n-                int[] indexes = perturbationContext.getRandom().ints(0, vector.length)\n-                        .limit(maxPerturbationSize).toArray();\n-                for (int idx : indexes) {\n-                    if (values[idx] != 0) {\n-                        values[idx] = 0;\n-                    } else {\n-                        values[idx]--;\n-                    }\n-                }\n-            }\n-            return new Value(values);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            double[] vector = new double[5];\n-            for (int i = 0; i < vector.length; i++) {\n-                vector[i] = perturbationContext.getRandom().nextDouble();\n-            }\n-            return new Value(vector);\n-        }\n-    },\n-\n-    UNDEFINED(\"undefined\") {\n-        @Override\n-        public Value drop(Value value) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, underlyingObject.getType().drop(underlyingObject.getValue())));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            if (value.getUnderlyingObject() instanceof Feature) {\n-                Feature underlyingObject = (Feature) value.getUnderlyingObject();\n-                Type type = underlyingObject.getType();\n-                Value perturbedValue = type.perturb(underlyingObject.getValue(), perturbationContext);\n-                value = new Value(FeatureFactory.copyOf(underlyingObject, perturbedValue));\n-            } else {\n-                value = new Value(null);\n-            }\n-            return value;\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            return new Value(new Object());\n-        }\n-    },\n-\n-    COMPOSITE(\"composite\") {\n-        @Override\n-        public Value drop(Value value) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newFeatures = new ArrayList<>(composite.size());\n-            for (Feature f : composite) {\n-                newFeatures.add(FeatureFactory.copyOf(f, f.getType().drop(f.getValue())));\n-            }\n-            return new Value(newFeatures);\n-        }\n-\n-        private List<Feature> getFeatures(Value value) {\n-            List<Feature> features;\n-            try {\n-                features = (List<Feature>) value.getUnderlyingObject();\n-            } catch (ClassCastException cce) {\n-                features = new LinkedList<>();\n-            }\n-            return features;\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Feature> composite = getFeatures(value);\n-            List<Feature> newList = DataUtils.perturbFeatures(composite, perturbationContext);\n-            return new Value(newList);\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            List<Feature> composite = getFeatures(target);\n-            int i = 0;\n-            List<List<double[]>> multiColumns = new LinkedList<>();\n-            for (Feature f : composite) {\n-                int finalI = i;\n-                List<double[]> subColumn = f.getType().encode(params, f.getValue(), Arrays.stream(values)\n-                        .map(v -> (List<Feature>) v.getUnderlyingObject())\n-                        .map(l -> l.get(finalI).getValue()).toArray(Value[]::new));\n-                multiColumns.add(subColumn);\n-                i++;\n-            }\n-            List<double[]> result = new LinkedList<>();\n-\n-            for (int j = 0; j < values.length; j++) {\n-                List<Double> vector = new LinkedList<>();\n-                for (List<double[]> multiColumn : multiColumns) {\n-                    double[] doubles = multiColumn.get(j);\n-                    vector.addAll(Arrays.asList(ArrayUtils.toObject(doubles)));\n-                }\n-                double[] doubles = new double[vector.size()];\n-                for (int d = 0; d < doubles.length; d++) {\n-                    doubles[d] = vector.get(d);\n-                }\n-                result.add(doubles);\n-            }\n-            return result;\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            Type[] types = Type.values();\n-            List<Object> values = new LinkedList<>();\n-            Type nestedType = types[perturbationContext.getRandom().nextInt(types.length - 1)];\n-            for (int i = 0; i < 5; i++) {\n-                Feature f = new Feature(\"f_\" + i, nestedType, nestedType.randomValue(perturbationContext));\n-                values.add(f);\n-            }\n-            return new Value(values);\n-        }\n-    },\n-\n-    CURRENCY(\"currency\") {\n-        @Override\n-        public Value drop(Value value) {\n-            return new Value(null);\n-        }\n-\n-        @Override\n-        public Value perturb(Value value, PerturbationContext perturbationContext) {\n-            List<Currency> availableCurrencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            if (value.getUnderlyingObject() instanceof Currency) {\n-                Currency current = (Currency) value.getUnderlyingObject();\n-                availableCurrencies.removeIf(current::equals);\n-            }\n-            return new Value(availableCurrencies.get(perturbationContext.getRandom().nextInt(availableCurrencies.size())));\n-        }\n-\n-        @Override\n-        public List<double[]> encode(EncodingParams params, Value target, Value... values) {\n-            return encodeEquals(target, values);\n-        }\n-\n-        @Override\n-        public Value randomValue(PerturbationContext perturbationContext) {\n-            ArrayList<Currency> currencies = new ArrayList<>(Currency.getAvailableCurrencies());\n-            return new Value(currencies.get(perturbationContext.getRandom().nextInt(currencies.size() - 1)));\n-        }\n-    };\n-\n-    static List<double[]> encodeEquals(Value target, Value[] values) {\n-        List<double[]> result = new ArrayList<>(values.length);\n-        for (Value value : values) {\n-            double[] data = new double[1];\n-            if (target.getUnderlyingObject().equals(value.getUnderlyingObject())) {\n-                data[0] = 1d;\n-            } else {\n-                data[0] = 0d;\n-            }\n-            result.add(data);\n-        }\n-        return result;\n-    }\n-\n-    private final String value;\n-\n-    Type(String value) {\n-        this.value = value;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return String.valueOf(value);\n-    }\n-\n-    /**\n-     * Drop a given {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a non existent/empty/void/dropped {@code Type}-specific instance.\n-     *\n-     * @param value the value to drop\n-     * @return the dropped value\n-     */\n-    public abstract Value drop(Value value);\n-\n-    /**\n-     * Perturb a {@code Value}. Implementations of this method should generate a new {@code Value} whose\n-     * {@code Value#getUnderlyingObject} should represent a perturbed/changed copy of the original value.\n-     *\n-     * @param value the value to perturb\n-     * @param perturbationContext the context holding metadata about how perturbations should be performed\n-     * @return the perturbed value\n-     */\n-    public abstract Value perturb(Value value, PerturbationContext perturbationContext);\n-\n-    /**\n-     * Encode some {@code Value}s with respect to a target value. Implementations of this method should generate a list\n-     * of vectors for each value. The target value represents the \"encoding reference\" to be used to decide how to encode\n-     * each value, e.g. values that are equals to the target one might get encoded as {@code double[1]{1d}} whereas\n-     * different values (wrt to {@code target}) might get encoded as {@code double[1]{0d}}.\n-     *\n-     * @param target the target reference value\n-     * @param values the values to be encoded\n-     * @return a list of vectors\n-     */\n-    public abstract List<double[]> encode(EncodingParams params, Value target, Value... values);\n-\n-    /**\n-     * Generate a random {@code Value} (depending on the underlying {@code Type}).\n-     *\n-     * @param perturbationContext context object used to randomize values\n-     * @return a random Value\n-     */\n-    public abstract Value randomValue(PerturbationContext perturbationContext);\n-\n-    private static String randomString(Random random) {\n-        return Long.toHexString(Double.doubleToLongBits(random.nextDouble()));\n-    }\n-}\n\\ No newline at end of file\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3MTkzMQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542471931", "body": "```suggestion\r\n    void testPrequalificationDMNExplanation() throws ExecutionException, InterruptedException, TimeoutException {\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                void testPrequalificationDMNExplanation1() throws ExecutionException, InterruptedException, TimeoutException {\n          \n          \n            \n                void testPrequalificationDMNExplanation() throws ExecutionException, InterruptedException, TimeoutException {", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-k\">void</span> <span class=\"x x-first x-last\">testPrequalificationDMNExplanation1</span>() throws <span class=\"pl-smi\">ExecutionException</span>, <span class=\"pl-smi\">InterruptedException</span>, <span class=\"pl-smi\">TimeoutException</span> {</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-k\">void</span> <span class=\"x x-first x-last\">testPrequalificationDMNExplanation</span>() throws <span class=\"pl-smi\">ExecutionException</span>, <span class=\"pl-smi\">InterruptedException</span>, <span class=\"pl-smi\">TimeoutException</span> {</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "r00ta", "createdAt": "2020-12-14T15:26:11Z", "path": "explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.kogito.explainability.explainability.integrationtests.dmn;\n+\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeoutException;\n+\n+import org.junit.jupiter.api.Test;\n+import org.kie.dmn.api.core.DMNRuntime;\n+import org.kie.kogito.decision.DecisionModel;\n+import org.kie.kogito.dmn.DMNKogito;\n+import org.kie.kogito.dmn.DmnDecisionModel;\n+import org.kie.kogito.explainability.Config;\n+import org.kie.kogito.explainability.local.lime.LimeConfig;\n+import org.kie.kogito.explainability.local.lime.LimeExplainer;\n+import org.kie.kogito.explainability.model.Feature;\n+import org.kie.kogito.explainability.model.FeatureFactory;\n+import org.kie.kogito.explainability.model.FeatureImportance;\n+import org.kie.kogito.explainability.model.PerturbationContext;\n+import org.kie.kogito.explainability.model.Prediction;\n+import org.kie.kogito.explainability.model.PredictionInput;\n+import org.kie.kogito.explainability.model.PredictionOutput;\n+import org.kie.kogito.explainability.model.PredictionProvider;\n+import org.kie.kogito.explainability.model.Saliency;\n+import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n+import org.kie.kogito.explainability.utils.ValidationUtils;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+\n+class PrequalificationDmnLimeExplainerTest {\n+\n+    @Test\n+    void testPrequalificationDMNExplanation1() throws ExecutionException, InterruptedException, TimeoutException {", "originalCommit": "4a42ffb10648bb781ec9fd7f0b74b1e80b5fcfe2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3MzQyMQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542473421", "bodyText": "And rename Prequalification-1 file?", "author": "r00ta", "createdAt": "2020-12-14T15:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3MTkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUxMzgyMQ==", "url": "https://github.com/kiegroup/kogito-apps/pull/530#discussion_r542513821", "bodyText": "sure, thanks :)", "author": "tteofili", "createdAt": "2020-12-14T16:18:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3MTkzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "fae3e0ba7ac4e7d113e1cf11e10ba5ff0949b3fe", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\nindex 5885d6831..edb534877 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n", "chunk": "@@ -52,7 +52,7 @@ import static org.junit.jupiter.api.Assertions.assertNotNull;\n class PrequalificationDmnLimeExplainerTest {\n \n     @Test\n-    void testPrequalificationDMNExplanation1() throws ExecutionException, InterruptedException, TimeoutException {\n+    void testPrequalificationDMNExplanation() throws ExecutionException, InterruptedException, TimeoutException {\n         DMNRuntime dmnRuntime = DMNKogito.createGenericDMNRuntime(new InputStreamReader(getClass().getResourceAsStream(\"/dmn/Prequalification-1.dmn\")));\n         assertEquals(1, dmnRuntime.getModels().size());\n \n", "next_change": {"commit": "d526a5b102313d804d1906c1520b5495d729f2c5", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\nindex edb534877..45e7a8f91 100644\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n+++ b/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n", "chunk": "@@ -53,36 +60,22 @@ class PrequalificationDmnLimeExplainerTest {\n \n     @Test\n     void testPrequalificationDMNExplanation() throws ExecutionException, InterruptedException, TimeoutException {\n-        DMNRuntime dmnRuntime = DMNKogito.createGenericDMNRuntime(new InputStreamReader(getClass().getResourceAsStream(\"/dmn/Prequalification-1.dmn\")));\n-        assertEquals(1, dmnRuntime.getModels().size());\n+        PredictionProvider model = getModel();\n \n-        final String NS = \"http://www.trisotech.com/definitions/_f31e1f8e-d4ce-4a3a-ac3b-747efa6b3401\";\n-        final String NAME = \"Prequalification\";\n-        DecisionModel decisionModel = new DmnDecisionModel(dmnRuntime, NS, NAME);\n-\n-        final Map<String, Object> borrower = new HashMap<>();\n-        borrower.put(\"Monthly Other Debt\", 1000);\n-        borrower.put(\"Monthly Income\", 10000);\n-        final Map<String, Object> contextVariables = new HashMap<>();\n-        contextVariables.put(\"Appraised Value\", 500000);\n-        contextVariables.put(\"Loan Amount\", 300000);\n-        contextVariables.put(\"Credit Score\", 600);\n-        contextVariables.put(\"Borrower\", borrower);\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCompositeFeature(\"context\", contextVariables));\n-        PredictionInput predictionInput = new PredictionInput(features);\n-\n-        PredictionProvider model = new DecisionModelWrapper(decisionModel);\n+        PredictionInput predictionInput = getTestInput();\n \n         Random random = new Random();\n-        random.setSeed(4);\n-        LimeConfig limeConfig = new LimeConfig().withSamples(3000)\n-                .withPerturbationContext(new PerturbationContext(random, 3));\n+\n+        random.setSeed(0);\n+        PerturbationContext perturbationContext = new PerturbationContext(random, 1);\n+        LimeConfig limeConfig = new LimeConfig()\n+                .withSamples(10)\n+                .withPerturbationContext(perturbationContext);\n         LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n \n         List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(predictionInput))\n                 .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        Prediction prediction = new Prediction(predictionInput, predictionOutputs.get(0));\n+        Prediction prediction = new SimplePrediction(predictionInput, predictionOutputs.get(0));\n         Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n                 .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n         for (Saliency saliency : saliencyMap.values()) {\n", "next_change": {"commit": "bbb22c06d37e77b97aae6496d74abe43a8cfc965", "changed_code": [{"header": "diff --git a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java b/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\ndeleted file mode 100644\nindex 45e7a8f91..000000000\n--- a/explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n+++ /dev/null\n", "chunk": "@@ -1,152 +0,0 @@\n-/*\n- * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *       http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.kie.kogito.explainability.explainability.integrationtests.dmn;\n-\n-import java.io.InputStreamReader;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.TimeoutException;\n-\n-import org.assertj.core.api.AssertionsForClassTypes;\n-import org.junit.jupiter.api.Test;\n-import org.kie.dmn.api.core.DMNRuntime;\n-import org.kie.kogito.decision.DecisionModel;\n-import org.kie.kogito.dmn.DMNKogito;\n-import org.kie.kogito.dmn.DmnDecisionModel;\n-import org.kie.kogito.explainability.Config;\n-import org.kie.kogito.explainability.local.lime.LimeConfig;\n-import org.kie.kogito.explainability.local.lime.LimeExplainer;\n-import org.kie.kogito.explainability.local.lime.optim.LimeConfigOptimizer;\n-import org.kie.kogito.explainability.model.DataDistribution;\n-import org.kie.kogito.explainability.model.Feature;\n-import org.kie.kogito.explainability.model.FeatureFactory;\n-import org.kie.kogito.explainability.model.FeatureImportance;\n-import org.kie.kogito.explainability.model.PerturbationContext;\n-import org.kie.kogito.explainability.model.Prediction;\n-import org.kie.kogito.explainability.model.PredictionInput;\n-import org.kie.kogito.explainability.model.PredictionInputsDataDistribution;\n-import org.kie.kogito.explainability.model.PredictionOutput;\n-import org.kie.kogito.explainability.model.PredictionProvider;\n-import org.kie.kogito.explainability.model.Saliency;\n-import org.kie.kogito.explainability.model.SimplePrediction;\n-import org.kie.kogito.explainability.utils.DataUtils;\n-import org.kie.kogito.explainability.utils.ExplainabilityMetrics;\n-import org.kie.kogito.explainability.utils.ValidationUtils;\n-\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-\n-class PrequalificationDmnLimeExplainerTest {\n-\n-    @Test\n-    void testPrequalificationDMNExplanation() throws ExecutionException, InterruptedException, TimeoutException {\n-        PredictionProvider model = getModel();\n-\n-        PredictionInput predictionInput = getTestInput();\n-\n-        Random random = new Random();\n-\n-        random.setSeed(0);\n-        PerturbationContext perturbationContext = new PerturbationContext(random, 1);\n-        LimeConfig limeConfig = new LimeConfig()\n-                .withSamples(10)\n-                .withPerturbationContext(perturbationContext);\n-        LimeExplainer limeExplainer = new LimeExplainer(limeConfig);\n-\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(List.of(predictionInput))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        Prediction prediction = new SimplePrediction(predictionInput, predictionOutputs.get(0));\n-        Map<String, Saliency> saliencyMap = limeExplainer.explainAsync(prediction, model)\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        for (Saliency saliency : saliencyMap.values()) {\n-            assertNotNull(saliency);\n-            List<FeatureImportance> topFeatures = saliency.getTopFeatures(2);\n-            if (!topFeatures.isEmpty()) {\n-                assertThat(ExplainabilityMetrics.impactScore(model, prediction, topFeatures)).isPositive();\n-            }\n-        }\n-\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, prediction, limeExplainer, 1,\n-                0.3, 0.3));\n-\n-        String decision = \"LLPA\";\n-        List<PredictionInput> inputs = new ArrayList<>();\n-        for (int n = 0; n < 10; n++) {\n-            inputs.add(new PredictionInput(DataUtils.perturbFeatures(predictionInput.getFeatures(), perturbationContext)));\n-        }\n-        DataDistribution distribution = new PredictionInputsDataDistribution(inputs);\n-        int k = 2;\n-        int chunkSize = 2;\n-        double f1 = ExplainabilityMetrics.getLocalSaliencyF1(decision, model, limeExplainer, distribution, k, chunkSize);\n-        AssertionsForClassTypes.assertThat(f1).isBetween(0.5d, 1d);\n-\n-    }\n-\n-    @Test\n-    void testExplanationStabilityWithOptimization() throws ExecutionException, InterruptedException, TimeoutException {\n-        PredictionProvider model = getModel();\n-\n-        List<PredictionInput> samples = DmnTestUtils.randomPrequalificationInputs();\n-        List<PredictionOutput> predictionOutputs = model.predictAsync(samples.subList(0, 10)).get();\n-        List<Prediction> predictions = DataUtils.getPredictions(samples, predictionOutputs);\n-        LimeConfigOptimizer limeConfigOptimizer = new LimeConfigOptimizer().withSampling(false);\n-        Random random = new Random();\n-        random.setSeed(0);\n-        LimeConfig initialConfig = new LimeConfig().withSamples(10);\n-        LimeConfig optimizedConfig = limeConfigOptimizer.optimize(initialConfig, predictions, model);\n-        assertThat(optimizedConfig).isNotSameAs(initialConfig);\n-\n-        LimeExplainer limeExplainer = new LimeExplainer(optimizedConfig);\n-        PredictionInput testPredictionInput = getTestInput();\n-        List<PredictionOutput> testPredictionOutputs = model.predictAsync(List.of(testPredictionInput))\n-                .get(Config.INSTANCE.getAsyncTimeout(), Config.INSTANCE.getAsyncTimeUnit());\n-        Prediction instance = new SimplePrediction(testPredictionInput, testPredictionOutputs.get(0));\n-\n-        assertDoesNotThrow(() -> ValidationUtils.validateLocalSaliencyStability(model, instance, limeExplainer, 1,\n-                0.4, 0.4));\n-    }\n-\n-    private PredictionInput getTestInput() {\n-        final Map<String, Object> borrower = new HashMap<>();\n-        borrower.put(\"Monthly Other Debt\", 1000);\n-        borrower.put(\"Monthly Income\", 10000);\n-        final Map<String, Object> contextVariables = new HashMap<>();\n-        contextVariables.put(\"Appraised Value\", 500000);\n-        contextVariables.put(\"Loan Amount\", 300000);\n-        contextVariables.put(\"Credit Score\", 600);\n-        contextVariables.put(\"Borrower\", borrower);\n-        List<Feature> features = new LinkedList<>();\n-        features.add(FeatureFactory.newCompositeFeature(\"context\", contextVariables));\n-        return new PredictionInput(features);\n-    }\n-\n-    private PredictionProvider getModel() {\n-        DMNRuntime dmnRuntime = DMNKogito.createGenericDMNRuntime(new InputStreamReader(getClass().getResourceAsStream(\"/dmn/Prequalification-1.dmn\")));\n-        assertEquals(1, dmnRuntime.getModels().size());\n-\n-        final String NS = \"http://www.trisotech.com/definitions/_f31e1f8e-d4ce-4a3a-ac3b-747efa6b3401\";\n-        final String NAME = \"Prequalification\";\n-        DecisionModel decisionModel = new DmnDecisionModel(dmnRuntime, NS, NAME);\n-        return new DecisionModelWrapper(decisionModel);\n-    }\n-}\n", "next_change": null}]}}]}}]}}, {"oid": "b4f49c55decbc6258ece0a5376082bbed9252a53", "url": "https://github.com/kiegroup/kogito-apps/commit/b4f49c55decbc6258ece0a5376082bbed9252a53", "message": "Update explainability/explainability-integrationtests/explainability-integrationtests-dmn/src/test/java/org/kie/kogito/explainability/explainability/integrationtests/dmn/PrequalificationDmnLimeExplainerTest.java\n\nCo-authored-by: Jacopo Rota <jacopo.r00ta@gmail.com>", "committedDate": "2020-12-14T16:17:39Z", "type": "commit"}, {"oid": "839f28843cc3c2e8f6df4c6bfea037eb55de4fbb", "url": "https://github.com/kiegroup/kogito-apps/commit/839f28843cc3c2e8f6df4c6bfea037eb55de4fbb", "message": "KOGITO-3763 - fixed NaN encoding issue", "committedDate": "2020-12-14T16:48:43Z", "type": "commit"}]}