{"pr_number": 9935, "pr_title": "optimize announceHistoricalSegments", "pr_author": "xiangqiao123", "pr_createdAt": "2020-05-27T04:51:34Z", "pr_url": "https://github.com/apache/druid/pull/9935", "timeline": [{"oid": "110b9a346632248d0053298d2683f7fcc27ffeb6", "url": "https://github.com/apache/druid/commit/110b9a346632248d0053298d2683f7fcc27ffeb6", "message": "optimize announceHistoricalSegment", "committedDate": "2020-05-27T04:37:07Z", "type": "commit"}, {"oid": "909081e1ee4ed9243b3c490a3197aa5fafce1e94", "url": "https://github.com/apache/druid/commit/909081e1ee4ed9243b3c490a3197aa5fafce1e94", "message": "optimize announceHistoricalSegment", "committedDate": "2020-05-27T06:19:54Z", "type": "commit"}, {"oid": "c6c4230ad8122b3762b25fbbee8104786a3b4484", "url": "https://github.com/apache/druid/commit/c6c4230ad8122b3762b25fbbee8104786a3b4484", "message": "Merge branch 'optimize_announceHistoricalSegment' of github.com:xiangqiao123/druid into optimize_announceHistoricalSegment", "committedDate": "2020-05-27T06:21:54Z", "type": "commit"}, {"oid": "e7061f281f275b752e4d770cd667f345f2504476", "url": "https://github.com/apache/druid/commit/e7061f281f275b752e4d770cd667f345f2504476", "message": "revert offline SegmentTransactionalInsertAction uses a separate lock", "committedDate": "2020-05-27T09:14:07Z", "type": "commit"}, {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "url": "https://github.com/apache/druid/commit/605bc5b32f25ae2a1b88af09a89417ac2450f87f", "message": "optimize segmentExistsBatch: Avoid too many elements in the in condition", "committedDate": "2020-05-28T05:21:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw==", "url": "https://github.com/apache/druid/pull/9935#discussion_r440493023", "body": "Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.", "bodyText": "Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.", "bodyHTML": "<p dir=\"auto\">Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.</p>", "author": "jihoonson", "createdAt": "2020-06-15T23:02:11Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTQxOQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446461419", "bodyText": "There is a typo in ANNOUNCE_HISTORICAL_SEGMENG_BATCH (SEGMENG). I would recommend to rename to be more clear such as MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "author": "jihoonson", "createdAt": "2020-06-27T00:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3MjI5Ng==", "url": "https://github.com/apache/druid/pull/9935#discussion_r449572296", "bodyText": "The number 100 is just experience.\nVariable name has been changed to MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "author": "xiangqiao123", "createdAt": "2020-07-03T13:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463433", "body": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\r\n\r\n```java\r\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\r\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\r\n      for (DataSegment segment : segments) {\r\n        if (!existedSegments.contains(segment.getId().toString())) {\r\n          toInsertSegments.add(segment);\r\n        }\r\n      }\r\n```", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }", "bodyHTML": "<p dir=\"auto\">This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"      Set&lt;String&gt; existedSegments = segmentExistsBatch(handle, segments);\n      log.info(&quot;Found these segments already exist in DB: %s&quot;, existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }\n\"><pre>      <span class=\"pl-k\">Set&lt;<span class=\"pl-smi\">String</span>&gt;</span> existedSegments <span class=\"pl-k\">=</span> segmentExistsBatch(handle, segments);\n      log<span class=\"pl-k\">.</span>info(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Found these segments already exist in DB: %s<span class=\"pl-pds\">\"</span></span>, existedSegments);\n      <span class=\"pl-k\">for</span> (<span class=\"pl-smi\">DataSegment</span> segment <span class=\"pl-k\">:</span> segments) {\n        <span class=\"pl-k\">if</span> (<span class=\"pl-k\">!</span>existedSegments<span class=\"pl-k\">.</span>contains(segment<span class=\"pl-k\">.</span>getId()<span class=\"pl-k\">.</span>toString())) {\n          toInsertSegments<span class=\"pl-k\">.</span>add(segment);\n        }\n      }</pre></div>", "author": "jihoonson", "createdAt": "2020-06-27T00:49:45Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NjE2NQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446466165", "bodyText": "There is a log.infoSegments method that takes a collection of segments that should be used if logging a potentially large list of segments.", "author": "clintropolis", "createdAt": "2020-06-27T01:14:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NzI2NQ==", "url": "https://github.com/apache/druid/pull/9935#discussion_r449577265", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }\n\nModified as required.", "author": "xiangqiao123", "createdAt": "2020-07-03T13:13:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzYyNg==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463626", "body": "This will copy the whole set of `toInsertSegments` which doesn't seem necessary. Can we use `toInsertSegment` instead in the [below for loop](https://github.com/apache/druid/pull/9935/files#diff-7dab61c46825adfbdc9166d4fb8a30a0R966)?", "bodyText": "This will copy the whole set of toInsertSegments which doesn't seem necessary. Can we use toInsertSegment instead in the below for loop?", "bodyHTML": "<p dir=\"auto\">This will copy the whole set of <code>toInsertSegments</code> which doesn't seem necessary. Can we use <code>toInsertSegment</code> instead in the <a href=\"https://github.com/apache/druid/pull/9935/files#diff-7dab61c46825adfbdc9166d4fb8a30a0R966\">below for loop</a>?</p>", "author": "jihoonson", "createdAt": "2020-06-27T00:51:38Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTIwMA==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465200", "body": "This will corrupt the logs too. How about modifying as the below?\r\n\r\n```java\r\n      final List<List<DataSegment>> partitionedSegments = Lists.partition(\r\n          new ArrayList<>(toInsertSegments),\r\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\r\n      );\r\n\r\n      PreparedBatch preparedBatch = handle.prepareBatch(\r\n          StringUtils.format(\r\n              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\r\n                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\r\n              dbTables.getSegmentsTable(),\r\n              connector.getQuoteString()\r\n          )\r\n      );\r\n      \r\n      for (List<DataSegment> partition : partitionedSegments) {\r\n        for (DataSegment segment : partition) {\r\n          preparedBatch.add()\r\n                       .bind(\"id\", segment.getId().toString())\r\n                       .bind(\"dataSource\", segment.getDataSource())\r\n                       .bind(\"created_date\", DateTimes.nowUtc().toString())\r\n                       .bind(\"start\", segment.getInterval().getStart().toString())\r\n                       .bind(\"end\", segment.getInterval().getEnd().toString())\r\n                       .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\r\n                       .bind(\"version\", segment.getVersion())\r\n                       .bind(\"used\", usedSegments.contains(segment))\r\n                       .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\r\n        }\r\n        final int[] affectedRows = preparedBatch.execute();\r\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -> eachAffectedRows == 1);\r\n        if (succeeded) {\r\n          log.infoSegments(partition, \"Published segments to DB\");\r\n        } else {\r\n          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())\r\n                                                             .filter(i -> affectedRows[i] != 1)\r\n                                                             .mapToObj(partition::get)\r\n                                                             .collect(Collectors.toList());\r\n          throw new ISE(\r\n              \"Failed to publish segments to DB: %s\",\r\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\r\n          );\r\n        }\r\n      }\r\n    }\r\n```", "bodyText": "This will corrupt the logs too. How about modifying as the below?\n      final List<List<DataSegment>> partitionedSegments = Lists.partition(\n          new ArrayList<>(toInsertSegments),\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\n      );\n\n      PreparedBatch preparedBatch = handle.prepareBatch(\n          StringUtils.format(\n              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n              dbTables.getSegmentsTable(),\n              connector.getQuoteString()\n          )\n      );\n      \n      for (List<DataSegment> partition : partitionedSegments) {\n        for (DataSegment segment : partition) {\n          preparedBatch.add()\n                       .bind(\"id\", segment.getId().toString())\n                       .bind(\"dataSource\", segment.getDataSource())\n                       .bind(\"created_date\", DateTimes.nowUtc().toString())\n                       .bind(\"start\", segment.getInterval().getStart().toString())\n                       .bind(\"end\", segment.getInterval().getEnd().toString())\n                       .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n                       .bind(\"version\", segment.getVersion())\n                       .bind(\"used\", usedSegments.contains(segment))\n                       .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n        }\n        final int[] affectedRows = preparedBatch.execute();\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -> eachAffectedRows == 1);\n        if (succeeded) {\n          log.infoSegments(partition, \"Published segments to DB\");\n        } else {\n          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())\n                                                             .filter(i -> affectedRows[i] != 1)\n                                                             .mapToObj(partition::get)\n                                                             .collect(Collectors.toList());\n          throw new ISE(\n              \"Failed to publish segments to DB: %s\",\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }", "bodyHTML": "<p dir=\"auto\">This will corrupt the logs too. How about modifying as the below?</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"      final List&lt;List&lt;DataSegment&gt;&gt; partitionedSegments = Lists.partition(\n          new ArrayList&lt;&gt;(toInsertSegments),\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\n      );\n\n      PreparedBatch preparedBatch = handle.prepareBatch(\n          StringUtils.format(\n              &quot;INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) &quot;\n                  + &quot;VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)&quot;,\n              dbTables.getSegmentsTable(),\n              connector.getQuoteString()\n          )\n      );\n      \n      for (List&lt;DataSegment&gt; partition : partitionedSegments) {\n        for (DataSegment segment : partition) {\n          preparedBatch.add()\n                       .bind(&quot;id&quot;, segment.getId().toString())\n                       .bind(&quot;dataSource&quot;, segment.getDataSource())\n                       .bind(&quot;created_date&quot;, DateTimes.nowUtc().toString())\n                       .bind(&quot;start&quot;, segment.getInterval().getStart().toString())\n                       .bind(&quot;end&quot;, segment.getInterval().getEnd().toString())\n                       .bind(&quot;partitioned&quot;, (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n                       .bind(&quot;version&quot;, segment.getVersion())\n                       .bind(&quot;used&quot;, usedSegments.contains(segment))\n                       .bind(&quot;payload&quot;, jsonMapper.writeValueAsBytes(segment));\n        }\n        final int[] affectedRows = preparedBatch.execute();\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -&gt; eachAffectedRows == 1);\n        if (succeeded) {\n          log.infoSegments(partition, &quot;Published segments to DB&quot;);\n        } else {\n          final List&lt;DataSegment&gt; failedToPublish = IntStream.range(0, partition.size())\n                                                             .filter(i -&gt; affectedRows[i] != 1)\n                                                             .mapToObj(partition::get)\n                                                             .collect(Collectors.toList());\n          throw new ISE(\n              &quot;Failed to publish segments to DB: %s&quot;,\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }\n\"><pre>      <span class=\"pl-k\">final</span> <span class=\"pl-k\">List&lt;<span class=\"pl-k\">List&lt;<span class=\"pl-smi\">DataSegment</span>&gt;</span>&gt;</span> partitionedSegments <span class=\"pl-k\">=</span> <span class=\"pl-smi\">Lists</span><span class=\"pl-k\">.</span>partition(\n          <span class=\"pl-k\">new</span> <span class=\"pl-k\">ArrayList&lt;&gt;</span>(toInsertSegments),\n          <span class=\"pl-c1\">MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE</span>\n      );\n\n      <span class=\"pl-smi\">PreparedBatch</span> preparedBatch <span class=\"pl-k\">=</span> handle<span class=\"pl-k\">.</span>prepareBatch(\n          <span class=\"pl-smi\">StringUtils</span><span class=\"pl-k\">.</span>format(\n              <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) <span class=\"pl-pds\">\"</span></span>\n                  <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)<span class=\"pl-pds\">\"</span></span>,\n              dbTables<span class=\"pl-k\">.</span>getSegmentsTable(),\n              connector<span class=\"pl-k\">.</span>getQuoteString()\n          )\n      );\n      \n      <span class=\"pl-k\">for</span> (<span class=\"pl-k\">List&lt;<span class=\"pl-smi\">DataSegment</span>&gt;</span> partition <span class=\"pl-k\">:</span> partitionedSegments) {\n        <span class=\"pl-k\">for</span> (<span class=\"pl-smi\">DataSegment</span> segment <span class=\"pl-k\">:</span> partition) {\n          preparedBatch<span class=\"pl-k\">.</span>add()\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>id<span class=\"pl-pds\">\"</span></span>, segment<span class=\"pl-k\">.</span>getId()<span class=\"pl-k\">.</span>toString())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataSource<span class=\"pl-pds\">\"</span></span>, segment<span class=\"pl-k\">.</span>getDataSource())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>created_date<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-smi\">DateTimes</span><span class=\"pl-k\">.</span>nowUtc()<span class=\"pl-k\">.</span>toString())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>start<span class=\"pl-pds\">\"</span></span>, segment<span class=\"pl-k\">.</span>getInterval()<span class=\"pl-k\">.</span>getStart()<span class=\"pl-k\">.</span>toString())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>end<span class=\"pl-pds\">\"</span></span>, segment<span class=\"pl-k\">.</span>getInterval()<span class=\"pl-k\">.</span>getEnd()<span class=\"pl-k\">.</span>toString())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>partitioned<span class=\"pl-pds\">\"</span></span>, (segment<span class=\"pl-k\">.</span>getShardSpec() <span class=\"pl-k\">instanceof</span> <span class=\"pl-smi\">NoneShardSpec</span>) <span class=\"pl-k\">?</span> <span class=\"pl-c1\">false</span> <span class=\"pl-k\">:</span> <span class=\"pl-c1\">true</span>)\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>version<span class=\"pl-pds\">\"</span></span>, segment<span class=\"pl-k\">.</span>getVersion())\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>used<span class=\"pl-pds\">\"</span></span>, usedSegments<span class=\"pl-k\">.</span>contains(segment))\n                       .bind(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>payload<span class=\"pl-pds\">\"</span></span>, jsonMapper<span class=\"pl-k\">.</span>writeValueAsBytes(segment));\n        }\n        <span class=\"pl-k\">final</span> <span class=\"pl-k\">int</span>[] affectedRows <span class=\"pl-k\">=</span> preparedBatch<span class=\"pl-k\">.</span>execute();\n        <span class=\"pl-k\">final</span> <span class=\"pl-k\">boolean</span> succeeded <span class=\"pl-k\">=</span> <span class=\"pl-smi\">Arrays</span><span class=\"pl-k\">.</span>stream(affectedRows)<span class=\"pl-k\">.</span>allMatch(eachAffectedRows <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> eachAffectedRows <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>);\n        <span class=\"pl-k\">if</span> (succeeded) {\n          log<span class=\"pl-k\">.</span>infoSegments(partition, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Published segments to DB<span class=\"pl-pds\">\"</span></span>);\n        } <span class=\"pl-k\">else</span> {\n          <span class=\"pl-k\">final</span> <span class=\"pl-k\">List&lt;<span class=\"pl-smi\">DataSegment</span>&gt;</span> failedToPublish <span class=\"pl-k\">=</span> <span class=\"pl-smi\">IntStream</span><span class=\"pl-k\">.</span>range(<span class=\"pl-c1\">0</span>, partition<span class=\"pl-k\">.</span>size())\n                                                             .filter(i <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> affectedRows[i] <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">1</span>)\n                                                             .mapToObj(partition<span class=\"pl-k\">::</span>get)\n                                                             .collect(<span class=\"pl-smi\">Collectors</span><span class=\"pl-k\">.</span>toList());\n          <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">ISE</span>(\n              <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Failed to publish segments to DB: %s<span class=\"pl-pds\">\"</span></span>,\n              <span class=\"pl-smi\">SegmentUtils</span><span class=\"pl-k\">.</span>commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }</pre></div>", "author": "jihoonson", "createdAt": "2020-06-27T01:04:57Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTMxOA==", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465318", "body": "Same here. Better to be `log.errorSegments(segments, \"Exception inserting segments\");`", "bodyText": "Same here. Better to be log.errorSegments(segments, \"Exception inserting segments\");", "bodyHTML": "<p dir=\"auto\">Same here. Better to be <code>log.errorSegments(segments, \"Exception inserting segments\");</code></p>", "author": "jihoonson", "createdAt": "2020-06-27T01:06:09Z", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            } else {\n+              throw new ISE(\n+                  \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            }\n+          }\n+        }\n       }\n     }\n     catch (Exception e) {\n-      log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), used);\n+      for (DataSegment segment : segments) {\n+        log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), usedSegments.contains(segment));\n+      }", "originalCommit": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "835f0d9977764ee9a22799de063de402befc8299", "url": "https://github.com/apache/druid/commit/835f0d9977764ee9a22799de063de402befc8299", "message": "add unit test && Modified according to cr", "committedDate": "2020-07-03T14:23:53Z", "type": "commit"}, {"oid": "a702cd11a406bbf5c63c04d102ed258b527e3704", "url": "https://github.com/apache/druid/commit/a702cd11a406bbf5c63c04d102ed258b527e3704", "message": "Merge branch 'master' into optimize_announceHistoricalSegment", "committedDate": "2020-07-03T14:40:44Z", "type": "commit"}]}