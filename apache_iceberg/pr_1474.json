{"pr_number": 1474, "pr_title": "FIX: fix reader not getting closed.", "pr_author": "sudssf", "pr_createdAt": "2020-09-17T23:13:31Z", "pr_url": "https://github.com/apache/iceberg/pull/1474", "timeline": [{"oid": "0c47ba83c089d16dabc6be6b22ff20252e0698ed", "url": "https://github.com/apache/iceberg/commit/0c47ba83c089d16dabc6be6b22ff20252e0698ed", "message": "FIX: fix reader not getting closed.\n\nthis lead to s3a connection pool timeout", "committedDate": "2020-09-17T23:12:25Z", "type": "commit"}, {"oid": "a427eef4fcb8de7a6060be6ee2d49283200041a1", "url": "https://github.com/apache/iceberg/commit/a427eef4fcb8de7a6060be6ee2d49283200041a1", "message": "no need to create new reader just use row group from existing reader", "committedDate": "2020-09-17T23:17:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzAxOA==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490617018", "body": "@sudssf This needs to initialize a new reader as we want to get row positions relative to the start of the file and not start of the split. I think the only change that is necessary in this PR is to close the reader.", "bodyText": "@sudssf This needs to initialize a new reader as we want to get row positions relative to the start of the file and not start of the split. I think the only change that is necessary in this PR is to close the reader.", "bodyHTML": "<p dir=\"auto\"><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/sudssf/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sudssf\">@sudssf</a> This needs to initialize a new reader as we want to get row positions relative to the start of the file and not start of the split. I think the only change that is necessary in this PR is to close the reader.</p>", "author": "shardulm94", "createdAt": "2020-09-17T23:30:27Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -165,12 +165,12 @@ ParquetFileReader reader() {\n     return shouldSkip;\n   }\n \n-  private Map<Long, Long> generateOffsetToStartPos() {\n-    ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build());", "originalCommit": "a427eef4fcb8de7a6060be6ee2d49283200041a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzU1Mg==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490617552", "bodyText": "ok I will revert to previous commit thanks for reply", "author": "sudssf", "createdAt": "2020-09-17T23:32:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzAxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMTIyOQ==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490621229", "bodyText": "Can you remove the extra newline here?", "author": "rdblue", "createdAt": "2020-09-17T23:44:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzU1OQ==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490617559", "body": "The reason for a new reader is to create one that doesn't have a filter pushed down through `ParquetReadOptions`.\r\n\r\nWhen building a reader, a [file range can be added to `ParquetReadOptions`](https://github.com/apache/iceberg/blob/master/parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java#L590), which will filter Parquet row groups as they are read. Iceberg uses this to handle file split ranges, and Parquet avoids reading the row group metadata in case there are a lot of row groups in the file.\r\n\r\nThere are a couple options to fix this. We could stop pushing the file range to Parquet so that it reads all the row groups and use your implementation here. That would also require applying the range in `ReadConf` and setting `shouldSkip` appropriately.\r\n\r\nA second option is to properly close the file that is opened here, like this:\r\n\r\n```java\r\n  private Map<Long, Long> generateOffsetToStartPos() {\r\n    try (ParquetFileReader fileReader = newReader(file, ParquetReadOptions.builder().build())) {\r\n      Map<Long, Long> offsetToStartPos = Maps.newHashMap();\r\n\r\n      long curRowCount = 0;\r\n      for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\r\n        BlockMetaData meta = fileReader.getRowGroups().get(i);\r\n        offsetToStartPos.put(meta.getStartingPos(), curRowCount);\r\n        curRowCount += meta.getRowCount();\r\n      }\r\n\r\n      return offsetToStartPos;\r\n\r\n    } catch (IOException e) {\r\n      throw new UncheckedIOException(\"Failed to close reader for file: \" + file, e);\r\n    }\r\n  }\r\n```\r\n\r\nI think it's a good idea to go with the first option, but we might want to fix this with the second option in the mean time.", "bodyText": "The reason for a new reader is to create one that doesn't have a filter pushed down through ParquetReadOptions.\nWhen building a reader, a file range can be added to ParquetReadOptions, which will filter Parquet row groups as they are read. Iceberg uses this to handle file split ranges, and Parquet avoids reading the row group metadata in case there are a lot of row groups in the file.\nThere are a couple options to fix this. We could stop pushing the file range to Parquet so that it reads all the row groups and use your implementation here. That would also require applying the range in ReadConf and setting shouldSkip appropriately.\nA second option is to properly close the file that is opened here, like this:\n  private Map<Long, Long> generateOffsetToStartPos() {\n    try (ParquetFileReader fileReader = newReader(file, ParquetReadOptions.builder().build())) {\n      Map<Long, Long> offsetToStartPos = Maps.newHashMap();\n\n      long curRowCount = 0;\n      for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\n        BlockMetaData meta = fileReader.getRowGroups().get(i);\n        offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n        curRowCount += meta.getRowCount();\n      }\n\n      return offsetToStartPos;\n\n    } catch (IOException e) {\n      throw new UncheckedIOException(\"Failed to close reader for file: \" + file, e);\n    }\n  }\nI think it's a good idea to go with the first option, but we might want to fix this with the second option in the mean time.", "bodyHTML": "<p dir=\"auto\">The reason for a new reader is to create one that doesn't have a filter pushed down through <code>ParquetReadOptions</code>.</p>\n<p dir=\"auto\">When building a reader, a <a href=\"https://github.com/apache/iceberg/blob/master/parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java#L590\">file range can be added to <code>ParquetReadOptions</code></a>, which will filter Parquet row groups as they are read. Iceberg uses this to handle file split ranges, and Parquet avoids reading the row group metadata in case there are a lot of row groups in the file.</p>\n<p dir=\"auto\">There are a couple options to fix this. We could stop pushing the file range to Parquet so that it reads all the row groups and use your implementation here. That would also require applying the range in <code>ReadConf</code> and setting <code>shouldSkip</code> appropriately.</p>\n<p dir=\"auto\">A second option is to properly close the file that is opened here, like this:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"  private Map&lt;Long, Long&gt; generateOffsetToStartPos() {\n    try (ParquetFileReader fileReader = newReader(file, ParquetReadOptions.builder().build())) {\n      Map&lt;Long, Long&gt; offsetToStartPos = Maps.newHashMap();\n\n      long curRowCount = 0;\n      for (int i = 0; i &lt; fileReader.getRowGroups().size(); i += 1) {\n        BlockMetaData meta = fileReader.getRowGroups().get(i);\n        offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n        curRowCount += meta.getRowCount();\n      }\n\n      return offsetToStartPos;\n\n    } catch (IOException e) {\n      throw new UncheckedIOException(&quot;Failed to close reader for file: &quot; + file, e);\n    }\n  }\n\"><pre>  <span class=\"pl-k\">private</span> <span class=\"pl-k\">Map&lt;<span class=\"pl-smi\">Long</span>, <span class=\"pl-smi\">Long</span>&gt;</span> generateOffsetToStartPos() {\n    <span class=\"pl-k\">try</span> (<span class=\"pl-smi\">ParquetFileReader</span> fileReader <span class=\"pl-k\">=</span> newReader(file, <span class=\"pl-smi\">ParquetReadOptions</span><span class=\"pl-k\">.</span>builder()<span class=\"pl-k\">.</span>build())) {\n      <span class=\"pl-k\">Map&lt;<span class=\"pl-smi\">Long</span>, <span class=\"pl-smi\">Long</span>&gt;</span> offsetToStartPos <span class=\"pl-k\">=</span> <span class=\"pl-smi\">Maps</span><span class=\"pl-k\">.</span>newHashMap();\n\n      <span class=\"pl-k\">long</span> curRowCount <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>;\n      <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> fileReader<span class=\"pl-k\">.</span>getRowGroups()<span class=\"pl-k\">.</span>size(); i <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>) {\n        <span class=\"pl-smi\">BlockMetaData</span> meta <span class=\"pl-k\">=</span> fileReader<span class=\"pl-k\">.</span>getRowGroups()<span class=\"pl-k\">.</span>get(i);\n        offsetToStartPos<span class=\"pl-k\">.</span>put(meta<span class=\"pl-k\">.</span>getStartingPos(), curRowCount);\n        curRowCount <span class=\"pl-k\">+=</span> meta<span class=\"pl-k\">.</span>getRowCount();\n      }\n\n      <span class=\"pl-k\">return</span> offsetToStartPos;\n\n    } <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">IOException</span> e) {\n      <span class=\"pl-k\">throw</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">UncheckedIOException</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Failed to close reader for file: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> file, e);\n    }\n  }</pre></div>\n<p dir=\"auto\">I think it's a good idea to go with the first option, but we might want to fix this with the second option in the mean time.</p>", "author": "rdblue", "createdAt": "2020-09-17T23:32:16Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -165,12 +165,12 @@ ParquetFileReader reader() {\n     return shouldSkip;\n   }\n \n-  private Map<Long, Long> generateOffsetToStartPos() {\n-    ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build());", "originalCommit": "a427eef4fcb8de7a6060be6ee2d49283200041a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzgxNA==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490617814", "bodyText": "thanks let me revert to option1", "author": "sudssf", "createdAt": "2020-09-17T23:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxNzU1OQ=="}], "type": "inlineReview"}, {"oid": "86d3dec8fc87b8545e476b550d7e9646724405e0", "url": "https://github.com/apache/iceberg/commit/86d3dec8fc87b8545e476b550d7e9646724405e0", "message": "Revert \"FIX: fix reader not getting closed.\"\n\nThis reverts commit 0c47ba83c089d16dabc6be6b22ff20252e0698ed.", "committedDate": "2020-09-17T23:35:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMDg3NQ==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490620875", "body": "Nit: missing space between `try` and `(`. This will probably fail validation.", "bodyText": "Nit: missing space between try and (. This will probably fail validation.", "bodyHTML": "<p dir=\"auto\">Nit: missing space between <code>try</code> and <code>(</code>. This will probably fail validation.</p>", "author": "rdblue", "createdAt": "2020-09-17T23:43:24Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -166,15 +167,19 @@ ParquetFileReader reader() {\n   }\n \n   private Map<Long, Long> generateOffsetToStartPos() {\n-    ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build());\n+\n     Map<Long, Long> offsetToStartPos = new HashMap<>();\n-    long curRowCount = 0;\n-    for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\n-      BlockMetaData meta = fileReader.getRowGroups().get(i);\n-      offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n-      curRowCount += meta.getRowCount();\n-    }\n \n+    try(ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build())) {", "originalCommit": "6b08ce32455b5cb830bcc396f93ab6b32fd0ced9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMTA2NQ==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490621065", "bodyText": "While you're changing this, can you remove this. from the variable reference? We use this. to distinguish setting instance fields, not getting instance fields.", "author": "rdblue", "createdAt": "2020-09-17T23:44:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMDg3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMjY1OA==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490622658", "bodyText": "yup I will revert to code you posted above :)", "author": "sudssf", "createdAt": "2020-09-17T23:49:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMDg3NQ=="}], "type": "inlineReview"}, {"oid": "aa8f42aa56b23bfe332e1d0be0b1ac21ece21a3e", "url": "https://github.com/apache/iceberg/commit/aa8f42aa56b23bfe332e1d0be0b1ac21ece21a3e", "message": "throw UncheckedIOException", "committedDate": "2020-09-17T23:44:30Z", "type": "forcePushed"}, {"oid": "2b257dba9dcc18596e10ae5aefd6e53eea950ebf", "url": "https://github.com/apache/iceberg/commit/2b257dba9dcc18596e10ae5aefd6e53eea950ebf", "message": "throw UncheckedIOException", "committedDate": "2020-09-17T23:53:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMzkxNA==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490623914", "body": "Maps.newHashMap() was not auto resolved by imports so going to use default constructor ", "bodyText": "Maps.newHashMap() was not auto resolved by imports so going to use default constructor", "bodyHTML": "<p dir=\"auto\">Maps.newHashMap() was not auto resolved by imports so going to use default constructor</p>", "author": "sudssf", "createdAt": "2020-09-17T23:54:15Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -166,17 +167,22 @@ ParquetFileReader reader() {\n   }\n \n   private Map<Long, Long> generateOffsetToStartPos() {\n-    ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build());\n-    Map<Long, Long> offsetToStartPos = new HashMap<>();\n-    long curRowCount = 0;\n-    for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\n-      BlockMetaData meta = fileReader.getRowGroups().get(i);\n-      offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n-      curRowCount += meta.getRowCount();\n-    }\n+      try (ParquetFileReader fileReader = newReader(file, ParquetReadOptions.builder().build())) {\n+        Map<Long, Long> offsetToStartPos = new HashMap<>();", "originalCommit": "2b257dba9dcc18596e10ae5aefd6e53eea950ebf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyODY1NQ==", "url": "https://github.com/apache/iceberg/pull/1474#discussion_r490628655", "body": "The message may be \"failed to create reader\"?", "bodyText": "The message may be \"failed to create reader\"?", "bodyHTML": "<p dir=\"auto\">The message may be \"failed to create reader\"?</p>", "author": "chenjunjiedada", "createdAt": "2020-09-18T00:11:54Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -166,17 +167,22 @@ ParquetFileReader reader() {\n   }\n \n   private Map<Long, Long> generateOffsetToStartPos() {\n-    ParquetFileReader fileReader = newReader(this.file, ParquetReadOptions.builder().build());\n-    Map<Long, Long> offsetToStartPos = new HashMap<>();\n-    long curRowCount = 0;\n-    for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\n-      BlockMetaData meta = fileReader.getRowGroups().get(i);\n-      offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n-      curRowCount += meta.getRowCount();\n-    }\n+      try (ParquetFileReader fileReader = newReader(file, ParquetReadOptions.builder().build())) {\n+        Map<Long, Long> offsetToStartPos = new HashMap<>();\n \n-    return offsetToStartPos;\n-  }\n+        long curRowCount = 0;\n+        for (int i = 0; i < fileReader.getRowGroups().size(); i += 1) {\n+          BlockMetaData meta = fileReader.getRowGroups().get(i);\n+          offsetToStartPos.put(meta.getStartingPos(), curRowCount);\n+          curRowCount += meta.getRowCount();\n+        }\n+\n+        return offsetToStartPos;\n+\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(\"Failed to close reader for file: \" + file, e);", "originalCommit": "2b257dba9dcc18596e10ae5aefd6e53eea950ebf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e7444ce62f3b2344d5f4c24991cec9eb9f4e548", "url": "https://github.com/apache/iceberg/commit/0e7444ce62f3b2344d5f4c24991cec9eb9f4e548", "message": "throw UncheckedIOException", "committedDate": "2020-09-18T00:16:32Z", "type": "commit"}, {"oid": "0e7444ce62f3b2344d5f4c24991cec9eb9f4e548", "url": "https://github.com/apache/iceberg/commit/0e7444ce62f3b2344d5f4c24991cec9eb9f4e548", "message": "throw UncheckedIOException", "committedDate": "2020-09-18T00:16:32Z", "type": "forcePushed"}]}