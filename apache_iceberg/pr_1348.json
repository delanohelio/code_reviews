{"pr_number": 1348, "pr_title": "Flink: Support table sink.", "pr_author": "openinx", "pr_createdAt": "2020-08-17T12:58:06Z", "pr_url": "https://github.com/apache/iceberg/pull/1348", "timeline": [{"oid": "3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "url": "https://github.com/apache/iceberg/commit/3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "message": "Introduce the catalog name.", "committedDate": "2020-08-20T13:41:51Z", "type": "forcePushed"}, {"oid": "948ea352e1e828f5f265b9a0ae602b73c62808c2", "url": "https://github.com/apache/iceberg/commit/948ea352e1e828f5f265b9a0ae602b73c62808c2", "message": "Introduce the catalog name.", "committedDate": "2020-08-20T14:24:28Z", "type": "forcePushed"}, {"oid": "4072210b430aad79a91de2103da516c9dc616912", "url": "https://github.com/apache/iceberg/commit/4072210b430aad79a91de2103da516c9dc616912", "message": "Rebase to IcebergFilesCommitter", "committedDate": "2020-08-25T10:35:16Z", "type": "forcePushed"}, {"oid": "0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "url": "https://github.com/apache/iceberg/commit/0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "message": "Rebase to master", "committedDate": "2020-08-26T09:27:46Z", "type": "forcePushed"}, {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "url": "https://github.com/apache/iceberg/commit/0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "message": "Rebase to flink-committer", "committedDate": "2020-08-27T12:19:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgzMjE4NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r479832185", "body": "Since we are already returning the DataStream, would it make sense to avoid the discarding sink and possibly let people stream the iceberg commit files instead? Like what if I wanted to also feed them into kafka?", "bodyText": "Since we are already returning the DataStream, would it make sense to avoid the discarding sink and possibly let people stream the iceberg commit files instead? Like what if I wanted to also feed them into kafka?", "bodyHTML": "<p dir=\"auto\">Since we are already returning the DataStream, would it make sense to avoid the discarding sink and possibly let people stream the iceberg commit files instead? Like what if I wanted to also feed them into kafka?</p>", "author": "kbendick", "createdAt": "2020-08-30T23:54:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n+          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      DataStream<RowData> inputStream = rowInput != null ? convert() : rowDataInput;\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = inputStream\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(inputStream.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n+          .setParallelism(1)\n+          .setMaxParallelism(1);\n+\n+      return returnStream.addSink(new DiscardingSink())\n+          .name(String.format(\"IcebergSink %s\", table.toString()))\n+          .setParallelism(1);", "originalCommit": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg1OTQ2Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r479859466", "bodyText": "You mean you want to feed the committed data files to kafka ?  Is that meaningful for users ?  It will be better to understand if we have such user cases I guess.\nSome context\nin the first sink version,  I made the IcebergFilesCommitter implemented the SinkFucntion, then we could chain the function by addSink directly,  while we found that it did not work for bounded stream because there was no interface/method to indicate that this stream is a bounded one, then we have no way to commit those data files into iceberg table when the stream has reached its end.  So we have to turn to AbstractStreamOperator  and implemented a BoundedOneInput interface.   Finally, int this version,  we will transform the data stream twice (the first one:  rowdata -> dataFiles, the second one: datafiles -> void), and finally add a discarding sink.", "author": "openinx", "createdAt": "2020-08-31T02:38:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgzMjE4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "afb913e94ca33c3cedea0a1fe0d7dc979748414d", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 979bfba5a..96ce0ba6d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -105,42 +155,20 @@ public class FlinkSink {\n       return this;\n     }\n \n-    private DataStream<RowData> convert() {\n-      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n-\n-      RowType rowType;\n-      DataType[] fieldDataTypes;\n-      if (tableSchema != null) {\n-        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n-        fieldDataTypes = tableSchema.getFieldDataTypes();\n-      } else {\n-        rowType = FlinkSchemaUtil.convert(table.schema());\n-        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n-      }\n-\n-      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n-\n-      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n-    }\n-\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n-      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n-          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n-      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n-          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n       Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n-      DataStream<RowData> inputStream = rowInput != null ? convert() : rowDataInput;\n-\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n       IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n \n-      DataStream<Void> returnStream = inputStream\n+      DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n-          .setParallelism(inputStream.getParallelism())\n+          .setParallelism(rowDataInput.getParallelism())\n           .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n           .setParallelism(1)\n           .setMaxParallelism(1);\n", "next_change": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 96ce0ba6d..8571d499c 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -155,16 +158,29 @@ public class FlinkSink {\n       return this;\n     }\n \n+    public Builder overwrite(boolean newOverwrite) {\n+      this.overwrite = newOverwrite;\n+      return this;\n+    }\n+\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);\n+        }\n+      }\n+\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n-      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf, overwrite);\n \n       DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 8571d499c..96ce0ba6d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -158,29 +155,16 @@ public class FlinkSink {\n       return this;\n     }\n \n-    public Builder overwrite(boolean newOverwrite) {\n-      this.overwrite = newOverwrite;\n-      return this;\n-    }\n-\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n-      if (table == null) {\n-        tableLoader.open(hadoopConf);\n-        try (TableLoader loader = tableLoader) {\n-          this.table = loader.loadTable();\n-        } catch (IOException e) {\n-          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);\n-        }\n-      }\n-\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n-      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf, overwrite);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n \n       DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n", "next_change": {"commit": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 96ce0ba6d..4d067eea3 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -159,10 +161,18 @@ public class FlinkSink {\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);\n+        }\n+      }\n+\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n       IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n \n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 4d067eea3..8571d499c 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -174,7 +180,7 @@ public class FlinkSink {\n       }\n \n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n-      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf, overwrite);\n \n       DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n", "next_change": null}]}}]}}]}}]}}]}}, {"oid": "afb913e94ca33c3cedea0a1fe0d7dc979748414d", "url": "https://github.com/apache/iceberg/commit/afb913e94ca33c3cedea0a1fe0d7dc979748414d", "message": "Flink: Support table sink.", "committedDate": "2020-08-31T01:58:13Z", "type": "forcePushed"}, {"oid": "4712393d6d4b862cf801875d8f206399c6662ec9", "url": "https://github.com/apache/iceberg/commit/4712393d6d4b862cf801875d8f206399c6662ec9", "message": "Minior fixes", "committedDate": "2020-08-31T11:23:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNDI2NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481624265", "body": "I think it is better to just pass a table loader to sink, source and sink can reuse this loader creation function, just like in:\r\nhttps://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61\r\nWhat do you think?", "bodyText": "I think it is better to just pass a table loader to sink, source and sink can reuse this loader creation function, just like in:\nhttps://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61\nWhat do you think?", "bodyHTML": "<p dir=\"auto\">I think it is better to just pass a table loader to sink, source and sink can reuse this loader creation function, just like in:<br>\n<a href=\"https://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61\">https://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61</a><br>\nWhat do you think?</p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:47:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.StreamTableSinkFactory;\n+import org.apache.flink.table.sinks.StreamTableSink;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+\n+public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n+  private final FlinkCatalog catalog;\n+\n+  public FlinkTableFactory(FlinkCatalog catalog) {\n+    this.catalog = catalog;\n+  }\n+\n+  @Override\n+  public StreamTableSink<RowData> createTableSink(Context context) {\n+    ObjectIdentifier identifier = context.getObjectIdentifier();\n+    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n+    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n+    try {\n+      Table table = catalog.loadIcebergTable(objectPath);\n+      return new IcebergTableSink(icebergIdentifier, table,", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTc4Njg3NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481786875", "bodyText": "Make sense to me,  we also don't need to pass the icebergIdentifier  to IcebergTableSink, that makes code more simplier.", "author": "openinx", "createdAt": "2020-09-02T06:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNDI2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\nindex af9bdb864..49a94f548 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n", "chunk": "@@ -40,17 +37,10 @@ public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n \n   @Override\n   public StreamTableSink<RowData> createTableSink(Context context) {\n-    ObjectIdentifier identifier = context.getObjectIdentifier();\n-    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n-    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n-    try {\n-      Table table = catalog.loadIcebergTable(objectPath);\n-      return new IcebergTableSink(icebergIdentifier, table,\n-          catalog.getCatalogLoader(), catalog.getHadoopConf(),\n-          FlinkSchemaUtil.toSchema(table.schema()));\n-    } catch (TableNotExistException e) {\n-      throw new ValidationException(String.format(\"Iceberg table(%s) not exist.\", icebergIdentifier), e);\n-    }\n+    ObjectPath objectPath = context.getObjectIdentifier().toObjectPath();\n+    TableLoader tableLoader = createTableLoader(objectPath);\n+    TableSchema tableSchema = getPhysicalSchema(context);\n+    return new IcebergTableSink(context.isBounded(), tableLoader, catalog.getHadoopConf(), tableSchema);\n   }\n \n   @Override\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\nindex 49a94f548..11e0c4295 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n", "chunk": "@@ -37,10 +40,17 @@ public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n \n   @Override\n   public StreamTableSink<RowData> createTableSink(Context context) {\n-    ObjectPath objectPath = context.getObjectIdentifier().toObjectPath();\n-    TableLoader tableLoader = createTableLoader(objectPath);\n-    TableSchema tableSchema = getPhysicalSchema(context);\n-    return new IcebergTableSink(context.isBounded(), tableLoader, catalog.getHadoopConf(), tableSchema);\n+    ObjectIdentifier identifier = context.getObjectIdentifier();\n+    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n+    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n+    try {\n+      Table table = catalog.getIcebergTable(objectPath);\n+      return new IcebergTableSink(icebergIdentifier, table,\n+          catalog.getCatalogLoader(), catalog.getHadoopConf(),\n+          FlinkSchemaUtil.toSchema(table.schema()));\n+    } catch (TableNotExistException e) {\n+      throw new ValidationException(String.format(\"Iceberg table(%s) not exist.\", icebergIdentifier), e);\n+    }\n   }\n \n   @Override\n", "next_change": {"commit": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\nindex 11e0c4295..cb25bbb35 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n", "chunk": "@@ -40,17 +37,10 @@ public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n \n   @Override\n   public StreamTableSink<RowData> createTableSink(Context context) {\n-    ObjectIdentifier identifier = context.getObjectIdentifier();\n-    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n-    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n-    try {\n-      Table table = catalog.getIcebergTable(objectPath);\n-      return new IcebergTableSink(icebergIdentifier, table,\n-          catalog.getCatalogLoader(), catalog.getHadoopConf(),\n-          FlinkSchemaUtil.toSchema(table.schema()));\n-    } catch (TableNotExistException e) {\n-      throw new ValidationException(String.format(\"Iceberg table(%s) not exist.\", icebergIdentifier), e);\n-    }\n+    ObjectPath objectPath = context.getObjectIdentifier().toObjectPath();\n+    TableLoader tableLoader = createTableLoader(objectPath);\n+    TableSchema tableSchema = getPhysicalSchema(context);\n+    return new IcebergTableSink(tableLoader, catalog.getHadoopConf(), tableSchema);\n   }\n \n   @Override\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\nindex cb25bbb35..49a94f548 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java\n", "chunk": "@@ -40,7 +40,7 @@ public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n     ObjectPath objectPath = context.getObjectIdentifier().toObjectPath();\n     TableLoader tableLoader = createTableLoader(objectPath);\n     TableSchema tableSchema = getPhysicalSchema(context);\n-    return new IcebergTableSink(tableLoader, catalog.getHadoopConf(), tableSchema);\n+    return new IcebergTableSink(context.isBounded(), tableLoader, catalog.getHadoopConf(), tableSchema);\n   }\n \n   @Override\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNTg2OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481625868", "body": "This is a deprecated method, no one will call it, you can just `return this`.", "bodyText": "This is a deprecated method, no one will call it, you can just return this.", "bodyHTML": "<p dir=\"auto\">This is a deprecated method, no one will call it, you can just <code>return this</code>.</p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:49:10Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n+  private final TableIdentifier tableIdentifier;\n+  private final Table table;\n+  private final CatalogLoader catalogLoader;\n+  private final TableSchema tableSchema;\n+  private final Configuration hadoopConf;\n+\n+  public IcebergTableSink(TableIdentifier tableIdentifier, Table table,\n+                          CatalogLoader catalogLoader, Configuration hadoopConf,\n+                          TableSchema tableSchema) {\n+    this.tableIdentifier = tableIdentifier;\n+    this.table = table;\n+    this.catalogLoader = catalogLoader;\n+    this.hadoopConf = hadoopConf;\n+    this.tableSchema = tableSchema;\n+  }\n+\n+  @Override\n+  public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n+    return FlinkSink.forRowData(dataStream)\n+        .table(table)\n+        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n+        .hadoopConf(hadoopConf)\n+        .tableSchema(tableSchema)\n+        .build();\n+  }\n+\n+  @Override\n+  public DataType getConsumedDataType() {\n+    return tableSchema.toRowDataType().bridgedTo(RowData.class);\n+  }\n+\n+  @Override\n+  public TableSchema getTableSchema() {\n+    return this.tableSchema;\n+  }\n+\n+  @Override\n+  public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n+    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgwMzQ1OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481803458", "bodyText": "OK, I see.  will do .", "author": "openinx", "createdAt": "2020-09-02T06:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNTg2OA=="}], "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 9f08eb404..22c96b5f3 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -73,18 +77,12 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n \n   @Override\n   public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {\n-      String expectedFieldNames = Arrays.toString(tableSchema.getFieldNames());\n-      String actualFieldNames = Arrays.toString(fieldNames);\n-      throw new ValidationException(\"The field names is mismatched. Expected: \" +\n-          expectedFieldNames + \" But was: \" + actualFieldNames);\n-    }\n-    if (!Arrays.equals(tableSchema.getFieldTypes(), fieldTypes)) {\n-      String expectedFieldTypes = Arrays.toString(tableSchema.getFieldTypes());\n-      String actualFieldTypes = Arrays.toString(fieldNames);\n-      throw new ValidationException(\"Field types are mismatched. Expected: \" +\n-          expectedFieldTypes + \" But was: \" + actualFieldTypes);\n-    }\n+    // This method has been deprecated and it will be removed in future version, so left the empty implementation here.\n     return this;\n   }\n+\n+  @Override\n+  public void setOverwrite(boolean overwrite) {\n+    this.overwrite = overwrite;\n+  }\n }\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 22c96b5f3..9f08eb404 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -77,12 +73,18 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData>, Overwri\n \n   @Override\n   public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-    // This method has been deprecated and it will be removed in future version, so left the empty implementation here.\n+    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {\n+      String expectedFieldNames = Arrays.toString(tableSchema.getFieldNames());\n+      String actualFieldNames = Arrays.toString(fieldNames);\n+      throw new ValidationException(\"The field names is mismatched. Expected: \" +\n+          expectedFieldNames + \" But was: \" + actualFieldNames);\n+    }\n+    if (!Arrays.equals(tableSchema.getFieldTypes(), fieldTypes)) {\n+      String expectedFieldTypes = Arrays.toString(tableSchema.getFieldTypes());\n+      String actualFieldTypes = Arrays.toString(fieldNames);\n+      throw new ValidationException(\"Field types are mismatched. Expected: \" +\n+          expectedFieldTypes + \" But was: \" + actualFieldTypes);\n+    }\n     return this;\n   }\n-\n-  @Override\n-  public void setOverwrite(boolean overwrite) {\n-    this.overwrite = overwrite;\n-  }\n }\n", "next_change": {"commit": "aa3326689f99238dd007603931db5a4e7262d151", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 9f08eb404..e851c50b3 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -73,18 +62,7 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n \n   @Override\n   public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {\n-      String expectedFieldNames = Arrays.toString(tableSchema.getFieldNames());\n-      String actualFieldNames = Arrays.toString(fieldNames);\n-      throw new ValidationException(\"The field names is mismatched. Expected: \" +\n-          expectedFieldNames + \" But was: \" + actualFieldNames);\n-    }\n-    if (!Arrays.equals(tableSchema.getFieldTypes(), fieldTypes)) {\n-      String expectedFieldTypes = Arrays.toString(tableSchema.getFieldTypes());\n-      String actualFieldTypes = Arrays.toString(fieldNames);\n-      throw new ValidationException(\"Field types are mismatched. Expected: \" +\n-          expectedFieldTypes + \" But was: \" + actualFieldTypes);\n-    }\n+    // This method has been deprecated and it will be removed in future version, so left the empty implementation here.\n     return this;\n   }\n }\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex e851c50b3..5ef234098 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -65,4 +75,9 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n     // This method has been deprecated and it will be removed in future version, so left the empty implementation here.\n     return this;\n   }\n+\n+  @Override\n+  public void setOverwrite(boolean overwrite) {\n+    this.overwrite = overwrite;\n+  }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 5ef234098..5c2576965 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -80,4 +82,9 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData>, Overwri\n   public void setOverwrite(boolean overwrite) {\n     this.overwrite = overwrite;\n   }\n+\n+  @Override\n+  public void setStaticPartition(Map<String, String> partitions) {\n+    // The flink's PartitionFanoutWriter will handle the static partition write policy automatically.\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODA3Nw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628077", "body": "Can we use Parameterized for batch too?", "bodyText": "Can we use Parameterized for batch too?", "bodyHTML": "<p dir=\"auto\">Can we use Parameterized for batch too?</p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:51:29Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgxNTUxMw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481815513", "bodyText": "That's a great idea, we could reuse almost all of the codes then.", "author": "openinx", "createdAt": "2020-09-02T07:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODA3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -61,71 +54,76 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+  private TableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n+  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 2, false},\n+        new Object[] {\"parquet\", 2, true}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n+  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n+    String warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings settings = EnvironmentSettings\n+    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n         .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+        .inBatchMode()\n+        .useBlinkPlanner();\n+\n+    if (isStreamingJob) {\n+      settingsBuilder.inStreamingMode();\n+      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+      env.enableCheckpointing(400);\n+      env.setParallelism(parallelism);\n+      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+    } else {\n+      settingsBuilder.inBatchMode();\n+      tEnv = TableEnvironment.create(settingsBuilder.build());\n+    }\n+\n+    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n+        warehouse);\n+    sql(\"use catalog iceberg_catalog\");\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -54,76 +61,71 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.flink.table.api.Expressions.$;\n+\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private TableEnvironment tEnv;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n-  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n+  public TestFlinkTableSink(String format, int parallelism) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n-    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+    warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n \n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -61,67 +54,73 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+  private TableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n+  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 2, false},\n+        new Object[] {\"parquet\", 2, true}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n+  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n+    String warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings settings = EnvironmentSettings\n+    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n         .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n+        .inBatchMode()\n+        .useBlinkPlanner();\n+\n+    if (isStreamingJob) {\n+      settingsBuilder.inStreamingMode();\n+      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+      env.enableCheckpointing(400);\n+      env.setParallelism(parallelism);\n+      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+    } else {\n+      settingsBuilder.inBatchMode();\n+      tEnv = TableEnvironment.create(settingsBuilder.build());\n+    }\n+\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -124,7 +122,6 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -119,9 +121,9 @@ public class TestFlinkTableSink extends AbstractTestBase {\n       tEnv = TableEnvironment.create(settingsBuilder.build());\n     }\n \n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n+        warehouse);\n+    sql(\"use catalog iceberg_catalog\");\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}, {"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -133,34 +132,23 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n-  }\n-\n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n-  }\n-\n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n+    List<RowData> expected = Lists.newArrayList(\n+        SimpleDataUtil.createRowData(1, \"hello\"),\n+        SimpleDataUtil.createRowData(2, \"world\"),\n+        SimpleDataUtil.createRowData(3, \"foo\"),\n+        SimpleDataUtil.createRowData(4, \"bar\")\n+    );\n \n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -131,46 +133,78 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n \n-    // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, \"foo\"),\n-        Expressions.row(4, \"bar\")\n-    );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n \n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n   }\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -168,39 +156,7 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n-  }\n-\n-  @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   private static void waitComplete(TableResult result) {\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODg2NA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628864", "body": "Looks like there is no dynamic table options. (Table hints)", "bodyText": "Looks like there is no dynamic table options. (Table hints)", "bodyHTML": "<p dir=\"auto\">Looks like there is no dynamic table options. (Table hints)</p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:52:11Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTg2ODEzMw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481868133", "bodyText": "OK, it could be removed now.", "author": "openinx", "createdAt": "2020-09-02T08:11:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODg2NA=="}], "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -61,71 +54,76 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+  private TableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n+  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 2, false},\n+        new Object[] {\"parquet\", 2, true}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n+  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n+    String warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings settings = EnvironmentSettings\n+    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n         .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+        .inBatchMode()\n+        .useBlinkPlanner();\n+\n+    if (isStreamingJob) {\n+      settingsBuilder.inStreamingMode();\n+      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+      env.enableCheckpointing(400);\n+      env.setParallelism(parallelism);\n+      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+    } else {\n+      settingsBuilder.inBatchMode();\n+      tEnv = TableEnvironment.create(settingsBuilder.build());\n+    }\n+\n+    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n+        warehouse);\n+    sql(\"use catalog iceberg_catalog\");\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -54,76 +61,71 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.flink.table.api.Expressions.$;\n+\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private TableEnvironment tEnv;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n-  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n+  public TestFlinkTableSink(String format, int parallelism) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n-    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+    warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n \n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -61,67 +54,73 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+  private TableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n+  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 2, false},\n+        new Object[] {\"parquet\", 2, true}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n+  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n+    String warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings settings = EnvironmentSettings\n+    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n         .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n+        .inBatchMode()\n+        .useBlinkPlanner();\n+\n+    if (isStreamingJob) {\n+      settingsBuilder.inStreamingMode();\n+      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+      env.enableCheckpointing(400);\n+      env.setParallelism(parallelism);\n+      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+    } else {\n+      settingsBuilder.inBatchMode();\n+      tEnv = TableEnvironment.create(settingsBuilder.build());\n+    }\n+\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -124,7 +122,6 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -119,9 +121,9 @@ public class TestFlinkTableSink extends AbstractTestBase {\n       tEnv = TableEnvironment.create(settingsBuilder.build());\n     }\n \n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n+        warehouse);\n+    sql(\"use catalog iceberg_catalog\");\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}, {"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -133,34 +132,23 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n-  }\n-\n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n-  }\n-\n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n+    List<RowData> expected = Lists.newArrayList(\n+        SimpleDataUtil.createRowData(1, \"hello\"),\n+        SimpleDataUtil.createRowData(2, \"world\"),\n+        SimpleDataUtil.createRowData(3, \"foo\"),\n+        SimpleDataUtil.createRowData(4, \"bar\")\n+    );\n \n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -131,46 +133,78 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n \n-    // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, \"foo\"),\n-        Expressions.row(4, \"bar\")\n-    );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n \n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n   }\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -168,39 +156,7 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n-  }\n-\n-  @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   private static void waitComplete(TableResult result) {\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyOTc3NA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481629774", "body": "Can we use `TableEnvironment.fromValues`?", "bodyText": "Can we use TableEnvironment.fromValues?", "bodyHTML": "<p dir=\"auto\">Can we use <code>TableEnvironment.fromValues</code>?</p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:53:07Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -133,78 +131,46 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n-  }\n-\n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n-  }\n-\n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n+    List<RowData> expected = Lists.newArrayList(\n+        SimpleDataUtil.createRowData(1, \"hello\"),\n+        SimpleDataUtil.createRowData(2, \"world\"),\n+        SimpleDataUtil.createRowData(3, \"foo\"),\n+        SimpleDataUtil.createRowData(4, \"bar\")\n+    );\n \n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n-    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -131,46 +133,78 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n \n-    // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, \"foo\"),\n-        Expressions.row(4, \"bar\")\n-    );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n \n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n   }\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -168,39 +156,7 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n-  }\n-\n-  @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   private static void waitComplete(TableResult result) {\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzMzIzOA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481633238", "body": "You can just add a method like:\r\n```\r\n  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\r\n    tEnv.executeSql(insert).getJobClient.get\r\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\r\n      .get\r\n  }\r\n```", "bodyText": "You can just add a method like:\n  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\n    tEnv.executeSql(insert).getJobClient.get\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n      .get\n  }", "bodyHTML": "<p dir=\"auto\">You can just add a method like:</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\n    tEnv.executeSql(insert).getJobClient.get\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n      .get\n  }\n\"><pre><code>  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\n    tEnv.executeSql(insert).getJobClient.get\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n      .get\n  }\n</code></pre></div>", "author": "JingsongLi", "createdAt": "2020-09-02T03:56:40Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+\n+    // Redirect the records from source table to destination table.\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+  }\n+\n+  @Test\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+  }\n+\n+  private static void waitComplete(TableResult result) {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -133,78 +131,46 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n-  }\n-\n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n-  }\n-\n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n+    List<RowData> expected = Lists.newArrayList(\n+        SimpleDataUtil.createRowData(1, \"hello\"),\n+        SimpleDataUtil.createRowData(2, \"world\"),\n+        SimpleDataUtil.createRowData(3, \"foo\"),\n+        SimpleDataUtil.createRowData(4, \"bar\")\n+    );\n \n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n-    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -131,46 +133,78 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n \n-    // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, \"foo\"),\n-        Expressions.row(4, \"bar\")\n-    );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n \n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n   }\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -168,39 +156,7 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n-  }\n-\n-  @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+    SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n   private static void waitComplete(TableResult result) {\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzNjM4Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481636386", "body": "We can add TODO for these interfaces:\r\nImplement `OverwritableTableSink`, so in the Flink SQL, user can write these SQLs:\r\n`INSERT OVERWRITE t ...`\r\nImplement `PartitionableTableSink`, user can write:\r\n`INSERT OVERWRITE/INTO t PARTITION(...)`", "bodyText": "We can add TODO for these interfaces:\nImplement OverwritableTableSink, so in the Flink SQL, user can write these SQLs:\nINSERT OVERWRITE t ...\nImplement PartitionableTableSink, user can write:\nINSERT OVERWRITE/INTO t PARTITION(...)", "bodyHTML": "<p dir=\"auto\">We can add TODO for these interfaces:<br>\nImplement <code>OverwritableTableSink</code>, so in the Flink SQL, user can write these SQLs:<br>\n<code>INSERT OVERWRITE t ...</code><br>\nImplement <code>PartitionableTableSink</code>, user can write:<br>\n<code>INSERT OVERWRITE/INTO t PARTITION(...)</code></p>", "author": "JingsongLi", "createdAt": "2020-09-02T03:59:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgwMDk3NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481800975", "bodyText": "Thanks for the remainding, before we add the TODO comment, I will try to implement those two interfaces in the next path.", "author": "openinx", "createdAt": "2020-09-02T06:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzNjM4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 9f08eb404..22c96b5f3 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -19,45 +19,49 @@\n \n package org.apache.iceberg.flink;\n \n-import java.util.Arrays;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.api.ValidationException;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n import org.apache.flink.table.sinks.TableSink;\n import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.flink.sink.FlinkSink;\n \n-public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n-  private final TableIdentifier tableIdentifier;\n-  private final Table table;\n-  private final CatalogLoader catalogLoader;\n+/**\n+ * TODO we will need to implement the {@link PartitionableTableSink} if pull request #1393 get merged, that pr is the\n+ * dependency because we need to get the partition keys from the catalog table.\n+ */\n+public class IcebergTableSink implements AppendStreamTableSink<RowData>, OverwritableTableSink {\n+  private final boolean isBounded;\n+  private final TableLoader tableLoader;\n   private final TableSchema tableSchema;\n   private final Configuration hadoopConf;\n \n-  public IcebergTableSink(TableIdentifier tableIdentifier, Table table,\n-                          CatalogLoader catalogLoader, Configuration hadoopConf,\n+  private boolean overwrite = false;\n+\n+  public IcebergTableSink(boolean isBounded, TableLoader tableLoader, Configuration hadoopConf,\n                           TableSchema tableSchema) {\n-    this.tableIdentifier = tableIdentifier;\n-    this.table = table;\n-    this.catalogLoader = catalogLoader;\n+    this.isBounded = isBounded;\n+    this.tableLoader = tableLoader;\n     this.hadoopConf = hadoopConf;\n     this.tableSchema = tableSchema;\n   }\n \n   @Override\n   public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n+    Preconditions.checkState(!overwrite || isBounded, \"Unbounded data stream don't support overwrite operation.\");\n+\n     return FlinkSink.forRowData(dataStream)\n-        .table(table)\n-        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n+        .tableLoader(tableLoader)\n         .hadoopConf(hadoopConf)\n         .tableSchema(tableSchema)\n+        .overwrite(overwrite)\n         .build();\n   }\n \n", "next_change": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 22c96b5f3..9f08eb404 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -19,49 +19,45 @@\n \n package org.apache.iceberg.flink;\n \n+import java.util.Arrays;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.sinks.AppendStreamTableSink;\n-import org.apache.flink.table.sinks.OverwritableTableSink;\n-import org.apache.flink.table.sinks.PartitionableTableSink;\n import org.apache.flink.table.sinks.TableSink;\n import org.apache.flink.table.types.DataType;\n-import org.apache.flink.util.Preconditions;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.flink.sink.FlinkSink;\n \n-/**\n- * TODO we will need to implement the {@link PartitionableTableSink} if pull request #1393 get merged, that pr is the\n- * dependency because we need to get the partition keys from the catalog table.\n- */\n-public class IcebergTableSink implements AppendStreamTableSink<RowData>, OverwritableTableSink {\n-  private final boolean isBounded;\n-  private final TableLoader tableLoader;\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n+  private final TableIdentifier tableIdentifier;\n+  private final Table table;\n+  private final CatalogLoader catalogLoader;\n   private final TableSchema tableSchema;\n   private final Configuration hadoopConf;\n \n-  private boolean overwrite = false;\n-\n-  public IcebergTableSink(boolean isBounded, TableLoader tableLoader, Configuration hadoopConf,\n+  public IcebergTableSink(TableIdentifier tableIdentifier, Table table,\n+                          CatalogLoader catalogLoader, Configuration hadoopConf,\n                           TableSchema tableSchema) {\n-    this.isBounded = isBounded;\n-    this.tableLoader = tableLoader;\n+    this.tableIdentifier = tableIdentifier;\n+    this.table = table;\n+    this.catalogLoader = catalogLoader;\n     this.hadoopConf = hadoopConf;\n     this.tableSchema = tableSchema;\n   }\n \n   @Override\n   public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n-    Preconditions.checkState(!overwrite || isBounded, \"Unbounded data stream don't support overwrite operation.\");\n-\n     return FlinkSink.forRowData(dataStream)\n-        .tableLoader(tableLoader)\n+        .table(table)\n+        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n         .hadoopConf(hadoopConf)\n         .tableSchema(tableSchema)\n-        .overwrite(overwrite)\n         .build();\n   }\n \n", "next_change": {"commit": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 9f08eb404..399ef8690 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -54,8 +46,7 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n   @Override\n   public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n     return FlinkSink.forRowData(dataStream)\n-        .table(table)\n-        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n+        .tableLoader(tableLoader)\n         .hadoopConf(hadoopConf)\n         .tableSchema(tableSchema)\n         .build();\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex 399ef8690..5ef234098 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n", "chunk": "@@ -45,10 +50,13 @@ public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n \n   @Override\n   public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n+    Preconditions.checkState(!overwrite || isBounded, \"Unbounded data stream don't support overwrite operation.\");\n+\n     return FlinkSink.forRowData(dataStream)\n         .tableLoader(tableLoader)\n         .hadoopConf(hadoopConf)\n         .tableSchema(tableSchema)\n+        .overwrite(overwrite)\n         .build();\n   }\n \n", "next_change": null}]}}]}}]}}]}}, {"oid": "ea63017d5efeb3964192c67903abb502ed53c1d2", "url": "https://github.com/apache/iceberg/commit/ea63017d5efeb3964192c67903abb502ed53c1d2", "message": "Add TODO to implement PartitionedTableSink", "committedDate": "2020-09-02T11:19:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk5OTMxMA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481999310", "body": "Think about this unit test again, we'd better to extend the `FlinkCatalogTestBase`  so that we could cover both hive and hadoop catalog cases. ", "bodyText": "Think about this unit test again, we'd better to extend the FlinkCatalogTestBase  so that we could cover both hive and hadoop catalog cases.", "bodyHTML": "<p dir=\"auto\">Think about this unit test again, we'd better to extend the <code>FlinkCatalogTestBase</code>  so that we could cover both hive and hadoop catalog cases.</p>", "author": "openinx", "createdAt": "2020-09-02T11:32:15Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {", "originalCommit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -54,76 +61,71 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.flink.table.api.Expressions.$;\n+\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private TableEnvironment tEnv;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n-  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n+  public TestFlinkTableSink(String format, int parallelism) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n-    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+    warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n \n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -61,67 +54,73 @@ import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n public class TestFlinkTableSink extends AbstractTestBase {\n   private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n   private static final String TABLE_NAME = \"flink_table\";\n \n   @Rule\n   public TemporaryFolder tempFolder = new TemporaryFolder();\n   private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+  private TableEnvironment tEnv;\n \n   private final FileFormat format;\n   private final int parallelism;\n+  private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n   public static Iterable<Object[]> data() {\n     return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 2, false},\n+        new Object[] {\"parquet\", 2, true}\n     );\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n+  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n     this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n     this.parallelism = parallelism;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n   @Before\n   public void before() throws IOException {\n     File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n+    String warehouse = folder.getAbsolutePath();\n \n     tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n     Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n \n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n \n-    EnvironmentSettings settings = EnvironmentSettings\n+    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n         .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n+        .inBatchMode()\n+        .useBlinkPlanner();\n+\n+    if (isStreamingJob) {\n+      settingsBuilder.inStreamingMode();\n+      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+      env.enableCheckpointing(400);\n+      env.setParallelism(parallelism);\n+      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+    } else {\n+      settingsBuilder.inBatchMode();\n+      tEnv = TableEnvironment.create(settingsBuilder.build());\n+    }\n+\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -124,7 +122,6 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n         \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n     tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -119,9 +121,9 @@ public class TestFlinkTableSink extends AbstractTestBase {\n       tEnv = TableEnvironment.create(settingsBuilder.build());\n     }\n \n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n+        warehouse);\n+    sql(\"use catalog iceberg_catalog\");\n \n     catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n         SimpleDataUtil.SCHEMA,\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}, {"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..f1ae87f43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -133,34 +132,23 @@ public class TestFlinkTableSink extends AbstractTestBase {\n         properties);\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n-  }\n-\n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n-  }\n-\n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n+    List<RowData> expected = Lists.newArrayList(\n+        SimpleDataUtil.createRowData(1, \"hello\"),\n+        SimpleDataUtil.createRowData(2, \"world\"),\n+        SimpleDataUtil.createRowData(3, \"foo\"),\n+        SimpleDataUtil.createRowData(4, \"bar\")\n+    );\n \n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n", "next_change": {"commit": "5393428404f5ab8724381e6f85ad458cb70c9504", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex f1ae87f43..860c737ee 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -152,15 +149,14 @@ public class TestFlinkTableSink extends AbstractTestBase {\n \n     // Redirect the records from source table to destination table.\n     String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    executeSQLAndWaitResult(tEnv, insertSQL);\n \n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n+  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n+    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 860c737ee..7fd1a59da 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -155,6 +157,19 @@ public class TestFlinkTableSink extends AbstractTestBase {\n     SimpleDataUtil.assertTableRows(tablePath, expected);\n   }\n \n+  @Test\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n+\n+    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n+    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n+    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n+  }\n+\n   private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n     tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n       try {\n", "next_change": {"commit": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 7fd1a59da..882b707d7 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -161,17 +160,17 @@ public class TestFlinkTableSink extends AbstractTestBase {\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME));\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n \n-    executeSQLAndWaitResult(tEnv, String.format(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME));\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n     SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n     org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n     Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n   }\n \n-  private static void executeSQLAndWaitResult(TableEnvironment tEnv, String statement) {\n-    tEnv.executeSql(statement).getJobClient().ifPresent(jobClient -> {\n+  private void sql(String statement, Object... args) {\n+    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n       try {\n         jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n       } catch (InterruptedException | ExecutionException e) {\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 882b707d7..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -33,149 +28,126 @@ import org.apache.flink.table.api.Expressions;\n import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.Assert;\n+import org.junit.After;\n import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n   private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n   private final boolean isStreamingJob;\n \n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}, isStreamingJob={3}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1, false},\n-        new Object[] {\"avro\", 1, true},\n-        new Object[] {\"avro\", 2, false},\n-        new Object[] {\"avro\", 2, true},\n-        new Object[] {\"orc\", 1, false},\n-        new Object[] {\"orc\", 1, true},\n-        new Object[] {\"orc\", 2, false},\n-        new Object[] {\"orc\", 2, true},\n-        new Object[] {\"parquet\", 1, false},\n-        new Object[] {\"parquet\", 1, true},\n-        new Object[] {\"parquet\", 2, false},\n-        new Object[] {\"parquet\", 2, true}\n-    );\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism, boolean isStreamingJob) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n     this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    String warehouse = folder.getAbsolutePath();\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n \n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    Catalog catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner();\n-\n-    if (isStreamingJob) {\n-      settingsBuilder.inStreamingMode();\n-      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-      env.enableCheckpointing(400);\n-      env.setParallelism(parallelism);\n-      tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-    } else {\n-      settingsBuilder.inBatchMode();\n-      tEnv = TableEnvironment.create(settingsBuilder.build());\n-    }\n-\n-    sql(\"create catalog iceberg_catalog with ('type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\",\n-        warehouse);\n-    sql(\"use catalog iceberg_catalog\");\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n \n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    List<RowData> expected = Lists.newArrayList(\n-        SimpleDataUtil.createRowData(1, \"hello\"),\n-        SimpleDataUtil.createRowData(2, \"world\"),\n-        SimpleDataUtil.createRowData(3, \"foo\"),\n-        SimpleDataUtil.createRowData(4, \"bar\")\n-    );\n-\n     // Register the rows into a temporary table.\n-    Table sourceTable = tEnv.fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n         Expressions.row(1, \"hello\"),\n         Expressions.row(2, \"world\"),\n         Expressions.row(3, \"foo\"),\n         Expressions.row(4, \"bar\")\n     );\n-    tEnv.createTemporaryView(\"sourceTable\", sourceTable);\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n     sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRows(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n   public void testOverwriteTable() throws Exception {\n     Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n \n-    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(1, \"hello\")));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'world'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList(SimpleDataUtil.createRowData(2, \"world\")));\n-    org.apache.iceberg.Table table = new HadoopTables().load(tablePath);\n-    Assert.assertEquals(\"overwrite\", table.currentSnapshot().operation());\n-  }\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n \n-  private void sql(String statement, Object... args) {\n-    tEnv.executeSql(String.format(statement, args)).getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MjY4OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482082688", "body": "We could use flink DDL to create table here if https://github.com/apache/iceberg/pull/1393 get merged.", "bodyText": "We could use flink DDL to create table here if #1393 get merged.", "bodyHTML": "<p dir=\"auto\">We could use flink DDL to create table here if <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"686955824\" data-permission-text=\"Title is private\" data-url=\"https://github.com/apache/iceberg/issues/1393\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/apache/iceberg/pull/1393/hovercard\" href=\"https://github.com/apache/iceberg/pull/1393\">#1393</a> get merged.</p>", "author": "openinx", "createdAt": "2020-09-02T13:48:19Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog", "originalCommit": "d6e7c259184f19d6b611ebdfc19910335aa8bf3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NDMzOQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482594339", "bodyText": "It was merged!", "author": "rdblue", "createdAt": "2020-09-02T23:22:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MjY4OA=="}], "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex e172fbe51..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -19,135 +19,197 @@\n \n package org.apache.iceberg.flink;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n import java.util.List;\n+import java.util.Locale;\n import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Expressions;\n-import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.After;\n-import org.junit.Assume;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.flink.table.api.Expressions.$;\n+\n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends FlinkCatalogTestBase {\n-  private static final String TABLE_NAME = \"test_table\";\n-  private TableEnvironment tEnv;\n-  private org.apache.iceberg.Table icebergTable;\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n \n   private final FileFormat format;\n-  private final boolean isStreamingJob;\n-\n-  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n-  public static Iterable<Object[]> parameters() {\n-    List<Object[]> parameters = Lists.newArrayList();\n-    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n-      for (Boolean isStreaming : new Boolean[] {true, false}) {\n-        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n-          String catalogName = (String) catalogParams[0];\n-          String[] baseNamespace = (String[]) catalogParams[1];\n-          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n-        }\n-      }\n-    }\n-    return parameters;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n   }\n \n-  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n-    super(catalogName, baseNamespace);\n-    this.format = format;\n-    this.isStreamingJob = isStreamingJob;\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n   }\n \n-  @Override\n-  protected TableEnvironment getTableEnv() {\n-    if (tEnv == null) {\n-      synchronized (this) {\n-        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-            .newInstance()\n-            .useBlinkPlanner();\n-        if (isStreamingJob) {\n-          settingsBuilder.inStreamingMode();\n-          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-          env.enableCheckpointing(400);\n-          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-        } else {\n-          settingsBuilder.inBatchMode();\n-          tEnv = TableEnvironment.create(settingsBuilder.build());\n-        }\n-      }\n-    }\n-    return tEnv;\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n   }\n \n-  @Before\n-  public void before() {\n-    sql(\"CREATE DATABASE %s\", flinkDatabase);\n-    sql(\"USE CATALOG %s\", catalogName);\n-    sql(\"USE %s\", DATABASE);\n-\n-    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    this.icebergTable = validationCatalog\n-        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n-            SimpleDataUtil.SCHEMA,\n-            PartitionSpec.unpartitioned(),\n-            properties);\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n   }\n \n-  @After\n-  public void clean() {\n-    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n-    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    // Register the rows into a temporary table.\n-    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, (String) null),\n-        Expressions.row(null, \"bar\")\n-    );\n-    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(1, \"hello\"),\n-        SimpleDataUtil.createRecord(2, \"world\"),\n-        SimpleDataUtil.createRecord(3, null),\n-        SimpleDataUtil.createRecord(null, \"bar\")\n-    ));\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n-\n-    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(1, \"a\")\n-    ));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(2, \"b\")\n-    ));\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+  }\n+\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n+      try {\n+        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n+      } catch (InterruptedException | ExecutionException e) {\n+        throw new RuntimeException(e);\n+      }\n+    });\n   }\n }\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -19,197 +19,135 @@\n \n package org.apache.iceberg.flink;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.Arrays;\n import java.util.List;\n-import java.util.Locale;\n import java.util.Map;\n-import java.util.concurrent.ExecutionException;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.TimeCharacteristic;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.util.FiniteTestSource;\n import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n-import org.apache.flink.table.api.TableResult;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.api.config.TableConfigOptions;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.util.DataFormatConverters;\n-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.flink.types.Row;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.data.Record;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.util.Pair;\n-import org.junit.Assert;\n+import org.junit.After;\n+import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n-\n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n-    );\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n-\n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n-\n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n-\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n   }\n \n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n-\n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n-    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n-  }\n-\n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU4OTg4OQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482589889", "body": "Minor: it would be nice to have more context here. Maybe the table loader should define a `toString` that could be used in the error message here.", "bodyText": "Minor: it would be nice to have more context here. Maybe the table loader should define a toString that could be used in the error message here.", "bodyHTML": "<p dir=\"auto\">Minor: it would be nice to have more context here. Maybe the table loader should define a <code>toString</code> that could be used in the error message here.</p>", "author": "rdblue", "createdAt": "2020-09-02T23:15:35Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -155,16 +158,29 @@ public Builder tableSchema(TableSchema newTableSchema) {\n       return this;\n     }\n \n+    public Builder overwrite(boolean newOverwrite) {\n+      this.overwrite = newOverwrite;\n+      return this;\n+    }\n+\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY2MDU1MQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482660551", "bodyText": "Defining the toString sounds good to me.", "author": "openinx", "createdAt": "2020-09-03T02:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU4OTg4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 8571d499c..96ce0ba6d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -158,29 +155,16 @@ public class FlinkSink {\n       return this;\n     }\n \n-    public Builder overwrite(boolean newOverwrite) {\n-      this.overwrite = newOverwrite;\n-      return this;\n-    }\n-\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n-      if (table == null) {\n-        tableLoader.open(hadoopConf);\n-        try (TableLoader loader = tableLoader) {\n-          this.table = loader.loadTable();\n-        } catch (IOException e) {\n-          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);\n-        }\n-      }\n-\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n-      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf, overwrite);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n \n       DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n", "next_change": {"commit": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 96ce0ba6d..4d067eea3 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -159,10 +161,18 @@ public class FlinkSink {\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);\n+        }\n+      }\n+\n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n       IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n \n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 4d067eea3..8571d499c 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n", "chunk": "@@ -174,7 +180,7 @@ public class FlinkSink {\n       }\n \n       IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n-      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf, overwrite);\n \n       DataStream<Void> returnStream = rowDataInput\n           .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5MjAxMA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592010", "body": "I just want to note that we don't encourage the use of `ReplacePartitions` because the data it deletes is implicit. It is better to specify what data should be overwritten, like in the new API for Spark:\r\n\r\n```scala\r\ndf.writeTo(\"iceberg.db.table\").overwrite($\"date\" === \"2020-09-01\")\r\n```\r\n\r\nIf Flink's semantics are to replace partitions for overwrite, then it should be okay. But I highly recommend being more explicit about data replacement.", "bodyText": "I just want to note that we don't encourage the use of ReplacePartitions because the data it deletes is implicit. It is better to specify what data should be overwritten, like in the new API for Spark:\ndf.writeTo(\"iceberg.db.table\").overwrite($\"date\" === \"2020-09-01\")\nIf Flink's semantics are to replace partitions for overwrite, then it should be okay. But I highly recommend being more explicit about data replacement.", "bodyHTML": "<p dir=\"auto\">I just want to note that we don't encourage the use of <code>ReplacePartitions</code> because the data it deletes is implicit. It is better to specify what data should be overwritten, like in the new API for Spark:</p>\n<div class=\"highlight highlight-source-scala position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"df.writeTo(&quot;iceberg.db.table&quot;).overwrite($&quot;date&quot; === &quot;2020-09-01&quot;)\n\"><pre>df.writeTo(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iceberg.db.table<span class=\"pl-pds\">\"</span></span>).overwrite($<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>date<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">===</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2020-09-01<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">If Flink's semantics are to replace partitions for overwrite, then it should be okay. But I highly recommend being more explicit about data replacement.</p>", "author": "rdblue", "createdAt": "2020-09-02T23:18:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -164,16 +168,51 @@ private void commitUpToCheckpoint(long checkpointId) {\n       pendingDataFiles.addAll(dataFiles);\n     }\n \n-    AppendFiles appendFiles = table.newAppend();\n-    pendingDataFiles.forEach(appendFiles::appendFile);\n-    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n-    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n-    appendFiles.commit();\n+    if (replacePartitions) {\n+      replacePartitions(pendingDataFiles, checkpointId);\n+    } else {\n+      append(pendingDataFiles, checkpointId);\n+    }\n \n     // Clear the committed data files from dataFilesPerCheckpoint.\n     pendingFileMap.clear();\n   }\n \n+  private void replacePartitions(List<DataFile> dataFiles, long checkpointId) {\n+    ReplacePartitions dynamicOverwrite = table.newReplacePartitions();", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY4ODg5NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482688895", "bodyText": "Yes, Flink's semantics are to replace partitions for overwrite, here is the 1, 2", "author": "openinx", "createdAt": "2020-09-03T03:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5MjAxMA=="}], "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java b/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\nindex c1bb2ecde..e0cfbafff 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n", "chunk": "@@ -168,51 +164,16 @@ class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n       pendingDataFiles.addAll(dataFiles);\n     }\n \n-    if (replacePartitions) {\n-      replacePartitions(pendingDataFiles, checkpointId);\n-    } else {\n-      append(pendingDataFiles, checkpointId);\n-    }\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n \n     // Clear the committed data files from dataFilesPerCheckpoint.\n     pendingFileMap.clear();\n   }\n \n-  private void replacePartitions(List<DataFile> dataFiles, long checkpointId) {\n-    ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n-\n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      dynamicOverwrite.addFile(file);\n-    }\n-\n-    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", checkpointId);\n-  }\n-\n-  private void append(List<DataFile> dataFiles, long checkpointId) {\n-    AppendFiles appendFiles = table.newAppend();\n-\n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      appendFiles.appendFile(file);\n-    }\n-\n-    commitOperation(appendFiles, numFiles, \"append\", checkpointId);\n-  }\n-\n-  private void commitOperation(SnapshotUpdate<?> operation, int numFiles, String description, long checkpointId) {\n-    LOG.info(\"Committing {} with {} files to table {}\", description, numFiles, table);\n-    operation.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n-    operation.set(FLINK_JOB_ID, flinkJobId);\n-\n-    long start = System.currentTimeMillis();\n-    operation.commit(); // abort is automatically called if this fails.\n-    long duration = System.currentTimeMillis() - start;\n-    LOG.info(\"Committed in {} ms\", duration);\n-  }\n-\n   @Override\n   public void processElement(StreamRecord<DataFile> element) {\n     this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n", "next_change": {"commit": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "changed_code": [{"header": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java b/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\nindex e0cfbafff..30091c3ac 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java\n", "chunk": "@@ -164,16 +168,51 @@ class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n       pendingDataFiles.addAll(dataFiles);\n     }\n \n-    AppendFiles appendFiles = table.newAppend();\n-    pendingDataFiles.forEach(appendFiles::appendFile);\n-    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n-    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n-    appendFiles.commit();\n+    if (replacePartitions) {\n+      replacePartitions(pendingDataFiles, checkpointId);\n+    } else {\n+      append(pendingDataFiles, checkpointId);\n+    }\n \n     // Clear the committed data files from dataFilesPerCheckpoint.\n     pendingFileMap.clear();\n   }\n \n+  private void replacePartitions(List<DataFile> dataFiles, long checkpointId) {\n+    ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+    int numFiles = 0;\n+    for (DataFile file : dataFiles) {\n+      numFiles += 1;\n+      dynamicOverwrite.addFile(file);\n+    }\n+\n+    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", checkpointId);\n+  }\n+\n+  private void append(List<DataFile> dataFiles, long checkpointId) {\n+    AppendFiles appendFiles = table.newAppend();\n+\n+    int numFiles = 0;\n+    for (DataFile file : dataFiles) {\n+      appendFiles.appendFile(file);\n+      numFiles++;\n+    }\n+\n+    commitOperation(appendFiles, numFiles, \"append\", checkpointId);\n+  }\n+\n+  private void commitOperation(SnapshotUpdate<?> operation, int numFiles, String description, long checkpointId) {\n+    LOG.info(\"Committing {} with {} files to table {}\", description, numFiles, table);\n+    operation.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    operation.set(FLINK_JOB_ID, flinkJobId);\n+\n+    long start = System.currentTimeMillis();\n+    operation.commit(); // abort is automatically called if this fails.\n+    long duration = System.currentTimeMillis() - start;\n+    LOG.info(\"Committed in {} ms\", duration);\n+  }\n+\n   @Override\n   public void processElement(StreamRecord<DataFile> element) {\n     this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5Mjk3Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592976", "body": "Can this be done automatically when a write completes, or is this a completely separate copy of the table?", "bodyText": "Can this be done automatically when a write completes, or is this a completely separate copy of the table?", "bodyHTML": "<p dir=\"auto\">Can this be done automatically when a write completes, or is this a completely separate copy of the table?</p>", "author": "rdblue", "createdAt": "2020-09-02T23:20:09Z", "path": "flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java", "diffHunk": "@@ -126,12 +126,16 @@ public static void assertTableRows(String tablePath, List<RowData> expected) thr\n     assertTableRecords(tablePath, expectedRecords);\n   }\n \n-  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n-    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n-    Table newTable = new HadoopTables().load(tablePath);\n-    try (CloseableIterable<Record> iterable = IcebergGenerics.read(newTable).build()) {\n+  public static void assertTableRecords(Table table, List<Record> expected) throws IOException {\n+    table.refresh();", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY5MzY1Mw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482693653", "bodyText": "Since we don't support to scan table by flink sql,   so we have to read records from iceberg table by iceberg Java API in unit tests.  In this test,  we get the icebergTable instance firstly, then the following test methods will commit the iceberg table by flink sql,  the icebergTable need a fresh to catch the latest changes.", "author": "openinx", "createdAt": "2020-09-03T04:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5Mjk3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java b/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\nindex 3eb7f1643..b377e54cd 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n", "chunk": "@@ -126,16 +126,12 @@ public class SimpleDataUtil {\n     assertTableRecords(tablePath, expectedRecords);\n   }\n \n-  public static void assertTableRecords(Table table, List<Record> expected) throws IOException {\n-    table.refresh();\n-    try (CloseableIterable<Record> iterable = IcebergGenerics.read(table).build()) {\n+  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n+    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n+    Table newTable = new HadoopTables().load(tablePath);\n+    try (CloseableIterable<Record> iterable = IcebergGenerics.read(newTable).build()) {\n       Assert.assertEquals(\"Should produce the expected record\",\n           Sets.newHashSet(expected), Sets.newHashSet(iterable));\n     }\n   }\n-\n-  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n-    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n-    assertTableRecords(new HadoopTables().load(tablePath), expected);\n-  }\n }\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java b/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\nindex b377e54cd..631ccb922 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n", "chunk": "@@ -126,12 +126,16 @@ public class SimpleDataUtil {\n     assertTableRecords(tablePath, expectedRecords);\n   }\n \n-  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n-    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n-    Table newTable = new HadoopTables().load(tablePath);\n-    try (CloseableIterable<Record> iterable = IcebergGenerics.read(newTable).build()) {\n+  public static void assertTableRecords(Table table, List<Record> expected) throws IOException {\n+    table.refresh();\n+    try (CloseableIterable<Record> iterable = IcebergGenerics.read(table).build()) {\n       Assert.assertEquals(\"Should produce the expected record\",\n           Sets.newHashSet(expected), Sets.newHashSet(iterable));\n     }\n   }\n+\n+  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n+    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n+    assertTableRecords(new HadoopTables().load(tablePath), expected);\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NDE5Mg==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482594192", "body": "It would be good to also have a partitioned test.", "bodyText": "It would be good to also have a partitioned test.", "bodyHTML": "<p dir=\"auto\">It would be good to also have a partitioned test.</p>", "author": "rdblue", "createdAt": "2020-09-02T23:21:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    // Register the rows into a temporary table.\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, (String) null),\n+        Expressions.row(null, \"bar\")\n+    );\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n+\n+    // Redirect the records from source table to destination table.\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, null),\n+        SimpleDataUtil.createRecord(null, \"bar\")\n+    ));\n+  }\n+\n+  @Test\n+  public void testOverwriteTable() throws Exception {", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a328cf866a622a867201be07e01826b04e8960e", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex e172fbe51..57a596aff 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -19,135 +19,197 @@\n \n package org.apache.iceberg.flink;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n import java.util.List;\n+import java.util.Locale;\n import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Expressions;\n-import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.After;\n-import org.junit.Assume;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.flink.table.api.Expressions.$;\n+\n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends FlinkCatalogTestBase {\n-  private static final String TABLE_NAME = \"test_table\";\n-  private TableEnvironment tEnv;\n-  private org.apache.iceberg.Table icebergTable;\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n \n   private final FileFormat format;\n-  private final boolean isStreamingJob;\n-\n-  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n-  public static Iterable<Object[]> parameters() {\n-    List<Object[]> parameters = Lists.newArrayList();\n-    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n-      for (Boolean isStreaming : new Boolean[] {true, false}) {\n-        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n-          String catalogName = (String) catalogParams[0];\n-          String[] baseNamespace = (String[]) catalogParams[1];\n-          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n-        }\n-      }\n-    }\n-    return parameters;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n   }\n \n-  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n-    super(catalogName, baseNamespace);\n-    this.format = format;\n-    this.isStreamingJob = isStreamingJob;\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n   }\n \n-  @Override\n-  protected TableEnvironment getTableEnv() {\n-    if (tEnv == null) {\n-      synchronized (this) {\n-        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n-            .newInstance()\n-            .useBlinkPlanner();\n-        if (isStreamingJob) {\n-          settingsBuilder.inStreamingMode();\n-          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-          env.enableCheckpointing(400);\n-          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n-        } else {\n-          settingsBuilder.inBatchMode();\n-          tEnv = TableEnvironment.create(settingsBuilder.build());\n-        }\n-      }\n-    }\n-    return tEnv;\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n   }\n \n-  @Before\n-  public void before() {\n-    sql(\"CREATE DATABASE %s\", flinkDatabase);\n-    sql(\"USE CATALOG %s\", catalogName);\n-    sql(\"USE %s\", DATABASE);\n-\n-    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    this.icebergTable = validationCatalog\n-        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n-            SimpleDataUtil.SCHEMA,\n-            PartitionSpec.unpartitioned(),\n-            properties);\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n   }\n \n-  @After\n-  public void clean() {\n-    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n-    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    // Register the rows into a temporary table.\n-    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n-        Expressions.row(1, \"hello\"),\n-        Expressions.row(2, \"world\"),\n-        Expressions.row(3, (String) null),\n-        Expressions.row(null, \"bar\")\n-    );\n-    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n \n     // Redirect the records from source table to destination table.\n-    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(1, \"hello\"),\n-        SimpleDataUtil.createRecord(2, \"world\"),\n-        SimpleDataUtil.createRecord(3, null),\n-        SimpleDataUtil.createRecord(null, \"bar\")\n-    ));\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n   }\n \n   @Test\n-  public void testOverwriteTable() throws Exception {\n-    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n-\n-    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(1, \"a\")\n-    ));\n-\n-    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n-    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n-        SimpleDataUtil.createRecord(2, \"b\")\n-    ));\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+  }\n+\n+  private static void waitComplete(TableResult result) {\n+    result.getJobClient().ifPresent(jobClient -> {\n+      try {\n+        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n+      } catch (InterruptedException | ExecutionException e) {\n+        throw new RuntimeException(e);\n+      }\n+    });\n   }\n }\n", "next_change": {"commit": "53a16d957035e970a6416ca6712972625a258a17", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 57a596aff..443cba9fa 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -19,197 +19,135 @@\n \n package org.apache.iceberg.flink;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.Arrays;\n import java.util.List;\n-import java.util.Locale;\n import java.util.Map;\n-import java.util.concurrent.ExecutionException;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.TimeCharacteristic;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.util.FiniteTestSource;\n import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n import org.apache.flink.table.api.TableEnvironment;\n-import org.apache.flink.table.api.TableResult;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n-import org.apache.flink.table.api.config.TableConfigOptions;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.util.DataFormatConverters;\n-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n-import org.apache.flink.test.util.AbstractTestBase;\n-import org.apache.flink.types.Row;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.data.Record;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.util.Pair;\n-import org.junit.Assert;\n+import org.junit.After;\n+import org.junit.Assume;\n import org.junit.Before;\n-import org.junit.Rule;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n-import static org.apache.flink.table.api.Expressions.$;\n-\n @RunWith(Parameterized.class)\n-public class TestFlinkTableSink extends AbstractTestBase {\n-  private static final Configuration CONF = new Configuration();\n-  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n-      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n-\n-  private static final String TABLE_NAME = \"flink_table\";\n-\n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n-  private String tablePath;\n-  private String warehouse;\n-  private Map<String, String> properties;\n-  private Catalog catalog;\n-  private StreamExecutionEnvironment env;\n-  private StreamTableEnvironment tEnv;\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n \n   private final FileFormat format;\n-  private final int parallelism;\n-\n-  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n-  public static Iterable<Object[]> data() {\n-    return Arrays.asList(\n-        new Object[] {\"avro\", 1},\n-        new Object[] {\"avro\", 2},\n-        new Object[] {\"orc\", 1},\n-        new Object[] {\"orc\", 2},\n-        new Object[] {\"parquet\", 1},\n-        new Object[] {\"parquet\", 2}\n-    );\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n   }\n \n-  public TestFlinkTableSink(String format, int parallelism) {\n-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n-    this.parallelism = parallelism;\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n   }\n \n-  @Before\n-  public void before() throws IOException {\n-    File folder = tempFolder.newFolder();\n-    warehouse = folder.getAbsolutePath();\n-\n-    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n-    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n-\n-    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n-    catalog = new HadoopCatalog(CONF, warehouse);\n-\n-    env = StreamExecutionEnvironment.getExecutionEnvironment();\n-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n-    env.enableCheckpointing(400);\n-    env.setParallelism(parallelism);\n-\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .useBlinkPlanner()\n-        .inStreamingMode()\n-        .build();\n-    tEnv = StreamTableEnvironment.create(env, settings);\n-    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    tEnv.executeSql(\"use catalog iceberg_catalog\");\n-    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n   }\n \n-  private DataStream<RowData> generateInputStream(List<Row> rows) {\n-    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n-    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n-        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n   }\n \n-  private Pair<List<Row>, List<Record>> generateData() {\n-    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n-    List<Row> rows = Lists.newArrayList();\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < worlds.length; i++) {\n-      rows.add(Row.of(i + 1, worlds[i]));\n-      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n-      expected.add(record);\n-      expected.add(record);\n-    }\n-    return Pair.of(rows, expected);\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n   }\n \n   @Test\n   public void testStreamSQL() throws Exception {\n-    Pair<List<Row>, List<Record>> data = generateData();\n-    List<Row> rows = data.first();\n-    List<Record> expected = data.second();\n-    DataStream<RowData> stream = generateInputStream(rows);\n-\n-    // Register the rows into a temporary table named 'sourceTable'.\n-    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+    // Register the rows into a temporary table.\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, \"foo\"),\n+        Expressions.row(4, \"bar\")\n+    );\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n \n     // Redirect the records from source table to destination table.\n-    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n-    TableResult result = tEnv.executeSql(insertSQL);\n-    waitComplete(result);\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n \n     // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, \"foo\"),\n+        SimpleDataUtil.createRecord(4, \"bar\")\n+    ));\n   }\n \n   @Test\n-  public void testBatchSQL() throws Exception {\n-    EnvironmentSettings settings = EnvironmentSettings\n-        .newInstance()\n-        .inBatchMode()\n-        .useBlinkPlanner()\n-        .build();\n-    TableEnvironment batchEnv = TableEnvironment.create(settings);\n-    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n-        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n-    batchEnv.executeSql(\"use catalog batch_catalog\");\n-    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n-\n-    // Create source table.\n-    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n-        SimpleDataUtil.SCHEMA,\n-        PartitionSpec.unpartitioned(),\n-        properties);\n-\n-    TableResult result;\n-    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n-    List<Record> expected = Lists.newArrayList();\n-    for (int i = 0; i < words.length; i++) {\n-      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n-      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n-      waitComplete(result);\n-    }\n-\n-    // Assert the table records as expected.\n-    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n-  }\n-\n-  private static void waitComplete(TableResult result) {\n-    result.getJobClient().ifPresent(jobClient -> {\n-      try {\n-        jobClient.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n-      } catch (InterruptedException | ExecutionException e) {\n-        throw new RuntimeException(e);\n-      }\n-    });\n+  public void testOverwriteTable() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 2, 'b'\", TABLE_NAME);\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(2, \"b\")\n+    ));\n   }\n }\n", "next_change": {"commit": "7587cb1643d92759751be85517cd1844ff8937d5", "changed_code": [{"header": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex 443cba9fa..0e211584e 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n", "chunk": "@@ -150,4 +142,79 @@ public class TestFlinkTableSink extends FlinkCatalogTestBase {\n         SimpleDataUtil.createRecord(2, \"b\")\n     ));\n   }\n+\n+  @Test\n+  public void testReplacePartitions() throws Exception {\n+    Assume.assumeFalse(\"Flink unbounded streaming does not support overwrite operation\", isStreamingJob);\n+    String tableName = \"test_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    sql(\"INSERT INTO %s SELECT 1, 'a'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 2, 'b'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 3, 'c'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s SELECT 4, 'b'\", tableName);\n+    sql(\"INSERT OVERWRITE %s SELECT 5, 'a'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(5, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"INSERT OVERWRITE %s PARTITION (data='a') SELECT 6\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(6, \"a\"),\n+        SimpleDataUtil.createRecord(4, \"b\"),\n+        SimpleDataUtil.createRecord(3, \"c\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n+\n+  @Test\n+  public void testInsertIntoPartition() throws Exception {\n+    String tableName = \"test_insert_into_partition\";\n+\n+    sql(\"CREATE TABLE %s(id INT, data VARCHAR) PARTITIONED BY (data) WITH ('write.format.default'='%s')\",\n+        tableName, format.name());\n+\n+    Table partitionedTable = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, tableName));\n+\n+    // Full partition.\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 1\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='a') SELECT 2\", tableName);\n+    sql(\"INSERT INTO %s PARTITION (data='b') SELECT 3\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\")\n+    ));\n+\n+    // Partial partition.\n+    sql(\"INSERT INTO %s SELECT 4, 'c'\", tableName);\n+    sql(\"INSERT INTO %s SELECT 5, 'd'\", tableName);\n+\n+    SimpleDataUtil.assertTableRecords(partitionedTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"a\"),\n+        SimpleDataUtil.createRecord(2, \"a\"),\n+        SimpleDataUtil.createRecord(3, \"b\"),\n+        SimpleDataUtil.createRecord(4, \"c\"),\n+        SimpleDataUtil.createRecord(5, \"d\")\n+    ));\n+\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, tableName);\n+  }\n }\n", "next_change": null}]}}]}}]}}, {"oid": "7a328cf866a622a867201be07e01826b04e8960e", "url": "https://github.com/apache/iceberg/commit/7a328cf866a622a867201be07e01826b04e8960e", "message": "Flink: Support table sink.", "committedDate": "2020-09-03T01:48:32Z", "type": "commit"}, {"oid": "23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "url": "https://github.com/apache/iceberg/commit/23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "message": "Minior fixes", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "f733b3e6c578632428d222b8d7a52621230e7459", "url": "https://github.com/apache/iceberg/commit/f733b3e6c578632428d222b8d7a52621230e7459", "message": "Remove the public modifier", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "url": "https://github.com/apache/iceberg/commit/17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "message": "Pass the table loader rather than CatalogLoader to IcebergTabelSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "aa3326689f99238dd007603931db5a4e7262d151", "url": "https://github.com/apache/iceberg/commit/aa3326689f99238dd007603931db5a4e7262d151", "message": "Remove the implementation for IcebergTableSink#configure because it is deprecated.", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "url": "https://github.com/apache/iceberg/commit/f9760c31094f8b1e7f99c4d9220b6116748bb355", "message": "Refactor the unit tests.", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "5393428404f5ab8724381e6f85ad458cb70c9504", "url": "https://github.com/apache/iceberg/commit/5393428404f5ab8724381e6f85ad458cb70c9504", "message": "Create an executeSQLAndWaitResult to execute sql", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "url": "https://github.com/apache/iceberg/commit/9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "message": "Implement the OverwritableTableSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "url": "https://github.com/apache/iceberg/commit/9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "message": "Add TODO to implement PartitionedTableSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "53a16d957035e970a6416ca6712972625a258a17", "url": "https://github.com/apache/iceberg/commit/53a16d957035e970a6416ca6712972625a258a17", "message": "Make the case extend the FlinkCatalogTestBase", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "01bc54d013dfec407fce9289cfebed2055ee6244", "url": "https://github.com/apache/iceberg/commit/01bc54d013dfec407fce9289cfebed2055ee6244", "message": "Fix the broken unit tests.", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "url": "https://github.com/apache/iceberg/commit/afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "message": "Minor changes", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "message": "Addressing comments from Ryan", "committedDate": "2020-09-03T03:56:48Z", "type": "commit"}, {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "message": "Addressing comments from Ryan", "committedDate": "2020-09-03T03:56:48Z", "type": "forcePushed"}, {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "message": "Minior changes.", "committedDate": "2020-09-03T04:27:33Z", "type": "commit"}, {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "message": "Minior changes.", "committedDate": "2020-09-03T04:27:33Z", "type": "forcePushed"}]}