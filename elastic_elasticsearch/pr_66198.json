{"pr_number": 66198, "pr_title": "Improve searchable snapshot mount time", "pr_author": "henningandersen", "pr_createdAt": "2020-12-11T09:16:33Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/66198", "timeline": [{"oid": "df3e8a010d9aa89ecbecc14453667371d7da8780", "url": "https://github.com/elastic/elasticsearch/commit/df3e8a010d9aa89ecbecc14453667371d7da8780", "message": "Improve searchable snapshot mount time\n\nReduce the range sizes we fetch during mounting to speed up mount time\nuntil shard started.\nOn resource constrained setups (rate limiter, disk or network), the time\nto mount multiple shards is proportional to the amount of data to fetch\nand for most files in a snapshot, we need to fetch only a small piece of\nthe files to start the shard.", "committedDate": "2020-12-11T09:13:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI3OTY0MA==", "url": "https://github.com/elastic/elasticsearch/pull/66198#discussion_r542279640", "body": "Suggest a comment so we remember why we are doing this:\r\n\r\n```suggestion\r\n    /**\r\n     * Starting up a shard involves reading small parts of some files from the repository, independently of the pre-warming process. If we\r\n     * expand those ranges using {@link CacheService#SNAPSHOT_CACHE_RANGE_SIZE_SETTING} then we end up reading quite a few 32MB ranges. If\r\n     * we read enough of these ranges for the restore throttling rate limiter to kick in then all the read threads will end up waiting on\r\n     * the throttle, blocking subsequent reads. By using a smaller read size during restore we avoid clogging up the rate limiter so much.\r\n     */\r\n    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\r\n```\r\n\r\nAlso suggest a similar comment on the other setting since this came up as a question in the investigation that led to this PR.\r\n\r\n```\r\n    /**\r\n     * If a search needs data from the repository then we expand it to a larger contiguous range whose size is determined by this setting,\r\n     * in anticipation of needing nearby data in subsequent reads. Repository reads typically have quite high latency (think ~100ms) and\r\n     * the default of 32MB for this setting represents the approximate point at which size starts to matter. In other words, reads of\r\n     * ranges smaller than 32MB don't usually happen much quicker, so we may as well expand all the way to 32MB ranges.\r\n     */\r\n    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\r\n```", "bodyText": "Suggest a comment so we remember why we are doing this:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\n          \n          \n            \n                /**\n          \n          \n            \n                 * Starting up a shard involves reading small parts of some files from the repository, independently of the pre-warming process. If we\n          \n          \n            \n                 * expand those ranges using {@link CacheService#SNAPSHOT_CACHE_RANGE_SIZE_SETTING} then we end up reading quite a few 32MB ranges. If\n          \n          \n            \n                 * we read enough of these ranges for the restore throttling rate limiter to kick in then all the read threads will end up waiting on\n          \n          \n            \n                 * the throttle, blocking subsequent reads. By using a smaller read size during restore we avoid clogging up the rate limiter so much.\n          \n          \n            \n                 */\n          \n          \n            \n                public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\n          \n      \n    \n    \n  \n\nAlso suggest a similar comment on the other setting since this came up as a question in the investigation that led to this PR.\n    /**\n     * If a search needs data from the repository then we expand it to a larger contiguous range whose size is determined by this setting,\n     * in anticipation of needing nearby data in subsequent reads. Repository reads typically have quite high latency (think ~100ms) and\n     * the default of 32MB for this setting represents the approximate point at which size starts to matter. In other words, reads of\n     * ranges smaller than 32MB don't usually happen much quicker, so we may as well expand all the way to 32MB ranges.\n     */\n    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RANGE_SIZE_SETTING = Setting.byteSizeSetting(", "bodyHTML": "<p dir=\"auto\">Suggest a comment so we remember why we are doing this:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"97\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-k x x-first\">public</span><span class=\"x\"> </span><span class=\"pl-k x\">static</span><span class=\"x\"> </span><span class=\"pl-k x\">final</span><span class=\"x\"> </span><span class=\"pl-k\"><span class=\"x\">Setting&lt;</span><span class=\"pl-smi x\">ByteSizeValue</span><span class=\"x\">&gt;</span></span><span class=\"x\"> </span><span class=\"pl-c1 x\">SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING</span><span class=\"x\"> </span><span class=\"pl-k x\">=</span><span class=\"x\"> </span><span class=\"pl-smi x\">Setting</span><span class=\"pl-k x\">.</span><span class=\"x x-last\">byteSizeSetting(</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"97\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-c\"><span class=\"pl-c x x-first x-last\">/**</span></span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"98\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> <span class=\"pl-smi\">Starting</span> up a shard involves reading small parts of some files from the repository, independently of the pre<span class=\"pl-k\">-</span>warming process. <span class=\"pl-smi\">If</span> we</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"99\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> expand those ranges using {<span class=\"pl-k\">@link</span> <span class=\"pl-smi\">CacheService</span>#<span class=\"pl-c1\">SNAPSHOT_CACHE_RANGE_SIZE_SETTING</span>} then we end up reading quite a few 32MB ranges. <span class=\"pl-smi\">If</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"100\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> we read enough of these ranges <span class=\"pl-k\">for</span> the restore throttling rate limiter to kick in then all the read threads will end up waiting on</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"101\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*</span> the throttle, blocking subsequent reads. <span class=\"pl-smi\">By</span> using a smaller read size during restore we avoid clogging up the rate limiter so much.</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"102\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">     <span class=\"pl-k\">*/</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"103\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">final</span> <span class=\"pl-k\">Setting&lt;<span class=\"pl-smi\">ByteSizeValue</span>&gt;</span> <span class=\"pl-c1\">SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING</span> <span class=\"pl-k\">=</span> <span class=\"pl-smi\">Setting</span><span class=\"pl-k\">.</span>byteSizeSetting(</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n\n<p dir=\"auto\">Also suggest a similar comment on the other setting since this came up as a question in the investigation that led to this PR.</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"    /**\n     * If a search needs data from the repository then we expand it to a larger contiguous range whose size is determined by this setting,\n     * in anticipation of needing nearby data in subsequent reads. Repository reads typically have quite high latency (think ~100ms) and\n     * the default of 32MB for this setting represents the approximate point at which size starts to matter. In other words, reads of\n     * ranges smaller than 32MB don't usually happen much quicker, so we may as well expand all the way to 32MB ranges.\n     */\n    public static final Setting&lt;ByteSizeValue&gt; SNAPSHOT_CACHE_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\"><pre><code>    /**\n     * If a search needs data from the repository then we expand it to a larger contiguous range whose size is determined by this setting,\n     * in anticipation of needing nearby data in subsequent reads. Repository reads typically have quite high latency (think ~100ms) and\n     * the default of 32MB for this setting represents the approximate point at which size starts to matter. In other words, reads of\n     * ranges smaller than 32MB don't usually happen much quicker, so we may as well expand all the way to 32MB ranges.\n     */\n    public static final Setting&lt;ByteSizeValue&gt; SNAPSHOT_CACHE_RANGE_SIZE_SETTING = Setting.byteSizeSetting(\n</code></pre></div>", "author": "DaveCTurner", "createdAt": "2020-12-14T10:35:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -71,6 +71,13 @@\n         MAX_SNAPSHOT_CACHE_RANGE_SIZE,                          // max\n         Setting.Property.NodeScope\n     );\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING = Setting.byteSizeSetting(", "originalCommit": "df3e8a010d9aa89ecbecc14453667371d7da8780", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6912d35a371ff84e1eba7416164ff84955fba868", "url": "https://github.com/elastic/elasticsearch/commit/6912d35a371ff84e1eba7416164ff84955fba868", "message": "Merge remote-tracking branch 'origin/master' into enhance_searchable_snapshot_mount_time", "committedDate": "2020-12-14T16:06:10Z", "type": "commit"}, {"oid": "7febee9a88af5ec5b4c1d049ce326b313eaaacc6", "url": "https://github.com/elastic/elasticsearch/commit/7febee9a88af5ec5b4c1d049ce326b313eaaacc6", "message": "Comments", "committedDate": "2020-12-14T16:08:04Z", "type": "commit"}]}