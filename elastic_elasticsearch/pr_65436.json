{"pr_number": 65436, "pr_title": "Also reroute after shard snapshot size fetch failure", "pr_author": "tlrx", "pr_createdAt": "2020-11-24T15:53:02Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/65436", "timeline": [{"oid": "9a9f1a51a4fca549952c027627247142c187e6f2", "url": "https://github.com/elastic/elasticsearch/commit/9a9f1a51a4fca549952c027627247142c187e6f2", "message": "Also reroute after shard snapshot size fetch failure", "committedDate": "2020-11-24T15:17:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwNTM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535505386", "body": "Should this go outside the assertBusy()?", "bodyText": "Should this go outside the assertBusy()?", "bodyHTML": "<p dir=\"auto\">Should this go outside the assertBusy()?</p>", "author": "henningandersen", "createdAt": "2020-12-03T19:07:19Z", "path": "x-pack/plugin/ccr/src/internalClusterTest/java/org/elasticsearch/xpack/ccr/CcrRepositoryIT.java", "diffHunk": "@@ -613,83 +602,76 @@ public void testCcrRepositoryFailsToFetchSnapshotShardSizes() throws Exception {\n                         && indicesStatsRequest.search() == false\n                         && indicesStatsRequest.fieldData() == false\n                     ) {\n-                        indicesStatsRequestsCount.incrementAndGet();\n+                        simulatedFailures.incrementAndGet();\n                         channel.sendResponse(new ElasticsearchException(\"simulated\"));\n-                        return;\n                     }\n                 }\n                 handler.messageReceived(request, channel, task);\n             });\n-            transportServices.add(mockTransportService);\n         }\n \n         final String followerIndex = \"follower\";\n         try {\n-            final String leaderCluster = CcrRepository.NAME_PREFIX + \"leader_cluster\";\n-            final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class);\n-            final Repository repository = repositoriesService.repository(leaderCluster);\n-            assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE));\n-            assertThat(repository.getMetadata().name(), equalTo(leaderCluster));\n-\n-            for (int i = 0; i < numberOfShards; i++) {\n-                final Index index = indexMetadata.getIndex();\n-                final int shardId = i;\n-                ElasticsearchException exception = expectThrows(ElasticsearchException.class,\n-                    () -> repository.getShardSnapshotStatus(\n-                        new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST),\n-                        new IndexId(index.getName(), index.getUUID()),\n-                        new ShardId(index, shardId)));\n-                assertThat(exception.getMessage(), equalTo(\"simulated\"));\n-            }\n-            assertThat(indicesStatsRequestsCount.getAndSet(0), equalTo(numberOfShards));\n-\n-            final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class);\n-            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n+            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n \n-            final PlainActionFuture<IndexRoutingTable> waitForRestoreInProgress = PlainActionFuture.newFuture();\n+            final PlainActionFuture<List<Long>> waitForAllShardSnapshotSizesFailures = PlainActionFuture.newFuture();\n             final ClusterStateListener listener = event -> {\n                 RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 if (restoreInProgress != null\n                     && restoreInProgress.isEmpty() == false\n                     && event.state().routingTable().hasIndex(followerIndex)) {\n-                    waitForRestoreInProgress.onResponse(event.state().routingTable().index(followerIndex));\n+                    try {\n+                        final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex);\n+                        assertBusy(() -> {\n+                            List<Long> sizes = indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).stream()\n+                                .filter(shard -> shard.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA)\n+                                .sorted(Comparator.comparingInt(ShardRouting::getId))\n+                                .map(shard -> snapshotsInfoService.snapshotShardSizes().getShardSize(shard))\n+                                .filter(Objects::nonNull)\n+                                .collect(Collectors.toList());\n+                            assertThat(sizes, hasSize(numberOfShards));\n+                            waitForAllShardSnapshotSizesFailures.onResponse(sizes);", "originalCommit": "9a9f1a51a4fca549952c027627247142c187e6f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk1MDAzNA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535950034", "bodyText": "Yes, actually there's no much value to capture the non-null sizes here and check again later that they are all equal to UNAVAILABLE_EXPECTED_SHARD_SIZE. We can check everything here, so I pushed 7c4480b", "author": "tlrx", "createdAt": "2020-12-04T09:16:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwNTM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUyNjA5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535526097", "body": "I wonder if we could use a smaller timeout like 1 minute or so?", "bodyText": "I wonder if we could use a smaller timeout like 1 minute or so?", "bodyHTML": "<p dir=\"auto\">I wonder if we could use a smaller timeout like 1 minute or so?</p>", "author": "henningandersen", "createdAt": "2020-12-03T19:33:44Z", "path": "x-pack/plugin/ccr/src/internalClusterTest/java/org/elasticsearch/xpack/ccr/CcrRepositoryIT.java", "diffHunk": "@@ -613,83 +602,76 @@ public void testCcrRepositoryFailsToFetchSnapshotShardSizes() throws Exception {\n                         && indicesStatsRequest.search() == false\n                         && indicesStatsRequest.fieldData() == false\n                     ) {\n-                        indicesStatsRequestsCount.incrementAndGet();\n+                        simulatedFailures.incrementAndGet();\n                         channel.sendResponse(new ElasticsearchException(\"simulated\"));\n-                        return;\n                     }\n                 }\n                 handler.messageReceived(request, channel, task);\n             });\n-            transportServices.add(mockTransportService);\n         }\n \n         final String followerIndex = \"follower\";\n         try {\n-            final String leaderCluster = CcrRepository.NAME_PREFIX + \"leader_cluster\";\n-            final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class);\n-            final Repository repository = repositoriesService.repository(leaderCluster);\n-            assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE));\n-            assertThat(repository.getMetadata().name(), equalTo(leaderCluster));\n-\n-            for (int i = 0; i < numberOfShards; i++) {\n-                final Index index = indexMetadata.getIndex();\n-                final int shardId = i;\n-                ElasticsearchException exception = expectThrows(ElasticsearchException.class,\n-                    () -> repository.getShardSnapshotStatus(\n-                        new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST),\n-                        new IndexId(index.getName(), index.getUUID()),\n-                        new ShardId(index, shardId)));\n-                assertThat(exception.getMessage(), equalTo(\"simulated\"));\n-            }\n-            assertThat(indicesStatsRequestsCount.getAndSet(0), equalTo(numberOfShards));\n-\n-            final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class);\n-            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n+            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n \n-            final PlainActionFuture<IndexRoutingTable> waitForRestoreInProgress = PlainActionFuture.newFuture();\n+            final PlainActionFuture<List<Long>> waitForAllShardSnapshotSizesFailures = PlainActionFuture.newFuture();\n             final ClusterStateListener listener = event -> {\n                 RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 if (restoreInProgress != null\n                     && restoreInProgress.isEmpty() == false\n                     && event.state().routingTable().hasIndex(followerIndex)) {\n-                    waitForRestoreInProgress.onResponse(event.state().routingTable().index(followerIndex));\n+                    try {\n+                        final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex);\n+                        assertBusy(() -> {\n+                            List<Long> sizes = indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).stream()\n+                                .filter(shard -> shard.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA)\n+                                .sorted(Comparator.comparingInt(ShardRouting::getId))\n+                                .map(shard -> snapshotsInfoService.snapshotShardSizes().getShardSize(shard))\n+                                .filter(Objects::nonNull)\n+                                .collect(Collectors.toList());\n+                            assertThat(sizes, hasSize(numberOfShards));\n+                            waitForAllShardSnapshotSizesFailures.onResponse(sizes);\n+                        });\n+                    } catch (Exception e) {\n+                        throw new AssertionError(\"Failed to retrieve all snapshot shard sizes\", e);\n+                    }\n                 }\n             };\n+\n+            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n             clusterService.addListener(listener);\n \n-            final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST)\n-                .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern(\"^(.*)$\")\n-                .renameReplacement(followerIndex)\n-                .masterNodeTimeout(TimeValue.MAX_VALUE)\n-                .indexSettings(Settings.builder()\n-                    .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex)\n-                    .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true));\n-            restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture());\n+            logger.debug(\"--> creating follower index [{}]\", followerIndex);\n+            followerClient().execute(PutFollowAction.INSTANCE, putFollow(leaderIndex, followerIndex, ActiveShardCount.NONE));\n \n-            final IndexRoutingTable indexRoutingTable = waitForRestoreInProgress.get(30L, TimeUnit.SECONDS);\n+            final List<Long> allShardSnapshotSizes = waitForAllShardSnapshotSizesFailures.get(30L, TimeUnit.SECONDS);\n             clusterService.removeListener(listener);\n \n-            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n-            assertBusy(() -> {\n-                SnapshotShardSizeInfo snapshotShardSizeInfo = snapshotsInfoService.snapshotShardSizes();\n-                for (int shardId = 0; shardId < numberOfShards; shardId++) {\n-                    final ShardRouting primary = indexRoutingTable.shard(shardId).primaryShard();\n-                    assertThat(snapshotShardSizeInfo.getShardSize(primary), equalTo(ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));\n-                    final long randomSize = randomNonNegativeLong();\n-                    assertThat(snapshotShardSizeInfo.getShardSize(primary, randomSize), equalTo(randomSize));\n-                }\n-            }, 60L, TimeUnit.SECONDS);\n-        } finally {\n-            transportServices.forEach(MockTransportService::clearAllRules);\n-        }\n+            assertTrue(allShardSnapshotSizes.stream().allMatch(size -> ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE == size));\n+            assertThat(simulatedFailures.get(), equalTo(numberOfShards));\n \n-        assertThat(indicesStatsRequestsCount.get(), equalTo(numberOfShards));\n-        blockCcrRestore.countDown();\n+            logger.debug(\"--> checking that SnapshotsInfoService does not know the real sizes fof snapshot shards\");\n+            final SnapshotShardSizeInfo snapshotShardSizeInfo = snapshotsInfoService.snapshotShardSizes();\n+            for (int shardId = 0; shardId < numberOfShards; shardId++) {\n+                final ShardRouting primary = clusterService.state().routingTable().index(followerIndex).shard(shardId).primaryShard();\n+                assertThat(snapshotShardSizeInfo.getShardSize(primary), equalTo(ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));\n+                final long randomSize = randomNonNegativeLong();\n+                assertThat(snapshotShardSizeInfo.getShardSize(primary, randomSize), equalTo(randomSize));\n+            }\n \n-        followerClient().admin().cluster().prepareReroute().get();\n-        ensureFollowerGreen(followerIndex);\n+            if (randomBoolean()) {\n+                logger.debug(\"--> create a random index to generate more cluster state updates\");\n+                final String randomIndex = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+                assertAcked(followerClient().admin().indices().prepareCreate(randomIndex).setMasterNodeTimeout(TimeValue.MAX_VALUE));", "originalCommit": "9a9f1a51a4fca549952c027627247142c187e6f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk1MDEzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535950135", "bodyText": "This timeout is imposed by CcrIntegTestCase.setupMasterNodeRequestsValidatorOnFollowerCluster() (see #60070 for more background)", "author": "tlrx", "createdAt": "2020-12-04T09:17:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUyNjA5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODEzNjMzNA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r538136334", "bodyText": "\ud83d\udc4d thanks for the clarification.", "author": "henningandersen", "createdAt": "2020-12-08T08:32:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUyNjA5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUyNjQ0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535526445", "body": "nit:\r\n```suggestion\r\n            logger.debug(\"--> checking that SnapshotsInfoService does not know the real sizes of snapshot shards\");\r\n```", "bodyText": "nit:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        logger.debug(\"--> checking that SnapshotsInfoService does not know the real sizes fof snapshot shards\");\n          \n          \n            \n                        logger.debug(\"--> checking that SnapshotsInfoService does not know the real sizes of snapshot shards\");", "bodyHTML": "<p dir=\"auto\">nit:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">            logger<span class=\"pl-k\">.</span>debug(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--&gt; checking that SnapshotsInfoService does not know the real sizes <span class=\"x x-first x-last\">fof</span> snapshot shards<span class=\"pl-pds\">\"</span></span>);</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">            logger<span class=\"pl-k\">.</span>debug(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--&gt; checking that SnapshotsInfoService does not know the real sizes <span class=\"x x-first x-last\">of</span> snapshot shards<span class=\"pl-pds\">\"</span></span>);</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "henningandersen", "createdAt": "2020-12-03T19:34:07Z", "path": "x-pack/plugin/ccr/src/internalClusterTest/java/org/elasticsearch/xpack/ccr/CcrRepositoryIT.java", "diffHunk": "@@ -613,83 +602,76 @@ public void testCcrRepositoryFailsToFetchSnapshotShardSizes() throws Exception {\n                         && indicesStatsRequest.search() == false\n                         && indicesStatsRequest.fieldData() == false\n                     ) {\n-                        indicesStatsRequestsCount.incrementAndGet();\n+                        simulatedFailures.incrementAndGet();\n                         channel.sendResponse(new ElasticsearchException(\"simulated\"));\n-                        return;\n                     }\n                 }\n                 handler.messageReceived(request, channel, task);\n             });\n-            transportServices.add(mockTransportService);\n         }\n \n         final String followerIndex = \"follower\";\n         try {\n-            final String leaderCluster = CcrRepository.NAME_PREFIX + \"leader_cluster\";\n-            final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class);\n-            final Repository repository = repositoriesService.repository(leaderCluster);\n-            assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE));\n-            assertThat(repository.getMetadata().name(), equalTo(leaderCluster));\n-\n-            for (int i = 0; i < numberOfShards; i++) {\n-                final Index index = indexMetadata.getIndex();\n-                final int shardId = i;\n-                ElasticsearchException exception = expectThrows(ElasticsearchException.class,\n-                    () -> repository.getShardSnapshotStatus(\n-                        new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST),\n-                        new IndexId(index.getName(), index.getUUID()),\n-                        new ShardId(index, shardId)));\n-                assertThat(exception.getMessage(), equalTo(\"simulated\"));\n-            }\n-            assertThat(indicesStatsRequestsCount.getAndSet(0), equalTo(numberOfShards));\n-\n-            final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class);\n-            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n+            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n \n-            final PlainActionFuture<IndexRoutingTable> waitForRestoreInProgress = PlainActionFuture.newFuture();\n+            final PlainActionFuture<List<Long>> waitForAllShardSnapshotSizesFailures = PlainActionFuture.newFuture();\n             final ClusterStateListener listener = event -> {\n                 RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 if (restoreInProgress != null\n                     && restoreInProgress.isEmpty() == false\n                     && event.state().routingTable().hasIndex(followerIndex)) {\n-                    waitForRestoreInProgress.onResponse(event.state().routingTable().index(followerIndex));\n+                    try {\n+                        final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex);\n+                        assertBusy(() -> {\n+                            List<Long> sizes = indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).stream()\n+                                .filter(shard -> shard.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA)\n+                                .sorted(Comparator.comparingInt(ShardRouting::getId))\n+                                .map(shard -> snapshotsInfoService.snapshotShardSizes().getShardSize(shard))\n+                                .filter(Objects::nonNull)\n+                                .collect(Collectors.toList());\n+                            assertThat(sizes, hasSize(numberOfShards));\n+                            waitForAllShardSnapshotSizesFailures.onResponse(sizes);\n+                        });\n+                    } catch (Exception e) {\n+                        throw new AssertionError(\"Failed to retrieve all snapshot shard sizes\", e);\n+                    }\n                 }\n             };\n+\n+            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n             clusterService.addListener(listener);\n \n-            final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST)\n-                .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern(\"^(.*)$\")\n-                .renameReplacement(followerIndex)\n-                .masterNodeTimeout(TimeValue.MAX_VALUE)\n-                .indexSettings(Settings.builder()\n-                    .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex)\n-                    .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true));\n-            restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture());\n+            logger.debug(\"--> creating follower index [{}]\", followerIndex);\n+            followerClient().execute(PutFollowAction.INSTANCE, putFollow(leaderIndex, followerIndex, ActiveShardCount.NONE));\n \n-            final IndexRoutingTable indexRoutingTable = waitForRestoreInProgress.get(30L, TimeUnit.SECONDS);\n+            final List<Long> allShardSnapshotSizes = waitForAllShardSnapshotSizesFailures.get(30L, TimeUnit.SECONDS);\n             clusterService.removeListener(listener);\n \n-            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n-            assertBusy(() -> {\n-                SnapshotShardSizeInfo snapshotShardSizeInfo = snapshotsInfoService.snapshotShardSizes();\n-                for (int shardId = 0; shardId < numberOfShards; shardId++) {\n-                    final ShardRouting primary = indexRoutingTable.shard(shardId).primaryShard();\n-                    assertThat(snapshotShardSizeInfo.getShardSize(primary), equalTo(ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE));\n-                    final long randomSize = randomNonNegativeLong();\n-                    assertThat(snapshotShardSizeInfo.getShardSize(primary, randomSize), equalTo(randomSize));\n-                }\n-            }, 60L, TimeUnit.SECONDS);\n-        } finally {\n-            transportServices.forEach(MockTransportService::clearAllRules);\n-        }\n+            assertTrue(allShardSnapshotSizes.stream().allMatch(size -> ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE == size));\n+            assertThat(simulatedFailures.get(), equalTo(numberOfShards));\n \n-        assertThat(indicesStatsRequestsCount.get(), equalTo(numberOfShards));\n-        blockCcrRestore.countDown();\n+            logger.debug(\"--> checking that SnapshotsInfoService does not know the real sizes fof snapshot shards\");", "originalCommit": "9a9f1a51a4fca549952c027627247142c187e6f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk1MDA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535950066", "bodyText": "fhanks :) I pushed 3b0ab8f", "author": "tlrx", "createdAt": "2020-12-04T09:16:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUyNjQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzNDE5NA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535534194", "body": "It seems a bit delicate to wait inside the cluster-listener, I think that could lead to strange failures. On the other hand, it seems quite deliberate, since the old version did the assertBusy outside the listener. I would be curious to know more details on why this needed to be like this?", "bodyText": "It seems a bit delicate to wait inside the cluster-listener, I think that could lead to strange failures. On the other hand, it seems quite deliberate, since the old version did the assertBusy outside the listener. I would be curious to know more details on why this needed to be like this?", "bodyHTML": "<p dir=\"auto\">It seems a bit delicate to wait inside the cluster-listener, I think that could lead to strange failures. On the other hand, it seems quite deliberate, since the old version did the assertBusy outside the listener. I would be curious to know more details on why this needed to be like this?</p>", "author": "henningandersen", "createdAt": "2020-12-03T19:42:58Z", "path": "x-pack/plugin/ccr/src/internalClusterTest/java/org/elasticsearch/xpack/ccr/CcrRepositoryIT.java", "diffHunk": "@@ -613,83 +602,76 @@ public void testCcrRepositoryFailsToFetchSnapshotShardSizes() throws Exception {\n                         && indicesStatsRequest.search() == false\n                         && indicesStatsRequest.fieldData() == false\n                     ) {\n-                        indicesStatsRequestsCount.incrementAndGet();\n+                        simulatedFailures.incrementAndGet();\n                         channel.sendResponse(new ElasticsearchException(\"simulated\"));\n-                        return;\n                     }\n                 }\n                 handler.messageReceived(request, channel, task);\n             });\n-            transportServices.add(mockTransportService);\n         }\n \n         final String followerIndex = \"follower\";\n         try {\n-            final String leaderCluster = CcrRepository.NAME_PREFIX + \"leader_cluster\";\n-            final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class);\n-            final Repository repository = repositoriesService.repository(leaderCluster);\n-            assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE));\n-            assertThat(repository.getMetadata().name(), equalTo(leaderCluster));\n-\n-            for (int i = 0; i < numberOfShards; i++) {\n-                final Index index = indexMetadata.getIndex();\n-                final int shardId = i;\n-                ElasticsearchException exception = expectThrows(ElasticsearchException.class,\n-                    () -> repository.getShardSnapshotStatus(\n-                        new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST),\n-                        new IndexId(index.getName(), index.getUUID()),\n-                        new ShardId(index, shardId)));\n-                assertThat(exception.getMessage(), equalTo(\"simulated\"));\n-            }\n-            assertThat(indicesStatsRequestsCount.getAndSet(0), equalTo(numberOfShards));\n-\n-            final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class);\n-            final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class);\n+            final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class);\n \n-            final PlainActionFuture<IndexRoutingTable> waitForRestoreInProgress = PlainActionFuture.newFuture();\n+            final PlainActionFuture<List<Long>> waitForAllShardSnapshotSizesFailures = PlainActionFuture.newFuture();\n             final ClusterStateListener listener = event -> {\n                 RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 if (restoreInProgress != null\n                     && restoreInProgress.isEmpty() == false\n                     && event.state().routingTable().hasIndex(followerIndex)) {\n-                    waitForRestoreInProgress.onResponse(event.state().routingTable().index(followerIndex));\n+                    try {\n+                        final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex);\n+                        assertBusy(() -> {", "originalCommit": "9a9f1a51a4fca549952c027627247142c187e6f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk0OTk4NA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535949984", "bodyText": "InternalSnapshotsInfoService listens for cluster state updates while shard snapshot size fetchings are executing in another thread pool (triggering more cluster state updates when the fetching finishes). With batched cluster states and intermediate reroutes, it is hard to catch the moment where all shards are still unassigned with FETCHING_SHARD_DATA while the InternalSnapshotsInfoService has the result of the size fetchings...\nBecause size fetching results are cleared up once corresponding shards are assigned, using assertBusy() here is the only way I can think of to catch the exact values fetched by the service.\nThe previous test was inadapted as shards remain unassigned until a reroute was triggered, whereas we want InternalSnapshotsInfoService to move the allocation of shards forward even in case of failure.", "author": "tlrx", "createdAt": "2020-12-04T09:16:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzNDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODEzNDQ1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r538134453", "bodyText": "Can we add a comment on the importance of the ordering of cluster service listeners? Something like:\n// this assertBusy completes because the listener is added after the InternalSnapshotsInfoService and ClusterService preserves the order of listeners.\nI think that such a comment can help if this breaks in the future...", "author": "henningandersen", "createdAt": "2020-12-08T08:29:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzNDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODE0MTg5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r538141893", "bodyText": "Sure, thanks, I'll borrow your comment.", "author": "tlrx", "createdAt": "2020-12-08T08:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzNDE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDk0NA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535540944", "body": "I think we could change the implementation to only have one map now, by putting the `UNAVAILABLE_EXPECTED_SHARD_SIZE` value into the map when fetching fails. But we can leave that to follow-ups, we may want to refine the failure handling and that could benefit from having them in separate maps.", "bodyText": "I think we could change the implementation to only have one map now, by putting the UNAVAILABLE_EXPECTED_SHARD_SIZE value into the map when fetching fails. But we can leave that to follow-ups, we may want to refine the failure handling and that could benefit from having them in separate maps.", "bodyHTML": "<p dir=\"auto\">I think we could change the implementation to only have one map now, by putting the <code>UNAVAILABLE_EXPECTED_SHARD_SIZE</code> value into the map when fetching fails. But we can leave that to follow-ups, we may want to refine the failure handling and that could benefit from having them in separate maps.</p>", "author": "henningandersen", "createdAt": "2020-12-03T19:53:41Z", "path": "server/src/main/java/org/elasticsearch/snapshots/InternalSnapshotsInfoService.java", "diffHunk": "@@ -120,7 +120,7 @@ private void setMaxConcurrentFetches(Integer maxConcurrentFetches) {\n     @Override\n     public SnapshotShardSizeInfo snapshotShardSizes() {\n         synchronized (mutex){\n-            final ImmutableOpenMap.Builder<SnapshotShard, Long> snapshotShardSizes = ImmutableOpenMap.builder(knownSnapshotShardSizes);\n+            final ImmutableOpenMap.Builder<SnapshotShard, Long> snapshotShardSizes = ImmutableOpenMap.builder(knownSnapshotShards);\n             for (SnapshotShard snapshotShard : failedSnapshotShards) {", "originalCommit": "9a9f1a51a4fca549952c027627247142c187e6f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk0OTg4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r535949883", "bodyText": "Having two maps makes it easier (at least for me) to reason about how failures are handled, so I'd prefer to keep it like this for now if that's OK for you.", "author": "tlrx", "createdAt": "2020-12-04T09:16:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDk0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODEyNTE3OA==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r538125178", "bodyText": "That is fine, but could we then check whether failedSnapshotShards.isEmpty() and return knownSnapshotShards directly if it is? Just to not waste cpu cycles unnecessarily on the happy case.", "author": "henningandersen", "createdAt": "2020-12-08T08:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDk0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODE0MzI0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/65436#discussion_r538143243", "bodyText": "Makes sense.", "author": "tlrx", "createdAt": "2020-12-08T08:42:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDk0NA=="}], "type": "inlineReview"}, {"oid": "a7494327c556aacf1d48020386af1f971bc4a7ed", "url": "https://github.com/elastic/elasticsearch/commit/a7494327c556aacf1d48020386af1f971bc4a7ed", "message": "Merge branch 'master' into reroute-after-snapshot-shard-size-failure", "committedDate": "2020-12-04T08:29:46Z", "type": "commit"}, {"oid": "7c4480bcf0af405532382578f418ba634fa5da0f", "url": "https://github.com/elastic/elasticsearch/commit/7c4480bcf0af405532382578f418ba634fa5da0f", "message": "waitForAllShardSnapshotSizesFailures", "committedDate": "2020-12-04T08:38:43Z", "type": "commit"}, {"oid": "3b0ab8fa2a80e91ce4e48df03d7ea76245fb76d3", "url": "https://github.com/elastic/elasticsearch/commit/3b0ab8fa2a80e91ce4e48df03d7ea76245fb76d3", "message": "fof", "committedDate": "2020-12-04T08:40:36Z", "type": "commit"}, {"oid": "6930102cfdc1fdd271cc6852b49d5f54bcad6074", "url": "https://github.com/elastic/elasticsearch/commit/6930102cfdc1fdd271cc6852b49d5f54bcad6074", "message": "Merge branch 'master' into reroute-after-snapshot-shard-size-failure", "committedDate": "2020-12-08T08:39:24Z", "type": "commit"}, {"oid": "cb19a836035249f56976f487433216dbbef6b2f5", "url": "https://github.com/elastic/elasticsearch/commit/cb19a836035249f56976f487433216dbbef6b2f5", "message": "feedback", "committedDate": "2020-12-08T08:56:40Z", "type": "commit"}]}