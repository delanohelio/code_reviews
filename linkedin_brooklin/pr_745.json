{"pr_number": 745, "pr_title": "Stop all the tasks on Session Expiry", "pr_author": "vmaheshw", "pr_createdAt": "2020-08-18T17:33:51Z", "pr_url": "https://github.com/linkedin/brooklin/pull/745", "timeline": [{"oid": "c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "url": "https://github.com/linkedin/brooklin/commit/c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "message": "Merge pull request #1 from linkedin/master\n\nPull latest", "committedDate": "2019-11-18T20:06:44Z", "type": "commit"}, {"oid": "7097d2756bab879f33c6b621322ac6eafb69b05f", "url": "https://github.com/linkedin/brooklin/commit/7097d2756bab879f33c6b621322ac6eafb69b05f", "message": "Merge branch 'master' of github.com:linkedin/brooklin", "committedDate": "2020-08-14T03:52:00Z", "type": "commit"}, {"oid": "4dd82a464c3e124e9376f3f8517f83b2d053ccbd", "url": "https://github.com/linkedin/brooklin/commit/4dd82a464c3e124e9376f3f8517f83b2d053ccbd", "message": "Stop all tasks on session expiry", "committedDate": "2020-08-18T17:30:37Z", "type": "commit"}, {"oid": "db6288803df23d5931583a1e00bec8506bf4a978", "url": "https://github.com/linkedin/brooklin/commit/db6288803df23d5931583a1e00bec8506bf4a978", "message": "Add tests and call onDatastreamChange during session expired", "committedDate": "2020-08-25T20:33:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU3NzM2OQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477577369", "body": "Let's add a comment here saying that access to this should be synchronized?", "bodyText": "Let's add a comment here saying that access to this should be synchronized?", "bodyHTML": "<p dir=\"auto\">Let's add a comment here saying that access to this should be synchronized?</p>", "author": "somandal", "createdAt": "2020-08-26T20:42:57Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -168,7 +168,7 @@\n   private final CheckpointProvider _cpProvider;\n   private final Map<String, TransportProviderAdmin> _transportProviderAdmins = new HashMap<>();\n   private final CoordinatorEventBlockingQueue _eventQueue;\n-  private final CoordinatorEventProcessor _eventThread;\n+  private CoordinatorEventProcessor _eventThread;", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU4ODkxOQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477588919", "body": "Can you move this function closer to where you have `waitForEventThreadToJoin`?", "bodyText": "Can you move this function closer to where you have waitForEventThreadToJoin?", "bodyHTML": "<p dir=\"auto\">Can you move this function closer to where you have <code>waitForEventThreadToJoin</code>?</p>", "author": "somandal", "createdAt": "2020-08-26T21:05:43Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -231,6 +233,25 @@ public Coordinator(CachedDatastreamReader datastreamCache, CoordinatorConfig con\n     _metrics = new CoordinatorMetrics(this);\n   }\n \n+  private synchronized void createEventThread() {\n+    _eventThread = new CoordinatorEventProcessor();\n+    _eventThread.setDaemon(true);\n+  }\n+\n+  private synchronized boolean stopEventThread() {", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU5MDAzMg==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477590032", "body": "nit, not your fault but can you fix the comment\r\n\"Exception caught while waiting for the event thread to stop\"", "bodyText": "nit, not your fault but can you fix the comment\n\"Exception caught while waiting for the event thread to stop\"", "bodyHTML": "<p dir=\"auto\">nit, not your fault but can you fix the comment<br>\n\"Exception caught while waiting for the event thread to stop\"</p>", "author": "somandal", "createdAt": "2020-08-26T21:08:00Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -323,6 +339,16 @@ public void stop() {\n     _log.info(\"Coordinator stopped\");\n   }\n \n+  private synchronized boolean waitForEventThreadToJoin() {\n+    try {\n+      _eventThread.join(EVENT_THREAD_LONG_JOIN_TIMEOUT);\n+    } catch (InterruptedException e) {\n+      _log.warn(\"Exception caught while waiting event thread to stop\", e);", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU5MTExMQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477591111", "body": "nit, not your fault but can you fix the comment:\r\n\"Exception caught while stopping the CoordinatorEventProcessor\"", "bodyText": "nit, not your fault but can you fix the comment:\n\"Exception caught while stopping the CoordinatorEventProcessor\"", "bodyHTML": "<p dir=\"auto\">nit, not your fault but can you fix the comment:<br>\n\"Exception caught while stopping the CoordinatorEventProcessor\"</p>", "author": "somandal", "createdAt": "2020-08-26T21:10:06Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -231,6 +233,25 @@ public Coordinator(CachedDatastreamReader datastreamCache, CoordinatorConfig con\n     _metrics = new CoordinatorMetrics(this);\n   }\n \n+  private synchronized void createEventThread() {\n+    _eventThread = new CoordinatorEventProcessor();\n+    _eventThread.setDaemon(true);\n+  }\n+\n+  private synchronized boolean stopEventThread() {\n+    // interrupt the thread if it's not gracefully shutdown\n+    while (_eventThread.isAlive()) {\n+      try {\n+        _eventThread.interrupt();\n+        _eventThread.join(EVENT_THREAD_SHORT_JOIN_TIMEOUT);\n+      } catch (InterruptedException e) {\n+        _log.warn(\"Exception caught while stopping coordinator\", e);", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU5MzQzMQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477593431", "body": "nit: remove extra line", "bodyText": "nit: remove extra line", "bodyHTML": "<p dir=\"auto\">nit: remove extra line</p>", "author": "somandal", "createdAt": "2020-08-26T21:14:47Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -1550,6 +1643,12 @@ public String getClusterName() {\n     return _clusterName;\n   }\n \n+  @VisibleForTesting\n+  CoordinatorEventProcessor getEventThread() {\n+    return _eventThread;\n+  }\n+\n+", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU5NTgzNw==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477595837", "body": "Can you update the diagram at the top of this file to add the new listener onSessionExpired() event?", "bodyText": "Can you update the diagram at the top of this file to add the new listener onSessionExpired() event?", "bodyHTML": "<p dir=\"auto\">Can you update the diagram at the top of this file to add the new listener onSessionExpired() event?</p>", "author": "somandal", "createdAt": "2020-08-26T21:19:43Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1299,6 +1299,11 @@ public static String parseHostnameFromZkInstance(String instance) {\n      * @param notifyTimestamp the timestamp that partitionMovement is triggered\n      */\n     void onPartitionMovement(Long notifyTimestamp);\n+\n+    /**\n+     * onSessionExpired is called when the zookeeper session expires.\n+     */\n+    void onSessionExpired();", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYwODUxNw==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477608517", "body": "Wondering if we should use a variable for better readability. One needs to really look at the callers to make sense of what true/false mean here (or maybe I'm just slow today). Can we potentially use a variable `exceptionCaught`, default it to `false` and set it to `true` when we catch the exception?\r\n\r\nIf you like this suggestion, can you modify `waitForEventThreadToJoin` to do the same?", "bodyText": "Wondering if we should use a variable for better readability. One needs to really look at the callers to make sense of what true/false mean here (or maybe I'm just slow today). Can we potentially use a variable exceptionCaught, default it to false and set it to true when we catch the exception?\nIf you like this suggestion, can you modify waitForEventThreadToJoin to do the same?", "bodyHTML": "<p dir=\"auto\">Wondering if we should use a variable for better readability. One needs to really look at the callers to make sense of what true/false mean here (or maybe I'm just slow today). Can we potentially use a variable <code>exceptionCaught</code>, default it to <code>false</code> and set it to <code>true</code> when we catch the exception?</p>\n<p dir=\"auto\">If you like this suggestion, can you modify <code>waitForEventThreadToJoin</code> to do the same?</p>", "author": "somandal", "createdAt": "2020-08-26T21:48:00Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -231,6 +233,25 @@ public Coordinator(CachedDatastreamReader datastreamCache, CoordinatorConfig con\n     _metrics = new CoordinatorMetrics(this);\n   }\n \n+  private synchronized void createEventThread() {\n+    _eventThread = new CoordinatorEventProcessor();\n+    _eventThread.setDaemon(true);\n+  }\n+\n+  private synchronized boolean stopEventThread() {\n+    // interrupt the thread if it's not gracefully shutdown\n+    while (_eventThread.isAlive()) {\n+      try {\n+        _eventThread.interrupt();\n+        _eventThread.join(EVENT_THREAD_SHORT_JOIN_TIMEOUT);\n+      } catch (InterruptedException e) {\n+        _log.warn(\"Exception caught while stopping coordinator\", e);\n+        return true;\n+      }\n+    }\n+    return false;", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYwOTAwNA==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477609004", "body": "nit: add an empty line before this?", "bodyText": "nit: add an empty line before this?", "bodyHTML": "<p dir=\"auto\">nit: add an empty line before this?</p>", "author": "somandal", "createdAt": "2020-08-26T21:49:14Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -286,22 +313,11 @@ public void stop() {\n     _eventQueue.put(CoordinatorEvent.NO_OP_EVENT);\n \n     // wait for eventThread to gracefully finish\n-    try {\n-      _eventThread.join(EVENT_THREAD_LONG_JOIN_TIMEOUT);\n-    } catch (InterruptedException e) {\n-      _log.warn(\"Exception caught while waiting event thread to stop\", e);\n+    if (waitForEventThreadToJoin()) {\n       return;\n     }\n-\n-    // interrupt the thread if it's not gracefully shutdown\n-    while (_eventThread.isAlive()) {\n-      try {\n-        _eventThread.interrupt();\n-        _eventThread.join(EVENT_THREAD_SHORT_JOIN_TIMEOUT);\n-      } catch (InterruptedException e) {\n-        _log.warn(\"Exception caught while stopping coordinator\", e);\n-        return;\n-      }\n+    if (stopEventThread()) {", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxMzEwMw==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477613103", "body": "Just to make sure, setting this to false now vs. after we cancel the future shouldn't make a difference?", "bodyText": "Just to make sure, setting this to false now vs. after we cancel the future shouldn't make a difference?", "bodyHTML": "<p dir=\"auto\">Just to make sure, setting this to false now vs. after we cancel the future shouldn't make a difference?</p>", "author": "somandal", "createdAt": "2020-08-26T21:58:40Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -415,6 +441,74 @@ public void onPartitionMovement(Long notifyTimestamp) {\n     _eventQueue.put(CoordinatorEvent.createPartitionMovementEvent(notifyTimestamp));\n     _log.info(\"Coordinator::onPartitionMovement completed successfully\");\n   }\n+\n+  /**\n+   * {@inheritDoc}\n+   * Stop all the tasks and wait for new session to connect.\n+   */\n+  @Override\n+  public void onSessionExpired() {\n+    if (_shutdown) {\n+      return;\n+    }\n+    stopEventThread();\n+\n+    _leaderDatastreamAddOrDeleteEventScheduled.set(false);", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDUyOTg3Mw==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r480529873", "bodyText": "right.", "author": "vmaheshw", "createdAt": "2020-09-01T01:07:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxMzEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNzI4NA==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477617284", "body": "In the past we've seen some issues with handleLeaderDoAssignment() getting interrupted during cleanup and causing bad state such that we couldn't do future leader assignments anymore for BMM. This would require a full cluster restart to fix the state. We don't know the root cause yet, so we don't know how to  fix this scenario. Won't only calling stopEventThread() make such occurrences more frequent? If for some reason we cannot give more time to the eventThread to stop naturally, then how do we avoid that problem?\r\n\r\nHave we added the relevant debug logs we needed for that issue (I recall it being an action item in one of the ZK session expiry documents that listed out ZK related issues)? If not, feel free to assign that item to me so that if it happens again I'll have something to debug with. Don't want to make that problem more frequent without appropriate logs in place. I've wasted 2-3 debugging sessions on it in the past to find out that it was related to early killing of handleLeaderDoAssignment, and after the timeout was increased, we stopped seeing it (so far). Which is why I haven't had a chance to root cause it since. Unit test don't help either.", "bodyText": "In the past we've seen some issues with handleLeaderDoAssignment() getting interrupted during cleanup and causing bad state such that we couldn't do future leader assignments anymore for BMM. This would require a full cluster restart to fix the state. We don't know the root cause yet, so we don't know how to  fix this scenario. Won't only calling stopEventThread() make such occurrences more frequent? If for some reason we cannot give more time to the eventThread to stop naturally, then how do we avoid that problem?\nHave we added the relevant debug logs we needed for that issue (I recall it being an action item in one of the ZK session expiry documents that listed out ZK related issues)? If not, feel free to assign that item to me so that if it happens again I'll have something to debug with. Don't want to make that problem more frequent without appropriate logs in place. I've wasted 2-3 debugging sessions on it in the past to find out that it was related to early killing of handleLeaderDoAssignment, and after the timeout was increased, we stopped seeing it (so far). Which is why I haven't had a chance to root cause it since. Unit test don't help either.", "bodyHTML": "<p dir=\"auto\">In the past we've seen some issues with handleLeaderDoAssignment() getting interrupted during cleanup and causing bad state such that we couldn't do future leader assignments anymore for BMM. This would require a full cluster restart to fix the state. We don't know the root cause yet, so we don't know how to  fix this scenario. Won't only calling stopEventThread() make such occurrences more frequent? If for some reason we cannot give more time to the eventThread to stop naturally, then how do we avoid that problem?</p>\n<p dir=\"auto\">Have we added the relevant debug logs we needed for that issue (I recall it being an action item in one of the ZK session expiry documents that listed out ZK related issues)? If not, feel free to assign that item to me so that if it happens again I'll have something to debug with. Don't want to make that problem more frequent without appropriate logs in place. I've wasted 2-3 debugging sessions on it in the past to find out that it was related to early killing of handleLeaderDoAssignment, and after the timeout was increased, we stopped seeing it (so far). Which is why I haven't had a chance to root cause it since. Unit test don't help either.</p>", "author": "somandal", "createdAt": "2020-08-26T22:09:41Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -415,6 +441,74 @@ public void onPartitionMovement(Long notifyTimestamp) {\n     _eventQueue.put(CoordinatorEvent.createPartitionMovementEvent(notifyTimestamp));\n     _log.info(\"Coordinator::onPartitionMovement completed successfully\");\n   }\n+\n+  /**\n+   * {@inheritDoc}\n+   * Stop all the tasks and wait for new session to connect.\n+   */\n+  @Override\n+  public void onSessionExpired() {\n+    if (_shutdown) {\n+      return;\n+    }\n+    stopEventThread();", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1Mjc2Mg==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483252762", "bodyText": "Can you open a bug (or find an existing one) to add logging for the StickyMulticastStrategy and assign it to me? I'll take this up so that we have something to work with next time we hit this issue. Otherwise we'll again waste time on debugging known issues without being able to fix them. It may become a bigger concern now because our change capture connectors have been moved to using StickyMulticastStrategy too.", "author": "somandal", "createdAt": "2020-09-03T21:03:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNzI4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA3NzgzOA==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r485077838", "bodyText": "done.", "author": "vmaheshw", "createdAt": "2020-09-08T17:18:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNzI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyMjY4OA==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477622688", "body": "Where is the retry logic? I don't see any retries happening.\r\nIn general, what should the protocol be for TimeoutException? And other exceptions like interrupt? Can this get interrupted?\r\n\r\nBasically, we need the tasks to stop, what if they don't? E.g. assignment taking too long so we cancel the futures, or our thread gets interrupted. The debounce timer assumption for the persistent lock changes is that the tasks will be brought down in 30 seconds.", "bodyText": "Where is the retry logic? I don't see any retries happening.\nIn general, what should the protocol be for TimeoutException? And other exceptions like interrupt? Can this get interrupted?\nBasically, we need the tasks to stop, what if they don't? E.g. assignment taking too long so we cancel the futures, or our thread gets interrupted. The debounce timer assumption for the persistent lock changes is that the tasks will be brought down in 30 seconds.", "bodyHTML": "<p dir=\"auto\">Where is the retry logic? I don't see any retries happening.<br>\nIn general, what should the protocol be for TimeoutException? And other exceptions like interrupt? Can this get interrupted?</p>\n<p dir=\"auto\">Basically, we need the tasks to stop, what if they don't? E.g. assignment taking too long so we cancel the futures, or our thread gets interrupted. The debounce timer assumption for the persistent lock changes is that the tasks will be brought down in 30 seconds.</p>", "author": "somandal", "createdAt": "2020-08-26T22:24:09Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -415,6 +441,74 @@ public void onPartitionMovement(Long notifyTimestamp) {\n     _eventQueue.put(CoordinatorEvent.createPartitionMovementEvent(notifyTimestamp));\n     _log.info(\"Coordinator::onPartitionMovement completed successfully\");\n   }\n+\n+  /**\n+   * {@inheritDoc}\n+   * Stop all the tasks and wait for new session to connect.\n+   */\n+  @Override\n+  public void onSessionExpired() {\n+    if (_shutdown) {\n+      return;\n+    }\n+    stopEventThread();\n+\n+    _leaderDatastreamAddOrDeleteEventScheduled.set(false);\n+    if (_leaderDatastreamAddOrDeleteEventScheduledFuture != null) {\n+      _leaderDatastreamAddOrDeleteEventScheduledFuture.cancel(true);\n+      _leaderDatastreamAddOrDeleteEventScheduledFuture = null;\n+    }\n+\n+    _leaderDoAssignmentScheduled.set(false);\n+    if (_leaderDoAssignmentScheduledFuture != null) {\n+      _leaderDoAssignmentScheduledFuture.cancel(true);\n+      _leaderDoAssignmentScheduledFuture = null;\n+    }\n+\n+    _eventQueue.clear();\n+\n+    // Stopping all the connectors so that they stop producing.\n+    List<Future<Boolean>> assignmentChangeFutures = _connectors.keySet().stream()\n+        .map(connectorType -> {\n+          _assignmentChangeThreadPool.get(connectorType).shutdownNow();\n+          _assignmentChangeThreadPool.put(connectorType, Executors.newSingleThreadExecutor());\n+          return dispatchAssignmentChangeIfNeeded(connectorType, new ArrayList<>(), false, false);\n+        })\n+        .filter(Objects::nonNull)\n+        .collect(Collectors.toList());\n+\n+    // Wait till all the futures are complete or timeout.\n+    ExecutorService threadPoolExecutor = Executors.newFixedThreadPool(1);\n+    threadPoolExecutor.submit(() -> {\n+      Instant start = Instant.now();\n+      try {\n+        getAssignmentsFuture(assignmentChangeFutures, start);\n+      } catch (Exception e) {\n+        // if it's timeout then we will retry", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyNTQ0NQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477625445", "body": "Can we use Objects.requireNonNull() in place of `datastream.getMetadata() != null`?", "bodyText": "Can we use Objects.requireNonNull() in place of datastream.getMetadata() != null?", "bodyHTML": "<p dir=\"auto\">Can we use Objects.requireNonNull() in place of <code>datastream.getMetadata() != null</code>?</p>", "author": "somandal", "createdAt": "2020-08-26T22:31:31Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -622,8 +714,8 @@ private boolean getCustomCheckpointing(DatastreamTask task) {\n     boolean customCheckpointing = _connectors.get(task.getConnectorType()).isCustomCheckpointing();\n \n     Datastream datastream = task.getDatastreams().get(0);\n-    if (datastream.hasMetadata()\n-        && datastream.getMetadata().containsKey(DatastreamMetadataConstants.CUSTOM_CHECKPOINT)) {\n+    if (datastream.hasMetadata() && datastream.getMetadata() != null &&", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyNzU0Nw==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r477627547", "body": "nit: space after '//'", "bodyText": "nit: space after '//'", "bodyHTML": "<p dir=\"auto\">nit: space after '//'</p>", "author": "somandal", "createdAt": "2020-08-26T22:37:03Z", "path": "datastream-server-restli/src/test/java/com/linkedin/datastream/server/TestCoordinator.java", "diffHunk": "@@ -2597,6 +2595,36 @@ public void testCoordinatorLeaderCleanupTasksPostElection() throws Exception {\n     instance2.stop();\n   }\n \n+  @Test\n+  public void testOnSessionExpired() throws Exception {\n+    String testCluster = \"testCoordinationSmoke3\";\n+    String testConnectorType = \"testConnectorType\";\n+    String datastreamName = \"datastreamNameSessionExpired\";\n+\n+    Coordinator instance1 = createCoordinator(_zkConnectionString, testCluster);\n+    instance1.addTransportProvider(DummyTransportProviderAdminFactory.PROVIDER_NAME,\n+        new DummyTransportProviderAdminFactory().createTransportProviderAdmin(\n+            DummyTransportProviderAdminFactory.PROVIDER_NAME, new Properties()));\n+\n+    TestHookConnector connector1 = new TestHookConnector(\"connector1\", testConnectorType);\n+    instance1.addConnector(testConnectorType, connector1, new BroadcastStrategy(Optional.empty()), false,\n+        new SourceBasedDeduper(), null);\n+    instance1.start();\n+\n+    ZkClient zkClient = new ZkClient(_zkConnectionString);\n+    DatastreamTestUtils.createAndStoreDatastreams(zkClient, testCluster, testConnectorType, datastreamName);\n+    //verify the assignment", "originalCommit": "db6288803df23d5931583a1e00bec8506bf4a978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "69b6d79693d3a3defffffa1b4f48e6efe91568ec", "url": "https://github.com/linkedin/brooklin/commit/69b6d79693d3a3defffffa1b4f48e6efe91568ec", "message": "Address review comments", "committedDate": "2020-08-27T23:38:04Z", "type": "commit"}, {"oid": "978123f5dbfe35ea023d280ea32302289bb28e54", "url": "https://github.com/linkedin/brooklin/commit/978123f5dbfe35ea023d280ea32302289bb28e54", "message": "Add Session expiry metrics and fix unit-tests", "committedDate": "2020-09-01T00:58:27Z", "type": "commit"}, {"oid": "a232e327e511b683367909bbce566a180256906a", "url": "https://github.com/linkedin/brooklin/commit/a232e327e511b683367909bbce566a180256906a", "message": "Merge branch 'master' of github.com:linkedin/brooklin into AbortAllTask", "committedDate": "2020-09-01T21:54:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1MTU4MQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483251581", "body": "Remove \"//(enabled = false)\"", "bodyText": "Remove \"//(enabled = false)\"", "bodyHTML": "<p dir=\"auto\">Remove \"//(enabled = false)\"</p>", "author": "somandal", "createdAt": "2020-09-03T21:00:41Z", "path": "datastream-server-restli/src/test/java/com/linkedin/datastream/server/TestCoordinator.java", "diffHunk": "@@ -1880,9 +1933,11 @@ public void testSimpleAssignmentStrategyIndependent() throws Exception {\n     instance1.stop();\n     instance2.stop();\n     zkClient.close();\n+    instance1.getDatastreamCache().getZkclient().close();\n+    instance2.getDatastreamCache().getZkclient().close();\n   }\n \n-  @Test\n+  @Test //(enabled = false)", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1MzYyMA==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483253620", "body": "Just leaving this comment here for tracking. As discussed offline, this is not sufficient as this only shutdown the producer for currently assigned tasks. Previously assigned tasks may have been unassigned from a producer and that producer will never get closed. The fix is involved so will be done as a separate PR. From our discussion it sounded like we will need to modify the interface for the TransportProvider, feel free to assign these to me, I'd be happy to help.\r\n\r\nProblematic race condition:\r\n\r\n> We create a pool of producers which is shared by all tasks for a given connecter -> Kafka broker mapping. There may have been a sequence of previously assigned tasks that are assigned producers that aren't assigned to any of the current tasks. Shouldn't we close all producers to be safe?\r\n> For example, say we create 10 producers per connector -> Kafka broker mapping.\r\n> initialize task t1 -> producer p1\r\n> initialize task t2 -> producer p2\r\n> initialize task t3 -> producer p3\r\n> assignment at this point in time: t1, t2, t3, unused producers from p4-10\r\n> uninitialize task t2 -> which will unassign t2 from producer p2's task list\r\n> assignment at this point in time: t1, t3, unused producers from p4-10, and p2 should also be unused, but wasn't closed\r\n> ZK session expiry occurs\r\n> We'll call task.getEventProducer()).shutdown(true) for t1 and t3, which will close producer p1 and p3. Producer p2 may still be running (send is taking a long time) in this scenario, since t2 was only removed from the task list of p2 and the producer was not actually shutdown.", "bodyText": "Just leaving this comment here for tracking. As discussed offline, this is not sufficient as this only shutdown the producer for currently assigned tasks. Previously assigned tasks may have been unassigned from a producer and that producer will never get closed. The fix is involved so will be done as a separate PR. From our discussion it sounded like we will need to modify the interface for the TransportProvider, feel free to assign these to me, I'd be happy to help.\nProblematic race condition:\n\nWe create a pool of producers which is shared by all tasks for a given connecter -> Kafka broker mapping. There may have been a sequence of previously assigned tasks that are assigned producers that aren't assigned to any of the current tasks. Shouldn't we close all producers to be safe?\nFor example, say we create 10 producers per connector -> Kafka broker mapping.\ninitialize task t1 -> producer p1\ninitialize task t2 -> producer p2\ninitialize task t3 -> producer p3\nassignment at this point in time: t1, t2, t3, unused producers from p4-10\nuninitialize task t2 -> which will unassign t2 from producer p2's task list\nassignment at this point in time: t1, t3, unused producers from p4-10, and p2 should also be unused, but wasn't closed\nZK session expiry occurs\nWe'll call task.getEventProducer()).shutdown(true) for t1 and t3, which will close producer p1 and p3. Producer p2 may still be running (send is taking a long time) in this scenario, since t2 was only removed from the task list of p2 and the producer was not actually shutdown.", "bodyHTML": "<p dir=\"auto\">Just leaving this comment here for tracking. As discussed offline, this is not sufficient as this only shutdown the producer for currently assigned tasks. Previously assigned tasks may have been unassigned from a producer and that producer will never get closed. The fix is involved so will be done as a separate PR. From our discussion it sounded like we will need to modify the interface for the TransportProvider, feel free to assign these to me, I'd be happy to help.</p>\n<p dir=\"auto\">Problematic race condition:</p>\n<blockquote>\n<p dir=\"auto\">We create a pool of producers which is shared by all tasks for a given connecter -&gt; Kafka broker mapping. There may have been a sequence of previously assigned tasks that are assigned producers that aren't assigned to any of the current tasks. Shouldn't we close all producers to be safe?<br>\nFor example, say we create 10 producers per connector -&gt; Kafka broker mapping.<br>\ninitialize task t1 -&gt; producer p1<br>\ninitialize task t2 -&gt; producer p2<br>\ninitialize task t3 -&gt; producer p3<br>\nassignment at this point in time: t1, t2, t3, unused producers from p4-10<br>\nuninitialize task t2 -&gt; which will unassign t2 from producer p2's task list<br>\nassignment at this point in time: t1, t3, unused producers from p4-10, and p2 should also be unused, but wasn't closed<br>\nZK session expiry occurs<br>\nWe'll call task.getEventProducer()).shutdown(true) for t1 and t3, which will close producer p1 and p3. Producer p2 may still be running (send is taking a long time) in this scenario, since t2 was only removed from the task list of p2 and the producer was not actually shutdown.</p>\n</blockquote>", "author": "somandal", "createdAt": "2020-09-03T21:05:16Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -415,6 +444,90 @@ public void onPartitionMovement(Long notifyTimestamp) {\n     _eventQueue.put(CoordinatorEvent.createPartitionMovementEvent(notifyTimestamp));\n     _log.info(\"Coordinator::onPartitionMovement completed successfully\");\n   }\n+\n+  /**\n+   * {@inheritDoc}\n+   * Stop all the tasks and wait for new session to connect.\n+   */\n+  @Override\n+  public void onSessionExpired() {\n+    _log.info(\"Coordinator::onSessionExpired is called\");\n+    _zkSessionExpired = true;\n+\n+    if (_shutdown) {\n+      return;\n+    }\n+    stopEventThread();\n+\n+    _leaderDatastreamAddOrDeleteEventScheduled.set(false);\n+    if (_leaderDatastreamAddOrDeleteEventScheduledFuture != null) {\n+      _leaderDatastreamAddOrDeleteEventScheduledFuture.cancel(true);\n+      _leaderDatastreamAddOrDeleteEventScheduledFuture = null;\n+    }\n+\n+    _leaderDoAssignmentScheduled.set(false);\n+    if (_leaderDoAssignmentScheduledFuture != null) {\n+      _leaderDoAssignmentScheduledFuture.cancel(true);\n+      _leaderDoAssignmentScheduledFuture = null;\n+    }\n+\n+    _eventQueue.clear();\n+\n+    // Stopping all the connectors so that they stop producing.\n+    List<Future<Boolean>> assignmentChangeFutures = _connectors.keySet().stream()\n+        .map(connectorType -> {\n+          _assignmentChangeThreadPool.get(connectorType).shutdownNow();\n+          _assignmentChangeThreadPool.put(connectorType, Executors.newSingleThreadExecutor());\n+          return dispatchAssignmentChangeIfNeeded(connectorType, new ArrayList<>(), false, false);\n+        })\n+        .filter(Objects::nonNull)\n+        .collect(Collectors.toList());\n+\n+    onDatastreamChange(new ArrayList<>());\n+    // Shutdown the event producer to stop any further production of records.\n+    // Event producer shutdown sequence does not need to wait for onAssignmentChange to complete.\n+    // This will ensure that even if any task thread does not respond to thread interruption, it will\n+    // still not be able to produce any records to destination.\n+    for (DatastreamTask task : _assignedDatastreamTasks.values()) {\n+      ((EventProducer) task.getEventProducer()).shutdown(true);", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1NTI5Mg==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483255292", "body": "Thanks for adding this!", "bodyText": "Thanks for adding this!", "bodyHTML": "<p dir=\"auto\">Thanks for adding this!</p>", "author": "somandal", "createdAt": "2020-09-03T21:08:59Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -1793,6 +1917,7 @@ private void registerGaugeMetrics() {\n           .put(MAX_PARTITION_COUNT_IN_TASK, MAX_PARTITION_COUNT::get)\n           .put(NUM_PAUSED_DATASTREAMS_GROUPS, PAUSED_DATASTREAMS_GROUPS::get)\n           .put(IS_LEADER, () -> _coordinator.getIsLeader().getAsBoolean() ? 1 : 0)\n+          .put(ZK_SESSION_EXPIRED, () -> _coordinator.isZkSessionExpired() ? 1 : 0)", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1NjQ0MQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483256441", "body": "You may want to add a comment on why we may want to skip checkpoint commit. (though this only affects if using the framework level checkpointing, which we don't use internally)\r\n\r\nFeel free to update the comment where you pass 'skipCheckpoint' as 'true' instead of here if it feel like a better place to discuss this aspect.", "bodyText": "You may want to add a comment on why we may want to skip checkpoint commit. (though this only affects if using the framework level checkpointing, which we don't use internally)\nFeel free to update the comment where you pass 'skipCheckpoint' as 'true' instead of here if it feel like a better place to discuss this aspect.", "bodyHTML": "<p dir=\"auto\">You may want to add a comment on why we may want to skip checkpoint commit. (though this only affects if using the framework level checkpointing, which we don't use internally)</p>\n<p dir=\"auto\">Feel free to update the comment where you pass 'skipCheckpoint' as 'true' instead of here if it feel like a better place to discuss this aspect.</p>", "author": "somandal", "createdAt": "2020-09-03T21:11:26Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/EventProducer.java", "diffHunk": "@@ -422,8 +422,10 @@ private void checkpoint(int partition, String checkpoint) {\n   /**\n    * Shuts down the event producer by flushing the checkpoints and closing the transport provider\n    */\n-  public void shutdown() {\n-    _checkpointProvider.flush();\n+  public void shutdown(boolean skipCheckpoint) {\n+    if (!skipCheckpoint) {\n+      _checkpointProvider.flush();\n+    }", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1NzQ4Ng==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483257486", "body": "nit: do you still need this comment? Won't the exiting comment be sufficient?", "bodyText": "nit: do you still need this comment? Won't the exiting comment be sufficient?", "bodyHTML": "<p dir=\"auto\">nit: do you still need this comment? Won't the exiting comment be sufficient?</p>", "author": "somandal", "createdAt": "2020-09-03T21:13:54Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1765,6 +1770,10 @@ void onSessionExpired() {\n     // cancel the lock clean up\n     _orphanLockCleanupFuture.cancel(true);\n     onBecomeFollower();\n+    if (_listener != null) {\n+      _listener.onSessionExpired();\n+    }\n+    // currently it will try to disconnect and fail. TODO: fix the connect and listen to handleNewSession.", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA3NTQ2MQ==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r485075461", "bodyText": "I have a TODO marker which makes it explicit.", "author": "vmaheshw", "createdAt": "2020-09-08T17:14:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1NzQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1ODEzNg==", "url": "https://github.com/linkedin/brooklin/pull/745#discussion_r483258136", "body": "Can you open a bug for fixing this and assign it to me? I've sent you an email about it too, so wondering if that approach looks sound? If so, I can take this up after we finish basic low-level certification.", "bodyText": "Can you open a bug for fixing this and assign it to me? I've sent you an email about it too, so wondering if that approach looks sound? If so, I can take this up after we finish basic low-level certification.", "bodyHTML": "<p dir=\"auto\">Can you open a bug for fixing this and assign it to me? I've sent you an email about it too, so wondering if that approach looks sound? If so, I can take this up after we finish basic low-level certification.</p>", "author": "somandal", "createdAt": "2020-09-03T21:15:24Z", "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "diffHunk": "@@ -317,7 +316,6 @@ void close(DatastreamTask task) {\n         shutdownProducer();\n       }\n     }\n-    ThreadUtils.shutdownExecutor(_producerCloseExecutorService, PRODUCER_CLOSE_EXECUTOR_SHUTDOWN_TIMEOUT, _log);", "originalCommit": "a232e327e511b683367909bbce566a180256906a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5bfb4c0eddc1c78a72ad3222912c5d088b2f4e5c", "url": "https://github.com/linkedin/brooklin/commit/5bfb4c0eddc1c78a72ad3222912c5d088b2f4e5c", "message": "Address review comments", "committedDate": "2020-09-08T17:20:31Z", "type": "commit"}]}