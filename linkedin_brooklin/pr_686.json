{"pr_number": 686, "pr_title": "Add diag support to report consumer groups and topic partitions", "pr_author": "vishwajith-s", "pr_createdAt": "2020-02-19T22:00:25Z", "pr_url": "https://github.com/linkedin/brooklin/pull/686", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc3NDk5MQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381774991", "body": "unnecessary check; it would prolly take a custom `List` impl to return a negative size :)", "bodyText": "unnecessary check; it would prolly take a custom List impl to return a negative size :)", "bodyHTML": "<p dir=\"auto\">unnecessary check; it would prolly take a custom <code>List</code> impl to return a negative size :)</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T05:40:23Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc3ODg0Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381778842", "body": "You can use either to improve the consumer group ID validation:\r\n```java\r\n// make sure you're importing org.apache.commons.lang3.StringUtils\r\n\r\n// This covers null/empty/all-whitespace\r\nStringUtils.isBlank(response.getConsumerGroupId())\r\n\r\n// This covers null/empty\r\nStringUtils.isEmpty(response.getConsumerGroupId())\r\n```", "bodyText": "You can use either to improve the consumer group ID validation:\n// make sure you're importing org.apache.commons.lang3.StringUtils\n\n// This covers null/empty/all-whitespace\nStringUtils.isBlank(response.getConsumerGroupId())\n\n// This covers null/empty\nStringUtils.isEmpty(response.getConsumerGroupId())", "bodyHTML": "<p dir=\"auto\">You can use either to improve the consumer group ID validation:</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"// make sure you're importing org.apache.commons.lang3.StringUtils\n\n// This covers null/empty/all-whitespace\nStringUtils.isBlank(response.getConsumerGroupId())\n\n// This covers null/empty\nStringUtils.isEmpty(response.getConsumerGroupId())\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> make sure you're importing org.apache.commons.lang3.StringUtils</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> This covers null/empty/all-whitespace</span>\n<span class=\"pl-smi\">StringUtils</span><span class=\"pl-k\">.</span>isBlank(response<span class=\"pl-k\">.</span>getConsumerGroupId())\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> This covers null/empty</span>\n<span class=\"pl-smi\">StringUtils</span><span class=\"pl-k\">.</span>isEmpty(response<span class=\"pl-k\">.</span>getConsumerGroupId())</pre></div>", "author": "ahmedahamid", "createdAt": "2020-02-20T05:48:06Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc4NTI3Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381785272", "body": "1. `putIfAbsent()` instantiates `new HashMap()` even if it's not needed. You can use `computeIfAbsent()` instead. \r\n2. This block can be simplified since `put/computeIfAbsent()` returns the value.\r\n    ```java\r\n    Map<String, Set<Integer>> topicPartitionsMap =\r\n        result.computeIfAbsent(response.getConsumerGroupId(), k -> new HashMap<>());\r\n\r\n    Set<TopicPartition> topicPartitions = response.getTopicPartitions();\r\n    topicPartitions.forEach(topicPartition -> {\r\n      Set<Integer> partitions = \r\n          topicPartitionsMap.computeIfAbsent(topicPartition.topic(), k -> new HashSet<>());\r\n      partitions.add(topicPartition.partition());\r\n    });\r\n    ```", "bodyText": "putIfAbsent() instantiates new HashMap() even if it's not needed. You can use computeIfAbsent() instead.\nThis block can be simplified since put/computeIfAbsent() returns the value.\nMap<String, Set<Integer>> topicPartitionsMap =\n    result.computeIfAbsent(response.getConsumerGroupId(), k -> new HashMap<>());\n\nSet<TopicPartition> topicPartitions = response.getTopicPartitions();\ntopicPartitions.forEach(topicPartition -> {\n  Set<Integer> partitions = \n      topicPartitionsMap.computeIfAbsent(topicPartition.topic(), k -> new HashSet<>());\n  partitions.add(topicPartition.partition());\n});", "bodyHTML": "<ol dir=\"auto\">\n<li><code>putIfAbsent()</code> instantiates <code>new HashMap()</code> even if it's not needed. You can use <code>computeIfAbsent()</code> instead.</li>\n<li>This block can be simplified since <code>put/computeIfAbsent()</code> returns the value.\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"Map&lt;String, Set&lt;Integer&gt;&gt; topicPartitionsMap =\n    result.computeIfAbsent(response.getConsumerGroupId(), k -&gt; new HashMap&lt;&gt;());\n\nSet&lt;TopicPartition&gt; topicPartitions = response.getTopicPartitions();\ntopicPartitions.forEach(topicPartition -&gt; {\n  Set&lt;Integer&gt; partitions = \n      topicPartitionsMap.computeIfAbsent(topicPartition.topic(), k -&gt; new HashSet&lt;&gt;());\n  partitions.add(topicPartition.partition());\n});\"><pre><span class=\"pl-k\">Map&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-k\">Set&lt;<span class=\"pl-smi\">Integer</span>&gt;</span>&gt;</span> topicPartitionsMap <span class=\"pl-k\">=</span>\n    result<span class=\"pl-k\">.</span>computeIfAbsent(response<span class=\"pl-k\">.</span>getConsumerGroupId(), k <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">new</span> <span class=\"pl-k\">HashMap&lt;&gt;</span>());\n\n<span class=\"pl-k\">Set&lt;<span class=\"pl-smi\">TopicPartition</span>&gt;</span> topicPartitions <span class=\"pl-k\">=</span> response<span class=\"pl-k\">.</span>getTopicPartitions();\ntopicPartitions<span class=\"pl-k\">.</span>forEach(topicPartition <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> {\n  <span class=\"pl-k\">Set&lt;<span class=\"pl-smi\">Integer</span>&gt;</span> partitions <span class=\"pl-k\">=</span> \n      topicPartitionsMap<span class=\"pl-k\">.</span>computeIfAbsent(topicPartition<span class=\"pl-k\">.</span>topic(), k <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">new</span> <span class=\"pl-k\">HashSet&lt;&gt;</span>());\n  partitions<span class=\"pl-k\">.</span>add(topicPartition<span class=\"pl-k\">.</span>partition());\n});</pre></div>\n</li>\n</ol>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:00:49Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {\n+            _logger.warn(\"Empty topic partition stats map from instance {}. Ignoring the result\", instance);\n+            return;\n+          }\n+          result.putIfAbsent(response.getConsumerGroupId(), new HashMap<>());\n+          Set<TopicPartition> topicPartitions = response.getTopicPartitions();\n+\n+          topicPartitions.forEach(topicPartition -> {\n+            Map<String, Set<Integer>> topicPartitionsMap = result.get(response.getConsumerGroupId());\n+            topicPartitionsMap.putIfAbsent(topicPartition.topic(), new HashSet<>());\n+            topicPartitionsMap.get(topicPartition.topic()).add(topicPartition.partition());\n+          });\n+        });\n+      }\n+    });", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc4OTc4Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381789783", "body": "nit: omit `result`", "bodyText": "nit: omit result", "bodyHTML": "<p dir=\"auto\">nit: omit <code>result</code></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:09:57Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -916,4 +924,31 @@ public static String getTaskMetadataGroupId(DatastreamTask task, CommonConnector\n \n     return null;\n   }\n-}\n+\n+  /**\n+   *\n+   *  Gets the KafkaTopicPartition tracker\n+   */\n+  public KafkaTopicPartitionTracker getKafkaTopicPartitionTracker() {\n+    return _kafkaTopicPartitionTracker;\n+  }\n+\n+  /**\n+   * Get Kafka group ID of given task\n+   * @param task Task for which group ID is generated.\n+   * @param groupIdConstructor GroupIdConstructor to use for generating group ID.\n+   * @param consumerMetrics CommonConnectorMetrics to use for reporting errors.\n+   * @param logger Logger for logging information.\n+   */\n+  @VisibleForTesting\n+  public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor groupIdConstructor,\n+      CommonConnectorMetrics consumerMetrics, Logger logger) {\n+    try {\n+      String result = groupIdConstructor.getTaskGroupId(task, Optional.of(logger));", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5NzQ0Ng==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381797446", "body": "unused", "bodyText": "unused", "bodyHTML": "<p dir=\"auto\">unused</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:24:57Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.jetbrains.annotations.NotNull;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * KafkaTopicPartitionTracker contains information about consumer groups, topic partitions and\n+ * their consumer offsets.\n+ *\n+ * The information stored can then be queried via the /diag endpoint for diagnostic and analytic purposes.\n+ */\n+\n+public class KafkaTopicPartitionTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaTopicPartitionTracker.class);", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5NzU2Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381797562", "body": "`final`", "bodyText": "final", "bodyHTML": "<p dir=\"auto\"><code>final</code></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:25:12Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5OTg0Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381799847", "body": "I think it would be useful to add an informational log statement at the beginning of this method", "bodyText": "I think it would be useful to add an informational log statement at the beginning of this method", "bodyHTML": "<p dir=\"auto\">I think it would be useful to add an informational log statement at the beginning of this method</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:29:25Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -428,6 +434,20 @@ private String processDatastreamStateRequest(URI request) {\n         .map(KafkaDatastreamStatesResponse::toJson).orElse(null);\n   }\n \n+  private String processTopicPartitionStatsRequest() {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMDkzOA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381800938", "body": "nit: remove empty line + `s/KafkaTopicPartition tracker/KafkaTopicPartitionTracker`", "bodyText": "nit: remove empty line + s/KafkaTopicPartition tracker/KafkaTopicPartitionTracker", "bodyHTML": "<p dir=\"auto\">nit: remove empty line + <code>s/KafkaTopicPartition tracker/KafkaTopicPartitionTracker</code></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:31:32Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -916,4 +924,31 @@ public static String getTaskMetadataGroupId(DatastreamTask task, CommonConnector\n \n     return null;\n   }\n-}\n+\n+  /**\n+   *\n+   *  Gets the KafkaTopicPartition tracker", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMTc3Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381801772", "body": "nit: I have nothing against `final` locals but it's inconsistent with the one on line 384", "bodyText": "nit: I have nothing against final locals but it's inconsistent with the one on line 384", "bodyHTML": "<p dir=\"auto\">nit: I have nothing against <code>final</code> locals but it's inconsistent with the one on line 384</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:33:09Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -382,6 +384,10 @@ public String process(String query) {\n         String response = processDatastreamStateRequest(uri);\n         _logger.trace(\"Query: {} returns response: {}\", query, response);\n         return response;\n+      } else if (path != null && path.equalsIgnoreCase(DiagnosticsRequestType.TOPICPARTITION_STATS.toString())) {\n+        final String response = processTopicPartitionStatsRequest();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMjc5OQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381802799", "body": "Great catch \ud83d\udc4d ", "bodyText": "Great catch \ud83d\udc4d", "bodyHTML": "<p dir=\"auto\">Great catch <g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:35:14Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaConnectorTask.java", "diffHunk": "@@ -153,22 +150,4 @@ protected DatastreamProducerRecord translate(ConsumerRecord<?, ?> fromKafka, Ins\n \n     return builder.build();\n   }\n-\n-  /**\n-   * Get Kafka group ID of given task\n-   * @param task Task for which group ID is generated.\n-   * @param groupIdConstructor GroupIdConstructor to use for generating group ID.\n-   * @param consumerMetrics CommonConnectorMetrics to use for reporting errors.\n-   * @param logger Logger for logging information.\n-   */\n-  @VisibleForTesting\n-  public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor groupIdConstructor,\n-      CommonConnectorMetrics consumerMetrics, Logger logger) {\n-    try {\n-      return groupIdConstructor.getTaskGroupId(task, Optional.of(logger));\n-    } catch (Exception e) {\n-      consumerMetrics.updateErrorRate(1, \"Can't find group ID\", e);\n-      throw e;\n-    }\n-  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwODY2Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381808667", "body": "There's an exact copy of this method in `TestKafkaMirrorMakerConnector`. I suggest keeping the one in this file and deleting the one over there.", "bodyText": "There's an exact copy of this method in TestKafkaMirrorMakerConnector. I suggest keeping the one in this file and deleting the one over there.", "bodyHTML": "<p dir=\"auto\">There's an exact copy of this method in <code>TestKafkaMirrorMakerConnector</code>. I suggest keeping the one in this file and deleting the one over there.</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:46:21Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTestUtils.java", "diffHunk": "@@ -159,4 +160,24 @@ static Properties getKafkaConsumerProperties() {\n     props.put(\"auditor.class\", NoOpAuditor.class.getCanonicalName());\n     return props;\n   }\n+\n+  /**\n+   * Get the default config properties of a Kafka-based connector\n+   * @param override Configuration properties to override default config properties\n+   */\n+  public static Properties getDefaultConfig(Optional<Properties> override) {\n+    Properties config = new Properties();\n+    config.put(KafkaBasedConnectorConfig.CONFIG_DEFAULT_KEY_SERDE, \"keySerde\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_DEFAULT_VALUE_SERDE, \"valueSerde\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_COMMIT_INTERVAL_MILLIS, \"10000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_COMMIT_TIMEOUT_MILLIS, \"1000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_POLL_TIMEOUT_MILLIS, \"5000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_CONSUMER_FACTORY_CLASS, LiKafkaConsumerFactory.class.getName());\n+    config.put(KafkaBasedConnectorConfig.CONFIG_PAUSE_PARTITION_ON_ERROR, Boolean.TRUE.toString());\n+    config.put(KafkaBasedConnectorConfig.CONFIG_RETRY_SLEEP_DURATION_MILLIS, \"1000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_PAUSE_ERROR_PARTITION_DURATION_MILLIS,\n+        String.valueOf(Duration.ofSeconds(5).toMillis()));\n+    override.ifPresent(config::putAll);\n+    return config;\n+  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgxMDkxMQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381810911", "body": "You can avoid having to fully qualify `getDefaultConfig()` throughout this file by using a static import.\r\n```java\r\nimport static com.linkedin.datastream.connectors.kafka.mirrormaker.KafkaMirrorMakerConnectorTestUtils.getDefaultConfig;\r\n```", "bodyText": "You can avoid having to fully qualify getDefaultConfig() throughout this file by using a static import.\nimport static com.linkedin.datastream.connectors.kafka.mirrormaker.KafkaMirrorMakerConnectorTestUtils.getDefaultConfig;", "bodyHTML": "<p dir=\"auto\">You can avoid having to fully qualify <code>getDefaultConfig()</code> throughout this file by using a static import.</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"import static com.linkedin.datastream.connectors.kafka.mirrormaker.KafkaMirrorMakerConnectorTestUtils.getDefaultConfig;\"><pre><span class=\"pl-k\">import static</span> <span class=\"pl-smi\">com.linkedin.datastream.connectors.kafka.mirrormaker.KafkaMirrorMakerConnectorTestUtils.getDefaultConfig</span>;</pre></div>", "author": "ahmedahamid", "createdAt": "2020-02-20T06:50:25Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaMirrorMakerConnector.java", "diffHunk": "@@ -102,7 +102,8 @@ public void testInitializeDatastream() throws Exception {\n     Datastream ds =\n         KafkaMirrorMakerConnectorTestUtils.createDatastream(\"testInitializeDatastream\", _broker, sourceRegex, metadata);\n     KafkaMirrorMakerConnector connector =\n-        new KafkaMirrorMakerConnector(\"testInitializeDatastream\", getDefaultConfig(Optional.empty()), \"testCluster\");\n+        new KafkaMirrorMakerConnector(\"testInitializeDatastream\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgzOTY3NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381839674", "body": "Could you, please, check if you can reuse this object for serialization and deserialization (e.g. declare as a `static final`)? I don't think we need to instantiate and configure one every time this method is invoked. Same goes for `fromJson()`.", "bodyText": "Could you, please, check if you can reuse this object for serialization and deserialization (e.g. declare as a static final)? I don't think we need to instantiate and configure one every time this method is invoked. Same goes for fromJson().", "bodyHTML": "<p dir=\"auto\">Could you, please, check if you can reuse this object for serialization and deserialization (e.g. declare as a <code>static final</code>)? I don't think we need to instantiate and configure one every time this method is invoked. Same goes for <code>fromJson()</code>.</p>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:12:11Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;\n+\n+  private Set<TopicPartition> _topicPartitions;\n+\n+  /**\n+   *  Default constructor for deserialization\n+   */\n+  public KafkaTopicPartitionStatsResponse() {\n+  }\n+\n+  /**\n+   * Constructor for KafkaTopicPartitionStatsResponse\n+   * @param consumerGroupId identifier for consumer group\n+   * @param topicPartitions a map of consumer offsets for topic partitions\n+   */\n+  public KafkaTopicPartitionStatsResponse(String consumerGroupId, Set<TopicPartition> topicPartitions) {\n+    _consumerGroupId = consumerGroupId;\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  @JsonGetter(\"_consumerGroupId\")\n+  public String getConsumerGroupId() {\n+    return _consumerGroupId;\n+  }\n+\n+  @JsonSetter(\"_consumerGroupId\")\n+  public void setConsumerGroupId(String consumerGroupId) {\n+    _consumerGroupId = consumerGroupId;\n+  }\n+\n+  @JsonGetter(\"_topicPartitions\")\n+  public Set<TopicPartition> getTopicPartitions() {\n+    return _topicPartitions;\n+  }\n+\n+  @JsonSetter(\"_topicPartitions\")\n+  public void setTopicPartitions(Set<TopicPartition> topicPartitions) {\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  /**\n+   * Serialize to JSON\n+   */\n+  public static String toJson(List<KafkaTopicPartitionStatsResponse> obj) {\n+    ObjectMapper mapper = new ObjectMapper();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5ODI4Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382198283", "bodyText": "I cannot make it as final. But I have made it static.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:05:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgzOTY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381841306", "body": "Could you, please, have the existing `fromJson()` and `toJson()` invoke these methods after reconciling all differences, e.g.\r\n\r\n```java\r\n  public static <T> T fromJson(String json, TypeReference<T> typeRef) {\r\n    return fromJson(json, typeRef, MAPPER);\r\n  }\r\n\r\n  public static <T> T fromJson(String json, TypeReference<T> typeRef, ObjectMapper mapper) {\r\n    Validate.notNull(json, \"null JSON string\");\r\n    Validate.notNull(typeRef, \"null type reference\");\r\n    T object = null;\r\n    try {\r\n      object = mapper.readValue(json, typeRef);\r\n    } catch (IOException e) {\r\n      String errorMessage = \"Failed to parse json: \" + json;\r\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\r\n    }\r\n    return object;\r\n  }\r\n\r\n  public static <T> String toJson(T object) {\r\n    return toJson(object, MAPPER);\r\n  }\r\n\r\n  public static <T> String toJson(T object, ObjectMapper mapper) {\r\n    Validate.notNull(object, \"null input object\");\r\n    StringWriter out = new StringWriter();   // Modified to match the old toJson()\r\n    try {\r\n      mapper.writeValue(out, object);        // ditto\r\n    } catch (IOException e) {\r\n      String errorMessage = \"Failed to serialize object: \" + object;\r\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\r\n    }\r\n    return out.toString();\r\n  }\r\n```", "bodyText": "Could you, please, have the existing fromJson() and toJson() invoke these methods after reconciling all differences, e.g.\n  public static <T> T fromJson(String json, TypeReference<T> typeRef) {\n    return fromJson(json, typeRef, MAPPER);\n  }\n\n  public static <T> T fromJson(String json, TypeReference<T> typeRef, ObjectMapper mapper) {\n    Validate.notNull(json, \"null JSON string\");\n    Validate.notNull(typeRef, \"null type reference\");\n    T object = null;\n    try {\n      object = mapper.readValue(json, typeRef);\n    } catch (IOException e) {\n      String errorMessage = \"Failed to parse json: \" + json;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return object;\n  }\n\n  public static <T> String toJson(T object) {\n    return toJson(object, MAPPER);\n  }\n\n  public static <T> String toJson(T object, ObjectMapper mapper) {\n    Validate.notNull(object, \"null input object\");\n    StringWriter out = new StringWriter();   // Modified to match the old toJson()\n    try {\n      mapper.writeValue(out, object);        // ditto\n    } catch (IOException e) {\n      String errorMessage = \"Failed to serialize object: \" + object;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return out.toString();\n  }", "bodyHTML": "<p dir=\"auto\">Could you, please, have the existing <code>fromJson()</code> and <code>toJson()</code> invoke these methods after reconciling all differences, e.g.</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"  public static &lt;T&gt; T fromJson(String json, TypeReference&lt;T&gt; typeRef) {\n    return fromJson(json, typeRef, MAPPER);\n  }\n\n  public static &lt;T&gt; T fromJson(String json, TypeReference&lt;T&gt; typeRef, ObjectMapper mapper) {\n    Validate.notNull(json, &quot;null JSON string&quot;);\n    Validate.notNull(typeRef, &quot;null type reference&quot;);\n    T object = null;\n    try {\n      object = mapper.readValue(json, typeRef);\n    } catch (IOException e) {\n      String errorMessage = &quot;Failed to parse json: &quot; + json;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return object;\n  }\n\n  public static &lt;T&gt; String toJson(T object) {\n    return toJson(object, MAPPER);\n  }\n\n  public static &lt;T&gt; String toJson(T object, ObjectMapper mapper) {\n    Validate.notNull(object, &quot;null input object&quot;);\n    StringWriter out = new StringWriter();   // Modified to match the old toJson()\n    try {\n      mapper.writeValue(out, object);        // ditto\n    } catch (IOException e) {\n      String errorMessage = &quot;Failed to serialize object: &quot; + object;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return out.toString();\n  }\"><pre>  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-smi\">T</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">T</span> fromJson(<span class=\"pl-smi\">String</span> json, <span class=\"pl-k\">TypeReference&lt;<span class=\"pl-smi\">T</span>&gt;</span> typeRef) {\n    <span class=\"pl-k\">return</span> fromJson(json, typeRef, <span class=\"pl-c1\">MAPPER</span>);\n  }\n\n  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-smi\">T</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">T</span> fromJson(<span class=\"pl-smi\">String</span> json, <span class=\"pl-k\">TypeReference&lt;<span class=\"pl-smi\">T</span>&gt;</span> typeRef, <span class=\"pl-smi\">ObjectMapper</span> mapper) {\n    <span class=\"pl-smi\">Validate</span><span class=\"pl-k\">.</span>notNull(json, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>null JSON string<span class=\"pl-pds\">\"</span></span>);\n    <span class=\"pl-smi\">Validate</span><span class=\"pl-k\">.</span>notNull(typeRef, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>null type reference<span class=\"pl-pds\">\"</span></span>);\n    <span class=\"pl-smi\">T</span> object <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;\n    <span class=\"pl-k\">try</span> {\n      object <span class=\"pl-k\">=</span> mapper<span class=\"pl-k\">.</span>readValue(json, typeRef);\n    } <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">IOException</span> e) {\n      <span class=\"pl-smi\">String</span> errorMessage <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Failed to parse json: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> json;\n      <span class=\"pl-smi\">ErrorLogger</span><span class=\"pl-k\">.</span>logAndThrowDatastreamRuntimeException(<span class=\"pl-c1\">LOG</span>, errorMessage, e);\n    }\n    <span class=\"pl-k\">return</span> object;\n  }\n\n  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-smi\">T</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">String</span> toJson(<span class=\"pl-smi\">T</span> object) {\n    <span class=\"pl-k\">return</span> toJson(object, <span class=\"pl-c1\">MAPPER</span>);\n  }\n\n  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-smi\">T</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">String</span> toJson(<span class=\"pl-smi\">T</span> object, <span class=\"pl-smi\">ObjectMapper</span> mapper) {\n    <span class=\"pl-smi\">Validate</span><span class=\"pl-k\">.</span>notNull(object, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>null input object<span class=\"pl-pds\">\"</span></span>);\n    <span class=\"pl-smi\">StringWriter</span> out <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StringWriter</span>();   <span class=\"pl-c\"><span class=\"pl-c\">//</span> Modified to match the old toJson()</span>\n    <span class=\"pl-k\">try</span> {\n      mapper<span class=\"pl-k\">.</span>writeValue(out, object);        <span class=\"pl-c\"><span class=\"pl-c\">//</span> ditto</span>\n    } <span class=\"pl-k\">catch</span> (<span class=\"pl-smi\">IOException</span> e) {\n      <span class=\"pl-smi\">String</span> errorMessage <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Failed to serialize object: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> object;\n      <span class=\"pl-smi\">ErrorLogger</span><span class=\"pl-k\">.</span>logAndThrowDatastreamRuntimeException(<span class=\"pl-c1\">LOG</span>, errorMessage, e);\n    }\n    <span class=\"pl-k\">return</span> out<span class=\"pl-k\">.</span>toString();\n  }</pre></div>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:16:12Z", "path": "datastream-common/src/main/java/com/linkedin/datastream/common/JsonUtils.java", "diffHunk": "@@ -96,6 +96,46 @@\n     return object;\n   }\n \n+  /**\n+   * Deserialize a JSON string into an object based on a type reference.\n+   * This method allows the caller to specify precisely the desired output\n+   * type for the target object.\n+   * @param json JSON string\n+   * @param typeRef type reference of the target object\n+   * @param  mapper the ObjectMapper to use\n+   * @return deserialized Java object\n+   */\n+  public static <T> T fromJson(String json, TypeReference<T> typeRef, ObjectMapper mapper) {\n+    Validate.notNull(json, \"null JSON string\");\n+    Validate.notNull(typeRef, \"null type reference\");\n+    T object = null;\n+    try {\n+      object = mapper.readValue(json, typeRef);\n+    } catch (IOException e) {\n+      String errorMessage = \"Failed to parse json: \" + json;\n+      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n+    }\n+    return object;\n+  }\n+\n+  /**\n+   * Serialize a Java object into JSON string.\n+   * @param object object to be serialized\n+   * @param  mapper the ObjectMapper to use\n+   * @return JSON string\n+   */\n+  public static <T> String toJson(T object, ObjectMapper mapper) {\n+    Validate.notNull(object, \"null input object\");\n+    String out = null;\n+    try {\n+      out = mapper.writeValueAsString(object);\n+    } catch (IOException e) {\n+      String errorMessage = \"Failed to serialize object: \" + object;\n+      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n+    }\n+    return out;\n+  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNjQ2NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382136464", "bodyText": "+1", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyMjUzNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382222536", "bodyText": "Good point. I should have done this at the start. I cannot combine the ones that takes class and typereference. But the rest is doable.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0NDUwMA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381844500", "body": "With the auto-field discovery and the custom deserializer, could you please double-check if you need these getter/setter annotations? I have a somewhat strong suspicion we don't really need them.\r\n\r\nI think we can do w/o `@JsonGetter`, `@JsonSetter`, and the default ctor in favor of the newer `@JsonCreator` and `@JsonProperty` annotations.\r\n```java\r\n  @JsonCreator\r\n  public KafkaTopicPartitionStatsResponse(\r\n      @JsonProperty(\"consumerGroupId\") String consumerGroupId,\r\n      @JsonProperty(\"topicPartitions\") Set<TopicPartition> topicPartitions) {\r\n    ...\r\n  }\r\n```", "bodyText": "With the auto-field discovery and the custom deserializer, could you please double-check if you need these getter/setter annotations? I have a somewhat strong suspicion we don't really need them.\nI think we can do w/o @JsonGetter, @JsonSetter, and the default ctor in favor of the newer @JsonCreator and @JsonProperty annotations.\n  @JsonCreator\n  public KafkaTopicPartitionStatsResponse(\n      @JsonProperty(\"consumerGroupId\") String consumerGroupId,\n      @JsonProperty(\"topicPartitions\") Set<TopicPartition> topicPartitions) {\n    ...\n  }", "bodyHTML": "<p dir=\"auto\">With the auto-field discovery and the custom deserializer, could you please double-check if you need these getter/setter annotations? I have a somewhat strong suspicion we don't really need them.</p>\n<p dir=\"auto\">I think we can do w/o <code>@JsonGetter</code>, <code>@JsonSetter</code>, and the default ctor in favor of the newer <code>@JsonCreator</code> and <code>@JsonProperty</code> annotations.</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"  @JsonCreator\n  public KafkaTopicPartitionStatsResponse(\n      @JsonProperty(&quot;consumerGroupId&quot;) String consumerGroupId,\n      @JsonProperty(&quot;topicPartitions&quot;) Set&lt;TopicPartition&gt; topicPartitions) {\n    ...\n  }\"><pre>  <span class=\"pl-k\">@JsonCreator</span>\n  <span class=\"pl-k\">public</span> KafkaTopicPartitionStatsResponse(\n      <span class=\"pl-k\">@JsonProperty</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>consumerGroupId<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-smi\">String</span> consumerGroupId,\n      <span class=\"pl-k\">@JsonProperty</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>topicPartitions<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">Set&lt;<span class=\"pl-smi\">TopicPartition</span>&gt;</span> topicPartitions) {\n    <span class=\"pl-c1\">...</span>\n  }</pre></div>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:23:43Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;\n+\n+  private Set<TopicPartition> _topicPartitions;\n+\n+  /**\n+   *  Default constructor for deserialization\n+   */\n+  public KafkaTopicPartitionStatsResponse() {\n+  }\n+\n+  /**\n+   * Constructor for KafkaTopicPartitionStatsResponse\n+   * @param consumerGroupId identifier for consumer group\n+   * @param topicPartitions a map of consumer offsets for topic partitions\n+   */\n+  public KafkaTopicPartitionStatsResponse(String consumerGroupId, Set<TopicPartition> topicPartitions) {\n+    _consumerGroupId = consumerGroupId;\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  @JsonGetter(\"_consumerGroupId\")", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NDc2Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381854767", "body": "nit: You don't need parenthesis for one liner lambdas\r\n```java\r\n    responseList.forEach(response -> actual.addAll(response.getTopicPartitions()));\r\n```", "bodyText": "nit: You don't need parenthesis for one liner lambdas\n    responseList.forEach(response -> actual.addAll(response.getTopicPartitions()));", "bodyHTML": "<p dir=\"auto\">nit: You don't need parenthesis for one liner lambdas</p>\n<div class=\"highlight highlight-source-java position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"    responseList.forEach(response -&gt; actual.addAll(response.getTopicPartitions()));\"><pre>    responseList<span class=\"pl-k\">.</span>forEach(response <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> actual<span class=\"pl-k\">.</span>addAll(response<span class=\"pl-k\">.</span>getTopicPartitions()));</pre></div>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:45:34Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {\n+    String jsonStr = connector.process(\"/topicpartition_stats\");\n+    List<KafkaTopicPartitionStatsResponse> responseList = KafkaTopicPartitionStatsResponse.fromJson(jsonStr);\n+\n+    Set<TopicPartition> actual = new HashSet<>();\n+    responseList.forEach(response -> {\n+      actual.addAll(response.getTopicPartitions());\n+    });", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwMDI4Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382200287", "bodyText": "Thanks! I will retain this as it makes it more clearer.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:09:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NDc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NTA2NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381855064", "body": "`private`", "bodyText": "private", "bodyHTML": "<p dir=\"auto\"><code>private</code></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:46:06Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NjI5Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381856293", "body": "Replace w/ `TOPICPARTITION_STATS_QUERY`", "bodyText": "Replace w/ TOPICPARTITION_STATS_QUERY", "bodyHTML": "<p dir=\"auto\">Replace w/ <code>TOPICPARTITION_STATS_QUERY</code></p>", "author": "ahmedahamid", "createdAt": "2020-02-20T08:48:31Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {\n+    String jsonStr = connector.process(\"/topicpartition_stats\");", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE0MjUxMA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382142510", "body": "nit: remove comma at the end? ", "bodyText": "nit: remove comma at the end?", "bodyHTML": "<p dir=\"auto\">nit: remove comma at the end?</p>", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:18:34Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -94,6 +95,7 @@ public Thread newThread(@NotNull Runnable r) {\n \n   enum DiagnosticsRequestType {\n     DATASTREAM_STATE,\n+    TOPICPARTITION_STATS,", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE1MTk5Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382151997", "body": "ConcurrentHashMap guarantees concurrent access at individual map level (not at the map level). So this can potentially result in inconsistent results when onPartitionsAssigned/onPartitionsRevoked gets called at the same time getTopicPartitions is called. Alternative is to make all the accesses \"synchronized\", but since the map gets accessed in hot path during rebalance (and we can potentially address this at caller level) its probably OK to leave it as it is. Just something to be aware of. \r\n", "bodyText": "ConcurrentHashMap guarantees concurrent access at individual map level (not at the map level). So this can potentially result in inconsistent results when onPartitionsAssigned/onPartitionsRevoked gets called at the same time getTopicPartitions is called. Alternative is to make all the accesses \"synchronized\", but since the map gets accessed in hot path during rebalance (and we can potentially address this at caller level) its probably OK to leave it as it is. Just something to be aware of.", "bodyHTML": "<p dir=\"auto\">ConcurrentHashMap guarantees concurrent access at individual map level (not at the map level). So this can potentially result in inconsistent results when onPartitionsAssigned/onPartitionsRevoked gets called at the same time getTopicPartitions is called. Alternative is to make all the accesses \"synchronized\", but since the map gets accessed in hot path during rebalance (and we can potentially address this at caller level) its probably OK to leave it as it is. Just something to be aware of.</p>", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:36:06Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.jetbrains.annotations.NotNull;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * KafkaTopicPartitionTracker contains information about consumer groups, topic partitions and\n+ * their consumer offsets.\n+ *\n+ * The information stored can then be queried via the /diag endpoint for diagnostic and analytic purposes.\n+ */\n+\n+public class KafkaTopicPartitionTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaTopicPartitionTracker.class);\n+\n+  private String _consumerGroupId;\n+\n+  @NotNull\n+  private Set<TopicPartition>  _topicPartitions = ConcurrentHashMap.newKeySet();\n+\n+  /**\n+   *  Constructor for KafkaTopicPartitionTracker\n+   *\n+   * @param consumerGroupId Identifier of the consumer group\n+   */\n+  public KafkaTopicPartitionTracker(String consumerGroupId) {\n+    _consumerGroupId = consumerGroupId;\n+  }\n+\n+  /**\n+   * Assigns paritions. This method should be called whenever the Connector's consumer\n+   * finishes assigning partitions.\n+   *\n+   * @param topicPartitions the topic partitions which have been assigned\n+   */\n+  public void onPartitionsAssigned(@NotNull Collection<TopicPartition> topicPartitions) {\n+    _topicPartitions.addAll(topicPartitions);\n+  }\n+\n+  /**\n+   * Frees partitions that have been revoked. This method should be called whenever the Connector's\n+   * consumer is about to re-balance (and thus unassign partitions).\n+   *\n+   * @param topicPartitions the topic partitions which were previously assigned\n+   */\n+  public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartitions) {\n+    _topicPartitions.removeAll(topicPartitions);\n+  }\n+\n+  public Set<TopicPartition> getTopicPartitions() {\n+    return Collections.unmodifiableSet(_topicPartitions);", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NTAyNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382165026", "body": "should we throw exception here so that caller of the endpoint will know about it? ", "bodyText": "should we throw exception here so that caller of the endpoint will know about it?", "bodyHTML": "<p dir=\"auto\">should we throw exception here so that caller of the endpoint will know about it?</p>", "author": "gauravrkulkarni", "createdAt": "2020-02-20T18:01:11Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {\n+            _logger.warn(\"Empty topic partition stats map from instance {}. Ignoring the result\", instance);\n+            return;", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwMzIzOQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382203239", "bodyText": "I think ignoring the result and proceeding further is good here.  At least we will return a warning and proceed further with best effort. This is a diag tool.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:14:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NTAyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382166615", "body": "Is the endpoint invoked for a particular datastream? If so, what are the cases where we would expect more than one consumer group? \r\n\r\nIf the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well? ", "bodyText": "Is the endpoint invoked for a particular datastream? If so, what are the cases where we would expect more than one consumer group?\nIf the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well?", "bodyHTML": "<p dir=\"auto\">Is the endpoint invoked for a particular datastream? If so, what are the cases where we would expect more than one consumer group?</p>\n<p dir=\"auto\">If the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well?</p>", "author": "gauravrkulkarni", "createdAt": "2020-02-20T18:04:22Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwNTgxMw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382205813", "bodyText": "The end point is not invoked for a particular datastream. It just aggregates the consumer groups across all brooklin instances, datastreams.\nIf the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well?\nI believe the intent here is to get enough info to query Kafka or lag monitor.  Since datastream is an internal brooklin concept it will not be reported.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNTA0MQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382225041", "bodyText": "I believe datastream name will be useful, as that's a fundamental unit of operation for brooklin (for example, if one needs to restart a datstream to recover from lags).\nAdmittedly there are other ways to map datastream to consumer group and some type of group IDs even have datastream names in them, but that can mean going through all the datastreams in a cluster.\nWe can discuss this offline and add the datastream in a separate PR if needed.", "author": "gauravrkulkarni", "createdAt": "2020-02-20T19:56:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ=="}], "type": "inlineReview"}, {"oid": "d5e14f482e26449ee65c65e24fdbc4ae6c549348", "url": "https://github.com/linkedin/brooklin/commit/d5e14f482e26449ee65c65e24fdbc4ae6c549348", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T00:54:36Z", "type": "forcePushed"}, {"oid": "88c44452280ea373126078eaed5642e79d0c929c", "url": "https://github.com/linkedin/brooklin/commit/88c44452280ea373126078eaed5642e79d0c929c", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:44:59Z", "type": "forcePushed"}, {"oid": "9650997ba8d59998f37d3bf7c3a55530224bfc34", "url": "https://github.com/linkedin/brooklin/commit/9650997ba8d59998f37d3bf7c3a55530224bfc34", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:49:32Z", "type": "forcePushed"}, {"oid": "81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "url": "https://github.com/linkedin/brooklin/commit/81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:59:25Z", "type": "commit"}, {"oid": "81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "url": "https://github.com/linkedin/brooklin/commit/81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:59:25Z", "type": "forcePushed"}]}