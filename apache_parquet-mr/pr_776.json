{"pr_number": 776, "pr_title": "PARQUET-1229: Parquet MR encryption", "pr_author": "ggershinsky", "pr_createdAt": "2020-04-01T07:43:37Z", "pr_url": "https://github.com/apache/parquet-mr/pull/776", "timeline": [{"oid": "45b917f7bf52cf41367c2ea7f6d2ec27e2ab0359", "url": "https://github.com/apache/parquet-mr/commit/45b917f7bf52cf41367c2ea7f6d2ec27e2ab0359", "message": "code format clean up", "committedDate": "2020-04-01T15:30:20Z", "type": "forcePushed"}, {"oid": "d2b93707fa04d9c66f9ef5914d4c60f76b7829b2", "url": "https://github.com/apache/parquet-mr/commit/d2b93707fa04d9c66f9ef5914d4c60f76b7829b2", "message": "mr encryption - initial push", "committedDate": "2020-04-22T10:17:43Z", "type": "commit"}, {"oid": "96d00dc60efc5b64ead41863273abda7b47a5805", "url": "https://github.com/apache/parquet-mr/commit/96d00dc60efc5b64ead41863273abda7b47a5805", "message": "PFR fix", "committedDate": "2020-04-22T10:17:43Z", "type": "commit"}, {"oid": "4eff3ba1190da37a6905c39e00fa1eedcc7ded67", "url": "https://github.com/apache/parquet-mr/commit/4eff3ba1190da37a6905c39e00fa1eedcc7ded67", "message": "PFR fix", "committedDate": "2020-04-22T10:17:43Z", "type": "commit"}, {"oid": "6b2bcffaba1f47b085e2ccb8e8675ee3aa79f9a5", "url": "https://github.com/apache/parquet-mr/commit/6b2bcffaba1f47b085e2ccb8e8675ee3aa79f9a5", "message": "format fix", "committedDate": "2020-04-22T10:17:43Z", "type": "commit"}, {"oid": "af87bcf0ebf52dd38c4ee0cf6c77397880e059d5", "url": "https://github.com/apache/parquet-mr/commit/af87bcf0ebf52dd38c4ee0cf6c77397880e059d5", "message": "javadoc annotations", "committedDate": "2020-04-22T10:32:02Z", "type": "commit"}, {"oid": "c8aa709738e538edd524e5dcaba2dd482065f23e", "url": "https://github.com/apache/parquet-mr/commit/c8aa709738e538edd524e5dcaba2dd482065f23e", "message": "code format clean up", "committedDate": "2020-04-22T10:32:02Z", "type": "commit"}, {"oid": "297ed1b12a78f196cf49a9a0b40d9215177f7351", "url": "https://github.com/apache/parquet-mr/commit/297ed1b12a78f196cf49a9a0b40d9215177f7351", "message": "use crypto exception instead of IOexception", "committedDate": "2020-04-22T10:32:02Z", "type": "commit"}, {"oid": "40484c6950702cc34484d056fba2434c6a990b2a", "url": "https://github.com/apache/parquet-mr/commit/40484c6950702cc34484d056fba2434c6a990b2a", "message": "remove hidden column exception", "committedDate": "2020-04-22T10:32:02Z", "type": "commit"}, {"oid": "c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "url": "https://github.com/apache/parquet-mr/commit/c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "message": "fix prettyJSON exception", "committedDate": "2020-04-22T10:32:02Z", "type": "commit"}, {"oid": "c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "url": "https://github.com/apache/parquet-mr/commit/c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "message": "fix prettyJSON exception", "committedDate": "2020-04-22T10:32:02Z", "type": "forcePushed"}, {"oid": "cacea5ea9e7ce759992e73e51aa535154e96fdaf", "url": "https://github.com/apache/parquet-mr/commit/cacea5ea9e7ce759992e73e51aa535154e96fdaf", "message": "bloom encryption test fix", "committedDate": "2020-04-22T12:39:45Z", "type": "commit"}, {"oid": "0245db23f5e8b8aca6b1c8520a1164b08eb4fba5", "url": "https://github.com/apache/parquet-mr/commit/0245db23f5e8b8aca6b1c8520a1164b08eb4fba5", "message": "indentation fix", "committedDate": "2020-04-22T12:54:31Z", "type": "commit"}, {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "url": "https://github.com/apache/parquet-mr/commit/fdb1d59461a06bbbc418d75a0dec257d1e11013d", "message": "remove travis-before_install-encryption (format master fetch)", "committedDate": "2020-04-23T12:54:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415429416", "body": "Since this is a public method, can we validate pageAAD also? ", "bodyText": "Since this is a public method, can we validate pageAAD also?", "bodyHTML": "<p dir=\"auto\">Since this is a public method, can we validate pageAAD also?</p>", "author": "shangxinli", "createdAt": "2020-04-26T23:43:09Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYxNzg3Mw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415617873", "bodyText": "Ok, I'll add this.", "author": "ggershinsky", "createdAt": "2020-04-27T08:32:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3MzcyMQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428673721", "bodyText": "I agree on validating the arguments is important. But the proper exception to be thrown for a null is a NullPointerException that would be thrown at line 134 anyway. If you really want to validate the argument for null at the first line of the method I would suggest using java.util.Objects.requireNonNull(Object).", "author": "gszadovszky", "createdAt": "2020-05-21T14:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\nindex 61e8ccc44..4d3328090 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n", "chunk": "@@ -91,22 +102,35 @@ public class AesCipher {\n     if (pageOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong page ordinal: \" + pageOrdinal);\n     }\n-    byte[] pageOrdinalBytes = shortToBytesLE(pageOrdinal);\n+    short shortPageOrdinal = (short) pageOrdinal;\n+    if (shortPageOrdinal != pageOrdinal) {\n+      throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n+          + \"more than Short.MAX_VALUE pages per chunk: \" + pageOrdinal);\n+    }\n+    byte[] pageOrdinalBytes = shortToBytesLE(shortPageOrdinal);\n     \n     return concatByteArrays(fileAAD, typeOrdinalBytes, rowGroupOrdinalBytes, columnOrdinalBytes, pageOrdinalBytes);\n   }\n \n   public static byte[] createFooterAAD(byte[] aadPrefixBytes) {\n-    return createModuleAAD(aadPrefixBytes, ModuleType.Footer, (short) -1, (short) -1, (short) -1);\n+    return createModuleAAD(aadPrefixBytes, ModuleType.Footer, -1, -1, -1);\n   }\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n-  public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+  public static void quickUpdatePageAAD(byte[] pageAAD, int newPageOrdinal) {\n+    if (null == pageAAD) {\n+      throw new IllegalArgumentException(\"Null pageAAD\");\n+    }\n     if (newPageOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong page ordinal: \" + newPageOrdinal);\n     }\n+    short shortPageOrdinal = (short) newPageOrdinal;\n+    if (shortPageOrdinal != newPageOrdinal) {\n+      throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n+          + \"more than Short.MAX_VALUE pages per chunk: \" + newPageOrdinal);\n+    }\n     \n-    byte[] pageOrdinalBytes = shortToBytesLE(newPageOrdinal);\n+    byte[] pageOrdinalBytes = shortToBytesLE(shortPageOrdinal);\n     System.arraycopy(pageOrdinalBytes, 0, pageAAD, pageAAD.length - 2, 2);\n   }\n \n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\nindex 4d3328090..6b9f24c45 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n", "chunk": "@@ -118,16 +118,14 @@ public class AesCipher {\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, int newPageOrdinal) {\n-    if (null == pageAAD) {\n-      throw new IllegalArgumentException(\"Null pageAAD\");\n-    }\n+    java.util.Objects.requireNonNull(pageAAD);\n     if (newPageOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong page ordinal: \" + newPageOrdinal);\n     }\n     short shortPageOrdinal = (short) newPageOrdinal;\n     if (shortPageOrdinal != newPageOrdinal) {\n       throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n-          + \"more than Short.MAX_VALUE pages per chunk: \" + newPageOrdinal);\n+          + \"more than \" + Short.MAX_VALUE + \" pages per chunk: \" + newPageOrdinal);\n     }\n     \n     byte[] pageOrdinalBytes = shortToBytesLE(shortPageOrdinal);\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415444590", "body": "Should we keep original addRowGroup() intact and just add a new one with InternalFileEncryptor to isolate the change's impact? There is not much duplicate code if doing so and we can refactor the existing code with helper functions.  In the majority use cases, they are non-encryption cases. ", "bodyText": "Should we keep original addRowGroup() intact and just add a new one with InternalFileEncryptor to isolate the change's impact? There is not much duplicate code if doing so and we can refactor the existing code with helper functions.  In the majority use cases, they are non-encryption cases.", "bodyHTML": "<p dir=\"auto\">Should we keep original addRowGroup() intact and just add a new one with InternalFileEncryptor to isolate the change's impact? There is not much duplicate code if doing so and we can refactor the existing code with helper functions.  In the majority use cases, they are non-encryption cases.</p>", "author": "shangxinli", "createdAt": "2020-04-27T00:57:22Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYyMTM5OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415621398", "bodyText": "encryption is isolated there, with if (null != fileEncryptor)  and if (encryptMetaData) - similar to the if (columnIndexRef != null)  and if (offsetIndexRef != null) in the same function.", "author": "ggershinsky", "createdAt": "2020-04-27T08:38:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NzM2OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r422577368", "bodyText": "Not all the changes are isolated. Generally, adding 'if/else' will add diverge the code and add the complexity. One other thing is regression thinking. If fileEncryptor is null, which would be most of the case, then it just executes the existing method without change. It would be less error prone.", "author": "shangxinli", "createdAt": "2020-05-10T03:16:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUwNzMzMg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r423507332", "bodyText": "Many functions can be run either with or without encryption. Duplicating them will result in hundreds or thousands of duplicate code lines. This will make code maintenance (changes/fixes)  a headache. Instead, we isolate encryption with if switches, without duplicating the existing code.\nThe same goes for other recent new features (column indexes and bloom filters) - they are isolated with an if switch, instead of code duplication. See if (columnIndexRef != null) and if (offsetIndexRef != null) in this addRowGroup function.", "author": "ggershinsky", "createdAt": "2020-05-12T07:05:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDY4NQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r432580685", "bodyText": "It is not a blocking comment and I am fine with it. But generally speaking, adding too much nested if/else diverges the code path and causes the complexity for reading. One way to avoid duplicating is to wrap them up in helper functions. I understand column indexes and bloom filters already did that but if we keep adding features like this, the code will become less and less readable.", "author": "shangxinli", "createdAt": "2020-05-29T15:55:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..eafce4305 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -487,13 +483,13 @@ public class ParquetMetadataConverter {\n   }\n \n   private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,\n-      InternalFileEncryptor fileEncryptor) throws IOException {\n+      InternalFileEncryptor fileEncryptor) {\n     \n     //rowGroup.total_byte_size = ;\n     List<ColumnChunkMetaData> columns = block.getColumns();\n     List<ColumnChunk> parquetColumns = new ArrayList<ColumnChunk>();\n-    short rowGroupOrdinal = (short) rowGroups.size();\n-    short columnOrdinal = -1;\n+    int rowGroupOrdinal = rowGroups.size();\n+    int columnOrdinal = -1;\n     ByteArrayOutputStream tempOutStream = null;\n     for (ColumnChunkMetaData columnMetaData : columns) {\n       ColumnChunk columnChunk = new ColumnChunk(columnMetaData.getFirstDataPageOffset()); // verify this is the right offset\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415446216", "body": "Move comments up", "bodyText": "Move comments up", "bodyHTML": "<p dir=\"auto\">Move comments up</p>", "author": "shangxinli", "createdAt": "2020-04-27T01:04:48Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYyMjg0NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415622844", "bodyText": "is there a requirement in the code formatting rules in this community to keep comments in separate lines?", "author": "ggershinsky", "createdAt": "2020-04-27T08:40:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTMyNg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r422575326", "bodyText": "No. I see most of them on up line but a few on the same line. It is not a must.", "author": "shangxinli", "createdAt": "2020-05-10T02:49:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyODA2MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427328060", "bodyText": "Actually, based on parquet-mr README:\n\nGenerally speaking, stick to the Sun Java Code Conventions\n\nBased on the related section both should be fine.", "author": "gszadovszky", "createdAt": "2020-05-19T14:05:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..eafce4305 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1339,7 +1340,7 @@ public class ParquetMetadataConverter {\n         fileDecryptor.setPlaintextFile();\n         // Done to detect files that were not encrypted by mistake\n         if (!fileDecryptor.plaintextFilesAllowed()) {\n-          throw new IOException(\"Applying decryptor on plaintext file\");\n+          throw new ParquetCryptoRuntimeException(\"Applying decryptor on plaintext file\");\n         }\n       } else {  // Encrypted file with plaintext footer\n         // if no fileDecryptor, can still read plaintext columns\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427284527", "body": "Why do we need it as a `short` instead of keeping it as an `int`? As per the parquet.thrift spec we never say that we cannot have more pages than `32767` even if it is unlikely to have such many.", "bodyText": "Why do we need it as a short instead of keeping it as an int? As per the parquet.thrift spec we never say that we cannot have more pages than 32767 even if it is unlikely to have such many.", "bodyHTML": "<p dir=\"auto\">Why do we need it as a <code>short</code> instead of keeping it as an <code>int</code>? As per the parquet.thrift spec we never say that we cannot have more pages than <code>32767</code> even if it is unlikely to have such many.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T13:03:52Z", "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "diffHunk": "@@ -49,6 +49,13 @@\n    * @return the index of the first row in the page\n    */\n   public long getFirstRowIndex(int pageIndex);\n+  \n+  /**\n+   * @param pageIndex\n+   *         the index of the page\n+   * @return the original ordinal of the page in the column chunk\n+   */\n+  public short getPageOrdinal(int pageIndex);", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0NTU3Ng==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427745576", "bodyText": "The background discussion is here,\n#776 (comment)\nIn the case of pages, encryption becomes an order (or two orders) of magnitude slower if the pages are small. Basically, the hardware acceleration does not kick in with small pages (and there are additional problems). This is another reason not to allow more than 32K pages in a chunk.", "author": "ggershinsky", "createdAt": "2020-05-20T05:14:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg0NTgyNw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427845827", "bodyText": "Plus - the page headers are also encrypted. These are small, so the hardware acceleration is not applied on them. Having dozens/hundreds of thousands (or more) of page headers will significantly affect the overall encryption time of a file.", "author": "ggershinsky", "createdAt": "2020-05-20T08:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java b/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java\nindex 44627c4cf..4bd925487 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java\n", "chunk": "@@ -55,7 +55,7 @@ public interface OffsetIndex {\n    *         the index of the page\n    * @return the original ordinal of the page in the column chunk\n    */\n-  public short getPageOrdinal(int pageIndex);\n+  public int getPageOrdinal(int pageIndex);\n \n   /**\n    * @param pageIndex\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427294307", "body": "Theoretically we don't give hard limits for the number of row groups, number of columns or the number of pages in the spec. There is a de facto limit that we use thrift lists where the size is an i32 meaning that we should allow java int values here.\r\nAlso, there was a post commit discussion in a [related PR](https://github.com/apache/parquet-format/pull/142#issuecomment-600294754). It is unfortunate that that time parquet-format was already released so I don't know if there is a way to properly fix this issue in the format. Anyway, I would not restrict these values to a `short`.", "bodyText": "Theoretically we don't give hard limits for the number of row groups, number of columns or the number of pages in the spec. There is a de facto limit that we use thrift lists where the size is an i32 meaning that we should allow java int values here.\nAlso, there was a post commit discussion in a related PR. It is unfortunate that that time parquet-format was already released so I don't know if there is a way to properly fix this issue in the format. Anyway, I would not restrict these values to a short.", "bodyHTML": "<p dir=\"auto\">Theoretically we don't give hard limits for the number of row groups, number of columns or the number of pages in the spec. There is a de facto limit that we use thrift lists where the size is an i32 meaning that we should allow java int values here.<br>\nAlso, there was a post commit discussion in a <a href=\"https://github.com/apache/parquet-format/pull/142#issuecomment-600294754\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/apache/parquet-format/pull/142/hovercard\">related PR</a>. It is unfortunate that that time parquet-format was already released so I don't know if there is a way to properly fix this issue in the format. Anyway, I would not restrict these values to a <code>short</code>.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T13:18:12Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0Mzg2MQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427743861", "bodyText": "The links to the discussion on this,\napache/parquet-format#114 (comment)\napache/parquet-format#114 (comment)\nhttp://mail-archives.apache.org/mod_mbox/parquet-dev/201901.mbox/%3CCAO4re1kM4xGMNT4CGrjvA43t-QgUmUwLMskTJfd8ivgCfF8rSw%40mail.gmail.com%3E\nThe parquet-cpp approach to this is to allow for any number of row groups in files without encryption, and to limit it to 32K in encrypted files,\napache/arrow@0c5168c\n\"While writing files with so many row groups is a bad idea, people will still do it... This .. enables reading the many-row-group files again. Files with encrypted row group metadata with that many row groups cannot be read\"", "author": "ggershinsky", "createdAt": "2020-05-20T05:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg0MzI5MQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427843291", "bodyText": "Also - like with the page numbers in the previous comment, having too many row groups will adversely affect encryption performance. There are per-rowgroup encryption operations, always performed on small  buffers - therefore, very slow (no hardware acceleration, etc). Having dozens/hundreds of thousands (or more) of them will significantly affect the overall encryption time of a file. Moreover, having lots of row groups might lead to having smaller data pages, which decreases the performance further.", "author": "ggershinsky", "createdAt": "2020-05-20T08:46:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMjMxMg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427922312", "bodyText": "Still, the variables in this function (and elsewhere) don't have to be short. After looking at the code, it seems ints are better suited for managing and checking these parameters (and for enabling any values in unencrypted files). I'll make this change.", "author": "ggershinsky", "createdAt": "2020-05-20T11:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyNTExMQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427925111", "bodyText": "I understand that large numbers of pages/row groups or columns would lead to significant performance drawbacks but it should not limit what the spec allows otherwise.\nSince it is discussed and approved already, I am fine with using short values for these. What I would suggest adding though is to have the conversion from int to short centralized and and have specific error messages so it is clear that the limit reached is a hard limit for the encryption feature. Also, if we will publish any description/example for the encryption feature these limitations shall be listed there.\nOne more thing: the check of intValue > Short.MAX_VALUE is not complete. In case of intValue is negative the cast may result in a valid positive short value. I would suggest using (short) intValue != intValue instead.", "author": "gszadovszky", "createdAt": "2020-05-20T11:07:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzMjQ3Ng==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427932476", "bodyText": "Thanks for the value checking tip, I'll update the code to use it. As for a centralization - I think this function (createModuleAAD) is the right place. In the encryption feature, ordinals are used only for integrity verification - performed via AADs, which are calculated here for both encryption and decryption. Everywhere in the code, the ordinals will be an int. Since the createModuleAAD is called only for encrypted files, an exception will be thrown only for them (if an ordinal exceeds the Short.MAX_VALUE).", "author": "ggershinsky", "createdAt": "2020-05-20T11:21:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzNDM0Ng==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427934346", "bodyText": "Sounds good to me.", "author": "gszadovszky", "createdAt": "2020-05-20T11:25:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\nindex 61e8ccc44..4d3328090 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n", "chunk": "@@ -66,7 +66,7 @@ public class AesCipher {\n   }\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n-      short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {\n+      int rowGroupOrdinal, int columnOrdinal, int pageOrdinal) {\n     \n     byte[] typeOrdinalBytes = new byte[1];\n     typeOrdinalBytes[0] = moduleType.getValue();\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5OTYxOA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427299618", "body": "I would keep `throws IOException` here. `InputStream` objects throw `IOException` so the caller shall be prepared handling these.", "bodyText": "I would keep throws IOException here. InputStream objects throw IOException so the caller shall be prepared handling these.", "bodyHTML": "<p dir=\"auto\">I would keep <code>throws IOException</code> here. <code>InputStream</code> objects throw <code>IOException</code> so the caller shall be prepared handling these.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T13:25:37Z", "path": "parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java", "diffHunk": "@@ -51,19 +49,17 @@\n      * Parquet Modular Encryption specification.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext - starts at offset 0 of the output value, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto problem encountered during decryption\n      */\n-    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD) throws IOException;\n+    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD);\n \n     /**\n      * Convenience decryption method that reads the length and ciphertext from the input stream.\n      * \n      * @param from Input stream with length and ciphertext.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext -  starts at offset 0 of the output, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto or IO problem encountered during decryption\n      */\n-    public byte[] decrypt(InputStream from, byte[] AAD) throws IOException;\n+    public byte[] decrypt(InputStream from, byte[] AAD);", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java b/parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java\nindex ccb9af673..37b0b583c 100755\n--- a/parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java\n+++ b/parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java\n", "chunk": "@@ -58,8 +59,9 @@ public interface BlockCipher{\n      * @param from Input stream with length and ciphertext.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext -  starts at offset 0 of the output, and fills up the entire byte array.\n+     * @throws IOException - Stream I/O problems\n      */\n-    public byte[] decrypt(InputStream from, byte[] AAD);\n+    public byte[] decrypt(InputStream from, byte[] AAD) throws IOException;\n   }\n }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU0Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300547", "body": "We should let the `IOException` thrown out.", "bodyText": "We should let the IOException thrown out.", "bodyHTML": "<p dir=\"auto\">We should let the <code>IOException</code> thrown out.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T13:27:00Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -98,16 +104,21 @@\n         ((lengthBuffer[0] & 0xff));\n \n     if (ciphertextLength < 1) {\n-      throw new IOException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n+      throw new ParquetCryptoRuntimeException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n     }\n \n     byte[] ciphertextBuffer = new byte[ciphertextLength];\n     gotBytes = 0;\n     // Read the encrypted structure contents\n     while (gotBytes < ciphertextLength) {\n-      int n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      } catch (IOException e) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\nindex 906d40518..4bc251abb 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n", "chunk": "@@ -111,12 +106,7 @@ public class AesGcmDecryptor extends AesCipher implements BlockCipher.Decryptor{\n     gotBytes = 0;\n     // Read the encrypted structure contents\n     while (gotBytes < ciphertextLength) {\n-      int n;\n-      try {\n-        n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n-      } catch (IOException e) {\n-        throw new ParquetCryptoRuntimeException(e);\n-      }\n+      int n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n       if (n <= 0) {\n         throw new ParquetCryptoRuntimeException(\"Tried to read \" + ciphertextLength + \" bytes, but only got \" + gotBytes + \" bytes.\");\n       }\n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\nindex 4bc251abb..1524d8e72 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n", "chunk": "@@ -108,7 +108,7 @@ public class AesGcmDecryptor extends AesCipher implements BlockCipher.Decryptor{\n     while (gotBytes < ciphertextLength) {\n       int n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n       if (n <= 0) {\n-        throw new ParquetCryptoRuntimeException(\"Tried to read \" + ciphertextLength + \" bytes, but only got \" + gotBytes + \" bytes.\");\n+        throw new IOException(\"Tried to read \" + ciphertextLength + \" bytes, but only got \" + gotBytes + \" bytes.\");\n       }\n       gotBytes += n;\n     }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU2NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300564", "body": "We should let the `IOException` thrown out.", "bodyText": "We should let the IOException thrown out.", "bodyHTML": "<p dir=\"auto\">We should let the <code>IOException</code> thrown out.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T13:27:01Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -70,23 +69,30 @@\n       if (null != AAD) cipher.updateAAD(AAD);\n \n       cipher.doFinal(ciphertext, inputOffset, inputLength, plainText, outputOffset);\n-    }  catch (GeneralSecurityException e) {\n-      throw new IOException(\"Failed to decrypt\", e);\n+    }  catch (AEADBadTagException e) {\n+      throw new TagVerificationException(\"GCM tag check failed\", e);\n+    } catch (GeneralSecurityException e) {\n+      throw new ParquetCryptoRuntimeException(\"Failed to decrypt\", e);\n     }\n \n     return plainText;\n   }\n \n   @Override\n-  public byte[] decrypt(InputStream from, byte[] AAD) throws IOException {\n+  public byte[] decrypt(InputStream from, byte[] AAD) {\n     byte[] lengthBuffer = new byte[SIZE_LENGTH];\n     int gotBytes = 0;\n \n     // Read the length of encrypted Thrift structure\n     while (gotBytes < SIZE_LENGTH) {\n-      int n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      } catch (IOException e) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\nindex 906d40518..4bc251abb 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n", "chunk": "@@ -79,18 +79,13 @@ public class AesGcmDecryptor extends AesCipher implements BlockCipher.Decryptor{\n   }\n \n   @Override\n-  public byte[] decrypt(InputStream from, byte[] AAD) {\n+  public byte[] decrypt(InputStream from, byte[] AAD) throws IOException {\n     byte[] lengthBuffer = new byte[SIZE_LENGTH];\n     int gotBytes = 0;\n \n     // Read the length of encrypted Thrift structure\n     while (gotBytes < SIZE_LENGTH) {\n-      int n;\n-      try {\n-        n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n-      } catch (IOException e) {\n-        throw new ParquetCryptoRuntimeException(e);\n-      }\n+      int n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n       if (n <= 0) {\n         throw new ParquetCryptoRuntimeException(\"Tried to read int (4 bytes), but only got \" + gotBytes + \" bytes.\");\n       }\n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\nindex 4bc251abb..1524d8e72 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java\n", "chunk": "@@ -87,7 +87,7 @@ public class AesGcmDecryptor extends AesCipher implements BlockCipher.Decryptor{\n     while (gotBytes < SIZE_LENGTH) {\n       int n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n       if (n <= 0) {\n-        throw new ParquetCryptoRuntimeException(\"Tried to read int (4 bytes), but only got \" + gotBytes + \" bytes.\");\n+        throw new IOException(\"Tried to read int (4 bytes), but only got \" + gotBytes + \" bytes.\");\n       }\n       gotBytes += n;\n     }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyOTk0OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427329948", "body": "Is `plaintext` a usual term for _un-encrypted_ files? I don't really like it but fine if it is commonly used in that sense. (Plaintext files for me are the `*.txt` files.)", "bodyText": "Is plaintext a usual term for un-encrypted files? I don't really like it but fine if it is commonly used in that sense. (Plaintext files for me are the *.txt files.)", "bodyHTML": "<p dir=\"auto\">Is <code>plaintext</code> a usual term for <em>un-encrypted</em> files? I don't really like it but fine if it is commonly used in that sense. (Plaintext files for me are the <code>*.txt</code> files.)</p>", "author": "gszadovszky", "createdAt": "2020-05-19T14:07:48Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc1OTEwMQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427759101", "bodyText": "In cryptography, plaintext is an opposite of ciphertext (the result of plaintext encryption).", "author": "ggershinsky", "createdAt": "2020-05-20T05:59:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyOTk0OA=="}], "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..eafce4305 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1339,7 +1340,7 @@ public class ParquetMetadataConverter {\n         fileDecryptor.setPlaintextFile();\n         // Done to detect files that were not encrypted by mistake\n         if (!fileDecryptor.plaintextFilesAllowed()) {\n-          throw new IOException(\"Applying decryptor on plaintext file\");\n+          throw new ParquetCryptoRuntimeException(\"Applying decryptor on plaintext file\");\n         }\n       } else {  // Encrypted file with plaintext footer\n         // if no fileDecryptor, can still read plaintext columns\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzMDMxNg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427330316", "body": "I think, `ParquetCryptoRuntimeException` would fit better here.", "bodyText": "I think, ParquetCryptoRuntimeException would fit better here.", "bodyHTML": "<p dir=\"auto\">I think, <code>ParquetCryptoRuntimeException</code> would fit better here.</p>", "author": "gszadovszky", "createdAt": "2020-05-19T14:08:17Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..eafce4305 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1339,7 +1340,7 @@ public class ParquetMetadataConverter {\n         fileDecryptor.setPlaintextFile();\n         // Done to detect files that were not encrypted by mistake\n         if (!fileDecryptor.plaintextFilesAllowed()) {\n-          throw new IOException(\"Applying decryptor on plaintext file\");\n+          throw new ParquetCryptoRuntimeException(\"Applying decryptor on plaintext file\");\n         }\n       } else {  // Encrypted file with plaintext footer\n         // if no fileDecryptor, can still read plaintext columns\n", "next_change": null}]}}, {"oid": "c9761c39a774caac1bde5875b41e3368e745c0b4", "url": "https://github.com/apache/parquet-mr/commit/c9761c39a774caac1bde5875b41e3368e745c0b4", "message": "address initial comments", "committedDate": "2020-05-20T13:35:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MTQxNg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427971416", "body": "There was no such check in the previous code. Strictly speaking it is a breaking change as a `NullPointerException` was thrown where an `IOException` is thrown today. ", "bodyText": "There was no such check in the previous code. Strictly speaking it is a breaking change as a NullPointerException was thrown where an IOException is thrown today.", "bodyHTML": "<p dir=\"auto\">There was no such check in the previous code. Strictly speaking it is a breaking change as a <code>NullPointerException</code> was thrown where an <code>IOException</code> is thrown today.</p>", "author": "gszadovszky", "createdAt": "2020-05-20T12:32:34Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..2c93d3157 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1409,9 +1410,6 @@ public class ParquetMetadataConverter {\n           boolean encryptedMetadata = false;\n           \n           if (null == cryptoMetaData) { // Plaintext column\n-            if (null == metaData) {\n-              throw new IOException(\"ColumnMetaData not set in plaintext column\");\n-            }\n             columnPath = getPath(metaData);\n             if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n               // mark this column as plaintext in encrypted file decryptor\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MjE4MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427972180", "body": "These `IOException`s seems to be thrown in cases of encryption related issues. Don't we want to use the specific exception instead?", "bodyText": "These IOExceptions seems to be thrown in cases of encryption related issues. Don't we want to use the specific exception instead?", "bodyHTML": "<p dir=\"auto\">These <code>IOException</code>s seems to be thrown in cases of encryption related issues. Don't we want to use the specific exception instead?</p>", "author": "gszadovszky", "createdAt": "2020-05-20T12:33:55Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");\n+            }\n+            columnPath = getPath(metaData);\n+            if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n+              // mark this column as plaintext in encrypted file decryptor\n+              fileDecryptor.setColumnCryptoMetadata(columnPath, false, false, (byte[]) null, columnOrdinal);\n+            }\n+          } else {  // Encrypted column\n+            boolean encryptedWithFooterKey = cryptoMetaData.isSetENCRYPTION_WITH_FOOTER_KEY();\n+            if (encryptedWithFooterKey) { // Column encrypted with footer key\n+              if (!encryptedFooter) {\n+                throw new IOException(\"Column encrypted with footer key in file with plaintext footer\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..2c93d3157 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1421,13 +1419,13 @@ public class ParquetMetadataConverter {\n             boolean encryptedWithFooterKey = cryptoMetaData.isSetENCRYPTION_WITH_FOOTER_KEY();\n             if (encryptedWithFooterKey) { // Column encrypted with footer key\n               if (!encryptedFooter) {\n-                throw new IOException(\"Column encrypted with footer key in file with plaintext footer\");\n+                throw new ParquetCryptoRuntimeException(\"Column encrypted with footer key in file with plaintext footer\");\n               }\n               if (null == metaData) {\n-                throw new IOException(\"ColumnMetaData not set in Encryption with Footer key\");\n+                throw new ParquetCryptoRuntimeException(\"ColumnMetaData not set in Encryption with Footer key\");\n               }\n               if (null == fileDecryptor) {\n-                throw new IOException(\"Column encrypted with footer key: No keys available\");\n+                throw new ParquetCryptoRuntimeException(\"Column encrypted with footer key: No keys available\");\n               }\n               columnPath = getPath(metaData);\n               fileDecryptor.setColumnCryptoMetadata(columnPath, true, true, (byte[]) null, columnOrdinal);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk4ODI2MQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427988261", "body": "Please, check your code to not to introduce any trailing whitespaces.", "bodyText": "Please, check your code to not to introduce any trailing whitespaces.", "bodyHTML": "<p dir=\"auto\">Please, check your code to not to introduce any trailing whitespaces.</p>", "author": "gszadovszky", "createdAt": "2020-05-20T12:59:10Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1465,23 +1674,38 @@ public void writeDataPageV2Header(\n             dataEncoding,\n             rlByteLength, dlByteLength), to);\n   }\n-\n+  ", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\nindex 117985899..2c93d3157 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java\n", "chunk": "@@ -1674,7 +1672,7 @@ public class ParquetMetadataConverter {\n             dataEncoding,\n             rlByteLength, dlByteLength), to);\n   }\n-  \n+\n   public void writeDataPageV1Header(\n       int uncompressedSize,\n       int compressedSize,\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428007885", "body": "Why do we introduce new public methods that are deprecated already?", "bodyText": "Why do we introduce new public methods that are deprecated already?", "bodyHTML": "<p dir=\"auto\">Why do we introduce new public methods that are deprecated already?</p>", "author": "gszadovszky", "createdAt": "2020-05-20T13:25:53Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -442,7 +460,13 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER);\n+    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(Configuration configuration, Path file, ", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ1NTA3MQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428455071", "bodyText": "These are encrypting versions of the existing readFooter functions, already marked as deprecated, but still actively used (eg in parquet-cli and in Spark). The deprecation comment says \"@ deprecated will be removed in 2.0.0\". Since we are not at parquet 2.0 yet, I've marked the encrypting versions of these functions as deprecated too, in order not to forget to handle them when working on parquet-2.0.", "author": "ggershinsky", "createdAt": "2020-05-21T05:39:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzNTc2Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428535767", "bodyText": "So, all of these new methods are used inside parquet-mr? If not, then I don't think we need them. If yes, then please, try to refactor the caller part to use the non-deprecated ones instead. If it does not require too much effort.", "author": "gszadovszky", "createdAt": "2020-05-21T09:07:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -460,13 +451,7 @@ public class ParquetFileReader implements Closeable {\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n-  }\n-\n-  @Deprecated\n-  public static final ParquetMetadata readFooter(Configuration configuration, Path file, \n-      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER, fileDecryptionProperties);\n+    return readFooter(configuration, file, NO_FILTER);\n   }\n \n   /**\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwODM1Mw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428008353", "body": "Why do we introduce new public methods that are deprecated already?", "bodyText": "Why do we introduce new public methods that are deprecated already?", "bodyHTML": "<p dir=\"auto\">Why do we introduce new public methods that are deprecated already?</p>", "author": "gszadovszky", "createdAt": "2020-05-20T13:26:28Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, ", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -528,38 +508,27 @@ public class ParquetFileReader implements Closeable {\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n-    return readFooter(file, filter, null);\n-  }\n-\n-  @Deprecated\n-  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, \n-      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetReadOptions options;\n     if (file instanceof HadoopInputFile) {\n       HadoopInputFile hadoopFile = (HadoopInputFile) file;\n-      options = HadoopReadOptions.builder(hadoopFile.getConfiguration())\n+      options = HadoopReadOptions.builder(hadoopFile.getConfiguration(), hadoopFile.getPath())\n           .withMetadataFilter(filter).build();\n-      if (null == fileDecryptionProperties) {\n-        fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());\n-      }\n     } else {\n       options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\n     }\n \n     try (SeekableInputStream in = file.newStream()) {\n-      return readFooter(file, options, in, fileDecryptionProperties);\n+      return readFooter(file, options, in);\n     }\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, \n-      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException {\n     ParquetMetadataConverter converter = new ParquetMetadataConverter(options);\n-    return readFooter(file, options, f, converter, fileDecryptionProperties);\n+    return readFooter(file, options, f, converter);\n   }\n \n   private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, \n-      SeekableInputStream f, ParquetMetadataConverter converter, \n-      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n+      SeekableInputStream f, ParquetMetadataConverter converter) throws IOException {\n \n     long fileLen = file.getLength();\n     String filePath = file.toString();\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAxMTg3MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428011870", "body": "I would use the specific crypto exception here.", "bodyText": "I would use the specific crypto exception here.", "bodyHTML": "<p dir=\"auto\">I would use the specific crypto exception here.</p>", "author": "gszadovszky", "createdAt": "2020-05-20T13:31:01Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetReadOptions options;\n     if (file instanceof HadoopInputFile) {\n-      options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration())\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      options = HadoopReadOptions.builder(hadoopFile.getConfiguration())\n           .withMetadataFilter(filter).build();\n+      if (null == fileDecryptionProperties) {\n+        fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());\n+      }\n     } else {\n       options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\n     }\n \n     try (SeekableInputStream in = file.newStream()) {\n-      return readFooter(file, options, in);\n+      return readFooter(file, options, in, fileDecryptionProperties);\n     }\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetMetadataConverter converter = new ParquetMetadataConverter(options);\n-    return readFooter(file, options, f, converter);\n+    return readFooter(file, options, f, converter, fileDecryptionProperties);\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, \n+      SeekableInputStream f, ParquetMetadataConverter converter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n+\n     long fileLen = file.getLength();\n+    String filePath = file.toString();\n     LOG.debug(\"File length {}\", fileLen);\n+\n     int FOOTER_LENGTH_SIZE = 4;\n     if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) { // MAGIC + data + footer + footerIndex + MAGIC\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file (too small length: \" + fileLen + \")\");\n+      throw new RuntimeException(filePath + \" is not a Parquet file (length is too low: \" + fileLen + \")\");\n     }\n-    long footerLengthIndex = fileLen - FOOTER_LENGTH_SIZE - MAGIC.length;\n-    LOG.debug(\"reading footer index at {}\", footerLengthIndex);\n \n-    f.seek(footerLengthIndex);\n-    int footerLength = readIntLittleEndian(f);\n+    // Read footer length and magic string - with a single seek\n     byte[] magic = new byte[MAGIC.length];\n+    long fileMetadataLengthIndex = fileLen - magic.length - FOOTER_LENGTH_SIZE;\n+    LOG.debug(\"reading footer index at {}\", fileMetadataLengthIndex);\n+    f.seek(fileMetadataLengthIndex);\n+    int fileMetadataLength = readIntLittleEndian(f);\n     f.readFully(magic);\n-    if (!Arrays.equals(MAGIC, magic)) {\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file. expected magic number at tail \" + Arrays.toString(MAGIC) + \" but found \" + Arrays.toString(magic));\n+\n+    boolean encryptedFooterMode;\n+    if (Arrays.equals(MAGIC, magic)) {\n+      encryptedFooterMode = false;\n+    } else if (Arrays.equals(EFMAGIC, magic)) {\n+      encryptedFooterMode = true;\n+    } else {\n+      throw new RuntimeException(filePath + \" is not a Parquet file. Expected magic number at tail, but found \" + Arrays.toString(magic));\n+    }\n+\n+    long fileMetadataIndex = fileMetadataLengthIndex - fileMetadataLength;\n+    LOG.debug(\"read footer length: {}, footer index: {}\", fileMetadataLength, fileMetadataIndex);\n+    if (fileMetadataIndex < magic.length || fileMetadataIndex >= fileMetadataLengthIndex) {\n+      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + fileMetadataIndex);\n     }\n-    long footerIndex = footerLengthIndex - footerLength;\n-    LOG.debug(\"read footer length: {}, footer index: {}\", footerLength, footerIndex);\n-    if (footerIndex < MAGIC.length || footerIndex >= footerLengthIndex) {\n-      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + footerIndex);\n+    f.seek(fileMetadataIndex);\n+\n+    InternalFileDecryptor fileDecryptor = null;\n+    if (null != fileDecryptionProperties) {\n+      fileDecryptor  = new InternalFileDecryptor(fileDecryptionProperties);\n     }\n-    f.seek(footerIndex);\n+\n     // Read all the footer bytes in one time to avoid multiple read operations,\n     // since it can be pretty time consuming for a single read operation in HDFS.\n-    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(footerLength);\n+    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(fileMetadataLength);\n     f.readFully(footerBytesBuffer);\n     LOG.debug(\"Finished to read all footer bytes.\");\n     footerBytesBuffer.flip();\n     InputStream footerBytesStream = ByteBufferInputStream.wrap(footerBytesBuffer);\n-    return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter());\n+\n+    // Regular file, or encrypted file with plaintext footer\n+    if (!encryptedFooterMode) {\n+      return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter(), fileDecryptor, false, \n+          fileMetadataLength);\n+    }\n+\n+    // Encrypted file with encrypted footer\n+    if (null == fileDecryptor) {\n+      throw new RuntimeException(\"Trying to read file with encrypted footer. No keys available\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -615,7 +585,7 @@ public class ParquetFileReader implements Closeable {\n \n     // Encrypted file with encrypted footer\n     if (null == fileDecryptor) {\n-      throw new RuntimeException(\"Trying to read file with encrypted footer. No keys available\");\n+      throw new ParquetCryptoRuntimeException(\"Trying to read file with encrypted footer. No keys available\");\n     }\n     FileCryptoMetaData fileCryptoMetaData = readFileCryptoMetaData(footerBytesStream);\n     fileDecryptor.setFileCryptoMetaData(fileCryptoMetaData.getEncryption_algorithm(), \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428034074", "body": "I think, it would be better to put the `FileDecryptionProperties` into `ParquetReadOptions`. `HadoopReadOptions` (the extension of `ParquetReadOptions`) already contains the hadoop conf so you may be able to create `FileDecryptionProperties` from there if not set.\r\nThis way you do not need to add new methods/constructors where `ParquetReadOptions` is already there as an argument.", "bodyText": "I think, it would be better to put the FileDecryptionProperties into ParquetReadOptions. HadoopReadOptions (the extension of ParquetReadOptions) already contains the hadoop conf so you may be able to create FileDecryptionProperties from there if not set.\nThis way you do not need to add new methods/constructors where ParquetReadOptions is already there as an argument.", "bodyHTML": "<p dir=\"auto\">I think, it would be better to put the <code>FileDecryptionProperties</code> into <code>ParquetReadOptions</code>. <code>HadoopReadOptions</code> (the extension of <code>ParquetReadOptions</code>) already contains the hadoop conf so you may be able to create <code>FileDecryptionProperties</code> from there if not set.<br>\nThis way you do not need to add new methods/constructors where <code>ParquetReadOptions</code> is already there as an argument.</p>", "author": "gszadovszky", "createdAt": "2020-05-20T13:58:26Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ2NTA5NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428465094", "bodyText": "We need the file Path in order to create the file decryption properties. ParquetReadOptions and HadoopReadOptions don't keep/handle the file paths.", "author": "ggershinsky", "createdAt": "2020-05-21T06:14:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzNzIyMA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428537220", "bodyText": "ParquetReadOptions was created to carry all the required properties for reading a parquet file. If the Path is necessary for the decryption then we might add it to the options as well. If we decide to not to add it still, I would use the options object to carry any other decryption properties.", "author": "gszadovszky", "createdAt": "2020-05-21T09:10:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU4NDk5OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428584998", "bodyText": "Sounds good. I'll check what can be done here.\nAlso, will check how the deprecated readFooter functions can be handled for encryption.", "author": "ggershinsky", "createdAt": "2020-05-21T11:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}], "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -797,26 +766,15 @@ public class ParquetFileReader implements Closeable {\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n-    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n-    this(file, options, null);\n-  }\n-\n-  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n-      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n-    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n-      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n-      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());\n-    }\n-\n     try {\n-      this.footer = readFooter(file, options, f, converter, fileDecryptionProperties);\n+      this.footer = readFooter(file, options, f, converter);\n     } catch (Exception e) {\n       // In case that reading footer throws an exception in the constructor, the new stream\n       // should be closed. Otherwise, there's no way to close this outside.\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA0MTI0OQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428041249", "body": "I think, we can replace it.", "bodyText": "I think, we can replace it.", "bodyHTML": "<p dir=\"auto\">I think, we can replace it.</p>", "author": "gszadovszky", "createdAt": "2020-05-20T14:07:23Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1021,18 +1148,44 @@ DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException {\n         !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {\n       return null;\n     }\n+    /** TODO Gabor - can be replaced with this?:\n+    if (!meta.hasDictionaryPage()) {\n+      return null;\n+    } */", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -1144,14 +1100,9 @@ public class ParquetFileReader implements Closeable {\n    * @throws IOException if there is an error while reading the dictionary\n    */\n   DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException {\n-    if (!meta.getEncodings().contains(Encoding.PLAIN_DICTIONARY) &&\n-        !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {\n-      return null;\n-    }\n-    /** TODO Gabor - can be replaced with this?:\n     if (!meta.hasDictionaryPage()) {\n       return null;\n-    } */\n+    }\n \n     // TODO: this should use getDictionaryPageOffset() but it isn't reliable.\n     if (f.getPos() != meta.getStartingPos()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428554021", "body": "@chenjunjiedada, could you please check this?", "bodyText": "@chenjunjiedada, could you please check this?", "bodyHTML": "<p dir=\"auto\"><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/chenjunjiedada/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/chenjunjiedada\">@chenjunjiedada</a>, could you please check this?</p>", "author": "gszadovszky", "createdAt": "2020-05-21T09:48:17Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1071,12 +1229,31 @@ public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n+\n+    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyMzg0OQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431823849", "bodyText": "@ggershinsky, please ensure this is correct and don't keep TODOs in the final code. (We usually don't fix them and they will be there forever.)", "author": "gszadovszky", "createdAt": "2020-05-28T13:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg1NjA5Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431856097", "bodyText": "yep, this works; also, makes sense to me - we should not proceed to read a bloom filter header (including seeking its offset), if there is no bloom filter in the file; this can be checked via the bloomFilterOffset variable - it can't be 0 if a bloom filter is present; and if no bloom filter, it can be 0 only.\nI'll remove the TODO comment.", "author": "ggershinsky", "createdAt": "2020-05-28T13:58:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "f3ed0b96643c097236ae66dbcde019da3331a13e", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..059063851 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -1229,14 +1180,11 @@ public class ParquetFileReader implements Closeable {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n-\n-    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?\n+    if (0 == bloomFilterOffset) {\n       return null;\n     }\n \n-    f.seek(bloomFilterOffset);\n-    BloomFilterHeader bloomFilterHeader;\n-\n+    // Prepare to decrypt Bloom filter (for encrypted columns)\n     BlockCipher.Decryptor bloomFilterDecryptor = null;\n     byte[] bloomFilterHeaderAAD = null;\n     byte[] bloomFilterBitsetAAD = null;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NjQ0Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428556447", "body": "I guess, it should be the specific crypto exception instead.", "bodyText": "I guess, it should be the specific crypto exception instead.", "bodyHTML": "<p dir=\"auto\">I guess, it should be the specific crypto exception instead.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T09:53:42Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1095,8 +1272,16 @@ public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException\n       return null;\n     }\n \n-    byte[] bitset = new byte[numBytes];\n-    f.readFully(bitset);\n+    byte[] bitset;\n+    if (null == bloomFilterDecryptor) {\n+      bitset = new byte[numBytes];\n+      f.readFully(bitset);\n+    } else {\n+      bitset = bloomFilterDecryptor.decrypt(f, bloomFilterBitsetAAD);\n+      if (bitset.length != numBytes) {\n+        throw new IOException(\"Wrong length of decrypted bloom filter bitset\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -1279,7 +1230,7 @@ public class ParquetFileReader implements Closeable {\n     } else {\n       bitset = bloomFilterDecryptor.decrypt(f, bloomFilterBitsetAAD);\n       if (bitset.length != numBytes) {\n-        throw new IOException(\"Wrong length of decrypted bloom filter bitset\");\n+        throw new ParquetCryptoRuntimeException(\"Wrong length of decrypted bloom filter bitset\");\n       }\n     }\n     return new BlockSplitBloomFilter(bitset);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDM2OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428560368", "body": "My understanding about this method is that it is invoked inside the `ColumnChunkMetaData` object when retrieving a value that might be encrypted. Why do we need to call it here?", "bodyText": "My understanding about this method is that it is invoked inside the ColumnChunkMetaData object when retrieving a value that might be encrypted. Why do we need to call it here?", "bodyHTML": "<p dir=\"auto\">My understanding about this method is that it is invoked inside the <code>ColumnChunkMetaData</code> object when retrieving a value that might be encrypted. Why do we need to call it here?</p>", "author": "gszadovszky", "createdAt": "2020-05-21T10:02:21Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1114,7 +1299,20 @@ public ColumnIndex readColumnIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));\n+\n+    column.decryptIfNeededed();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk2NDI1Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430964257", "bodyText": "found a better place for it, inside the ColumnChunkMetaData object", "author": "ggershinsky", "createdAt": "2020-05-27T08:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDM2OA=="}], "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -1300,7 +1251,6 @@ public class ParquetFileReader implements Closeable {\n     }\n     f.seek(ref.getOffset());\n \n-    column.decryptIfNeededed();\n     BlockCipher.Decryptor columnIndexDecryptor = null;\n     byte[] columnIndexAAD = null;\n     if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MjA3NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428562074", "body": "Same as above.", "bodyText": "Same as above.", "bodyHTML": "<p dir=\"auto\">Same as above.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T10:06:23Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1131,7 +1329,19 @@ public OffsetIndex readOffsetIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetOffsetIndex(Util.readOffsetIndex(f));\n+\n+    column.decryptIfNeededed();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\nindex 9bebd2bae..e20e44cb6 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java\n", "chunk": "@@ -1330,7 +1280,6 @@ public class ParquetFileReader implements Closeable {\n     }\n     f.seek(ref.getOffset());\n \n-    column.decryptIfNeededed();\n     BlockCipher.Decryptor offsetIndexDecryptor = null;\n     byte[] offsetIndexAAD = null;\n     if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2NDY1Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428564657", "body": "Just like for `MAGIC`:\r\n```suggestion\r\n  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\r\n```", "bodyText": "Just like for MAGIC:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));\n          \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(StandardCharsets.US_ASCII);", "bodyHTML": "<p dir=\"auto\">Just like for <code>MAGIC</code>:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">final</span> <span class=\"pl-k\">byte</span>[] <span class=\"pl-c1\">EFMAGIC</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">EF_MAGIC_STR</span><span class=\"pl-k\">.</span>getBytes(<span class=\"pl-smi x x-first\">Charset</span><span class=\"pl-k x\">.</span><span class=\"x\">forName(</span><span class=\"pl-s\"><span class=\"pl-pds x\">\"</span><span class=\"x\">ASCII</span><span class=\"pl-pds x\">\"</span></span><span class=\"x x-last\">)</span>);</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">final</span> <span class=\"pl-k\">byte</span>[] <span class=\"pl-c1\">EFMAGIC</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">EF_MAGIC_STR</span><span class=\"pl-k\">.</span>getBytes(<span class=\"pl-smi x x-first\">StandardCharsets</span><span class=\"pl-c1\"><span class=\"pl-k x\">.</span><span class=\"x x-last\">US_ASCII</span></span>);</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "gszadovszky", "createdAt": "2020-05-21T10:12:17Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -90,6 +102,8 @@\n   public static final String PARQUET_METADATA_FILE = \"_metadata\";\n   public static final String MAGIC_STR = \"PAR1\";\n   public static final byte[] MAGIC = MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\n+  public static final String EF_MAGIC_STR = \"PARE\";\n+  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\nindex 50f2c516c..9a80257b2 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n", "chunk": "@@ -103,7 +103,7 @@ public class ParquetFileWriter {\n   public static final String MAGIC_STR = \"PAR1\";\n   public static final byte[] MAGIC = MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\n   public static final String EF_MAGIC_STR = \"PARE\";\n-  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));\n+  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\n   public static final String PARQUET_COMMON_METADATA_FILE = \"_common_metadata\";\n   public static final int CURRENT_VERSION = 1;\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYwNzE2Mg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428607162", "body": "`blockSize` is a bit misleading. It is going to be used as ordinal so what about `rowGroupOrdinal` or similar?", "bodyText": "blockSize is a bit misleading. It is going to be used as ordinal so what about rowGroupOrdinal or similar?", "bodyHTML": "<p dir=\"auto\"><code>blockSize</code> is a bit misleading. It is going to be used as ordinal so what about <code>rowGroupOrdinal</code> or similar?</p>", "author": "gszadovszky", "createdAt": "2020-05-21T11:55:06Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -772,6 +860,11 @@ public void endBlock() throws IOException {\n     state = state.endBlock();\n     LOG.debug(\"{}: end block\", out.getPos());\n     currentBlock.setRowCount(currentRecordCount);\n+    int blockSize = blocks.size();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\nindex 50f2c516c..9a80257b2 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n", "chunk": "@@ -861,10 +861,7 @@ public class ParquetFileWriter {\n     LOG.debug(\"{}: end block\", out.getPos());\n     currentBlock.setRowCount(currentRecordCount);\n     int blockSize = blocks.size();\n-    if (fileEncryptor != null && blockSize > Short.MAX_VALUE) {\n-      throw new IOException(\"Number of row groups exceeds short max. Can't set ordinal\");\n-    }\n-    currentBlock.setOrdinal((short) blockSize);\n+    currentBlock.setOrdinal(blockSize);\n     blocks.add(currentBlock);\n     columnIndexes.add(currentColumnIndexes);\n     offsetIndexes.add(currentOffsetIndexes);\n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\nindex 9a80257b2..a1017ca23 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n", "chunk": "@@ -860,8 +860,7 @@ public class ParquetFileWriter {\n     state = state.endBlock();\n     LOG.debug(\"{}: end block\", out.getPos());\n     currentBlock.setRowCount(currentRecordCount);\n-    int blockSize = blocks.size();\n-    currentBlock.setOrdinal(blockSize);\n+    currentBlock.setOrdinal(blocks.size());\n     blocks.add(currentBlock);\n     columnIndexes.add(currentColumnIndexes);\n     offsetIndexes.add(currentOffsetIndexes);\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyMTg3Ng==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428621876", "body": "I don't know why we think that the footer length is an important information but this is the only case where we do not log it. We might want to add it here as well.", "bodyText": "I don't know why we think that the footer length is an important information but this is the only case where we do not log it. We might want to add it here as well.", "bodyHTML": "<p dir=\"auto\">I don't know why we think that the footer length is an important information but this is the only case where we do not log it. We might want to add it here as well.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T12:29:58Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -1035,20 +1154,89 @@ private static void serializeBloomFilters(\n \n         long offset = out.getPos();\n         column.setBloomFilterOffset(offset);\n-        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out);\n-        bloomFilter.writeTo(out);\n+        \n+        BlockCipher.Encryptor bloomFilterEncryptor = null;\n+        byte[] bloomFilterHeaderAAD = null;\n+        byte[] bloomFilterBitsetAAD = null;\n+        if (null != fileEncryptor) {\n+          InternalColumnEncryptionSetup columnEncryptionSetup = fileEncryptor.getColumnSetup(column.getPath(), false, (short) cIndex);\n+          if (columnEncryptionSetup.isEncrypted()) {\n+            bloomFilterEncryptor = columnEncryptionSetup.getMetaDataEncryptor();\n+            short columnOrdinal = columnEncryptionSetup.getOrdinal();\n+            bloomFilterHeaderAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterHeader, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+            bloomFilterBitsetAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterBitset, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+          }\n+        }\n+        \n+        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out, \n+            bloomFilterEncryptor, bloomFilterHeaderAAD);\n+        \n+        ByteArrayOutputStream tempOutStream = new ByteArrayOutputStream();\n+        bloomFilter.writeTo(tempOutStream);\n+        byte[] serializedBitset = tempOutStream.toByteArray();\n+        if (null != bloomFilterEncryptor) {\n+          serializedBitset = bloomFilterEncryptor.encrypt(serializedBitset, bloomFilterBitsetAAD);\n+        }\n+        out.write(serializedBitset);\n       }\n     }\n   }\n-\n-  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out) throws IOException {\n-    long footerIndex = out.getPos();\n+  \n+  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out,\n+      InternalFileEncryptor fileEncryptor) throws IOException {\n+    \n     ParquetMetadataConverter metadataConverter = new ParquetMetadataConverter();\n-    org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n-    writeFileMetaData(parquetMetadata, out);\n-    LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n-    BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n-    out.write(MAGIC);\n+    \n+    // Unencrypted file\n+    if (null == fileEncryptor) {\n+      long footerIndex = out.getPos();\n+      org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n+      writeFileMetaData(parquetMetadata, out);\n+      LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n+      BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n+      out.write(MAGIC);\n+      return;\n+    }\n+    \n+    org.apache.parquet.format.FileMetaData parquetMetadata =\n+        metadataConverter.toParquetMetadata(CURRENT_VERSION, footer, fileEncryptor);\n+    \n+    // Encrypted file with plaintext footer \n+    if (!fileEncryptor.isFooterEncrypted()) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyNjMyOQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428626329", "body": "I think, `createEncryptionProperties` or similar would be a better naming.", "bodyText": "I think, createEncryptionProperties or similar would be a better naming.", "bodyHTML": "<p dir=\"auto\">I think, <code>createEncryptionProperties</code> or similar would be a better naming.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T12:39:42Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "diffHunk": "@@ -539,4 +544,13 @@ public OutputCommitter getOutputCommitter(TaskAttemptContext context)\n   public synchronized static MemoryManager getMemoryManager() {\n     return memoryManager;\n   }\n+  \n+  private FileEncryptionProperties getEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath, ", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java\nindex ce81242b9..4eb040844 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java\n", "chunk": "@@ -545,8 +545,8 @@ public class ParquetOutputFormat<T> extends FileOutputFormat<Void, T> {\n     return memoryManager;\n   }\n   \n-  private FileEncryptionProperties getEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath, \n-      WriteContext fileWriteContext) throws IOException {\n+  private static FileEncryptionProperties createEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath, \n+      WriteContext fileWriteContext) {\n     EncryptionPropertiesFactory cryptoFactory = EncryptionPropertiesFactory.loadFactory(fileHadoopConfig);\n     if (null == cryptoFactory) {\n       return null;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyODc5Mg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428628792", "body": "It is not clear to me why we need to suppress this exception.", "bodyText": "It is not clear to me why we need to suppress this exception.", "bodyHTML": "<p dir=\"auto\">It is not clear to me why we need to suppress this exception.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T12:45:12Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java", "diffHunk": "@@ -188,7 +189,11 @@ private void checkDeltaByteArrayProblem(FileMetaData meta, Configuration conf, B\n       // this is okay if not using DELTA_BYTE_ARRAY with the bug\n       Set<Encoding> encodings = new HashSet<Encoding>();\n       for (ColumnChunkMetaData column : block.getColumns()) {\n-        encodings.addAll(column.getEncodings());\n+        try {\n+          encodings.addAll(column.getEncodings());\n+        } catch (KeyAccessDeniedException e) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk2MjYxOQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430962619", "bodyText": "removed the suppression", "author": "ggershinsky", "createdAt": "2020-05-27T08:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyODc5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java\nindex 38b890397..46534107a 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java\n", "chunk": "@@ -189,11 +188,7 @@ public class ParquetRecordReader<T> extends RecordReader<Void, T> {\n       // this is okay if not using DELTA_BYTE_ARRAY with the bug\n       Set<Encoding> encodings = new HashSet<Encoding>();\n       for (ColumnChunkMetaData column : block.getColumns()) {\n-        try {\n-          encodings.addAll(column.getEncodings());\n-        } catch (KeyAccessDeniedException e) {\n-          LOG.warn(\"checkDeltaByteArrayProblem: Bypass encrypted column {} because key unavailable\", column);\n-        }\n+        encodings.addAll(column.getEncodings());\n       }\n       for (Encoding encoding : encodings) {\n         if (CorruptDeltaByteArrays.requiresSequentialReads(meta.getCreatedBy(), encoding)) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyOTc0Nw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428629747", "body": "Similarly to another one of my comments. Instead of adding more deprecated methods/constructors, try to use the non-deprecated once in the code. If not used internally, we should not introduce these.", "bodyText": "Similarly to another one of my comments. Instead of adding more deprecated methods/constructors, try to use the non-deprecated once in the code. If not used internally, we should not introduce these.", "bodyHTML": "<p dir=\"auto\">Similarly to another one of my comments. Instead of adding more deprecated methods/constructors, try to use the non-deprecated once in the code. If not used internally, we should not introduce these.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T12:47:21Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -194,6 +195,23 @@ public ParquetWriter(\n         enableDictionary, validating, writerVersion, conf);\n   }\n \n+  @Deprecated\n+  public ParquetWriter(", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\nindex e8595b8a6..ecc12de07 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n", "chunk": "@@ -194,23 +194,6 @@ public class ParquetWriter<T> implements Closeable {\n         compressionCodecName, blockSize, pageSize, dictionaryPageSize,\n         enableDictionary, validating, writerVersion, conf);\n   }\n-\n-  @Deprecated\n-  public ParquetWriter(\n-      Path file,\n-      WriteSupport<T> writeSupport,\n-      CompressionCodecName compressionCodecName,\n-      int blockSize,\n-      int pageSize,\n-      int dictionaryPageSize,\n-      boolean enableDictionary,\n-      boolean validating,\n-      WriterVersion writerVersion,\n-      Configuration conf, FileEncryptionProperties encryptionProperties) throws IOException {\n-    this(file, ParquetFileWriter.Mode.CREATE, writeSupport,\n-        compressionCodecName, blockSize, pageSize, dictionaryPageSize,\n-        enableDictionary, validating, writerVersion, conf, encryptionProperties);\n-  }\n   \n   /**\n    * Create a new ParquetWriter.\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNDg4MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428634880", "body": "The column path is already part of `properties` which whole purpose is to save memory. If you need this for `EncryptedColumnChunkMetaData`, add it there instead.", "bodyText": "The column path is already part of properties which whole purpose is to save memory. If you need this for EncryptedColumnChunkMetaData, add it there instead.", "bodyHTML": "<p dir=\"auto\">The column path is already part of <code>properties</code> which whole purpose is to save memory. If you need this for <code>EncryptedColumnChunkMetaData</code>, add it there instead.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T12:58:00Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -34,6 +47,9 @@\n  * Column meta data for a block stored in the file footer and passed in the InputSplit\n  */\n abstract public class ColumnChunkMetaData {\n+  \n+  protected ColumnPath path;", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex 6bc2e06c9..f7066e115 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -49,7 +49,7 @@ import org.apache.yetus.audience.InterfaceAudience.Private;\n abstract public class ColumnChunkMetaData {\n   \n   protected ColumnPath path;\n-  protected short rowGroupOrdinal = -1;\n+  protected int rowGroupOrdinal = -1;\n \n   @Deprecated\n   public static ColumnChunkMetaData get(\n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex f7066e115..479144840 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -47,8 +47,6 @@ import org.apache.yetus.audience.InterfaceAudience.Private;\n  * Column meta data for a block stored in the file footer and passed in the InputSplit\n  */\n abstract public class ColumnChunkMetaData {\n-  \n-  protected ColumnPath path;\n   protected int rowGroupOrdinal = -1;\n \n   @Deprecated\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNjEwOQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428636109", "body": "The design seems to require this method to be only invoked inside this package. So please, make it _package private_.\r\n\r\nYou may have an empty implementation here so you don't need to add the empty implementations in the Int/Long classes.\r\n\r\nAlso, a bit too many \"_ed_\"s:\r\n```suggestion\r\n  abstract public void decryptIfNeeded();\r\n```", "bodyText": "The design seems to require this method to be only invoked inside this package. So please, make it package private.\nYou may have an empty implementation here so you don't need to add the empty implementations in the Int/Long classes.\nAlso, a bit too many \"ed\"s:\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              abstract public void decryptIfNeededed();\n          \n          \n            \n              abstract public void decryptIfNeeded();", "bodyHTML": "<p dir=\"auto\">The design seems to require this method to be only invoked inside this package. So please, make it <em>package private</em>.</p>\n<p dir=\"auto\">You may have an empty implementation here so you don't need to add the empty implementations in the Int/Long classes.</p>\n<p dir=\"auto\">Also, a bit too many \"<em>ed</em>\"s:</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">  <span class=\"pl-k\">abstract</span> <span class=\"pl-k\">public</span> <span class=\"pl-k\">void</span> <span class=\"x x-first x-last\">decryptIfNeededed</span>();</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">  <span class=\"pl-k\">abstract</span> <span class=\"pl-k\">public</span> <span class=\"pl-k\">void</span> <span class=\"x x-first x-last\">decryptIfNeeded</span>();</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "gszadovszky", "createdAt": "2020-05-21T13:00:31Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -241,6 +280,8 @@ public PrimitiveType getPrimitiveType() {\n    * @return the stats for this column\n    */\n   abstract public Statistics getStatistics();\n+  \n+  abstract public void decryptIfNeededed();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dded1bbfe9a588f7e74a6d80065c382151101844", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex 6bc2e06c9..5a0fbdd5b 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -281,7 +281,7 @@ abstract public class ColumnChunkMetaData {\n    */\n   abstract public Statistics getStatistics();\n   \n-  abstract public void decryptIfNeededed();\n+  abstract public void decryptIfNeeded();\n \n   /**\n    * @return the reference to the column index\n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex 5a0fbdd5b..479144840 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -280,14 +285,13 @@ abstract public class ColumnChunkMetaData {\n    * @return the stats for this column\n    */\n   abstract public Statistics getStatistics();\n-  \n-  abstract public void decryptIfNeeded();\n \n   /**\n    * @return the reference to the column index\n    */\n   @Private\n   public IndexReference getColumnIndexReference() {\n+    decryptIfNeeded();\n     return columnIndexReference;\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428642512", "body": "These are accessed only inside the same class so you may keep them `private`.", "bodyText": "These are accessed only inside the same class so you may keep them private.", "bodyHTML": "<p dir=\"auto\">These are accessed only inside the same class so you may keep them <code>private</code>.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:13:20Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkwMjQwMA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430902400", "bodyText": "these fields are accessed by the EncryptedColumnChunkMetaData class, that sets their values after decrypting the column metadata.", "author": "ggershinsky", "createdAt": "2020-05-27T07:11:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc1MTk4MQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431751981", "bodyText": "Yep, but EncryptedColumnChunkMetaData is defined inside the class ColumnChunkMetaData. It makes them access each other's private members. Even the classEncryptedColumnChunkMetaData can be declared private if you don't want to use the type outside of EncryptedColumnChunkMetaData.\nSee Nested Classes for details.", "author": "gszadovszky", "createdAt": "2020-05-28T10:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgwMjc2Mg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431802762", "bodyText": "hmm, when I change encodingStats to private, I get a compilation error\nThe field ColumnChunkMetaData.encodingStats is not visible\nat line 619,\nthis.encodingStats = shadowColumnChunkMetaData.encodingStats;", "author": "ggershinsky", "createdAt": "2020-05-28T12:38:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyMTgwMw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431821803", "bodyText": "OK, I got it now. So, these are not nested classes. There are 4 different classes next to each other in the same java file. This is valid since only one class is public but I've never seen such design in production. I've never realized this design in ColumnChunkMetaData. There is no reason why one would put multiple classes in the same file but not nesting them.\nSo, without refactoring this structure I'll accept encodingStats to not being private. But, we can narrow the visibility by using package private (no modifier).", "author": "gszadovszky", "createdAt": "2020-05-28T13:10:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg0ODQ2OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431848468", "bodyText": "Sure, will change this.", "author": "ggershinsky", "createdAt": "2020-05-28T13:48:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}], "type": "inlineReview", "revised_code": {"commit": "f3ed0b96643c097236ae66dbcde019da3331a13e", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex 6bc2e06c9..e816b2756 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -200,10 +202,10 @@ abstract public class ColumnChunkMetaData {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  protected EncodingStats encodingStats;\n+  EncodingStats encodingStats;\n \n   // we save 3 references by storing together the column properties that have few distinct values\n-  protected ColumnChunkProperties properties;\n+  ColumnChunkProperties properties;\n \n   private IndexReference columnIndexReference;\n   private IndexReference offsetIndexReference;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0OTAzMg==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428649032", "body": "I would expect some comments here why we need this object (to decrypt this metadata lazily instead of simply decrypt it at reading).", "bodyText": "I would expect some comments here why we need this object (to decrypt this metadata lazily instead of simply decrypt it at reading).", "bodyHTML": "<p dir=\"auto\">I would expect some comments here why we need this object (to decrypt this metadata lazily instead of simply decrypt it at reading).</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:25:37Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -141,11 +159,28 @@ public static ColumnChunkMetaData get(\n           totalUncompressedSize);\n     }\n   }\n+  \n+  public static ColumnChunkMetaData getWithEncryptedMetadata(ParquetMetadataConverter parquetMetadataConverter, ColumnPath path, ", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex 6bc2e06c9..f7066e115 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -162,17 +162,17 @@ abstract public class ColumnChunkMetaData {\n   \n   public static ColumnChunkMetaData getWithEncryptedMetadata(ParquetMetadataConverter parquetMetadataConverter, ColumnPath path, \n       PrimitiveType type, byte[] encryptedMetadata, byte[] columnKeyMetadata,\n-      InternalFileDecryptor fileDecryptor, short rowGroupOrdinal, short columnOrdinal, \n+      InternalFileDecryptor fileDecryptor, int rowGroupOrdinal, int columnOrdinal, \n       String createdBy) {\n     return new EncryptedColumnChunkMetaData(parquetMetadataConverter, path, type, encryptedMetadata, columnKeyMetadata,\n         fileDecryptor, rowGroupOrdinal, columnOrdinal, createdBy);\n   }\n \n-  public void setRowGroupOrdinal (short rowGroupOrdinal) {\n+  public void setRowGroupOrdinal (int rowGroupOrdinal) {\n     this.rowGroupOrdinal = rowGroupOrdinal;\n   }\n \n-  public short getRowGroupOrdinal() {\n+  public int getRowGroupOrdinal() {\n     return rowGroupOrdinal;\n   }\n \n", "next_change": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\nindex f7066e115..479144840 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n", "chunk": "@@ -180,7 +182,7 @@ abstract public class ColumnChunkMetaData {\n    * @return the offset of the first byte in the chunk\n    */\n   public long getStartingPos() {\n-    decryptIfNeededed();\n+    decryptIfNeeded();\n     long dictionaryPageOffset = getDictionaryPageOffset();\n     long firstDataPageOffset = getFirstDataPageOffset();\n     if (dictionaryPageOffset > 0 && dictionaryPageOffset < firstDataPageOffset) {\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428657633", "body": "I guess, this test and the one for column indexes were created by copy-pasting the original tests and adding the encryption. Since we already use parameterized testing in both I would suggest keeping the original tests and adding another dimension for _plain_/_encrypted_ or similar. This way we would have 4 runs for each tests: (V1 with plain), (V1 with encryption), (V2 with plain) and (V2 with encryption).", "bodyText": "I guess, this test and the one for column indexes were created by copy-pasting the original tests and adding the encryption. Since we already use parameterized testing in both I would suggest keeping the original tests and adding another dimension for plain/encrypted or similar. This way we would have 4 runs for each tests: (V1 with plain), (V1 with encryption), (V2 with plain) and (V2 with encryption).", "bodyHTML": "<p dir=\"auto\">I guess, this test and the one for column indexes were created by copy-pasting the original tests and adding the encryption. Since we already use parameterized testing in both I would suggest keeping the original tests and adding another dimension for <em>plain</em>/<em>encrypted</em> or similar. This way we would have 4 runs for each tests: (V1 with plain), (V1 with encryption), (V2 with plain) and (V2 with encryption).</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:40:40Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkwMTIzNw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430901237", "bodyText": "this will be a part of #782 (it will also remove the TestBloomEncryption.java and TestColumnIndexEncryption.java files).", "author": "ggershinsky", "createdAt": "2020-05-27T07:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM5Mzg1OA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r432393858", "bodyText": "Thank you, @gszadovszky , for the suggestion. We've added it to #782 .", "author": "andersonm-1", "createdAt": "2020-05-29T10:22:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1OTI2NA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428659264", "body": "I would suggest creating a temporary dir in temporary space instead of having a relative path depending on the current directory. This way it is a bit error prone.\r\n(There are a couple of ways to create and cleanup temporary directories in junit.)", "bodyText": "I would suggest creating a temporary dir in temporary space instead of having a relative path depending on the current directory. This way it is a bit error prone.\n(There are a couple of ways to create and cleanup temporary directories in junit.)", "bodyHTML": "<p dir=\"auto\">I would suggest creating a temporary dir in temporary space instead of having a relative path depending on the current directory. This way it is a bit error prone.<br>\n(There are a couple of ways to create and cleanup temporary directories in junit.)</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:43:20Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\ndeleted file mode 100644\nindex e636bfb4f..000000000\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.hadoop;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n-import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n-import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n-\n-import java.nio.charset.StandardCharsets;\n-import java.util.HashMap;\n-import java.util.Random;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.Path;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-import org.apache.parquet.crypto.ColumnEncryptionProperties;\n-import org.apache.parquet.crypto.FileDecryptionProperties;\n-import org.apache.parquet.crypto.FileEncryptionProperties;\n-import org.apache.parquet.crypto.ParquetCipher;\n-import org.apache.parquet.crypto.StringKeyIdRetriever;\n-import org.apache.parquet.example.data.Group;\n-import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n-\n-\n-import org.apache.parquet.hadoop.example.GroupReadSupport;\n-import org.apache.parquet.hadoop.example.GroupWriteSupport;\n-import org.apache.parquet.hadoop.metadata.ColumnPath;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.MessageType;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestEncryption {\n-\n-  @Test\n-  public void test() throws Exception {\n-    Configuration conf = new Configuration();\n-    Path root = new Path(\"target/tests/TestEncryption/\");\n-    enforceEmptyDir(conf, root);\n-\n-    Random random = new Random();\n-    int numberOfEncryptionModes = 5;\n-    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n-    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n-\n-    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n-    encryptionPropertiesList[0] = null;\n-    decryptionPropertiesList[0] = null;\n-\n-    // #1 Basic encryption setup\n-    byte[] encryptionKey = new byte[16];\n-    random.nextBytes(encryptionKey);\n-    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n-    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n-    encryptionPropertiesList[1] = encryptionProperties;\n-    decryptionPropertiesList[1] = decryptionProperties;\n-\n-    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n-    byte[] footerKey = new byte[16];\n-    random.nextBytes(footerKey);\n-    byte[] columnKey0 = new byte[16];\n-    random.nextBytes(columnKey0);\n-    byte[] columnKey1 = new byte[16];\n-    random.nextBytes(columnKey1);\n-    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n-    keyRetriever.putKey(\"fk\", footerKey);\n-    keyRetriever.putKey(\"ck0\", columnKey0);\n-    keyRetriever.putKey(\"ck1\", columnKey1);\n-    decryptionProperties = FileDecryptionProperties.builder()\n-        .withKeyRetriever(keyRetriever)\n-        .build();\n-    encryptionPropertiesList[2] = encryptionProperties;\n-    decryptionPropertiesList[2] = decryptionProperties;\n-\n-    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[3] = encryptionProperties;\n-    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n-\n-    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withPlaintextFooter()\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[4] = encryptionProperties;\n-    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n-\n-\n-    MessageType schema = parseMessageType(\n-        \"message test { \"\n-            + \"required binary binary_field; \"\n-            + \"required int32 int32_field; \"\n-            + \"required int64 int64_field; \"\n-            + \"required boolean boolean_field; \"\n-            + \"required float float_field; \"\n-            + \"required double double_field; \"\n-            + \"required fixed_len_byte_array(3) flba_field; \"\n-            + \"required int96 int96_field; \"\n-            + \"} \");\n-    GroupWriteSupport.setSchema(schema, conf);\n-    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n-\n-    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n-      System.out.println(\"MODE: \"+encryptionMode);\n-      \n-      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n-      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n-          file,\n-          new GroupWriteSupport(),\n-          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n-          encryptionPropertiesList[encryptionMode]);\n-      for (int i = 0; i < 1000; i++) {\n-        writer.write(\n-            f.newGroup()\n-            .append(\"binary_field\", \"test\" + i)\n-            .append(\"int32_field\", 32)\n-            .append(\"int64_field\", 64l)\n-            .append(\"boolean_field\", true)\n-            .append(\"float_field\", 1.0f)\n-            .append(\"double_field\", 2.0d)\n-            .append(\"flba_field\", \"foo\")\n-            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n-      }\n-      writer.close();\n-\n-      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n-      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n-          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n-      for (int i = 0; i < 1000; i++) {\n-        Group group = null;\n-        group= reader.read();\n-        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n-        assertEquals(32, group.getInteger(\"int32_field\", 0));\n-        assertEquals(64l, group.getLong(\"int64_field\", 0));\n-        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n-        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n-        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n-        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n-        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n-            group.getInt96(\"int96_field\",0));\n-      }\n-      reader.close();\n-    }\n-    enforceEmptyDir(conf, root);\n-  }\n-\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-}\n-\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MDg2Mw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428660863", "body": "Consider using try-with-resources. That construct would be less error prone (e.g. closing the writer/reader in case of an exception occurs).", "bodyText": "Consider using try-with-resources. That construct would be less error prone (e.g. closing the writer/reader in case of an exception occurs).", "bodyHTML": "<p dir=\"auto\">Consider using try-with-resources. That construct would be less error prone (e.g. closing the writer/reader in case of an exception occurs).</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:46:07Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\ndeleted file mode 100644\nindex e636bfb4f..000000000\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.hadoop;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n-import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n-import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n-\n-import java.nio.charset.StandardCharsets;\n-import java.util.HashMap;\n-import java.util.Random;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.Path;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-import org.apache.parquet.crypto.ColumnEncryptionProperties;\n-import org.apache.parquet.crypto.FileDecryptionProperties;\n-import org.apache.parquet.crypto.FileEncryptionProperties;\n-import org.apache.parquet.crypto.ParquetCipher;\n-import org.apache.parquet.crypto.StringKeyIdRetriever;\n-import org.apache.parquet.example.data.Group;\n-import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n-\n-\n-import org.apache.parquet.hadoop.example.GroupReadSupport;\n-import org.apache.parquet.hadoop.example.GroupWriteSupport;\n-import org.apache.parquet.hadoop.metadata.ColumnPath;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.MessageType;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestEncryption {\n-\n-  @Test\n-  public void test() throws Exception {\n-    Configuration conf = new Configuration();\n-    Path root = new Path(\"target/tests/TestEncryption/\");\n-    enforceEmptyDir(conf, root);\n-\n-    Random random = new Random();\n-    int numberOfEncryptionModes = 5;\n-    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n-    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n-\n-    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n-    encryptionPropertiesList[0] = null;\n-    decryptionPropertiesList[0] = null;\n-\n-    // #1 Basic encryption setup\n-    byte[] encryptionKey = new byte[16];\n-    random.nextBytes(encryptionKey);\n-    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n-    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n-    encryptionPropertiesList[1] = encryptionProperties;\n-    decryptionPropertiesList[1] = decryptionProperties;\n-\n-    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n-    byte[] footerKey = new byte[16];\n-    random.nextBytes(footerKey);\n-    byte[] columnKey0 = new byte[16];\n-    random.nextBytes(columnKey0);\n-    byte[] columnKey1 = new byte[16];\n-    random.nextBytes(columnKey1);\n-    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n-    keyRetriever.putKey(\"fk\", footerKey);\n-    keyRetriever.putKey(\"ck0\", columnKey0);\n-    keyRetriever.putKey(\"ck1\", columnKey1);\n-    decryptionProperties = FileDecryptionProperties.builder()\n-        .withKeyRetriever(keyRetriever)\n-        .build();\n-    encryptionPropertiesList[2] = encryptionProperties;\n-    decryptionPropertiesList[2] = decryptionProperties;\n-\n-    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[3] = encryptionProperties;\n-    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n-\n-    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withPlaintextFooter()\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[4] = encryptionProperties;\n-    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n-\n-\n-    MessageType schema = parseMessageType(\n-        \"message test { \"\n-            + \"required binary binary_field; \"\n-            + \"required int32 int32_field; \"\n-            + \"required int64 int64_field; \"\n-            + \"required boolean boolean_field; \"\n-            + \"required float float_field; \"\n-            + \"required double double_field; \"\n-            + \"required fixed_len_byte_array(3) flba_field; \"\n-            + \"required int96 int96_field; \"\n-            + \"} \");\n-    GroupWriteSupport.setSchema(schema, conf);\n-    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n-\n-    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n-      System.out.println(\"MODE: \"+encryptionMode);\n-      \n-      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n-      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n-          file,\n-          new GroupWriteSupport(),\n-          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n-          encryptionPropertiesList[encryptionMode]);\n-      for (int i = 0; i < 1000; i++) {\n-        writer.write(\n-            f.newGroup()\n-            .append(\"binary_field\", \"test\" + i)\n-            .append(\"int32_field\", 32)\n-            .append(\"int64_field\", 64l)\n-            .append(\"boolean_field\", true)\n-            .append(\"float_field\", 1.0f)\n-            .append(\"double_field\", 2.0d)\n-            .append(\"flba_field\", \"foo\")\n-            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n-      }\n-      writer.close();\n-\n-      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n-      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n-          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n-      for (int i = 0; i < 1000; i++) {\n-        Group group = null;\n-        group= reader.read();\n-        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n-        assertEquals(32, group.getInteger(\"int32_field\", 0));\n-        assertEquals(64l, group.getLong(\"int64_field\", 0));\n-        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n-        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n-        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n-        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n-        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n-            group.getInt96(\"int96_field\",0));\n-      }\n-      reader.close();\n-    }\n-    enforceEmptyDir(conf, root);\n-  }\n-\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-}\n-\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MjI2MA==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428662260", "body": "Yep, that's one good way I was talking about but you should use `temp` instead of hardcoding the directory at the beginning.", "bodyText": "Yep, that's one good way I was talking about but you should use temp instead of hardcoding the directory at the beginning.", "bodyHTML": "<p dir=\"auto\">Yep, that's one good way I was talking about but you should use <code>temp</code> instead of hardcoding the directory at the beginning.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:48:35Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();\n+\n+      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n+      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n+          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n+      for (int i = 0; i < 1000; i++) {\n+        Group group = null;\n+        group= reader.read();\n+        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n+        assertEquals(32, group.getInteger(\"int32_field\", 0));\n+        assertEquals(64l, group.getLong(\"int64_field\", 0));\n+        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n+        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n+        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n+        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n+        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n+            group.getInt96(\"int96_field\",0));\n+      }\n+      reader.close();\n+    }\n+    enforceEmptyDir(conf, root);\n+  }\n+\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\ndeleted file mode 100644\nindex e636bfb4f..000000000\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.hadoop;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n-import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n-import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n-\n-import java.nio.charset.StandardCharsets;\n-import java.util.HashMap;\n-import java.util.Random;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.Path;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-import org.apache.parquet.crypto.ColumnEncryptionProperties;\n-import org.apache.parquet.crypto.FileDecryptionProperties;\n-import org.apache.parquet.crypto.FileEncryptionProperties;\n-import org.apache.parquet.crypto.ParquetCipher;\n-import org.apache.parquet.crypto.StringKeyIdRetriever;\n-import org.apache.parquet.example.data.Group;\n-import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n-\n-\n-import org.apache.parquet.hadoop.example.GroupReadSupport;\n-import org.apache.parquet.hadoop.example.GroupWriteSupport;\n-import org.apache.parquet.hadoop.metadata.ColumnPath;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.MessageType;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestEncryption {\n-\n-  @Test\n-  public void test() throws Exception {\n-    Configuration conf = new Configuration();\n-    Path root = new Path(\"target/tests/TestEncryption/\");\n-    enforceEmptyDir(conf, root);\n-\n-    Random random = new Random();\n-    int numberOfEncryptionModes = 5;\n-    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n-    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n-\n-    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n-    encryptionPropertiesList[0] = null;\n-    decryptionPropertiesList[0] = null;\n-\n-    // #1 Basic encryption setup\n-    byte[] encryptionKey = new byte[16];\n-    random.nextBytes(encryptionKey);\n-    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n-    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n-    encryptionPropertiesList[1] = encryptionProperties;\n-    decryptionPropertiesList[1] = decryptionProperties;\n-\n-    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n-    byte[] footerKey = new byte[16];\n-    random.nextBytes(footerKey);\n-    byte[] columnKey0 = new byte[16];\n-    random.nextBytes(columnKey0);\n-    byte[] columnKey1 = new byte[16];\n-    random.nextBytes(columnKey1);\n-    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n-    keyRetriever.putKey(\"fk\", footerKey);\n-    keyRetriever.putKey(\"ck0\", columnKey0);\n-    keyRetriever.putKey(\"ck1\", columnKey1);\n-    decryptionProperties = FileDecryptionProperties.builder()\n-        .withKeyRetriever(keyRetriever)\n-        .build();\n-    encryptionPropertiesList[2] = encryptionProperties;\n-    decryptionPropertiesList[2] = decryptionProperties;\n-\n-    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[3] = encryptionProperties;\n-    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n-\n-    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withPlaintextFooter()\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[4] = encryptionProperties;\n-    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n-\n-\n-    MessageType schema = parseMessageType(\n-        \"message test { \"\n-            + \"required binary binary_field; \"\n-            + \"required int32 int32_field; \"\n-            + \"required int64 int64_field; \"\n-            + \"required boolean boolean_field; \"\n-            + \"required float float_field; \"\n-            + \"required double double_field; \"\n-            + \"required fixed_len_byte_array(3) flba_field; \"\n-            + \"required int96 int96_field; \"\n-            + \"} \");\n-    GroupWriteSupport.setSchema(schema, conf);\n-    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n-\n-    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n-      System.out.println(\"MODE: \"+encryptionMode);\n-      \n-      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n-      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n-          file,\n-          new GroupWriteSupport(),\n-          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n-          encryptionPropertiesList[encryptionMode]);\n-      for (int i = 0; i < 1000; i++) {\n-        writer.write(\n-            f.newGroup()\n-            .append(\"binary_field\", \"test\" + i)\n-            .append(\"int32_field\", 32)\n-            .append(\"int64_field\", 64l)\n-            .append(\"boolean_field\", true)\n-            .append(\"float_field\", 1.0f)\n-            .append(\"double_field\", 2.0d)\n-            .append(\"flba_field\", \"foo\")\n-            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n-      }\n-      writer.close();\n-\n-      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n-      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n-          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n-      for (int i = 0; i < 1000; i++) {\n-        Group group = null;\n-        group= reader.read();\n-        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n-        assertEquals(32, group.getInteger(\"int32_field\", 0));\n-        assertEquals(64l, group.getLong(\"int64_field\", 0));\n-        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n-        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n-        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n-        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n-        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n-            group.getInt96(\"int96_field\",0));\n-      }\n-      reader.close();\n-    }\n-    enforceEmptyDir(conf, root);\n-  }\n-\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-}\n-\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MzgyNw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428663827", "body": "I don't know if make to much sense to write the same data 1000 times. Most of our tests are working by generating random data in memory (e.g. a `List<Group>`) the write it and then test whether we can read back the same data we have in memory.", "bodyText": "I don't know if make to much sense to write the same data 1000 times. Most of our tests are working by generating random data in memory (e.g. a List<Group>) the write it and then test whether we can read back the same data we have in memory.", "bodyHTML": "<p dir=\"auto\">I don't know if make to much sense to write the same data 1000 times. Most of our tests are working by generating random data in memory (e.g. a <code>List&lt;Group&gt;</code>) the write it and then test whether we can read back the same data we have in memory.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T13:51:09Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {", "originalCommit": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODcxMDc5OQ==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428710799", "bodyText": "we're working on a better unitest for encryption, in #782 . it should be possible to drop this one.", "author": "ggershinsky", "createdAt": "2020-05-21T15:01:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MzgyNw=="}], "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\ndeleted file mode 100644\nindex e636bfb4f..000000000\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java\n+++ /dev/null\n", "chunk": "@@ -1,215 +0,0 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.hadoop;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n-import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n-import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n-\n-import java.nio.charset.StandardCharsets;\n-import java.util.HashMap;\n-import java.util.Random;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.Path;\n-import org.junit.Rule;\n-import org.junit.Test;\n-\n-import org.apache.parquet.crypto.ColumnEncryptionProperties;\n-import org.apache.parquet.crypto.FileDecryptionProperties;\n-import org.apache.parquet.crypto.FileEncryptionProperties;\n-import org.apache.parquet.crypto.ParquetCipher;\n-import org.apache.parquet.crypto.StringKeyIdRetriever;\n-import org.apache.parquet.example.data.Group;\n-import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n-\n-\n-import org.apache.parquet.hadoop.example.GroupReadSupport;\n-import org.apache.parquet.hadoop.example.GroupWriteSupport;\n-import org.apache.parquet.hadoop.metadata.ColumnPath;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.MessageType;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestEncryption {\n-\n-  @Test\n-  public void test() throws Exception {\n-    Configuration conf = new Configuration();\n-    Path root = new Path(\"target/tests/TestEncryption/\");\n-    enforceEmptyDir(conf, root);\n-\n-    Random random = new Random();\n-    int numberOfEncryptionModes = 5;\n-    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n-    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n-\n-    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n-    encryptionPropertiesList[0] = null;\n-    decryptionPropertiesList[0] = null;\n-\n-    // #1 Basic encryption setup\n-    byte[] encryptionKey = new byte[16];\n-    random.nextBytes(encryptionKey);\n-    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n-    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n-    encryptionPropertiesList[1] = encryptionProperties;\n-    decryptionPropertiesList[1] = decryptionProperties;\n-\n-    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n-    byte[] footerKey = new byte[16];\n-    random.nextBytes(footerKey);\n-    byte[] columnKey0 = new byte[16];\n-    random.nextBytes(columnKey0);\n-    byte[] columnKey1 = new byte[16];\n-    random.nextBytes(columnKey1);\n-    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n-    keyRetriever.putKey(\"fk\", footerKey);\n-    keyRetriever.putKey(\"ck0\", columnKey0);\n-    keyRetriever.putKey(\"ck1\", columnKey1);\n-    decryptionProperties = FileDecryptionProperties.builder()\n-        .withKeyRetriever(keyRetriever)\n-        .build();\n-    encryptionPropertiesList[2] = encryptionProperties;\n-    decryptionPropertiesList[2] = decryptionProperties;\n-\n-    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n-        .withFooterKeyID(\"fk\")\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[3] = encryptionProperties;\n-    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n-\n-    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n-    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n-        .withKey(columnKey0)\n-        .withKeyID(\"ck0\")\n-        .build();\n-    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n-        .withKey(columnKey1)\n-        .withKeyID(\"ck1\")\n-        .build();\n-    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n-    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n-    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n-    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n-        .withFooterKeyID(\"fk\")\n-        .withPlaintextFooter()\n-        .withAADPrefix(AADPrefix)\n-        .withEncryptedColumns(columnPropertiesMap)\n-        .build();\n-    encryptionPropertiesList[4] = encryptionProperties;\n-    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n-\n-\n-    MessageType schema = parseMessageType(\n-        \"message test { \"\n-            + \"required binary binary_field; \"\n-            + \"required int32 int32_field; \"\n-            + \"required int64 int64_field; \"\n-            + \"required boolean boolean_field; \"\n-            + \"required float float_field; \"\n-            + \"required double double_field; \"\n-            + \"required fixed_len_byte_array(3) flba_field; \"\n-            + \"required int96 int96_field; \"\n-            + \"} \");\n-    GroupWriteSupport.setSchema(schema, conf);\n-    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n-\n-    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n-      System.out.println(\"MODE: \"+encryptionMode);\n-      \n-      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n-      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n-          file,\n-          new GroupWriteSupport(),\n-          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n-          encryptionPropertiesList[encryptionMode]);\n-      for (int i = 0; i < 1000; i++) {\n-        writer.write(\n-            f.newGroup()\n-            .append(\"binary_field\", \"test\" + i)\n-            .append(\"int32_field\", 32)\n-            .append(\"int64_field\", 64l)\n-            .append(\"boolean_field\", true)\n-            .append(\"float_field\", 1.0f)\n-            .append(\"double_field\", 2.0d)\n-            .append(\"flba_field\", \"foo\")\n-            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n-      }\n-      writer.close();\n-\n-      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n-      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n-          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n-      for (int i = 0; i < 1000; i++) {\n-        Group group = null;\n-        group= reader.read();\n-        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n-        assertEquals(32, group.getInteger(\"int32_field\", 0));\n-        assertEquals(64l, group.getLong(\"int64_field\", 0));\n-        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n-        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n-        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n-        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n-        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n-            group.getInt96(\"int96_field\",0));\n-      }\n-      reader.close();\n-    }\n-    enforceEmptyDir(conf, root);\n-  }\n-\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-}\n-\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3NDEwMw==", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428674103", "body": "I think, writing the actual value instead of referencing a java constant is more informative.", "bodyText": "I think, writing the actual value instead of referencing a java constant is more informative.", "bodyHTML": "<p dir=\"auto\">I think, writing the actual value instead of referencing a java constant is more informative.</p>", "author": "gszadovszky", "createdAt": "2020-05-21T14:07:55Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -78,11 +78,22 @@\n     if (rowGroupOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong row group ordinal: \" + rowGroupOrdinal);\n     }\n-    byte[] rowGroupOrdinalBytes = shortToBytesLE(rowGroupOrdinal);\n+    short shortRGOrdinal = (short) rowGroupOrdinal;\n+    if (shortRGOrdinal != rowGroupOrdinal) {\n+      throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n+          + \"more than Short.MAX_VALUE row groups: \" + rowGroupOrdinal);", "originalCommit": "c9761c39a774caac1bde5875b41e3368e745c0b4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\nindex 4d3328090..6b9f24c45 100755\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java\n", "chunk": "@@ -81,7 +81,7 @@ public class AesCipher {\n     short shortRGOrdinal = (short) rowGroupOrdinal;\n     if (shortRGOrdinal != rowGroupOrdinal) {\n       throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n-          + \"more than Short.MAX_VALUE row groups: \" + rowGroupOrdinal);\n+          + \"more than \" + Short.MAX_VALUE + \" row groups: \" + rowGroupOrdinal);\n     }\n     byte[] rowGroupOrdinalBytes = shortToBytesLE(shortRGOrdinal);\n     \n", "next_change": null}]}}, {"oid": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "url": "https://github.com/apache/parquet-mr/commit/7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "message": "Update parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n\nCo-authored-by: Gabor Szadovszky <gabor@apache.org>", "committedDate": "2020-05-21T14:53:09Z", "type": "commit"}, {"oid": "dded1bbfe9a588f7e74a6d80065c382151101844", "url": "https://github.com/apache/parquet-mr/commit/dded1bbfe9a588f7e74a6d80065c382151101844", "message": "Update parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n\nCo-authored-by: Gabor Szadovszky <gabor@apache.org>", "committedDate": "2020-05-21T14:56:29Z", "type": "commit"}, {"oid": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "url": "https://github.com/apache/parquet-mr/commit/4a325b70b4deb434c9589edd209b1a0d22a1162a", "message": "address review comments", "committedDate": "2020-05-27T11:37:08Z", "type": "commit"}, {"oid": "f3ed0b96643c097236ae66dbcde019da3331a13e", "url": "https://github.com/apache/parquet-mr/commit/f3ed0b96643c097236ae66dbcde019da3331a13e", "message": "field scope, rm comment", "committedDate": "2020-05-29T05:57:53Z", "type": "commit"}]}