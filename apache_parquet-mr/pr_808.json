{"pr_number": 808, "pr_title": "PARQUET-1396: Example of using EncryptionPropertiesFactory and DecryptionPropertiesFactory", "pr_author": "shangxinli", "pr_createdAt": "2020-07-29T21:20:25Z", "pr_url": "https://github.com/apache/parquet-mr/pull/808", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5MzQ4MA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462893480", "body": "This package name break java naming conventions. It should not contain uppercase. I would suggest using e.g. `org.apache.parquet.crypto.propertiesfactory`", "bodyText": "This package name break java naming conventions. It should not contain uppercase. I would suggest using e.g. org.apache.parquet.crypto.propertiesfactory", "bodyHTML": "<p dir=\"auto\">This package name break java naming conventions. It should not contain uppercase. I would suggest using e.g. <code>org.apache.parquet.crypto.propertiesfactory</code></p>", "author": "gszadovszky", "createdAt": "2020-07-30T10:10:02Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;", "originalCommit": "9adb75a2356147a204d76c82eb39f43e9ee72b58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzcyNTA1Nw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463725057", "bodyText": "Sounds good!", "author": "shangxinli", "createdAt": "2020-07-31T17:00:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5MzQ4MA=="}], "type": "inlineReview", "revised_code": {"commit": "ed946b260730ab2456cbc8c940c528522e02138c", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nsimilarity index 92%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex 7b1779b9e..b21f94ea2 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;\n+package org.apache.parquet.crypto.propertiesfactory;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5OTcxNw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462899717", "body": "Is it necessary to allow setting the `WriteSupport`? The concept of the `ParquetWriter` implementations is to hide all these stuff from the user so it can simply create a `ParquetWriter<Group> writer = ExampleParquetWriter.builder(...).with(...)` without dealing with the logic required for converting a `Group` object to writable primitives. Also, allowing to set a simple `WriteSupport` allows to set one that is not compatible with the `Group` type breaking the whole logic.", "bodyText": "Is it necessary to allow setting the WriteSupport? The concept of the ParquetWriter implementations is to hide all these stuff from the user so it can simply create a ParquetWriter<Group> writer = ExampleParquetWriter.builder(...).with(...) without dealing with the logic required for converting a Group object to writable primitives. Also, allowing to set a simple WriteSupport allows to set one that is not compatible with the Group type breaking the whole logic.", "bodyHTML": "<p dir=\"auto\">Is it necessary to allow setting the <code>WriteSupport</code>? The concept of the <code>ParquetWriter</code> implementations is to hide all these stuff from the user so it can simply create a <code>ParquetWriter&lt;Group&gt; writer = ExampleParquetWriter.builder(...).with(...)</code> without dealing with the logic required for converting a <code>Group</code> object to writable primitives. Also, allowing to set a simple <code>WriteSupport</code> allows to set one that is not compatible with the <code>Group</code> type breaking the whole logic.</p>", "author": "gszadovszky", "createdAt": "2020-07-30T10:22:16Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -104,15 +105,19 @@ public Builder withExtraMetaData(Map<String, String> extraMetaData) {\n       return this;\n     }\n \n+    public Builder withWriteSupport(WriteSupport writeSupport) {\n+      this.writeSupport = writeSupport;\n+      return this;\n+    }\n+", "originalCommit": "9adb75a2356147a204d76c82eb39f43e9ee72b58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc3MjU0Mw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463772543", "bodyText": "If we don't do that, we need to write Builder class to extend ParquetWriter.Builder. It should be a thin wrapper. I think we can do it.", "author": "shangxinli", "createdAt": "2020-07-31T18:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5OTcxNw=="}], "type": "inlineReview", "revised_code": {"commit": "ed946b260730ab2456cbc8c940c528522e02138c", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex 6d502984d..a298e551f 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -105,11 +104,6 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n       return this;\n     }\n \n-    public Builder withWriteSupport(WriteSupport writeSupport) {\n-      this.writeSupport = writeSupport;\n-      return this;\n-    }\n-\n     @Override\n     protected Builder self() {\n       return this;\n", "next_change": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex a298e551f..6d502984d 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -104,6 +105,11 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n       return this;\n     }\n \n+    public Builder withWriteSupport(WriteSupport writeSupport) {\n+      this.writeSupport = writeSupport;\n+      return this;\n+    }\n+\n     @Override\n     protected Builder self() {\n       return this;\n", "next_change": {"commit": "7b68b0b0e901a1d6b6a7676d60df7507cc267380", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex 6d502984d..a298e551f 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -105,11 +104,6 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n       return this;\n     }\n \n-    public Builder withWriteSupport(WriteSupport writeSupport) {\n-      this.writeSupport = writeSupport;\n-      return this;\n-    }\n-\n     @Override\n     protected Builder self() {\n       return this;\n", "next_change": null}, {"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex 6d502984d..a298e551f 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -117,7 +111,7 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n \n     @Override\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n-      return writeSupport == null ? new GroupWriteSupport(type, extraMetaData) : writeSupport;\n+      return new GroupWriteSupport(type, extraMetaData);\n     }\n   }\n }\n", "next_change": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex a298e551f..12a67d301 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -113,5 +113,6 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n+\n   }\n }\n", "next_change": null}]}}]}}, {"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex a298e551f..6d502984d 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -111,7 +117,7 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n \n     @Override\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n-      return new GroupWriteSupport(type, extraMetaData);\n+      return writeSupport == null ? new GroupWriteSupport(type, extraMetaData) : writeSupport;\n     }\n   }\n }\n", "next_change": {"commit": "7b68b0b0e901a1d6b6a7676d60df7507cc267380", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex 6d502984d..a298e551f 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -117,7 +111,7 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n \n     @Override\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n-      return writeSupport == null ? new GroupWriteSupport(type, extraMetaData) : writeSupport;\n+      return new GroupWriteSupport(type, extraMetaData);\n     }\n   }\n }\n", "next_change": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex a298e551f..12a67d301 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -113,5 +113,6 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n+\n   }\n }\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkwOTMyMA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462909320", "body": "You should use the annotation `@Override` for every method that is overriden.", "bodyText": "You should use the annotation @Override for every method that is overriden.", "bodyHTML": "<p dir=\"auto\">You should use the annotation <code>@Override</code> for every method that is overriden.</p>", "author": "gszadovszky", "createdAt": "2020-07-30T10:41:42Z", "path": "parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.schema;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * This class decorates the class 'Type' by adding a Map field 'metadata'.\n+ *\n+ * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n+ * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n+ *\n+ * MessageType oldSchema = ...\n+ * Map metadata = ...\n+ * List newFields = new ArrayList();\n+ * for (Type field = oldSchema.getFields()) {\n+ *     Type newField = new ExtType(field);\n+ *     newField.setMetadata(metadata);\n+ *     newFields.add(newField);\n+ * }\n+ * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n+ *\n+ * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n+ * implementation of PrimitiveType or GroupType.\n+ */\n+public class ExtType<T> extends Type {\n+  private Type type;\n+  private Map<String, T> metadata;\n+\n+  public ExtType(Type type) {\n+    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n+    this.type = type;\n+  }\n+\n+  public ExtType(Type type, String name) {\n+    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n+    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n+  }\n+\n+  public Type withId(int id) {", "originalCommit": "9adb75a2356147a204d76c82eb39f43e9ee72b58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzcxOTQ0Nw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463719447", "bodyText": "Sounds good", "author": "shangxinli", "createdAt": "2020-07-31T16:49:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkwOTMyMA=="}], "type": "inlineReview", "revised_code": {"commit": "ed946b260730ab2456cbc8c940c528522e02138c", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\nindex 92c9cdd76..9feb4e520 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\n", "chunk": "@@ -54,88 +54,98 @@ public class ExtType<T> extends Type {\n     this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n   }\n \n+  @Override\n   public Type withId(int id) {\n     return this.type.withId(id);\n   }\n \n+  @Override\n   public boolean isPrimitive() {\n     return this.type.isPrimitive();\n   }\n \n+  @Override\n   public void writeToStringBuilder(StringBuilder sb, String indent) {\n     this.type.writeToStringBuilder(sb, indent);\n   }\n \n+  @Override\n   public void accept(TypeVisitor visitor) {\n     this.type.accept(visitor);\n   }\n \n   /** @deprecated */\n   @Deprecated\n+  @Override\n   protected int typeHashCode() {\n     return this.type.hashCode();\n   }\n \n   /** @deprecated */\n   @Deprecated\n+  @Override\n   protected boolean typeEquals(Type other) {\n     return this.type.typeEquals(other);\n   }\n \n+  @Override\n   protected boolean equals(Type other) {\n     return this.type.equals(other);\n   }\n \n+  @Override\n   public int getMaxRepetitionLevel(String[] path, int i) {\n     return this.type.getMaxRepetitionLevel(path, i);\n   }\n \n+  @Override\n   public int getMaxDefinitionLevel(String[] path, int i) {\n     return this.type.getMaxDefinitionLevel(path, i);\n   }\n \n+  @Override\n   public Type getType(String[] path, int i) {\n     return this.type.getType(path, i);\n   }\n \n+  @Override\n   protected List<String[]> getPaths(int depth) {\n     return this.type.getPaths(depth);\n   }\n \n+  @Override\n   void checkContains(Type subType) {\n     this.type.checkContains(subType);\n   }\n \n+  @Override\n   public <T> T convert(List<GroupType> path, TypeConverter<T> converter) {\n     return this.type.convert(path, converter);\n   }\n \n+  @Override\n   protected boolean containsPath(String[] path, int depth) {\n     return this.type.containsPath(path, depth);\n   }\n \n+  @Override\n   protected Type union(Type toMerge) {\n     return this.type.union(toMerge);\n   }\n \n+  @Override\n   protected Type union(Type toMerge, boolean strict) {\n     return this.type.union(toMerge, strict);\n   }\n \n+  @Override\n   public PrimitiveType asPrimitiveType() {\n-    if (!this.type.isPrimitive()) {\n-      throw new ClassCastException(this + \" is not primitive\");\n-    } else {\n-      return (PrimitiveType)this.type;\n-    }\n+    return this.type.asPrimitiveType();\n   }\n \n+  @Override\n   public GroupType asGroupType() {\n-    if (this.type.isPrimitive()) {\n-      throw new ClassCastException(this + \" is not a group\");\n-    } else {\n-      return (GroupType)this.type;\n-    }\n+    return this.type.asGroupType();\n   }\n \n   public void setMetadata(Map<String, T> metadata) {\n", "next_change": {"commit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\ndeleted file mode 100644\nindex 9feb4e520..000000000\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\n+++ /dev/null\n", "chunk": "@@ -1,158 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.schema;\n-\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * This class decorates the class 'Type' by adding a Map field 'metadata'.\n- *\n- * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n- * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n- *\n- * MessageType oldSchema = ...\n- * Map metadata = ...\n- * List newFields = new ArrayList();\n- * for (Type field = oldSchema.getFields()) {\n- *     Type newField = new ExtType(field);\n- *     newField.setMetadata(metadata);\n- *     newFields.add(newField);\n- * }\n- * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n- *\n- * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n- * implementation of PrimitiveType or GroupType.\n- */\n-public class ExtType<T> extends Type {\n-  private Type type;\n-  private Map<String, T> metadata;\n-\n-  public ExtType(Type type) {\n-    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n-    this.type = type;\n-  }\n-\n-  public ExtType(Type type, String name) {\n-    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n-    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n-  }\n-\n-  @Override\n-  public Type withId(int id) {\n-    return this.type.withId(id);\n-  }\n-\n-  @Override\n-  public boolean isPrimitive() {\n-    return this.type.isPrimitive();\n-  }\n-\n-  @Override\n-  public void writeToStringBuilder(StringBuilder sb, String indent) {\n-    this.type.writeToStringBuilder(sb, indent);\n-  }\n-\n-  @Override\n-  public void accept(TypeVisitor visitor) {\n-    this.type.accept(visitor);\n-  }\n-\n-  /** @deprecated */\n-  @Deprecated\n-  @Override\n-  protected int typeHashCode() {\n-    return this.type.hashCode();\n-  }\n-\n-  /** @deprecated */\n-  @Deprecated\n-  @Override\n-  protected boolean typeEquals(Type other) {\n-    return this.type.typeEquals(other);\n-  }\n-\n-  @Override\n-  protected boolean equals(Type other) {\n-    return this.type.equals(other);\n-  }\n-\n-  @Override\n-  public int getMaxRepetitionLevel(String[] path, int i) {\n-    return this.type.getMaxRepetitionLevel(path, i);\n-  }\n-\n-  @Override\n-  public int getMaxDefinitionLevel(String[] path, int i) {\n-    return this.type.getMaxDefinitionLevel(path, i);\n-  }\n-\n-  @Override\n-  public Type getType(String[] path, int i) {\n-    return this.type.getType(path, i);\n-  }\n-\n-  @Override\n-  protected List<String[]> getPaths(int depth) {\n-    return this.type.getPaths(depth);\n-  }\n-\n-  @Override\n-  void checkContains(Type subType) {\n-    this.type.checkContains(subType);\n-  }\n-\n-  @Override\n-  public <T> T convert(List<GroupType> path, TypeConverter<T> converter) {\n-    return this.type.convert(path, converter);\n-  }\n-\n-  @Override\n-  protected boolean containsPath(String[] path, int depth) {\n-    return this.type.containsPath(path, depth);\n-  }\n-\n-  @Override\n-  protected Type union(Type toMerge) {\n-    return this.type.union(toMerge);\n-  }\n-\n-  @Override\n-  protected Type union(Type toMerge, boolean strict) {\n-    return this.type.union(toMerge, strict);\n-  }\n-\n-  @Override\n-  public PrimitiveType asPrimitiveType() {\n-    return this.type.asPrimitiveType();\n-  }\n-\n-  @Override\n-  public GroupType asGroupType() {\n-    return this.type.asGroupType();\n-  }\n-\n-  public void setMetadata(Map<String, T> metadata) {\n-    this.metadata = metadata;\n-  }\n-\n-  public Map<String, T> getMetadata() {\n-    return this.metadata;\n-  }\n-}\n", "next_change": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\nnew file mode 100644\nindex 000000000..92c9cdd76\n--- /dev/null\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\n", "chunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.schema;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * This class decorates the class 'Type' by adding a Map field 'metadata'.\n+ *\n+ * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n+ * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n+ *\n+ * MessageType oldSchema = ...\n+ * Map metadata = ...\n+ * List newFields = new ArrayList();\n+ * for (Type field = oldSchema.getFields()) {\n+ *     Type newField = new ExtType(field);\n+ *     newField.setMetadata(metadata);\n+ *     newFields.add(newField);\n+ * }\n+ * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n+ *\n+ * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n+ * implementation of PrimitiveType or GroupType.\n+ */\n+public class ExtType<T> extends Type {\n+  private Type type;\n+  private Map<String, T> metadata;\n+\n+  public ExtType(Type type) {\n+    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n+    this.type = type;\n+  }\n+\n+  public ExtType(Type type, String name) {\n+    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n+    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n+  }\n+\n+  public Type withId(int id) {\n+    return this.type.withId(id);\n+  }\n+\n+  public boolean isPrimitive() {\n+    return this.type.isPrimitive();\n+  }\n+\n+  public void writeToStringBuilder(StringBuilder sb, String indent) {\n+    this.type.writeToStringBuilder(sb, indent);\n+  }\n+\n+  public void accept(TypeVisitor visitor) {\n+    this.type.accept(visitor);\n+  }\n+\n+  /** @deprecated */\n+  @Deprecated\n+  protected int typeHashCode() {\n+    return this.type.hashCode();\n+  }\n+\n+  /** @deprecated */\n+  @Deprecated\n+  protected boolean typeEquals(Type other) {\n+    return this.type.typeEquals(other);\n+  }\n+\n+  protected boolean equals(Type other) {\n+    return this.type.equals(other);\n+  }\n+\n+  public int getMaxRepetitionLevel(String[] path, int i) {\n+    return this.type.getMaxRepetitionLevel(path, i);\n+  }\n+\n+  public int getMaxDefinitionLevel(String[] path, int i) {\n+    return this.type.getMaxDefinitionLevel(path, i);\n+  }\n+\n+  public Type getType(String[] path, int i) {\n+    return this.type.getType(path, i);\n+  }\n+\n+  protected List<String[]> getPaths(int depth) {\n+    return this.type.getPaths(depth);\n+  }\n+\n+  void checkContains(Type subType) {\n+    this.type.checkContains(subType);\n+  }\n+\n+  public <T> T convert(List<GroupType> path, TypeConverter<T> converter) {\n+    return this.type.convert(path, converter);\n+  }\n+\n+  protected boolean containsPath(String[] path, int depth) {\n+    return this.type.containsPath(path, depth);\n+  }\n+\n+  protected Type union(Type toMerge) {\n+    return this.type.union(toMerge);\n+  }\n+\n+  protected Type union(Type toMerge, boolean strict) {\n+    return this.type.union(toMerge, strict);\n+  }\n+\n+  public PrimitiveType asPrimitiveType() {\n+    if (!this.type.isPrimitive()) {\n+      throw new ClassCastException(this + \" is not primitive\");\n+    } else {\n+      return (PrimitiveType)this.type;\n+    }\n+  }\n+\n+  public GroupType asGroupType() {\n+    if (this.type.isPrimitive()) {\n+      throw new ClassCastException(this + \" is not a group\");\n+    } else {\n+      return (GroupType)this.type;\n+    }\n+  }\n+\n+  public void setMetadata(Map<String, T> metadata) {\n+    this.metadata = metadata;\n+  }\n+\n+  public Map<String, T> getMetadata() {\n+    return this.metadata;\n+  }\n+}\n", "next_change": {"commit": "621437f72d46a95747826d6e02bb710d981de7d3", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java b/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\ndeleted file mode 100644\nindex 92c9cdd76..000000000\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java\n+++ /dev/null\n", "chunk": "@@ -1,148 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.schema;\n-\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * This class decorates the class 'Type' by adding a Map field 'metadata'.\n- *\n- * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n- * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n- *\n- * MessageType oldSchema = ...\n- * Map metadata = ...\n- * List newFields = new ArrayList();\n- * for (Type field = oldSchema.getFields()) {\n- *     Type newField = new ExtType(field);\n- *     newField.setMetadata(metadata);\n- *     newFields.add(newField);\n- * }\n- * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n- *\n- * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n- * implementation of PrimitiveType or GroupType.\n- */\n-public class ExtType<T> extends Type {\n-  private Type type;\n-  private Map<String, T> metadata;\n-\n-  public ExtType(Type type) {\n-    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n-    this.type = type;\n-  }\n-\n-  public ExtType(Type type, String name) {\n-    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n-    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n-  }\n-\n-  public Type withId(int id) {\n-    return this.type.withId(id);\n-  }\n-\n-  public boolean isPrimitive() {\n-    return this.type.isPrimitive();\n-  }\n-\n-  public void writeToStringBuilder(StringBuilder sb, String indent) {\n-    this.type.writeToStringBuilder(sb, indent);\n-  }\n-\n-  public void accept(TypeVisitor visitor) {\n-    this.type.accept(visitor);\n-  }\n-\n-  /** @deprecated */\n-  @Deprecated\n-  protected int typeHashCode() {\n-    return this.type.hashCode();\n-  }\n-\n-  /** @deprecated */\n-  @Deprecated\n-  protected boolean typeEquals(Type other) {\n-    return this.type.typeEquals(other);\n-  }\n-\n-  protected boolean equals(Type other) {\n-    return this.type.equals(other);\n-  }\n-\n-  public int getMaxRepetitionLevel(String[] path, int i) {\n-    return this.type.getMaxRepetitionLevel(path, i);\n-  }\n-\n-  public int getMaxDefinitionLevel(String[] path, int i) {\n-    return this.type.getMaxDefinitionLevel(path, i);\n-  }\n-\n-  public Type getType(String[] path, int i) {\n-    return this.type.getType(path, i);\n-  }\n-\n-  protected List<String[]> getPaths(int depth) {\n-    return this.type.getPaths(depth);\n-  }\n-\n-  void checkContains(Type subType) {\n-    this.type.checkContains(subType);\n-  }\n-\n-  public <T> T convert(List<GroupType> path, TypeConverter<T> converter) {\n-    return this.type.convert(path, converter);\n-  }\n-\n-  protected boolean containsPath(String[] path, int depth) {\n-    return this.type.containsPath(path, depth);\n-  }\n-\n-  protected Type union(Type toMerge) {\n-    return this.type.union(toMerge);\n-  }\n-\n-  protected Type union(Type toMerge, boolean strict) {\n-    return this.type.union(toMerge, strict);\n-  }\n-\n-  public PrimitiveType asPrimitiveType() {\n-    if (!this.type.isPrimitive()) {\n-      throw new ClassCastException(this + \" is not primitive\");\n-    } else {\n-      return (PrimitiveType)this.type;\n-    }\n-  }\n-\n-  public GroupType asGroupType() {\n-    if (this.type.isPrimitive()) {\n-      throw new ClassCastException(this + \" is not a group\");\n-    } else {\n-      return (GroupType)this.type;\n-    }\n-  }\n-\n-  public void setMetadata(Map<String, T> metadata) {\n-    this.metadata = metadata;\n-  }\n-\n-  public Map<String, T> getMetadata() {\n-    return this.metadata;\n-  }\n-}\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxMDY5Ng==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462910696", "body": "Using `defaultCharset()` would work just as if you would not set any. We usually set the charset to ensure that the result will always be the same on every environment (independently from the default charset). I would suggest using one of the constants of `StandardCharsets`.", "bodyText": "Using defaultCharset() would work just as if you would not set any. We usually set the charset to ensure that the result will always be the same on every environment (independently from the default charset). I would suggest using one of the constants of StandardCharsets.", "bodyHTML": "<p dir=\"auto\">Using <code>defaultCharset()</code> would work just as if you would not set any. We usually set the charset to ensure that the result will always be the same on every environment (independently from the default charset). I would suggest using one of the constants of <code>StandardCharsets</code>.</p>", "author": "gszadovszky", "createdAt": "2020-07-30T10:44:45Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.ExtType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());", "originalCommit": "9adb75a2356147a204d76c82eb39f43e9ee72b58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc3NDM0Mg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463774342", "bodyText": "Fix it.", "author": "shangxinli", "createdAt": "2020-07-31T18:48:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxMDY5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ed946b260730ab2456cbc8c940c528522e02138c", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\nsimilarity index 97%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\nindex b6d6e4d23..3a1230e17 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\n", "chunk": "@@ -52,10 +52,10 @@ public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactor\n   public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n   private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n     0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n-  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n   private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n     0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n-  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(Charset.defaultCharset());\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n \n   @Override\n   public FileEncryptionProperties getFileEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath,\n", "next_change": null}]}}, {"oid": "ed946b260730ab2456cbc8c940c528522e02138c", "url": "https://github.com/apache/parquet-mr/commit/ed946b260730ab2456cbc8c940c528522e02138c", "message": "Address feedbacks", "committedDate": "2020-08-01T00:15:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY0NDYwNQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465644605", "body": "I would expect method comments describing the purpose and usage of this metadata. (We should mention that this metadata is for the current parquet-mr runtime only and it won't be serialized to the file.)\r\n\r\nI understand this is the easiest way to add this map to the class but I don't really like it. For example by invoking this `setMetadata` the caller will not be informed if it overwrites any values already in. I would more like an approach where the user can set/get the metadata one-by-one.", "bodyText": "I would expect method comments describing the purpose and usage of this metadata. (We should mention that this metadata is for the current parquet-mr runtime only and it won't be serialized to the file.)\nI understand this is the easiest way to add this map to the class but I don't really like it. For example by invoking this setMetadata the caller will not be informed if it overwrites any values already in. I would more like an approach where the user can set/get the metadata one-by-one.", "bodyHTML": "<p dir=\"auto\">I would expect method comments describing the purpose and usage of this metadata. (We should mention that this metadata is for the current parquet-mr runtime only and it won't be serialized to the file.)</p>\n<p dir=\"auto\">I understand this is the easiest way to add this map to the class but I don't really like it. For example by invoking this <code>setMetadata</code> the caller will not be informed if it overwrites any values already in. I would more like an approach where the user can set/get the metadata one-by-one.</p>", "author": "gszadovszky", "createdAt": "2020-08-05T11:01:50Z", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -363,4 +365,11 @@ void checkContains(Type subType) {\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n \n+   public void setMetadata(Map<String, Object> metadata) {", "originalCommit": "b649eb56682451042c44821edc113e8091bcacfe", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 80b6f3add..e22b22df7 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -364,12 +362,4 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-   public void setMetadata(Map<String, Object> metadata) {\n-    this.metadata = metadata;\n-  }\n-\n-   public Map<String, Object> getMetadata() {\n-    return this.metadata;\n-  }\n-}\n+ }\n", "next_change": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex e22b22df7..310227ac2 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,5 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n- }\n+\n+}\n", "next_change": {"commit": "621437f72d46a95747826d6e02bb710d981de7d3", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 310227ac2..80b6f3add 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -363,4 +365,11 @@ abstract public class Type {\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n \n+   public void setMetadata(Map<String, Object> metadata) {\n+    this.metadata = metadata;\n+  }\n+\n+   public Map<String, Object> getMetadata() {\n+    return this.metadata;\n+  }\n }\n", "next_change": {"commit": "aea692ac3abb37bfbd0b79984cfc7a861d428825", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 80b6f3add..e22b22df7 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -364,12 +362,4 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-   public void setMetadata(Map<String, Object> metadata) {\n-    this.metadata = metadata;\n-  }\n-\n-   public Map<String, Object> getMetadata() {\n-    return this.metadata;\n-  }\n-}\n+ }\n", "next_change": {"commit": "306796e0b6eedcd9bfda63b2b498223d72c599bf", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex e22b22df7..9782a56fd 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,4 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n- }\n+}\n", "next_change": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 9782a56fd..310227ac2 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,5 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Mjc2Mw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465652763", "body": "nit: -> cry**p**toMetadatas\r\nI'm not sure about 's'. I think, data doesn't have plural. ", "bodyText": "nit: -> cryptoMetadatas\nI'm not sure about 's'. I think, data doesn't have plural.", "bodyHTML": "<p dir=\"auto\">nit: -&gt; cry<strong>p</strong>toMetadatas<br>\nI'm not sure about 's'. I think, data doesn't have plural.</p>", "author": "gszadovszky", "createdAt": "2020-08-05T11:15:09Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadatas = new HashMap<>();", "originalCommit": "b649eb56682451042c44821edc113e8091bcacfe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ3ODY1NA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r495478654", "bodyText": "resolved", "author": "shangxinli", "createdAt": "2020-09-26T17:37:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Mjc2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex 7907d8a61..a850f9400 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -63,7 +63,7 @@ public class SchemaControlEncryptionTest {\n   // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n   // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n   // store in HMS, or other metastore. \n-  private Map<String, Map<String, Object>> crytoMetadatas = new HashMap<>();\n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n   private Map<String, Object[]> testData = new HashMap<>();\n \n   @Before\n", "next_change": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nsimilarity index 74%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nindex a850f9400..7b1779b9e 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\n", "chunk": "@@ -63,7 +66,7 @@ public class SchemaControlEncryptionTest {\n   // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n   // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n   // store in HMS, or other metastore. \n-  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Map<String, String>> crytoMetadatas = new HashMap<>();\n   private Map<String, Object[]> testData = new HashMap<>();\n \n   @Before\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1NjA0OA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656048", "body": "I suggest using the already existing constant in `PropertiesDrivenCryptoFactory` directly.", "bodyText": "I suggest using the already existing constant in PropertiesDrivenCryptoFactory directly.", "bodyHTML": "<p dir=\"auto\">I suggest using the already existing constant in <code>PropertiesDrivenCryptoFactory</code> directly.</p>", "author": "gszadovszky", "createdAt": "2020-08-05T11:21:00Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";", "originalCommit": "b649eb56682451042c44821edc113e8091bcacfe", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nsimilarity index 95%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nindex 7522ab1f9..b6d6e4d23 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\n", "chunk": "@@ -50,10 +52,10 @@ public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactor\n   public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n   private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n     0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n-  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());\n   private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n     0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n-  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(Charset.defaultCharset());\n \n   @Override\n   public FileEncryptionProperties getFileEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath,\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Njg3Ng==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656876", "body": "Do you mean `\"parquet.encryption.plaintext.footer\"`? Please, use existing constants.", "bodyText": "Do you mean \"parquet.encryption.plaintext.footer\"? Please, use existing constants.", "bodyHTML": "<p dir=\"auto\">Do you mean <code>\"parquet.encryption.plaintext.footer\"</code>? Please, use existing constants.</p>", "author": "gszadovszky", "createdAt": "2020-08-05T11:22:46Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";", "originalCommit": "b649eb56682451042c44821edc113e8091bcacfe", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nsimilarity index 95%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nindex 7522ab1f9..b6d6e4d23 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\n", "chunk": "@@ -50,10 +52,10 @@ public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactor\n   public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n   private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n     0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n-  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());\n   private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n     0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n-  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(Charset.defaultCharset());\n \n   @Override\n   public FileEncryptionProperties getFileEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath,\n", "next_change": null}]}}, {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992", "url": "https://github.com/apache/parquet-mr/commit/94872443620c93f96ba4b7c182c9e0a1eca75992", "message": "Use Configuration to pass the setting", "committedDate": "2020-09-26T20:52:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExMzc1MA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509113750", "body": "Since the target class is already on the classpath I would use the class object directly and maybe the related conf setter method as well.", "bodyText": "Since the target class is already on the classpath I would use the class object directly and maybe the related conf setter method as well.", "bodyHTML": "<p dir=\"auto\">Since the target class is already on the classpath I would use the class object directly and maybe the related conf setter method as well.</p>", "author": "gszadovszky", "createdAt": "2020-10-21T09:05:20Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionWithFooter() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_FOOTER, true);\n+    runTest(conf);\n+  }\n+\n+  private void runTest(Configuration conf ) throws Exception {\n+    conf.set(EncryptionPropertiesFactory.CRYPTO_FACTORY_CLASS_PROPERTY_NAME,\n+      \"org.apache.parquet.crypto.propertiesfactory.SchemaCryptoPropertiesFactory\");", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2MDYxOQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510460619", "bodyText": "changed to SchemaCryptoPropertiesFactory.class.getName()", "author": "shangxinli", "createdAt": "2020-10-22T21:12:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExMzc1MA=="}], "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nsimilarity index 74%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nindex a850f9400..7b1779b9e 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\n", "chunk": "@@ -114,7 +117,7 @@ public class SchemaControlEncryptionTest {\n \n   private void runTest(Configuration conf ) throws Exception {\n     conf.set(EncryptionPropertiesFactory.CRYPTO_FACTORY_CLASS_PROPERTY_NAME,\n-      \"org.apache.parquet.crypto.propertiesfactory.SchemaCryptoPropertiesFactory\");\n+      \"org.apache.parquet.crypto.CryptoPropertiesFactoryTests.SchemaCryptoPropertiesFactory\");\n     String file = createTempFile(\"test\");\n     markEncryptColumns();\n     encryptParquetFile(file, conf);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ3NQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115475", "body": "I would suggest using the related constant instead.", "bodyText": "I would suggest using the related constant instead.", "bodyHTML": "<p dir=\"auto\">I would suggest using the related constant instead.</p>", "author": "gszadovszky", "createdAt": "2020-10-21T09:07:59Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1Nzg1NQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510457855", "bodyText": "fixed", "author": "shangxinli", "createdAt": "2020-10-22T21:07:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex a850f9400..e9749f0e8 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -94,14 +95,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n     runTest(conf);\n   }\n \n", "next_change": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex e9749f0e8..17fda97d1 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -95,14 +96,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_V1.toString());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_CTR_V1.toString());\n     runTest(conf);\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTU0Mg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115542", "body": "I would suggest using the related constant instead.", "bodyText": "I would suggest using the related constant instead.", "bodyHTML": "<p dir=\"auto\">I would suggest using the related constant instead.</p>", "author": "gszadovszky", "createdAt": "2020-10-21T09:08:05Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NzgyMA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510457820", "bodyText": "fixed", "author": "shangxinli", "createdAt": "2020-10-22T21:07:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTU0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex a850f9400..e9749f0e8 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -94,14 +95,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n     runTest(conf);\n   }\n \n", "next_change": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex e9749f0e8..17fda97d1 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -95,14 +96,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_V1.toString());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_CTR_V1.toString());\n     runTest(conf);\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNTEyMg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509135122", "body": "I guess it should be cry**p**toMetadata", "bodyText": "I guess it should be cryptoMetadata", "bodyHTML": "<p dir=\"auto\">I guess it should be cry<strong>p</strong>toMetadata</p>", "author": "gszadovszky", "createdAt": "2020-10-21T09:37:07Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0ODk2MQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509848961", "bodyText": "good catch", "author": "shangxinli", "createdAt": "2020-10-22T02:53:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNTEyMg=="}], "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nsimilarity index 74%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\nindex a850f9400..7b1779b9e 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java\n", "chunk": "@@ -63,7 +66,7 @@ public class SchemaControlEncryptionTest {\n   // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n   // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n   // store in HMS, or other metastore. \n-  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Map<String, String>> crytoMetadatas = new HashMap<>();\n   private Map<String, Object[]> testData = new HashMap<>();\n \n   @Before\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNjc5Ng==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509136796", "body": "I don't think it is a good practice to use the column name as a conf key directly. The chance of collisions are pretty high. I would suggest adding a constant prefix.", "bodyText": "I don't think it is a good practice to use the column name as a conf key directly. The chance of collisions are pretty high. I would suggest adding a constant prefix.", "bodyHTML": "<p dir=\"auto\">I don't think it is a good practice to use the column name as a conf key directly. The chance of collisions are pretty high. I would suggest adding a constant prefix.</p>", "author": "gszadovszky", "createdAt": "2020-10-21T09:39:40Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n+    0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+\n+  @Override\n+  public FileEncryptionProperties getFileEncryptionProperties(Configuration conf, Path tempFilePath,\n+                                                              WriteContext fileWriteContext) throws ParquetCryptoRuntimeException {\n+    MessageType schema = fileWriteContext.getSchema();\n+    List<String[]> paths = schema.getPaths();\n+    if (paths == null || paths.isEmpty()) {\n+      throw new ParquetCryptoRuntimeException(\"Null or empty fields is found\");\n+    }\n+\n+    Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap = new HashMap<>();\n+\n+    for (String[] path : paths) {\n+      getColumnEncryptionProperties(path, columnPropertyMap, conf);\n+    }\n+\n+    if (columnPropertyMap.size() == 0) {\n+      log.debug(\"No column is encrypted. Returning null so that Parquet can skip. Empty properties will cause Parquet exception\");\n+      return null;\n+    }\n+\n+    /**\n+     * Why we still need footerKeyMetadata even withEncryptedFooter as false? According to the\n+     * 'Plaintext Footer' section of\n+     * https://github.com/apache/parquet-format/blob/encryption/Encryption.md, the plaintext footer\n+     * is signed in order to prevent tampering with the FileMetaData contents. So footerKeyMetadata\n+     * is always needed. This signature will be verified if parquet-mr code is with parquet-1178.\n+     * Otherwise, it will be ignored.\n+     */\n+    boolean shouldEncryptFooter = getEncryptFooter(conf);\n+    FileEncryptionProperties.Builder encryptionPropertiesBuilder =\n+      FileEncryptionProperties.builder(FOOTER_KEY)\n+        .withFooterKeyMetadata(FOOTER_KEY_METADATA)\n+        .withAlgorithm(getParquetCipherOrDefault(conf))\n+        .withEncryptedColumns(columnPropertyMap);\n+    if (!shouldEncryptFooter) {\n+      encryptionPropertiesBuilder = encryptionPropertiesBuilder.withPlaintextFooter();\n+    }\n+    FileEncryptionProperties encryptionProperties = encryptionPropertiesBuilder.build();\n+    log.info(\n+      \"FileEncryptionProperties is built with, algorithm:{}, footerEncrypted:{}\",\n+      encryptionProperties.getAlgorithm(),\n+      encryptionProperties.encryptedFooter());\n+    return encryptionProperties;\n+  }\n+\n+  private ParquetCipher getParquetCipherOrDefault(Configuration conf) {\n+    String algorithm = conf.get(CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    log.debug(\"Encryption algorithm is {}\", algorithm);\n+    return ParquetCipher.valueOf(algorithm.toUpperCase());\n+  }\n+\n+  private boolean getEncryptFooter(Configuration conf) {\n+    boolean encryptFooter = conf.getBoolean(CONF_ENCRYPTION_FOOTER, false);\n+    log.debug(\"Encrypt Footer: {}\", encryptFooter);\n+    return encryptFooter;\n+  }\n+\n+  private void getColumnEncryptionProperties(String[] path, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n+                                             Configuration conf) throws ParquetCryptoRuntimeException {\n+    String pathName = String.join(\".\", path);\n+    String columnKeyName = conf.get(pathName, null);", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MzY1OA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509863658", "bodyText": "fixed", "author": "shangxinli", "createdAt": "2020-10-22T03:51:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNjc5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nsimilarity index 67%\nrename from parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\nrename to parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\nindex d1460cda2..b6d6e4d23 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java\n", "chunk": "@@ -110,18 +115,42 @@ public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactor\n     return encryptFooter;\n   }\n \n-  private void getColumnEncryptionProperties(String[] path, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n-                                             Configuration conf) throws ParquetCryptoRuntimeException {\n-    String pathName = String.join(\".\", path);\n-    String columnKeyName = conf.get(pathName, null);\n-    if (columnKeyName != null) {\n-      ColumnPath columnPath = ColumnPath.get(path);\n-      ColumnEncryptionProperties colEncProp = ColumnEncryptionProperties.builder(columnPath)\n-        .withKey(COL_KEY)\n-        .withKeyMetaData(COL_KEY_METADATA)\n-        .build();\n-      columnPropertyMap.put(columnPath, colEncProp);\n+  private void getColumnEncryptionProperties(Type field, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n+                                             List<String> currentPath) throws ParquetCryptoRuntimeException {\n+    String pathName = field.getName();\n+    currentPath.add(pathName);\n+    if (field.isPrimitive()) {\n+      if (field instanceof ExtType) {\n+        log.debug(\"Leaf node {} is being checked crypto settings\", field.getName());\n+        // leaf node\n+        Map<String, Object> metaData = ((ExtType<Object>) field).getMetadata();\n+        if (metaData != null && metaData.containsKey(\"encrypted\")) {\n+          boolean encryptFlag;\n+          if ((metaData.get(\"encrypted\") instanceof String)) {\n+            encryptFlag = Boolean.parseBoolean((String) metaData.get(\"encrypted\"));\n+          } else {\n+            encryptFlag = (boolean) metaData.get(\"encrypted\");\n+          }\n+          if (encryptFlag) {\n+            log.info(\"Field {} is to be in encrypted mode\", field.getName());\n+            ColumnPath path = ColumnPath.get(currentPath.toArray(new String[0]));\n+            ColumnEncryptionProperties colEncProp =\n+              ColumnEncryptionProperties.builder(path)\n+                .withKey(COL_KEY)\n+                .withKeyMetaData(COL_KEY_METADATA)\n+                .build();\n+            columnPropertyMap.put(path, colEncProp);\n+          }\n+        }\n+      }\n+    } else {\n+      // intermediate node containing child(ren)\n+      List<Type> fields = field.asGroupType().getFields();\n+      for (Type childField : fields) {\n+        getColumnEncryptionProperties(childField, columnPropertyMap, currentPath);\n+      }\n     }\n+    currentPath.remove(currentPath.size() - 1);\n   }\n \n   @Override\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509200667", "body": "This assumes that `file.toString()` returns the full file path. However, the `file` is a `public abstract interface org.apache.parquet.io.OutputFile`, which doesn't have such method, so `toString()` is up to the implementation; no guarantees it will return the path. Also, `new Path(string full_path)` is not aware of the right filesystem (?) Maybe can be handled with an upcast to a known implementing class - preferably one that already has a `Path getPath()` method. \r\nBut of course, this won't be very general.", "bodyText": "This assumes that file.toString() returns the full file path. However, the file is a public abstract interface org.apache.parquet.io.OutputFile, which doesn't have such method, so toString() is up to the implementation; no guarantees it will return the path. Also, new Path(string full_path) is not aware of the right filesystem (?) Maybe can be handled with an upcast to a known implementing class - preferably one that already has a Path getPath() method.\nBut of course, this won't be very general.", "bodyHTML": "<p dir=\"auto\">This assumes that <code>file.toString()</code> returns the full file path. However, the <code>file</code> is a <code>public abstract interface org.apache.parquet.io.OutputFile</code>, which doesn't have such method, so <code>toString()</code> is up to the implementation; no guarantees it will return the path. Also, <code>new Path(string full_path)</code> is not aware of the right filesystem (?) Maybe can be handled with an upcast to a known implementing class - preferably one that already has a <code>Path getPath()</code> method.<br>\nBut of course, this won't be very general.</p>", "author": "ggershinsky", "createdAt": "2020-10-21T11:28:50Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwNjI3OQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509206279", "bodyText": "eg upcasting to HadoopOutputFile. Or even better, adding Path getPath() method to the OutputFile - this should be general enough.", "author": "ggershinsky", "createdAt": "2020-10-21T11:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI3ODIwMg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510278202", "bodyText": "Good point. I like the idea of adding \"Path getPath()\" better. Since this feature is going to be in a major release. I think\nadding a new method to the interface could be fine.", "author": "shangxinli", "createdAt": "2020-10-22T15:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDY5Mjg4Mg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510692882", "bodyText": "looks good. please also replace new Path(file.toString()) with file.getPath()", "author": "ggershinsky", "createdAt": "2020-10-23T07:38:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\nindex 90507d62f..ec6d9e74f 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n", "chunk": "@@ -281,7 +281,7 @@ public class ParquetWriter<T> implements Closeable {\n \n     // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n     if (encryptionProperties == null) {\n-      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);\n     }\n \n     ParquetFileWriter fileWriter = new ParquetFileWriter(\n", "next_change": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\nindex ec6d9e74f..c571afd62 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n", "chunk": "@@ -281,7 +281,7 @@ public class ParquetWriter<T> implements Closeable {\n \n     // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n     if (encryptionProperties == null) {\n-      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);\n     }\n \n     ParquetFileWriter fileWriter = new ParquetFileWriter(\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMjk0NA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509202944", "body": "probably no need in changing this file", "bodyText": "probably no need in changing this file", "bodyHTML": "<p dir=\"auto\">probably no need in changing this file</p>", "author": "ggershinsky", "createdAt": "2020-10-21T11:32:46Z", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-}\n+ }", "originalCommit": "94872443620c93f96ba4b7c182c9e0a1eca75992", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0ODgxMQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509848811", "bodyText": "true", "author": "shangxinli", "createdAt": "2020-10-22T02:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMjk0NA=="}], "type": "inlineReview", "revised_code": {"commit": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex e22b22df7..310227ac2 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,5 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n- }\n+\n+}\n", "next_change": {"commit": "621437f72d46a95747826d6e02bb710d981de7d3", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 310227ac2..80b6f3add 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -363,4 +365,11 @@ abstract public class Type {\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n \n+   public void setMetadata(Map<String, Object> metadata) {\n+    this.metadata = metadata;\n+  }\n+\n+   public Map<String, Object> getMetadata() {\n+    return this.metadata;\n+  }\n }\n", "next_change": {"commit": "aea692ac3abb37bfbd0b79984cfc7a861d428825", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 80b6f3add..e22b22df7 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -364,12 +362,4 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-   public void setMetadata(Map<String, Object> metadata) {\n-    this.metadata = metadata;\n-  }\n-\n-   public Map<String, Object> getMetadata() {\n-    return this.metadata;\n-  }\n-}\n+ }\n", "next_change": {"commit": "306796e0b6eedcd9bfda63b2b498223d72c599bf", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex e22b22df7..9782a56fd 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,4 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n- }\n+}\n", "next_change": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 9782a56fd..310227ac2 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,5 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n+\n }\n", "next_change": null}]}}]}}]}}]}}]}}, {"oid": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "url": "https://github.com/apache/parquet-mr/commit/4be2d141f1df419ae53e320f01c80e1945f25fa1", "message": "Parquet-1396: Example of using EncryptionPropertiesFactory and DecryptionPropertiesFactory", "committedDate": "2020-10-22T03:46:38Z", "type": "commit"}, {"oid": "7b68b0b0e901a1d6b6a7676d60df7507cc267380", "url": "https://github.com/apache/parquet-mr/commit/7b68b0b0e901a1d6b6a7676d60df7507cc267380", "message": "Address feedbacks", "committedDate": "2020-10-22T03:47:12Z", "type": "commit"}, {"oid": "621437f72d46a95747826d6e02bb710d981de7d3", "url": "https://github.com/apache/parquet-mr/commit/621437f72d46a95747826d6e02bb710d981de7d3", "message": "Remove ExtType and add metadata to Type directly", "committedDate": "2020-10-22T03:47:12Z", "type": "commit"}, {"oid": "aea692ac3abb37bfbd0b79984cfc7a861d428825", "url": "https://github.com/apache/parquet-mr/commit/aea692ac3abb37bfbd0b79984cfc7a861d428825", "message": "Use Configuration to pass the setting", "committedDate": "2020-10-22T03:47:13Z", "type": "commit"}, {"oid": "306796e0b6eedcd9bfda63b2b498223d72c599bf", "url": "https://github.com/apache/parquet-mr/commit/306796e0b6eedcd9bfda63b2b498223d72c599bf", "message": "Address feedback", "committedDate": "2020-10-22T03:47:13Z", "type": "forcePushed"}, {"oid": "26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "url": "https://github.com/apache/parquet-mr/commit/26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "message": "Address feedback", "committedDate": "2020-10-22T16:58:08Z", "type": "forcePushed"}, {"oid": "9589c7593be119591cf802e3d37924d937d7ff50", "url": "https://github.com/apache/parquet-mr/commit/9589c7593be119591cf802e3d37924d937d7ff50", "message": "Address feedback", "committedDate": "2020-10-22T18:40:45Z", "type": "forcePushed"}, {"oid": "c056ee1db7832d7b36b98363ab668306a272e302", "url": "https://github.com/apache/parquet-mr/commit/c056ee1db7832d7b36b98363ab668306a272e302", "message": "Address feedback", "committedDate": "2020-10-22T20:04:26Z", "type": "forcePushed"}, {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99", "url": "https://github.com/apache/parquet-mr/commit/0246e50fec13681a1de0407ca660ca923e8a3f99", "message": "Address feedback", "committedDate": "2020-10-22T21:14:13Z", "type": "commit"}, {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99", "url": "https://github.com/apache/parquet-mr/commit/0246e50fec13681a1de0407ca660ca923e8a3f99", "message": "Address feedback", "committedDate": "2020-10-22T21:14:13Z", "type": "forcePushed"}, {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "url": "https://github.com/apache/parquet-mr/commit/2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "message": "Replace file.toString() with file.getPath()", "committedDate": "2020-10-23T13:34:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTMxNw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511805317", "body": "I don't know if I overlooked this one before or it is a new change. The module `parquet-column` should not depend on hadoop. That's why we have the separate module `parquet-hadoop`. We already have struggling issues that parquet-mr cannot be used without hadoop, let's not make it worse.", "bodyText": "I don't know if I overlooked this one before or it is a new change. The module parquet-column should not depend on hadoop. That's why we have the separate module parquet-hadoop. We already have struggling issues that parquet-mr cannot be used without hadoop, let's not make it worse.", "bodyHTML": "<p dir=\"auto\">I don't know if I overlooked this one before or it is a new change. The module <code>parquet-column</code> should not depend on hadoop. That's why we have the separate module <code>parquet-hadoop</code>. We already have struggling issues that parquet-mr cannot be used without hadoop, let's not make it worse.</p>", "author": "gszadovszky", "createdAt": "2020-10-26T09:01:40Z", "path": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java", "diffHunk": "@@ -31,4 +33,5 @@\n \n   long defaultBlockSize();\n \n+  Path getPath();", "originalCommit": "0246e50fec13681a1de0407ca660ca923e8a3f99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgzODgzOA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511838838", "bodyText": "apologies, this is a recent change done due to my suggestion; making common dependent on hadoop is indeed not a good idea. @shangxinli - I've checked the hadoop Path and FileSystem code, and it looks like having a Path class here is not a must. A regular String would do, eg String getPath in this interface (OutputFile).\nThen the ParquetWriter can use the new Path(String) call.", "author": "ggershinsky", "createdAt": "2020-10-26T09:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTMxNw=="}], "type": "inlineReview", "revised_code": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java b/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java\nindex 80e7a1fd3..11bb051b9 100644\n--- a/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java\n+++ b/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java\n", "chunk": "@@ -33,5 +33,5 @@ public interface OutputFile {\n \n   long defaultBlockSize();\n \n-  Path getPath();\n+  String getPath();\n }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODQ2MQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808461", "body": "I think it is nicer to use the enum `ParquetCypher` instead of the parquet-format generated class.", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "bodyHTML": "<p dir=\"auto\">I think it is nicer to use the enum <code>ParquetCypher</code> instead of the parquet-format generated class.</p>", "author": "gszadovszky", "createdAt": "2020-10-26T09:07:08Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());", "originalCommit": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex e0c3c94a5..17fda97d1 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -95,14 +96,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_V1.toString());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_CTR_V1.toString());\n     runTest(conf);\n   }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODUyMw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808523", "body": "I think it is nicer to use the enum `ParquetCypher` instead of the parquet-format generated class.", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "bodyHTML": "<p dir=\"auto\">I think it is nicer to use the enum <code>ParquetCypher</code> instead of the parquet-format generated class.</p>", "author": "gszadovszky", "createdAt": "2020-10-26T09:07:15Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());", "originalCommit": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\nindex e0c3c94a5..17fda97d1 100644\n--- a/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n+++ b/parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java\n", "chunk": "@@ -95,14 +96,14 @@ public class SchemaControlEncryptionTest {\n   @Test\n   public void testEncryptionGcm() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_V1.toString());\n     runTest(conf);\n   }\n \n   @Test\n   public void testEncryptionGcmCtr() throws Exception {\n     Configuration conf = new Configuration();\n-    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, ParquetCipher.AES_GCM_CTR_V1.toString());\n     runTest(conf);\n   }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0MDQzMw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511840433", "body": "per the previous comment, can be changed to `encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);`", "bodyText": "per the previous comment, can be changed to encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);", "bodyHTML": "<p dir=\"auto\">per the previous comment, can be changed to <code>encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);</code></p>", "author": "ggershinsky", "createdAt": "2020-10-26T09:59:04Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);", "originalCommit": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76d1b6872ffa38eff4d43e441419a9f362711934", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\nindex ec6d9e74f..c571afd62 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n", "chunk": "@@ -281,7 +281,7 @@ public class ParquetWriter<T> implements Closeable {\n \n     // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n     if (encryptionProperties == null) {\n-      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);\n     }\n \n     ParquetFileWriter fileWriter = new ParquetFileWriter(\n", "next_change": null}]}}, {"oid": "76d1b6872ffa38eff4d43e441419a9f362711934", "url": "https://github.com/apache/parquet-mr/commit/76d1b6872ffa38eff4d43e441419a9f362711934", "message": "Address feedback", "committedDate": "2020-10-26T14:53:10Z", "type": "commit"}, {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "url": "https://github.com/apache/parquet-mr/commit/cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "message": "fix build error", "committedDate": "2020-10-26T19:58:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTAwNg==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485006", "body": "It's unused.", "bodyText": "It's unused.", "bodyHTML": "<p dir=\"auto\">It's unused.</p>", "author": "gszadovszky", "createdAt": "2020-10-27T08:08:31Z", "path": "parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java", "diffHunk": "@@ -28,6 +28,7 @@\n import java.io.IOException;\n import java.util.Random;\n \n+import org.apache.hadoop.fs.Path;", "originalCommit": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4ODY4Nw==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512688687", "bodyText": "fixed", "author": "shangxinli", "createdAt": "2020-10-27T13:26:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTAwNg=="}], "type": "inlineReview", "revised_code": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java b/parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java\nindex ebb3223aa..f5613a21b 100644\n--- a/parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java\n+++ b/parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java\n", "chunk": "@@ -28,7 +28,6 @@ import static org.openjdk.jmh.annotations.Scope.Benchmark;\n import java.io.IOException;\n import java.util.Random;\n \n-import org.apache.hadoop.fs.Path;\n import org.apache.parquet.example.data.Group;\n import org.apache.parquet.example.data.GroupFactory;\n import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTQ5MA==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485490", "body": "nit: Please undo formatting changes in this file.", "bodyText": "nit: Please undo formatting changes in this file.", "bodyHTML": "<p dir=\"auto\">nit: Please undo formatting changes in this file.</p>", "author": "gszadovszky", "createdAt": "2020-10-27T08:09:22Z", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-", "originalCommit": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4OTkzNQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512689935", "bodyText": "fixed", "author": "shangxinli", "createdAt": "2020-10-27T13:27:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTQ5MA=="}], "type": "inlineReview", "revised_code": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\nindex 9782a56fd..310227ac2 100644\n--- a/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n+++ b/parquet-column/src/main/java/org/apache/parquet/schema/Type.java\n", "chunk": "@@ -362,4 +362,5 @@ abstract public class Type {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n+\n }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTczNQ==", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485735", "body": "nit: Please undo formatting changes in this file.", "bodyText": "nit: Please undo formatting changes in this file.", "bodyHTML": "<p dir=\"auto\">nit: Please undo formatting changes in this file.</p>", "author": "gszadovszky", "createdAt": "2020-10-27T08:09:47Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -113,6 +113,5 @@ protected Builder self() {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n-", "originalCommit": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "475b2228847263d11972539104c11fe31cb4c1fa", "changed_code": [{"header": "diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\nindex a298e551f..12a67d301 100644\n--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java\n", "chunk": "@@ -113,5 +113,6 @@ public class ExampleParquetWriter extends ParquetWriter<Group> {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n+\n   }\n }\n", "next_change": null}]}}, {"oid": "475b2228847263d11972539104c11fe31cb4c1fa", "url": "https://github.com/apache/parquet-mr/commit/475b2228847263d11972539104c11fe31cb4c1fa", "message": "Address more feedbacks", "committedDate": "2020-10-27T13:31:43Z", "type": "commit"}]}