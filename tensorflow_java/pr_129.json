{"pr_number": 129, "pr_title": "Add Losses", "pr_author": "JimClarke5", "pr_createdAt": "2020-10-08T22:57:20Z", "pr_url": "https://github.com/tensorflow/java/pull/129", "timeline": [{"oid": "c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "url": "https://github.com/tensorflow/java/commit/c57a2e741b23569d4b1ad33e18404dbe0dc814dc", "message": "Merge pull request #3 from tensorflow/master\n\nSync with master tensorflow on upstream", "committedDate": "2020-10-08T17:19:37Z", "type": "commit"}, {"oid": "9cc26757f102688789b58d32f18d6fd7e4941fc2", "url": "https://github.com/tensorflow/java/commit/9cc26757f102688789b58d32f18d6fd7e4941fc2", "message": "Initial checkin to rebase to Initialziers to pick up changes to ndarry Shape", "committedDate": "2020-10-08T18:07:11Z", "type": "commit"}, {"oid": "2508f5e58b59e18d3537d845491ce1e3f7afbd85", "url": "https://github.com/tensorflow/java/commit/2508f5e58b59e18d3537d845491ce1e3f7afbd85", "message": "Initial Checkin for losses", "committedDate": "2020-10-08T18:07:11Z", "type": "commit"}, {"oid": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "url": "https://github.com/tensorflow/java/commit/17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "message": "Fix reshape in sparseCategoricalCrossentropy()", "committedDate": "2020-10-08T22:25:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NjE5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502686196", "body": "perhaps \"of the predictions and result\"?", "bodyText": "perhaps \"of the predictions and result\"?", "bodyHTML": "<p dir=\"auto\">perhaps \"of the predictions and result\"?</p>", "author": "deansher", "createdAt": "2020-10-09T21:54:41Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjM0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842340", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-10-10T23:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NjE5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..e3e91a41 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -32,7 +32,7 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute error\n    */\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex e3e91a41..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -39,7 +39,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -38,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -53,7 +53,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -52,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf, labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NzIwMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502687203", "body": "The `Tuple` class name is uncomfortably vanilla for me. Perhaps `LossTuple`?", "bodyText": "The Tuple class name is uncomfortably vanilla for me. Perhaps LossTuple?", "bodyHTML": "<p dir=\"auto\">The <code>Tuple</code> class name is uncomfortably vanilla for me. Perhaps <code>LossTuple</code>?</p>", "author": "deansher", "createdAt": "2020-10-09T21:59:37Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjUzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842530", "bodyText": "This object will also be used in Metrics as many metrics are built using loss classes or Losses methods. I have changed it to LossTuple.", "author": "JimClarke5", "createdAt": "2020-10-10T23:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4NzIwMw=="}], "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\nsimilarity index 65%\nrename from tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java\nrename to tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\nindex 402cac96..596fb31c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\n", "chunk": "@@ -4,33 +4,33 @@ import org.tensorflow.Operand;\n import org.tensorflow.types.family.TNumber;\n \n /**\n- * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ * A helper class for loss methods to return  labels, target, and sampleWeights\n  *\n- * @param <T> the data type of the Tuple entries.\n+ * @param <T> the data type of the LossTuple entries.\n  */\n-public class Tuple<T extends TNumber> {\n+public class LossTuple<T extends TNumber> {\n   private final Operand<T> labels;\n   private final Operand<T> target;\n   private final Operand<T> sampleWeights;\n \n   /**\n-   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   * Creates a LossTuple of Operands for labels, target, and sampleWeights\n    *\n    * @param labels the labels\n    * @param target the losses or target\n    */\n-  public Tuple(Operand<T> labels, Operand<T> target) {\n+  public LossTuple(Operand<T> labels, Operand<T> target) {\n     this(labels, target, null);\n   }\n \n   /**\n-   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   * Creates a LossTuple of Operands for labels, target, and sampleWeights\n    *\n    * @param labels the labels\n    * @param target the losses or target\n    * @param sampleWeights the sample weights\n    */\n-  public Tuple(Operand<T> labels, Operand<T> target, Operand<T> sampleWeights) {\n+  public LossTuple(Operand<T> labels, Operand<T> target, Operand<T> sampleWeights) {\n     this.labels = labels;\n     this.target = target;\n     this.sampleWeights = sampleWeights;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODAxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688018", "body": "The Javadocs in this file are still partly in markdown.", "bodyText": "The Javadocs in this file are still partly in markdown.", "bodyHTML": "<p dir=\"auto\">The Javadocs in this file are still partly in markdown.</p>", "author": "deansher", "createdAt": "2020-10-09T22:03:44Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MzMyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502843327", "bodyText": "OK, I thought I caught them all, I will fix.", "author": "JimClarke5", "createdAt": "2020-10-10T23:24:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODAxOA=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -18,18 +18,21 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed with a sampleWeights of one.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n-   *       {@link #removeSqueezableDimensions}).\n-   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n-   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank\n+   *       differs by 1 (using {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar./li>\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n-   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n-   *     is null, (prediction, label) is returned.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n+   *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n+   *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n+   *     label) is returned.\n    */\n   public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n       Ops tf, Operand<T> labels, Operand<T> predictions) {\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -29,12 +29,12 @@ public class LossesImpl {\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n+   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n    *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n    *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n    *     label) is returned.\n    */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+  public static <T extends TNumber> LossTuple<T> squeezeOrExpandDimensions(\n       Ops tf, Operand<T> labels, Operand<T> predictions) {\n     return squeezeOrExpandDimensions(tf, labels, predictions, null);\n   }\n", "next_change": {"commit": "249b65194bb055decf02d61f56378e7771e6d05f", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..9cc77b50 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -29,8 +29,8 @@ public class LossesImpl {\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n-   *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n+   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will\n+   *     be null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n    *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n    *     label) is returned.\n    */\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502688796", "body": "For this method, the returned `sampleWeight` is always null.", "bodyText": "For this method, the returned sampleWeight is always null.", "bodyHTML": "<p dir=\"auto\">For this method, the returned <code>sampleWeight</code> is always null.</p>", "author": "deansher", "createdAt": "2020-10-09T22:07:20Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NDQwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502784409", "bodyText": "That is not always the case when we do Metrics.", "author": "JimClarke5", "createdAt": "2020-10-10T12:15:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgyMzc5MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502823790", "bodyText": "I'm just thinking our documentation for this method might take into account that the returned sampleWeight is always null.", "author": "deansher", "createdAt": "2020-10-10T19:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MzU3OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502843579", "bodyText": "Now I see what you  are talking about. I added a comment in the @return that sampleWeight will be null for this particular method signature.", "author": "JimClarke5", "createdAt": "2020-10-10T23:27:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4ODc5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -18,18 +18,21 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed with a sampleWeights of one.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n-   *       {@link #removeSqueezableDimensions}).\n-   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n-   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank\n+   *       differs by 1 (using {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar./li>\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n-   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n-   *     is null, (prediction, label) is returned.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n+   *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n+   *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n+   *     label) is returned.\n    */\n   public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n       Ops tf, Operand<T> labels, Operand<T> predictions) {\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -29,12 +29,12 @@ public class LossesImpl {\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n+   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n    *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n    *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n    *     label) is returned.\n    */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+  public static <T extends TNumber> LossTuple<T> squeezeOrExpandDimensions(\n       Ops tf, Operand<T> labels, Operand<T> predictions) {\n     return squeezeOrExpandDimensions(tf, labels, predictions, null);\n   }\n", "next_change": {"commit": "249b65194bb055decf02d61f56378e7771e6d05f", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..9cc77b50 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -29,8 +29,8 @@ public class LossesImpl {\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will be\n-   *     null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n+   * @return LossTuple of <code>prediction</code>, <code>label</code>,<code>sampleWeight</code> will\n+   *     be null. Each of them possibly has the last dimension squeezed, <code>sampleWeight</code>\n    *     could be extended by one dimension. If <code>sampleWeight</code> is null, (prediction,\n    *     label) is returned.\n    */\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689064", "body": "Is \"match\" the right way to describe the precondition relationship between `predictions` and `labels`?", "bodyText": "Is \"match\" the right way to describe the precondition relationship between predictions and labels?", "bodyHTML": "<p dir=\"auto\">Is \"match\" the right way to describe the precondition relationship between <code>predictions</code> and <code>labels</code>?</p>", "author": "deansher", "createdAt": "2020-10-09T22:08:36Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0NDYyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502844627", "bodyText": "It is definitely not the same Shape.  I was thinking of compatible, but that has specific meaning in Shape.isCompatibleWIth.  The description is saying the ranks must be equal or differ by one. I am not sure of one word that describes that.  match was the word used in the Python version of this method.", "author": "JimClarke5", "createdAt": "2020-10-10T23:43:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxNTYwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502915600", "bodyText": "Hmm, here's a suggestion:\n\nWe could decide what we want the convention to be, in terms of squeeze-or-expand plus maybe broadcasting.\nWrite this up carefully in the class javadoc for either Loss or Losses.\nMention that documentation in the class javadoc for every other loss class.\nAlso mention it in Loss#call.\nAnd be silent about it in the individual methods of Losses and LossesImpl.\nPerhaps?", "author": "deansher", "createdAt": "2020-10-11T13:26:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyMDQ2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502920467", "bodyText": "That said, it just occurred to me that we have another gap, and that filling that gap might help this issue.\nWe don't specify the behavior of these methods when labels and predictions don't have a permitted shape relationship. Nor do we make sure our behavior is consistent in that case.\nPerhaps we should\n\nspell out that there's an IllegalArgumentException for that in the statically-known-dimensions case,\nrename squeezeOrExpandDimensions into something like validateAndAdjustLossDimensions,\nhave that method throw IllegalArgumentException when appropriate,\nand then link to a fuller explanation in the documentation of the IllegalArgumentException?\n\nAlthough I have never been in the habit of subclassing IllegalArgumentException, I see Oracle does that sometimes. That could be an alternative way of pointing people to the fuller explanation.", "author": "deansher", "createdAt": "2020-10-11T14:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzMzc4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502933789", "bodyText": "match must mean that the shapes of the input operands are capable of being molded into the relationships defined for the result of this method. Again LossesImpl is intended to be marked as module private (JDK 11) and only should be accessible from the losses or metrics package. It is not intended to be a general use API.", "author": "JimClarke5", "createdAt": "2020-10-11T15:59:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDUxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500512", "bodyText": "We should probably note in the javadoc for the class that this is an internal implementation class and subject to change (and being locked off under the module system).", "author": "Craigacp", "createdAt": "2020-10-24T18:29:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjMzMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652331", "bodyText": "Added this comment for the LossesImpl class\n/**\n * These are helper methods for Losses and will be module private when\n * Java modularity is applied to TensorFlow Java.\n * These methods should not be used outside of the Loss package.\n */", "author": "JimClarke5", "createdAt": "2020-10-25T21:44:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTA2NA=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,36 +43,39 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return Tuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n   public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    Tuple<T> tuple = new Tuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n           tuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -77,19 +77,19 @@ public class LossesImpl {\n         // Use static rank for 'label' and 'prediction'.\n         if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n     if (sampleWeights == null) { // nothing more to do.\n-      return tuple;\n+      return lossTuple;\n     }\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -89,7 +88,7 @@ public class LossesImpl {\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new LossTuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTU4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502689589", "body": "For consistency, `labelsRank`.", "bodyText": "For consistency, labelsRank.", "bodyHTML": "<p dir=\"auto\">For consistency, <code>labelsRank</code>.</p>", "author": "deansher", "createdAt": "2020-10-09T22:11:07Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjc4OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502926789", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-10-11T15:01:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY4OTU4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,36 +43,39 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return Tuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n   public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    Tuple<T> tuple = new Tuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n           tuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -77,19 +77,19 @@ public class LossesImpl {\n         // Use static rank for 'label' and 'prediction'.\n         if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n     if (sampleWeights == null) { // nothing more to do.\n-      return tuple;\n+      return lossTuple;\n     }\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -89,7 +88,7 @@ public class LossesImpl {\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new LossTuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjI2MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502776261", "body": "Extraneous `*`", "bodyText": "Extraneous *", "bodyHTML": "<p dir=\"auto\">Extraneous <code>*</code></p>", "author": "deansher", "createdAt": "2020-10-10T10:40:51Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTQxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839418", "bodyText": "Deleted", "author": "JimClarke5", "createdAt": "2020-10-10T22:33:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjI2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex aa4e167c..c8be0463 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -59,7 +59,7 @@ public class BinaryCrossentropy extends Loss {\n   /**\n    * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n    * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n-   * Loss Reduction of {@link * Reduction#AUTO}\n+   * Loss Reduction of {@link Reduction#AUTO}\n    *\n    *\n    *\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex c8be0463..d194f084 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -59,9 +57,7 @@ public class BinaryCrossentropy extends Loss {\n   /**\n    * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n    * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n-   * Loss Reduction of {@link Reduction#AUTO}\n-   *\n-   *\n+   * Loss Reduction of {@link Loss#REDUCTION_DEFAULT}\n    *\n    * @param tf the TensorFlow Ops\n    */\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NjcwMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502786701", "body": "Extraneous `*`", "bodyText": "Extraneous *", "bodyHTML": "<p dir=\"auto\">Extraneous <code>*</code></p>", "author": "deansher", "createdAt": "2020-10-10T12:42:15Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTgyNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839825", "bodyText": "Removed all Extraneous @link *", "author": "JimClarke5", "createdAt": "2020-10-10T22:38:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NjcwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex b042a656..a7491285 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -63,7 +63,7 @@ public class CategoricalCrossentropy extends Loss {\n   /**\n    * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n    * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n-   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n    * #DEFAULT_AXIS}\n    *\n    * @param tf the TensorFlow Ops\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex a7491285..1550042d 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -63,7 +62,7 @@ public class CategoricalCrossentropy extends Loss {\n   /**\n    * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n    * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n-   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n    * #DEFAULT_AXIS}\n    *\n    * @param tf the TensorFlow Ops\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787006", "body": ". . . `, or null to use {@link Class#getSimpleName()}`", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "bodyHTML": "<p dir=\"auto\">. . . <code>, or null to use {@link Class#getSimpleName()}</code></p>", "author": "deansher", "createdAt": "2020-10-10T12:45:30Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzMzk2OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502933968", "bodyText": "Why would someone want to pass null, when there are other CTORs that handle that condition?", "author": "JimClarke5", "createdAt": "2020-10-11T16:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1OTYyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503259628", "bodyText": "For APIs that will get enough use to be worth some polish, I tend toward carefully documenting edge cases. I don't know whether we want to invest in that now.", "author": "deansher", "createdAt": "2020-10-12T12:25:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjIxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492216", "bodyText": "I think it's worth documenting it in case users build their own losses.", "author": "Craigacp", "createdAt": "2020-10-24T16:59:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjYxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652618", "bodyText": "OK, added this to name param, if null the name will be {@link Class#getSimpleName()}.", "author": "JimClarke5", "createdAt": "2020-10-25T21:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAwNg=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex 9c0976f2..b56f77e9 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -19,10 +21,10 @@ public abstract class Loss {\n   }\n \n   /**\n-   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   * Creates a Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}\n    *\n    * @param tf the TensorFlow Ops\n-   * @param name the name of this Loss\n+   * @param name the name of this Loss, if null the name will be {@link Class#getSimpleName()}.\n    */\n   protected Loss(Ops tf, String name) {\n     this(tf, name, Reduction.AUTO);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787035", "body": ". . . `, or null to use {@link Class#getSimpleName()}`", "bodyText": ". . . , or null to use {@link Class#getSimpleName()}", "bodyHTML": "<p dir=\"auto\">. . . <code>, or null to use {@link Class#getSimpleName()}</code></p>", "author": "deansher", "createdAt": "2020-10-10T12:45:46Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MjY5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511652698", "bodyText": "OK, added this to all  name param, if null the name will be {@link Class#getSimpleName()}.", "author": "JimClarke5", "createdAt": "2020-10-25T21:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzAzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex 9c0976f2..b56f77e9 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -32,7 +34,7 @@ public abstract class Loss {\n    * Creates a Loss\n    *\n    * @param tf the TensorFlow Ops\n-   * @param name the name of this loss\n+   * @param name the name of this loss, if null the name will be {@link Class#getSimpleName()}.\n    * @param reduction Type of Reduction to apply to the loss.\n    */\n   protected Loss(Ops tf, String name, Reduction reduction) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzA4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502787086", "body": "Actually, there's a separate `<U>` for the labels.", "bodyText": "Actually, there's a separate <U> for the labels.", "bodyHTML": "<p dir=\"auto\">Actually, there's a separate <code>&lt;U&gt;</code> for the labels.</p>", "author": "deansher", "createdAt": "2020-10-10T12:46:27Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MTgwOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502841808", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-10T23:03:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc4NzA4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex 9c0976f2..445ebf99 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -45,7 +45,8 @@ public abstract class Loss {\n    *\n    * @param labels the truth values or labels\n    * @param predictions the predictions\n-   * @param <T> The data type of the labels, predictions and loss.\n+   * @param <T> The data type of the predictions and loss.\n+   * @param <U> The data type of the labels.\n    * @return the loss\n    */\n   public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex 445ebf99..b56f77e9 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -54,7 +56,7 @@ public abstract class Loss {\n   }\n \n   /**\n-   * Calculates the loss\n+   * Generates an Operand that calculates the loss.\n    *\n    * @param labels the truth values or labels\n    * @param predictions the predictions\n", "next_change": {"commit": "02573b594ca552371b8f42fa9e53c019143e6931", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex b56f77e9..b9a08ad2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -60,12 +75,12 @@ public abstract class Loss {\n    *\n    * @param labels the truth values or labels\n    * @param predictions the predictions\n-   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n-   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   * @param sampleWeights Optional sampleWeights acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a tensor\n    *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n-   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n    *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n-   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all loss\n    *     functions reduce by 1 dimension, usually axis=-1.)\n    * @param <T> The data type of the predictions, sampleWeights and loss.\n    * @param <U> The data type of the labels.\n", "next_change": {"commit": "744e32463c4aa8def4456fac4bcec53536a04fa4", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex b9a08ad2..ae33d5df 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -79,7 +78,7 @@ public abstract class Loss {\n    *     provided, then the loss is simply scaled by the given value. If SampleWeights is a tensor\n    *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n    *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n-   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     [batch_size, d0, .. dN-1] (or can be broadcast to this shape), then each loss element of\n    *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all loss\n    *     functions reduce by 1 dimension, usually axis=-1.)\n    * @param <T> The data type of the predictions, sampleWeights and loss.\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808082", "body": "How would you feel about mnemonic/indicative type names like `L` for labels? Or even `LabelsT`?", "bodyText": "How would you feel about mnemonic/indicative type names like L for labels? Or even LabelsT?", "bodyHTML": "<p dir=\"auto\">How would you feel about mnemonic/indicative type names like <code>L</code> for labels? Or even <code>LabelsT</code>?</p>", "author": "deansher", "createdAt": "2020-10-10T16:35:25Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+public abstract class Loss {\n+  protected final Ops tf;\n+  protected final Reduction reduction;\n+\n+  /**\n+   * Creates a Loss using {@link Class#getSimpleName()}  as the name and a Loss Reduction of {@link\n+   * Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  protected Loss(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this Loss\n+   */\n+  protected Loss(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  protected Loss(Ops tf, String name, Reduction reduction) {\n+    this.tf = name != null ? tf.withSubScope(name) : tf.withSubScope(getClass().getSimpleName());\n+    this.reduction = reduction;\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param <T> The data type of the labels, predictions and loss.\n+   * @return the loss\n+   */\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(Operand<U> labels, Operand<T> predictions) {\n+    return call(labels, predictions, null);\n+  }\n+\n+  /**\n+   * Calculates the loss\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MjE3OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502842178", "bodyText": "The standard convention is:\n\nE - Element (used extensively by the Java Collections Framework)\nK - Key\nN - Number\nT - Type\nV - Value\nS,U,V etc. - 2nd, 3rd, 4th types\n\nGeneric Types", "author": "JimClarke5", "createdAt": "2020-10-10T23:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI2MzU0Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503263542", "bodyText": "I don't think of that list as being especially prescriptive. In the Oracle documentation you link above, the list is introduced as \"The most commonly used type parameter names are: ...\".\nGoogle's Java style guide says:\n\nEach type variable is named in one of two styles:\n\nA single capital letter, optionally followed by a single numeral (such as E, T, X, T2)\nA name in the form used for classes (see Section 5.2.2, Class names), followed by the capital letter T (examples: RequestT, FooBarT).\n\n\nPersonally, I'd lean toward using some of our own single-letter conventions for situations that are common in our own code, including L as the labels type.", "author": "deansher", "createdAt": "2020-10-12T12:33:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjUxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492519", "bodyText": "My vote would be to stick to the Java conventions Jim described.\nI particularly dislike the Google style form where the type name is a word that isn't all caps, but I tend to find type variables that are longer than a single character tricky to read anyway.", "author": "Craigacp", "createdAt": "2020-10-24T17:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwMjg4OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521102888", "bodyText": "Personally, I'd lean toward using some of our own single-letter conventions for situations that are common in our own code, including L as the labels type.\n\nThis may be hard to follow consistently once several letters have been used e.g. 'L' might be needed for something other than label type. Seems a tad more confusing than the standard type names", "author": "KartikChugh", "createdAt": "2020-11-11T04:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNDIyMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522204222", "bodyText": "Ok, sticking with the original plan! Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODA4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "02573b594ca552371b8f42fa9e53c019143e6931", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex 9c0976f2..b9a08ad2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -53,16 +71,16 @@ public abstract class Loss {\n   }\n \n   /**\n-   * Calculates the loss\n+   * Generates an Operand that calculates the loss.\n    *\n    * @param labels the truth values or labels\n    * @param predictions the predictions\n-   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n-   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   * @param sampleWeights Optional sampleWeights acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a tensor\n    *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n-   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n    *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n-   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all loss\n    *     functions reduce by 1 dimension, usually axis=-1.)\n    * @param <T> The data type of the predictions, sampleWeights and loss.\n    * @param <U> The data type of the labels.\n", "next_change": {"commit": "744e32463c4aa8def4456fac4bcec53536a04fa4", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\nindex b9a08ad2..ae33d5df 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Loss.java\n", "chunk": "@@ -79,7 +78,7 @@ public abstract class Loss {\n    *     provided, then the loss is simply scaled by the given value. If SampleWeights is a tensor\n    *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n    *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n-   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     [batch_size, d0, .. dN-1] (or can be broadcast to this shape), then each loss element of\n    *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all loss\n    *     functions reduce by 1 dimension, usually axis=-1.)\n    * @param <T> The data type of the predictions, sampleWeights and loss.\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODY5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808698", "body": "Inconsistency between accessing the superclass's `tf` directly and accessing its reduction via `getReduction`.", "bodyText": "Inconsistency between accessing the superclass's tf directly and accessing its reduction via getReduction.", "bodyHTML": "<p dir=\"auto\">Inconsistency between accessing the superclass's <code>tf</code> directly and accessing its reduction via <code>getReduction</code>.</p>", "author": "deansher", "createdAt": "2020-10-10T16:42:33Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ *\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link * Reduction#AUTO}\n+   *\n+   *\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link #REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using a reduction of {@link #REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+          Ops tf,  boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+      this(tf, null, fromLogits, labelSmoothing, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     correspond to heavier smoothing.\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(\n+      Ops tf, String name, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+      Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.binaryCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTUzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839536", "bodyText": "Changed to getTF()", "author": "JimClarke5", "createdAt": "2020-10-10T22:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODY5OA=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex aa4e167c..c8be0463 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -173,7 +173,7 @@ public class BinaryCrossentropy extends Loss {\n   public <T extends TNumber, U extends TNumber> Operand<T> call(\n       Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n     Operand<T> losses =\n-        Losses.binaryCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing);\n-    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);\n+        Losses.binaryCrossentropy(getTF(), labels, predictions, fromLogits, labelSmoothing);\n+    return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex c8be0463..d194f084 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -160,20 +156,58 @@ public class BinaryCrossentropy extends Loss {\n    *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n    *     correspond to heavier smoothing.\n    * @param reduction Type of Reduction to apply to the loss.\n+   * @throws IllegalArgumentException if labelSmoothing is not in the inclusive range of 0. - 1.\n    */\n   public BinaryCrossentropy(\n       Ops tf, String name, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n     super(tf, name, reduction);\n+    if(labelSmoothing < 0 || labelSmoothing > 1)\n+      throw new IllegalArgumentException(\"labelSmoothing must be >= 0. and <= 1, found \" + labelSmoothing);\n     this.fromLogits = fromLogits;\n     this.labelSmoothing = labelSmoothing;\n   }\n \n-  /** {@inheritDoc} */\n+  /**\n+   * Generates an Operand that calculates the loss.\n+   *\n+   * If run in Graph mode, the computation will throw {@link org.tensorflow.exceptions.TFInvalidArgumentException}\n+   * if the predictions values are outside the range o [0. to 1.]. In Eager Mode, this call\n+   * will throw {@link IllegalArgumentException}, if the predictions values are outside the range o [0. to 1.]\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions, values must be in the range [0. to 1.] inclusive.\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.\n+   * @return the loss\n+   * @throws IllegalArgumentException if the predictions are outside the range [0.-1.].\n+   */\n   @Override\n   public <T extends TNumber, U extends TNumber> Operand<T> call(\n       Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> lPredictions;\n+    if (!fromLogits) {\n+      // add predictions range check for 0 - 1\n+      lPredictions =\n+          LossesImpl.rangeCheck(\n+              getTF(),\n+              \"predictions range check [0-1]\",\n+              predictions,\n+              getTF().dtypes.cast(getTF().constant(0), predictions.asOutput().dataType()),\n+              getTF().dtypes.cast(getTF().constant(1), predictions.asOutput().dataType()));\n+\n+    } else {\n+      lPredictions = predictions;\n+    }\n+\n     Operand<T> losses =\n-        Losses.binaryCrossentropy(getTF(), labels, predictions, fromLogits, labelSmoothing);\n+        Losses.binaryCrossentropy(getTF(), labels, lPredictions, fromLogits, labelSmoothing);\n     return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex d194f084..a2261705 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -208,6 +225,6 @@ public class BinaryCrossentropy extends Loss {\n \n     Operand<T> losses =\n         Losses.binaryCrossentropy(getTF(), labels, lPredictions, fromLogits, labelSmoothing);\n-    return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n+    return LossesHelper.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODgwNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502808807", "body": "`tf` versus `getReduction` (but I'll stop mentioning these)", "bodyText": "tf versus getReduction (but I'll stop mentioning these)", "bodyHTML": "<p dir=\"auto\"><code>tf</code> versus <code>getReduction</code> (but I'll stop mentioning these)</p>", "author": "deansher", "createdAt": "2020-10-10T16:43:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link *\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link * Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link * Reduction#AUTO},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   * @param axis The channels axis. <code>axis=-1</code> corresponds to data format `Channels Last'\n+   *     and <code>axis=1</code> corresponds to data format 'Channels First'.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf,\n+      String name,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      Reduction reduction,\n+      int axis) {\n+    super(tf, name, reduction);\n+    this.fromLogits = fromLogits;\n+    this.labelSmoothing = labelSmoothing;\n+    this.axis = axis;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public <T extends TNumber, U extends TNumber> Operand<T> call(\n+          Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> losses =\n+        Losses.categoricalCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing, axis);\n+    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzOTg4NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502839884", "bodyText": "Changed to getTF()", "author": "JimClarke5", "createdAt": "2020-10-10T22:39:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwODgwNw=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex b042a656..a7491285 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -213,7 +213,7 @@ public class CategoricalCrossentropy extends Loss {\n   public <T extends TNumber, U extends TNumber> Operand<T> call(\n           Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n     Operand<T> losses =\n-        Losses.categoricalCrossentropy(tf, labels, predictions, fromLogits, labelSmoothing, axis);\n-    return LossesImpl.computeWeightedLoss(tf, losses, getReduction(), sampleWeights);\n+        Losses.categoricalCrossentropy(getTF(), labels, predictions, fromLogits, labelSmoothing, axis);\n+    return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex a7491285..1550042d 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -203,17 +202,53 @@ public class CategoricalCrossentropy extends Loss {\n       Reduction reduction,\n       int axis) {\n     super(tf, name, reduction);\n+    if(labelSmoothing < 0 || labelSmoothing > 1)\n+      throw new IllegalArgumentException(\"labelSmoothing must be >= 0. and <= 1, found \" + labelSmoothing);\n     this.fromLogits = fromLogits;\n     this.labelSmoothing = labelSmoothing;\n     this.axis = axis;\n   }\n \n-  /** {@inheritDoc} */\n+  /**\n+   * Generates an Operand that calculates the loss.\n+   *\n+   * If run in Graph mode, the computation will throw {@link org.tensorflow.exceptions.TFInvalidArgumentException}\n+   * if the predictions values are outside the range o [0. to 1.]. In Eager Mode, this call\n+   * will throw {@link IllegalArgumentException}, if the predictions values are outside the range o [0. to 1.]\n+   *\n+   * @param labels the truth values or labels\n+   * @param predictions the predictions, values must be in the range [0. to 1.] inclusive.\n+   * @param sampleWeights Optional sample_weight acts as a coefficient for the loss. If a scalar is\n+   *     provided, then the loss is simply scaled by the given value. If sample_weight is a tensor\n+   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by the\n+   *     corresponding element in the sample_weight vector. If the shape of sample_weight is\n+   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of\n+   *     predictions is scaled by the corresponding value of sample_weight. (Note on dN-1: all loss\n+   *     functions reduce by 1 dimension, usually axis=-1.)\n+   * @param <T> The data type of the predictions, sampleWeights and loss.\n+   * @param <U> The data type of the labels.\n+   * @return the loss\n+   * @throws IllegalArgumentException if the predictions are outside the range [0.-1.].\n+   */\n   @Override\n   public <T extends TNumber, U extends TNumber> Operand<T> call(\n           Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+    Operand<T> lPredictions;\n+    if (!fromLogits) {\n+      // add predictions range check for 0 - 1\n+      lPredictions =\n+              LossesImpl.rangeCheck(\n+                      getTF(),\n+                      \"predictions range check [0-1]\",\n+                      predictions,\n+                      getTF().dtypes.cast(getTF().constant(0), predictions.asOutput().dataType()),\n+                      getTF().dtypes.cast(getTF().constant(1), predictions.asOutput().dataType()));\n+\n+    } else {\n+      lPredictions = predictions;\n+    }\n     Operand<T> losses =\n-        Losses.categoricalCrossentropy(getTF(), labels, predictions, fromLogits, labelSmoothing, axis);\n+        Losses.categoricalCrossentropy(getTF(), labels, lPredictions, fromLogits, labelSmoothing, axis);\n     return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 1550042d..e6665ddc 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -237,18 +253,18 @@ public class CategoricalCrossentropy extends Loss {\n     if (!fromLogits) {\n       // add predictions range check for 0 - 1\n       lPredictions =\n-              LossesImpl.rangeCheck(\n+              LossesHelper.rangeCheck(\n                       getTF(),\n                       \"predictions range check [0-1]\",\n                       predictions,\n-                      getTF().dtypes.cast(getTF().constant(0), predictions.asOutput().dataType()),\n-                      getTF().dtypes.cast(getTF().constant(1), predictions.asOutput().dataType()));\n+                      cast(getTF(), getTF().constant(0), predictions.asOutput().dataType()),\n+                      cast(getTF(), getTF().constant(1), predictions.asOutput().dataType()));\n \n     } else {\n       lPredictions = predictions;\n     }\n     Operand<T> losses =\n         Losses.categoricalCrossentropy(getTF(), labels, lPredictions, fromLogits, labelSmoothing, axis);\n-    return LossesImpl.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n+    return LossesHelper.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": {"commit": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex e6665ddc..3306d16b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -248,23 +248,24 @@ public class CategoricalCrossentropy extends Loss {\n    */\n   @Override\n   public <T extends TNumber, U extends TNumber> Operand<T> call(\n-          Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+      Operand<U> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n     Operand<T> lPredictions;\n     if (!fromLogits) {\n       // add predictions range check for 0 - 1\n       lPredictions =\n-              LossesHelper.rangeCheck(\n-                      getTF(),\n-                      \"predictions range check [0-1]\",\n-                      predictions,\n-                      cast(getTF(), getTF().constant(0), predictions.asOutput().dataType()),\n-                      cast(getTF(), getTF().constant(1), predictions.asOutput().dataType()));\n+          LossesHelper.rangeCheck(\n+              getTF(),\n+              \"predictions range check [0-1]\",\n+              predictions,\n+              cast(getTF(), getTF().constant(0), predictions.asOutput().dataType()),\n+              cast(getTF(), getTF().constant(1), predictions.asOutput().dataType()));\n \n     } else {\n       lPredictions = predictions;\n     }\n     Operand<T> losses =\n-        Losses.categoricalCrossentropy(getTF(), labels, lPredictions, fromLogits, labelSmoothing, axis);\n+        Losses.categoricalCrossentropy(\n+            getTF(), labels, lPredictions, fromLogits, labelSmoothing, axis);\n     return LossesHelper.computeWeightedLoss(getTF(), losses, getReduction(), sampleWeights);\n   }\n }\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502809972", "body": "How would you feel about a line break after the whole `tf.math.abs(...)`, to make it easier to scan the parameters of `tf.math.mean`?", "bodyText": "How would you feel about a line break after the whole tf.math.abs(...), to make it easier to scan the parameters of tf.math.mean?", "bodyHTML": "<p dir=\"auto\">How would you feel about a line break after the whole <code>tf.math.abs(...)</code>, to make it easier to scan the parameters of <code>tf.math.mean</code>?</p>", "author": "deansher", "createdAt": "2020-10-10T16:56:11Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjc1Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206753", "bodyText": "minor -- we'll call it Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgwOTk3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -32,14 +32,14 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute error\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -38,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -53,7 +53,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -52,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf, labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMTM4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502811386", "body": "In current invocations of these constructors, the `target` argument always comes from a variable called `predictions`.", "bodyText": "In current invocations of these constructors, the target argument always comes from a variable called predictions.", "bodyHTML": "<p dir=\"auto\">In current invocations of these constructors, the <code>target</code> argument always comes from a variable called <code>predictions</code>.</p>", "author": "deansher", "createdAt": "2020-10-10T17:12:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ *\n+ * @param <T> the data type of the Tuple entries.\n+ */\n+public class Tuple<T extends TNumber> {\n+  private final Operand<T> labels;\n+  private final Operand<T> target;\n+  private final Operand<T> sampleWeights;\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param target the losses or target\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> target) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\nsimilarity index 65%\nrename from tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java\nrename to tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\nindex 402cac96..596fb31c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/Tuple.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossTuple.java\n", "chunk": "@@ -4,33 +4,33 @@ import org.tensorflow.Operand;\n import org.tensorflow.types.family.TNumber;\n \n /**\n- * A helper class for loss methods to return multiple labels, target, and sampleWeights\n+ * A helper class for loss methods to return  labels, target, and sampleWeights\n  *\n- * @param <T> the data type of the Tuple entries.\n+ * @param <T> the data type of the LossTuple entries.\n  */\n-public class Tuple<T extends TNumber> {\n+public class LossTuple<T extends TNumber> {\n   private final Operand<T> labels;\n   private final Operand<T> target;\n   private final Operand<T> sampleWeights;\n \n   /**\n-   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   * Creates a LossTuple of Operands for labels, target, and sampleWeights\n    *\n    * @param labels the labels\n    * @param target the losses or target\n    */\n-  public Tuple(Operand<T> labels, Operand<T> target) {\n+  public LossTuple(Operand<T> labels, Operand<T> target) {\n     this(labels, target, null);\n   }\n \n   /**\n-   * Creates a Tuple of Operands for labels, target, and sampleWeights\n+   * Creates a LossTuple of Operands for labels, target, and sampleWeights\n    *\n    * @param labels the labels\n    * @param target the losses or target\n    * @param sampleWeights the sample weights\n    */\n-  public Tuple(Operand<T> labels, Operand<T> target, Operand<T> sampleWeights) {\n+  public LossTuple(Operand<T> labels, Operand<T> target, Operand<T> sampleWeights) {\n     this.labels = labels;\n     this.target = target;\n     this.sampleWeights = sampleWeights;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812240", "body": "This is the first of these methods where we used (`target`, `output`) instead of (`labels`, `predictions`).", "bodyText": "This is the first of these methods where we used (target, output) instead of (labels, predictions).", "bodyHTML": "<p dir=\"auto\">This is the first of these methods where we used (<code>target</code>, <code>output</code>) instead of (<code>labels</code>, <code>predictions</code>).</p>", "author": "deansher", "createdAt": "2020-10-10T17:22:42Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjk3Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206976", "bodyText": "minor -- we'll call it Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjI0MA=="}], "type": "inlineReview", "revised_code": {"commit": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..cb6baa8c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -143,19 +143,19 @@ public class Losses {\n       Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n     if (labelSmoothing != 0.0f) {\n       tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n     }\n-    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n     return tf.math.mean(bce, tf.constant(-1));\n   }\n \n   /**\n-   * Compute binary crossentropy loss between labels and predictions.\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n    *\n    * @param tf the TensorFlow Ops\n    * @param target the target Operand\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502812533", "body": "`sub` will do broadcasting if needed. Do we feel good about applying `squeezeOrExpandDimensions` and then subsequent broadcasting? If so, is there a succinct description we could provide for overall treatment of dimensions?", "bodyText": "sub will do broadcasting if needed. Do we feel good about applying squeezeOrExpandDimensions and then subsequent broadcasting? If so, is there a succinct description we could provide for overall treatment of dimensions?", "bodyHTML": "<p dir=\"auto\"><code>sub</code> will do broadcasting if needed. Do we feel good about applying <code>squeezeOrExpandDimensions</code> and then subsequent broadcasting? If so, is there a succinct description we could provide for overall treatment of dimensions?</p>", "author": "deansher", "createdAt": "2020-10-10T17:25:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjM0OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522206348", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMjUzMw=="}], "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -32,14 +32,14 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute error\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -38,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -53,7 +53,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -52,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf, labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813233", "body": "Seems like we should document the required relationships between `labels` and `predictions` and the resulting transformations? (Given our use of `squeezeOrExpandDimensions` followed by broadcasting.)", "bodyText": "Seems like we should document the required relationships between labels and predictions and the resulting transformations? (Given our use of squeezeOrExpandDimensions followed by broadcasting.)", "bodyHTML": "<p dir=\"auto\">Seems like we should document the required relationships between <code>labels</code> and <code>predictions</code> and the resulting transformations? (Given our use of <code>squeezeOrExpandDimensions</code> followed by broadcasting.)</p>", "author": "deansher", "createdAt": "2020-10-10T17:34:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNTk5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522205999", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:44:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzIzMw=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..e3e91a41 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -32,7 +32,7 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute error\n    */\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex e3e91a41..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -39,7 +39,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -38,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -53,7 +53,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -52,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf, labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502813406", "body": "This is the first case in `Losses` where we haven't followed `squeezeOrExpandDimensions` with broadcasting. Do we want to add broadcasting here for consistency, or document the difference?", "bodyText": "This is the first case in Losses where we haven't followed squeezeOrExpandDimensions with broadcasting. Do we want to add broadcasting here for consistency, or document the difference?", "bodyHTML": "<p dir=\"auto\">This is the first case in <code>Losses</code> where we haven't followed <code>squeezeOrExpandDimensions</code> with broadcasting. Do we want to add broadcasting here for consistency, or document the difference?</p>", "author": "deansher", "createdAt": "2020-10-10T17:35:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzE0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207146", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxMzQwNg=="}], "type": "inlineReview", "revised_code": {"commit": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..cb6baa8c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -165,7 +165,7 @@ public class Losses {\n    * @param <T> the data type of the Operands\n    * @return the binary crossentropy loss.\n    */\n-  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n       Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n     if (fromLogits) {\n       return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -171,16 +171,19 @@ public class Losses {\n       return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n     }\n \n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n     if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n-      // TODO - this does not work, cannot walk back, work around is only go back 1.\n-      // output = backtrackIdentity(output);\n-      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n-        if (output.op().numOutputs() != 1)\n-          throw new IllegalArgumentException(\"output can only have 1 output\");\n-        output = output.op().output(0);\n-        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n-      }\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n     }\n+    */\n+\n     DataType<T> dataType = output.asOutput().dataType();\n     Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n     Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -185,8 +199,8 @@ public class Losses {\n     */\n \n     DataType<T> dataType = output.asOutput().dataType();\n-    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n-    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n \n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -199,8 +195,8 @@ public class Losses {\n     */\n \n     DataType<T> dataType = output.asOutput().dataType();\n-    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n-    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf, tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf, tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n \n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815067", "body": "Although the documentation of `softmaxCrossEntropyWithLogits` doesn't specify, I imagine it doesn't do broadcasting. So this would be another method in `Losses` that does `squeezeOrExpandDimensions` but does not broadcast.", "bodyText": "Although the documentation of softmaxCrossEntropyWithLogits doesn't specify, I imagine it doesn't do broadcasting. So this would be another method in Losses that does squeezeOrExpandDimensions but does not broadcast.", "bodyHTML": "<p dir=\"auto\">Although the documentation of <code>softmaxCrossEntropyWithLogits</code> doesn't specify, I imagine it doesn't do broadcasting. So this would be another method in <code>Losses</code> that does <code>squeezeOrExpandDimensions</code> but does not broadcast.</p>", "author": "deansher", "createdAt": "2020-10-10T17:53:59Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzMyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207327", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTA2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -222,7 +222,7 @@ public class Losses {\n       int axis) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -221,7 +237,7 @@ public class Losses {\n       float labelSmoothing,\n       int axis) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -238,7 +238,7 @@ public class Losses {\n       int axis) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = cast(tf,  labels, dataType);\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -237,13 +233,13 @@ public class Losses {\n       float labelSmoothing,\n       int axis) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    Operand<T> tLabels = cast(tf, labels, dataType);\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n     if (labelSmoothing != 0.0f) {\n-      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+      tLabels = smoothCategoricalLabels(tf, tLabels, labelSmoothing);\n     }\n     if (fromLogits) {\n       return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815385", "body": "Although in this internal case of this method, we do broadcast. I'll stop commenting on this issue.", "bodyText": "Although in this internal case of this method, we do broadcast. I'll stop commenting on this issue.", "bodyHTML": "<p dir=\"auto\">Although in this internal case of this method, we do broadcast. I'll stop commenting on this issue.</p>", "author": "deansher", "createdAt": "2020-10-10T17:57:21Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNzQ5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522207496", "bodyText": "We can return to the \"squeezeOrExpandDimensions followed by broadcasting\" topic when I work on #130 .\nResolved.", "author": "deansher", "createdAt": "2020-11-12T15:46:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTM4NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502815866", "body": "Do we want to avoid this cast in the case where `labels` already has the same data type?", "bodyText": "Do we want to avoid this cast in the case where labels already has the same data type?", "bodyHTML": "<p dir=\"auto\">Do we want to avoid this cast in the case where <code>labels</code> already has the same data type?</p>", "author": "deansher", "createdAt": "2020-10-10T18:03:36Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzNDEzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502934136", "bodyText": "I guess the question is what is the overhead of casting onto oneself vs the overhead of checking?  I would hope that tf.dtypes.cast already handles this, but I could be mistaken.", "author": "JimClarke5", "createdAt": "2020-10-11T16:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI3MTM0OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503271348", "bodyText": "The code for checking could be something like this:\n@SuppressWarnings(\"unchecked\")\nprivate static <T extends TNumber, U extends TNumber> Operand<T> castIfNecessary(\n    Operand<U> value, DataType<T> requiredType) {\n  return (value.asOutput().dataType() == requiredType) \n      ? (Operand<T>) value\n      : tf.dtypes.cast(value, requiredType);\n}\nSo the overhead of checking would be the function call plus value.asOutput().dataType() == requiredType.\nLooking at the code for tf.dtypes.cast, unless we think a cast is almost always needed, it would be cheaper to do the check to sometimes avoid it.\n  public <U extends TType, T extends TType> Cast<U> cast(Operand<T> x, DataType<U> DstT,\n      Cast.Options... options) {\n    return Cast.create(scope, x, DstT, options);\n  }\n\n  @Endpoint(describeByClass = true)\n  public static <U extends TType, T extends TType> Cast<U> create(Scope scope, Operand<T> x, DataType<U> DstT, Options... options) {\n    OperationBuilder opBuilder = scope.env().opBuilder(\"Cast\", scope.makeOpName(\"Cast\"));\n    opBuilder.addInput(x.asOutput());\n    opBuilder = scope.applyControlDependencies(opBuilder);\n    opBuilder.setAttr(\"DstT\", DstT);\n    if (options != null) {\n      for (Options opts : options) {\n        if (opts.Truncate != null) {\n          opBuilder.setAttr(\"Truncate\", opts.Truncate);\n        }\n      }\n    }\n    return new Cast<U>(opBuilder.build());\n  }", "author": "deansher", "createdAt": "2020-10-12T12:46:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Mjc2Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492766", "bodyText": "In graph construction mode the overhead is probably irrelevant because it's only called once during construction. In eager mode it could be faster as it could sidestep a JNI call in each step, but I suspect we've got other issues to get speed in eager mode.", "author": "Craigacp", "createdAt": "2020-10-24T17:05:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1MzE1MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511653151", "bodyText": "I like castIfNecessary as a general util method. It would be used almost everywhere, so it would be a huge change.\nPerhaps create a new PR for castIfNecessary, then once that is merged we can start retrofitting all packages under framework.", "author": "JimClarke5", "createdAt": "2020-10-25T21:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg5MjYyNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511892627", "bodyText": "In graph construction mode, an unnecessary call to cast creates an unnecessary graph operation.", "author": "deansher", "createdAt": "2020-10-26T11:31:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3NDcxNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512074714", "bodyText": "shrug it'll be a no-op most of the time and compiled away if we get XLA working. Given the relative size of the computation around it I suspect it won't be an issue.", "author": "Craigacp", "createdAt": "2020-10-26T15:59:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2Mzc0OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513863749", "bodyText": "I also vote for a explicit check in the code to avoid adding an extra operation to the graph when it is not required", "author": "karllessard", "createdAt": "2020-10-29T01:34:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMxNzY3OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r514317679", "bodyText": "OK, I will add a helper class in org.tensorflow.framework.utils, then retrofit the Loss classes.", "author": "JimClarke5", "createdAt": "2020-10-29T14:48:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMzMDA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r514330067", "bodyText": "Just a comment on @deansher proposed method here, the datatypes for <U> and <T> should not be restricted to TNumber because it is valid to cast to/from TNumber and TBool .", "author": "JimClarke5", "createdAt": "2020-10-29T15:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNTg2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..e3e91a41 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -32,7 +32,7 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute error\n    */\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex e3e91a41..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -39,7 +39,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -38,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -53,7 +53,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     return tf.math.mean(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -52,7 +52,7 @@ public class Losses {\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n-    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = cast(tf, labels, predictions.asOutput().dataType());\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjE5Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502816197", "body": "Can just use `dataType`.", "bodyText": "Can just use dataType.", "bodyHTML": "<p dir=\"auto\">Can just use <code>dataType</code>.</p>", "author": "deansher", "createdAt": "2020-10-10T18:06:57Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzNDE0OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502934149", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-11T16:02:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjE5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..e3e91a41 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -75,14 +75,14 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean absolute percentage error\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n     Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex e3e91a41..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -83,7 +83,7 @@ public class Losses {\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     Operand<T> diff =\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -82,7 +82,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -82,7 +96,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..ba641d19 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -97,7 +97,7 @@ public class Losses {\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = cast(tf,  labels, dataType);\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     Operand<T> diff =\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -96,7 +96,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    Operand<T> tLabels = cast(tf, labels, dataType);\n     LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": null}, {"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -104,10 +104,8 @@ public class Losses {\n         tf.math.abs(\n             tf.math.div(\n                 tf.math.sub(tLabels, predictions),\n-                tf.math.maximum(\n-                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n-    return tf.math.mul(\n-        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+                tf.math.maximum(tf.math.abs(tLabels), cast(tf, tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(cast(tf, tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n   }\n \n   /**\n", "next_change": null}]}}]}}, {"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -91,9 +105,9 @@ public class Losses {\n             tf.math.div(\n                 tf.math.sub(tLabels, predictions),\n                 tf.math.maximum(\n-                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n     return tf.math.mul(\n-        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n   }\n \n   /**\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -96,18 +96,16 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = cast(tf,  labels, dataType);\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    Operand<T> tLabels = cast(tf, labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n     Operand<T> diff =\n         tf.math.abs(\n             tf.math.div(\n                 tf.math.sub(tLabels, predictions),\n-                tf.math.maximum(\n-                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n-    return tf.math.mul(\n-        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+                tf.math.maximum(tf.math.abs(tLabels), cast(tf, tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(cast(tf, tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n   }\n \n   /**\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502818616", "body": "I tripped over this private method having the usual naming of a loss method, since I didn't notice that it was private and so expected it to follow the conventions of public loss methods, such as invoking `squeezeOrExpandDimensions`.  Also (if I'm navigating accurately through unfamiliar territory), this method doesn't compute a `binaryCrossentropy` since it depends on its caller to compute the mean at the end.", "bodyText": "I tripped over this private method having the usual naming of a loss method, since I didn't notice that it was private and so expected it to follow the conventions of public loss methods, such as invoking squeezeOrExpandDimensions.  Also (if I'm navigating accurately through unfamiliar territory), this method doesn't compute a binaryCrossentropy since it depends on its caller to compute the mean at the end.", "bodyHTML": "<p dir=\"auto\">I tripped over this private method having the usual naming of a loss method, since I didn't notice that it was private and so expected it to follow the conventions of public loss methods, such as invoking <code>squeezeOrExpandDimensions</code>.  Also (if I'm navigating accurately through unfamiliar territory), this method doesn't compute a <code>binaryCrossentropy</code> since it depends on its caller to compute the mean at the end.</p>", "author": "deansher", "createdAt": "2020-10-10T18:31:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropy(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Compute binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropy(", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTM4Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951386", "bodyText": "This method does the grunt work for the binaryCrossentropy after the operands have had their shapes and types manipulated and after smoothing the labels. Perhaps a new name would remove some of the confusion.", "author": "JimClarke5", "createdAt": "2020-10-11T18:45:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI3MTg0NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503271845", "bodyText": "Yes, I wonder if we want to call it something like binaryCrossentropyHelper?", "author": "deansher", "createdAt": "2020-10-12T12:47:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNDQ5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504134493", "bodyText": "OK, Changed", "author": "JimClarke5", "createdAt": "2020-10-13T17:30:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxODYxNg=="}], "type": "inlineReview", "revised_code": {"commit": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..cb6baa8c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -165,7 +165,7 @@ public class Losses {\n    * @param <T> the data type of the Operands\n    * @return the binary crossentropy loss.\n    */\n-  private static <T extends TNumber> Operand<T> binaryCrossentropy(\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n       Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n     if (fromLogits) {\n       return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -171,16 +171,19 @@ public class Losses {\n       return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n     }\n \n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n     if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n-      // TODO - this does not work, cannot walk back, work around is only go back 1.\n-      // output = backtrackIdentity(output);\n-      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n-        if (output.op().numOutputs() != 1)\n-          throw new IllegalArgumentException(\"output can only have 1 output\");\n-        output = output.op().output(0);\n-        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n-      }\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n     }\n+    */\n+\n     DataType<T> dataType = output.asOutput().dataType();\n     Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n     Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -185,8 +199,8 @@ public class Losses {\n     */\n \n     DataType<T> dataType = output.asOutput().dataType();\n-    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n-    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n \n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -199,8 +195,8 @@ public class Losses {\n     */\n \n     DataType<T> dataType = output.asOutput().dataType();\n-    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n-    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf, tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf, tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n \n", "next_change": null}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502819700", "body": "Should we follow the Python in documenting that labels are expected to be `0` or `1`?", "bodyText": "Should we follow the Python in documenting that labels are expected to be 0 or 1?", "bodyHTML": "<p dir=\"auto\">Should we follow the Python in documenting that labels are expected to be <code>0</code> or <code>1</code>?</p>", "author": "deansher", "createdAt": "2020-10-10T18:43:05Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg0MDk5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502840999", "bodyText": "Yes, The Python CategporicalHinge class does not mention that at all, but it is mentioned in the categorical_hinge method.\nI have added an entry to the class JavaDoc and to the Losses.categoricalHinge method.", "author": "JimClarke5", "createdAt": "2020-10-10T22:53:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjcyODgxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512728819", "bodyText": "Actually the values can be [-1, 0, 1]. [0,1] is converted to [-1,1]. I have added a value check to make sure the values are wholly contained in the allowed values set  [-1, 0, 1]. This will either throw TFInvalidArgumentException if run in Graph mode via a control dependency, and throw IllegalArgumentException if created in Eager mode with the call method.", "author": "JimClarke5", "createdAt": "2020-10-27T14:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwMTYxMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522201610", "bodyText": "Cool -- Resolved.", "author": "deansher", "createdAt": "2020-11-12T15:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3NzUwNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522577504", "bodyText": "What does it do if there are all three of [-1 0 1] present? That's probably an invalid input, does it throw?", "author": "Craigacp", "createdAt": "2020-11-13T02:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxOTcwMA=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java\nindex 6c828fd2..2b1f1e04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java\n", "chunk": "@@ -11,6 +11,8 @@ import org.tensorflow.types.family.TNumber;\n  * <p><code>loss = maximum(neg - pos + 1, 0)</code> where <code>neg=maximum((1-labels)*predictions)\n  * </code> and <code>pos=sum(labels*predictions)</code>\n  *\n+ * <p><code>labels</code> values are expected to be 0 or 1.</p>\n+ *\n  * <p>Standalone usage:\n  *\n  * <pre>\n", "next_change": null}]}}, {"oid": "ee1c48a443810260be7319caab94bde8a3dae529", "url": "https://github.com/tensorflow/java/commit/ee1c48a443810260be7319caab94bde8a3dae529", "message": "Apply various fixes to JavaDoc", "committedDate": "2020-10-11T16:05:56Z", "type": "commit"}, {"oid": "287c96e34eea177303716e6a2b72509c2c749333", "url": "https://github.com/tensorflow/java/commit/287c96e34eea177303716e6a2b72509c2c749333", "message": "Change Tuple to LossTuple", "committedDate": "2020-10-11T16:46:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODI5Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918292", "body": "Could just use `dataType`. I'll stop mentioning this.", "bodyText": "Could just use dataType. I'll stop mentioning this.", "bodyHTML": "<p dir=\"auto\">Could just use <code>dataType</code>. I'll stop mentioning this.</p>", "author": "deansher", "createdAt": "2020-10-11T13:50:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTY0NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951644", "bodyText": "Fixed, hopefully I have fixed them all.", "author": "JimClarke5", "createdAt": "2020-10-11T18:48:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODI5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..e3e91a41 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -104,7 +104,7 @@ public class Losses {\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean squared logarithmic percentage error\n    */\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex e3e91a41..36c04fb2 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -112,7 +112,7 @@ public class Losses {\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n     Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n", "next_change": {"commit": "642069c34d9e6b6c3df92cab4672c315029555de", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 36c04fb2..258c761a 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -111,7 +111,7 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 258c761a..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -111,13 +125,13 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n-    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n-    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n \n     Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n     Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -125,13 +123,13 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = cast(tf,  labels, dataType);\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    Operand<T> tLabels = cast(tf, labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n-    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n-    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf, tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf, tf.constant(1), dataType);\n \n     Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n     Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n", "next_change": null}]}}]}}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODQ1Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502918452", "body": "I think \"percentage\" is extraneous here.", "bodyText": "I think \"percentage\" is extraneous here.", "bodyHTML": "<p dir=\"auto\">I think \"percentage\" is extraneous here.</p>", "author": "deansher", "createdAt": "2020-10-11T13:52:06Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,683 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.Tuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic percentage error between labels and predictions.", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTc5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502951793", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-11T18:49:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkxODQ1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "642069c34d9e6b6c3df92cab4672c315029555de", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 9eeab135..258c761a 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -97,22 +97,22 @@ public class Losses {\n   }\n \n   /**\n-   * Calculates the mean squared logarithmic percentage error between labels and predictions.\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n    *\n    * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n    *\n    * @param tf The TensorFlow Ops\n    * @param labels the labels\n    * @param predictions the predictions\n-   * @param <T> the data type of the result\n+   * @param <T> the data type of the predictions and result\n    * @param <U> the data type of the labels\n    * @return the mean squared logarithmic percentage error\n    */\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n-    Tuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 258c761a..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -111,13 +125,13 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n     LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n-    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n-    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n \n     Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n     Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -125,13 +123,13 @@ public class Losses {\n   public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n       Ops tf, Operand<U> labels, Operand<T> predictions) {\n     DataType<T> dataType = predictions.asOutput().dataType();\n-    Operand<T> tLabels = cast(tf,  labels, dataType);\n-    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    Operand<T> tLabels = cast(tf, labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n     predictions = ops.getTarget();\n     tLabels = ops.getLabels();\n \n-    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n-    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf, tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf, tf.constant(1), dataType);\n \n     Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n     Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNDUyNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502924525", "body": "If the rank is unknown, then the size of the last dimension is guaranteed to be unknown, so `isCompatible` is guaranteed true. (But there may be some idiomatic reason for writing it this way, of which I am blissfully unaware.)", "bodyText": "If the rank is unknown, then the size of the last dimension is guaranteed to be unknown, so isCompatible is guaranteed true. (But there may be some idiomatic reason for writing it this way, of which I am blissfully unaware.)", "bodyHTML": "<p dir=\"auto\">If the rank is unknown, then the size of the last dimension is guaranteed to be unknown, so <code>isCompatible</code> is guaranteed true. (But there may be some idiomatic reason for writing it this way, of which I am blissfully unaware.)</p>", "author": "deansher", "createdAt": "2020-10-11T14:42:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MzUxMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502953511", "bodyText": "Correct, it should have been or not and.", "author": "JimClarke5", "createdAt": "2020-10-11T19:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNDUyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -183,7 +191,7 @@ public class LossesImpl {\n       } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n         labels = tf.squeeze(labels);\n       }\n-      return new Tuple<>(labels, predictions);\n+      return new LossTuple<>(labels, predictions);\n     }\n     // Use dynamic rank.\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjYwMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502926601", "body": "I'm pretty sure this logic is wrong. Perhaps either\r\n* document preconditions of `removeSqueezableDimensions` and check exactly those,\r\n* or (my leaning) just invoke `removeSqueezableDimensions` and make it however smart it needs to be.", "bodyText": "I'm pretty sure this logic is wrong. Perhaps either\n\ndocument preconditions of removeSqueezableDimensions and check exactly those,\nor (my leaning) just invoke removeSqueezableDimensions and make it however smart it needs to be.", "bodyHTML": "<p dir=\"auto\">I'm pretty sure this logic is wrong. Perhaps either</p>\n<ul dir=\"auto\">\n<li>document preconditions of <code>removeSqueezableDimensions</code> and check exactly those,</li>\n<li>or (my leaning) just invoke <code>removeSqueezableDimensions</code> and make it however smart it needs to be.</li>\n</ul>", "author": "deansher", "createdAt": "2020-10-11T14:59:53Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzODMyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504138328", "bodyText": "This logic is checking to see if both objects ranks are known (not Shape.unknown()). If both ranks are known, then it checks to see if the shapes are already in the right relationship or not. If not in the right relationship, then call removeSqueezableDimensions. It is basically an optimization to avoid doing the work in removeSqueezableDimensions if it does not need to be done.", "author": "JimClarke5", "createdAt": "2020-10-13T17:37:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNjYwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,36 +43,39 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return Tuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n   public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    Tuple<T> tuple = new Tuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n           tuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -77,19 +77,19 @@ public class LossesImpl {\n         // Use static rank for 'label' and 'prediction'.\n         if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n     if (sampleWeights == null) { // nothing more to do.\n-      return tuple;\n+      return lossTuple;\n     }\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -89,7 +88,7 @@ public class LossesImpl {\n     Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new LossTuple<>(labels, predictions, sampleWeights);\n+      return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzEwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927109", "body": "Specify dimension `-1L`. Also, I'd advocate doing our own check that the last dimension of `sampleWeight` has size `1`. The Python documentation for `tf.squeeze` says that, if axes are specified, then \"it is an error to squeeze a dimension that is not 1.\"", "bodyText": "Specify dimension -1L. Also, I'd advocate doing our own check that the last dimension of sampleWeight has size 1. The Python documentation for tf.squeeze says that, if axes are specified, then \"it is an error to squeeze a dimension that is not 1.\"", "bodyHTML": "<p dir=\"auto\">Specify dimension <code>-1L</code>. Also, I'd advocate doing our own check that the last dimension of <code>sampleWeight</code> has size <code>1</code>. The Python documentation for <code>tf.squeeze</code> says that, if axes are specified, then \"it is an error to squeeze a dimension that is not 1.\"</p>", "author": "deansher", "createdAt": "2020-10-11T15:03:28Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -77,33 +83,33 @@ public class LossesImpl {\n         tuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n-    if (sampleWeight == null) {\n+    if (sampleWeights == null) { // nothing more to do.\n       return tuple;\n     }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n+    Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new Tuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n \n       if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n+        sampleWeights = tf.squeeze(sampleWeights);\n       } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+        sampleWeights = tf.expandDims(sampleWeights, tf.constant(-1L));\n       }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new Tuple<>(labels, predictions, sampleWeights);\n     }\n     // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeights);\n     Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n+    sampleWeights =\n         tf.select(\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n+            sampleWeights,\n+            maybeAdjustWeights(tf, sampleWeights, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +109,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +108,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new LossTuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n   }\n \n   /**\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyNzgzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502927830", "body": "Do we also have to go dynamic in the case where the ranks are both known but the size of the last weight dimension is unknown?", "bodyText": "Do we also have to go dynamic in the case where the ranks are both known but the size of the last weight dimension is unknown?", "bodyHTML": "<p dir=\"auto\">Do we also have to go dynamic in the case where the ranks are both known but the size of the last weight dimension is unknown?</p>", "author": "deansher", "createdAt": "2020-10-11T15:09:18Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee1c48a443810260be7319caab94bde8a3dae529", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..089e264e 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -77,33 +83,33 @@ public class LossesImpl {\n         tuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n-    if (sampleWeight == null) {\n+    if (sampleWeights == null) { // nothing more to do.\n       return tuple;\n     }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n+    Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new Tuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n \n       if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n+        sampleWeights = tf.squeeze(sampleWeights);\n       } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+        sampleWeights = tf.expandDims(sampleWeights, tf.constant(-1L));\n       }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new Tuple<>(labels, predictions, sampleWeights);\n     }\n     // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeights);\n     Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n+    sampleWeights =\n         tf.select(\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n+            sampleWeights,\n+            maybeAdjustWeights(tf, sampleWeights, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 089e264e..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +109,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +108,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new LossTuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n   }\n \n   /**\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODE4NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928184", "body": "What, if anything, do we want to do with the possibility that the last dimension of `sampleWeight` may not have size `1`? (The Python documention for `tf.squeeze` says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of sampleWeight may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "bodyHTML": "<p dir=\"auto\">What, if anything, do we want to do with the possibility that the last dimension of <code>sampleWeight</code> may not have size <code>1</code>? (The Python documention for <code>tf.squeeze</code> says that if axes are provided, is an error to squeeze a dimension that is not 1.)</p>", "author": "deansher", "createdAt": "2020-10-11T15:11:55Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\ndeleted file mode 100644\nindex eb803256..00000000\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ /dev/null\n", "chunk": "@@ -1,304 +0,0 @@\n-package org.tensorflow.framework.losses.impl;\n-\n-import org.tensorflow.DataType;\n-import org.tensorflow.Operand;\n-import org.tensorflow.framework.losses.Reduction;\n-import org.tensorflow.ndarray.Shape;\n-import org.tensorflow.op.Ops;\n-import org.tensorflow.op.core.ReduceSum;\n-import org.tensorflow.op.core.Squeeze;\n-import org.tensorflow.types.TInt32;\n-import org.tensorflow.types.family.TNumber;\n-\n-import java.util.Collections;\n-\n-public class LossesImpl {\n-\n-  /**\n-   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n-   *\n-   * <ol type=\"1\">\n-   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n-   *       {@link #removeSqueezableDimensions}).\n-   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n-   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n-   * </ol>\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n-   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n-   *     is null, (prediction, label) is returned.\n-   */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions) {\n-    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n-  }\n-\n-  /**\n-   * Squeeze or expand last dimension if needed.\n-   *\n-   * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n-   * </ol>\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n-   *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n-   *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n-   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n-   *     returned.\n-   */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n-\n-    Shape predictionsShape = predictions.asOutput().shape();\n-    long predictionsRank = predictionsShape.numDimensions();\n-\n-    if (labels != null) {\n-      Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n-          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n-        }\n-      } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n-      }\n-    }\n-    if (sampleWeight == null) {\n-      return tuple;\n-    }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n-    long weightsRank = weightsShape.numDimensions();\n-    if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n-    }\n-\n-    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n-\n-      if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n-      } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n-      }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n-    }\n-    // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n-    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n-        tf.select(\n-            tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n-  }\n-\n-  /**\n-   * Squeeze or expand the sampleWeight based on the rank difference\n-   *\n-   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n-   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n-   * sampleWeight as is.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param sampleWeight the sample weights\n-   * @param rankDiff the difference in rank\n-   * @param <T> the data type for the Operands.\n-   * @return the adjusted sampleWeight\n-   */\n-  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n-      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n-    return tf.select(\n-        tf.math.equal(rankDiff, tf.constant(1)),\n-        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n-        maybeExpandWeights(tf, sampleWeight, rankDiff));\n-  }\n-\n-  /**\n-   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param sampleWeight the sample weights\n-   * @param rankDiff the difference in rank\n-   * @param <T> the data type for the Operands.\n-   * @return the adjusted sampleWeight\n-   */\n-  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n-      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n-    return tf.select(\n-        tf.math.equal(rankDiff, tf.constant(-1)),\n-        tf.expandDims(sampleWeight, tf.constant(-1)),\n-        sampleWeight);\n-  }\n-\n-  /**\n-   * Squeeze last dim if ranks differ from expected by exactly 1.\n-   *\n-   * @param tf the TensorFlowOps\n-   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n-   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n-   * @return `labels` and `predictions`, possibly with last dim squeezed.\n-   */\n-  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions) {\n-    return removeSqueezableDimensions(tf, labels, predictions, 0);\n-  }\n-\n-  /**\n-   * Squeeze last dim if ranks differ from expected by exactly 1.\n-   *\n-   * @param tf the TensorFlowOps\n-   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n-   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n-   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n-   * @return `labels` and `predictions`, possibly with last dim squeezed.\n-   */\n-  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n-\n-    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n-    Shape predictionsShape = predictions.asOutput().shape();\n-    int predictionsRank = predictionsShape.numDimensions();\n-    Shape labelsShape = labels.asOutput().shape();\n-    int labelsRank = labelsShape.numDimensions();\n-\n-    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n-      // Use static rank.\n-      int rankDiff = predictionsRank - labelsRank;\n-      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n-        predictions = tf.squeeze(predictions);\n-      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n-        labels = tf.squeeze(labels);\n-      }\n-      return new Tuple<>(labels, predictions);\n-    }\n-    // Use dynamic rank.\n-\n-    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n-    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n-      /*\n-       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n-       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n-       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n-       */\n-      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n-    }\n-    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n-      /*\n-       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n-       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n-       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n-       */\n-      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n-    }\n-    return new Tuple<>(labels, predictions);\n-  }\n-\n-  /**\n-   * Computes the weighted loss\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param loss the unweighted loss\n-   * @param reduction the type of reduction\n-   * @param sampleWeight the sample weight, if null then this defaults to one.\n-   * @param <T> the data type of the loss\n-   * @return the weighted loss\n-   */\n-  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n-      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n-    DataType<T> dataType = loss.asOutput().dataType();\n-    if (sampleWeight == null) {\n-      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n-    }\n-    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n-    loss = result.getTarget();\n-    sampleWeight = result.getSampleWeights();\n-\n-    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n-    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n-    return tf.dtypes.cast(loss, dataType);\n-  }\n-\n-  /**\n-   * Reduces the weighted loss based on the reduction type\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param weightedLoss the weighted loss\n-   * @param reduction the type of reduction\n-   * @param <T> the data type of the weighted loss\n-   * @return the reduced weighted loss\n-   */\n-  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n-      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n-    Operand<T> loss;\n-    if (reduction == Reduction.NONE) {\n-      loss = weightedLoss;\n-    } else {\n-      loss =\n-          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n-      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n-        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n-      }\n-    }\n-    return loss;\n-  }\n-\n-  /**\n-   * Computes a safe mean of the losses.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n-   * @param numElements The number of measurable elements in <code>losses</code>.\n-   * @param <T> the data type of the losses\n-   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n-   *     zero, then zero is returned.\n-   */\n-  public static <T extends TNumber> Operand<T> safeMean(\n-      Ops tf, Operand<T> losses, long numElements) {\n-    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n-    return tf.math.divNoNan(\n-        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n-  }\n-\n-  /**\n-   * Gets a Constant integer array representing all the axes of the operand.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param op the TensorFlow Ops\n-   * @param <T> the type of Operand\n-   * @return a Constant that represents all the axes of the operand.\n-   */\n-  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n-    int[] ranks = allAxis(op);\n-    return tf.constant(ranks);\n-  }\n-\n-  /**\n-   * Gets an integer array representing all the axes of the operand.\n-   *\n-   * @param op the Operand\n-   * @param <T> the type of Operand\n-   * @return the integer array representing all the axes of the operand.\n-   */\n-  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n-    int rank = op.asOutput().shape().numDimensions();\n-    int[] axes = new int[rank];\n-    for (int i = 0; i < rank; i++) {\n-      axes[i] = i;\n-    }\n-    return axes;\n-  }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODUzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928535", "body": "Need to specify squeezed axis `-1L`. But in that case:\r\n\r\nWhat, if anything, do we want to do with the possibility that the last dimension of `predictions` may not have size `1`? (The Python documention for `tf.squeeze` says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "bodyText": "Need to specify squeezed axis -1L. But in that case:\nWhat, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1? (The Python documention for tf.squeeze says that if axes are provided, is an error to squeeze a dimension that is not 1.)", "bodyHTML": "<p dir=\"auto\">Need to specify squeezed axis <code>-1L</code>. But in that case:</p>\n<p dir=\"auto\">What, if anything, do we want to do with the possibility that the last dimension of <code>predictions</code> may not have size <code>1</code>? (The Python documention for <code>tf.squeeze</code> says that if axes are provided, is an error to squeeze a dimension that is not 1.)</p>", "author": "deansher", "createdAt": "2020-10-11T15:15:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -183,7 +191,7 @@ public class LossesImpl {\n       } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n         labels = tf.squeeze(labels);\n       }\n-      return new Tuple<>(labels, predictions);\n+      return new LossTuple<>(labels, predictions);\n     }\n     // Use dynamic rank.\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODYwOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928609", "body": "Same comments as for `predictions` above.", "bodyText": "Same comments as for predictions above.", "bodyHTML": "<p dir=\"auto\">Same comments as for <code>predictions</code> above.</p>", "author": "deansher", "createdAt": "2020-10-11T15:15:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -183,7 +191,7 @@ public class LossesImpl {\n       } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n         labels = tf.squeeze(labels);\n       }\n-      return new Tuple<>(labels, predictions);\n+      return new LossTuple<>(labels, predictions);\n     }\n     // Use dynamic rank.\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyODg5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502928899", "body": "Do we need to also go fully dynamic in the case where the size of a last dimension is unknown?", "bodyText": "Do we need to also go fully dynamic in the case where the size of a last dimension is unknown?", "bodyHTML": "<p dir=\"auto\">Do we need to also go fully dynamic in the case where the size of a last dimension is unknown?</p>", "author": "deansher", "createdAt": "2020-10-11T15:17:17Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "249b65194bb055decf02d61f56378e7771e6d05f", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..9cc77b50 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -175,7 +182,7 @@ public class LossesImpl {\n     Shape labelsShape = labels.asOutput().shape();\n     int labelsRank = labelsShape.numDimensions();\n \n-    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+    if (predictionsRank != Shape.UNKNOWN_SIZE || labelsRank != Shape.UNKNOWN_SIZE) {\n       // Use static rank.\n       int rankDiff = predictionsRank - labelsRank;\n       if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTE5Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929196", "body": "We do have to verify `tf.math.equal(tf.constant(expectedRankDiff+1), rankDiff)`, right?", "bodyText": "We do have to verify tf.math.equal(tf.constant(expectedRankDiff+1), rankDiff), right?", "bodyHTML": "<p dir=\"auto\">We do have to verify <code>tf.math.equal(tf.constant(expectedRankDiff+1), rankDiff)</code>, right?</p>", "author": "deansher", "createdAt": "2020-10-11T15:19:54Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -183,7 +191,7 @@ public class LossesImpl {\n       } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n         labels = tf.squeeze(labels);\n       }\n-      return new Tuple<>(labels, predictions);\n+      return new LossTuple<>(labels, predictions);\n     }\n     // Use dynamic rank.\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTM4MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929381", "body": "What, if anything, do we want to do with the possibility that the last dimension of `predictions` may not have size `1`? ", "bodyText": "What, if anything, do we want to do with the possibility that the last dimension of predictions may not have size 1?", "bodyHTML": "<p dir=\"auto\">What, if anything, do we want to do with the possibility that the last dimension of <code>predictions</code> may not have size <code>1</code>?</p>", "author": "deansher", "createdAt": "2020-10-11T15:21:01Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\ndeleted file mode 100644\nindex eb803256..00000000\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ /dev/null\n", "chunk": "@@ -1,304 +0,0 @@\n-package org.tensorflow.framework.losses.impl;\n-\n-import org.tensorflow.DataType;\n-import org.tensorflow.Operand;\n-import org.tensorflow.framework.losses.Reduction;\n-import org.tensorflow.ndarray.Shape;\n-import org.tensorflow.op.Ops;\n-import org.tensorflow.op.core.ReduceSum;\n-import org.tensorflow.op.core.Squeeze;\n-import org.tensorflow.types.TInt32;\n-import org.tensorflow.types.family.TNumber;\n-\n-import java.util.Collections;\n-\n-public class LossesImpl {\n-\n-  /**\n-   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n-   *\n-   * <ol type=\"1\">\n-   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n-   *       {@link #removeSqueezableDimensions}).\n-   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n-   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n-   * </ol>\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n-   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n-   *     is null, (prediction, label) is returned.\n-   */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions) {\n-    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n-  }\n-\n-  /**\n-   * Squeeze or expand last dimension if needed.\n-   *\n-   * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n-   * </ol>\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n-   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n-   *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n-   *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n-   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n-   *     returned.\n-   */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n-\n-    Shape predictionsShape = predictions.asOutput().shape();\n-    long predictionsRank = predictionsShape.numDimensions();\n-\n-    if (labels != null) {\n-      Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n-          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n-        }\n-      } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n-      }\n-    }\n-    if (sampleWeight == null) {\n-      return tuple;\n-    }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n-    long weightsRank = weightsShape.numDimensions();\n-    if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n-    }\n-\n-    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n-\n-      if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n-      } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n-      }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n-    }\n-    // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n-    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n-        tf.select(\n-            tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n-  }\n-\n-  /**\n-   * Squeeze or expand the sampleWeight based on the rank difference\n-   *\n-   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n-   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n-   * sampleWeight as is.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param sampleWeight the sample weights\n-   * @param rankDiff the difference in rank\n-   * @param <T> the data type for the Operands.\n-   * @return the adjusted sampleWeight\n-   */\n-  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n-      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n-    return tf.select(\n-        tf.math.equal(rankDiff, tf.constant(1)),\n-        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n-        maybeExpandWeights(tf, sampleWeight, rankDiff));\n-  }\n-\n-  /**\n-   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param sampleWeight the sample weights\n-   * @param rankDiff the difference in rank\n-   * @param <T> the data type for the Operands.\n-   * @return the adjusted sampleWeight\n-   */\n-  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n-      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n-    return tf.select(\n-        tf.math.equal(rankDiff, tf.constant(-1)),\n-        tf.expandDims(sampleWeight, tf.constant(-1)),\n-        sampleWeight);\n-  }\n-\n-  /**\n-   * Squeeze last dim if ranks differ from expected by exactly 1.\n-   *\n-   * @param tf the TensorFlowOps\n-   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n-   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n-   * @return `labels` and `predictions`, possibly with last dim squeezed.\n-   */\n-  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions) {\n-    return removeSqueezableDimensions(tf, labels, predictions, 0);\n-  }\n-\n-  /**\n-   * Squeeze last dim if ranks differ from expected by exactly 1.\n-   *\n-   * @param tf the TensorFlowOps\n-   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n-   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n-   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n-   * @return `labels` and `predictions`, possibly with last dim squeezed.\n-   */\n-  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n-\n-    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n-    Shape predictionsShape = predictions.asOutput().shape();\n-    int predictionsRank = predictionsShape.numDimensions();\n-    Shape labelsShape = labels.asOutput().shape();\n-    int labelsRank = labelsShape.numDimensions();\n-\n-    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n-      // Use static rank.\n-      int rankDiff = predictionsRank - labelsRank;\n-      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n-        predictions = tf.squeeze(predictions);\n-      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n-        labels = tf.squeeze(labels);\n-      }\n-      return new Tuple<>(labels, predictions);\n-    }\n-    // Use dynamic rank.\n-\n-    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n-    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n-      /*\n-       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n-       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n-       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n-       */\n-      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n-    }\n-    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n-      /*\n-       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n-       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n-       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n-       */\n-      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n-    }\n-    return new Tuple<>(labels, predictions);\n-  }\n-\n-  /**\n-   * Computes the weighted loss\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param loss the unweighted loss\n-   * @param reduction the type of reduction\n-   * @param sampleWeight the sample weight, if null then this defaults to one.\n-   * @param <T> the data type of the loss\n-   * @return the weighted loss\n-   */\n-  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n-      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n-    DataType<T> dataType = loss.asOutput().dataType();\n-    if (sampleWeight == null) {\n-      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n-    }\n-    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n-    loss = result.getTarget();\n-    sampleWeight = result.getSampleWeights();\n-\n-    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n-    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n-    return tf.dtypes.cast(loss, dataType);\n-  }\n-\n-  /**\n-   * Reduces the weighted loss based on the reduction type\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param weightedLoss the weighted loss\n-   * @param reduction the type of reduction\n-   * @param <T> the data type of the weighted loss\n-   * @return the reduced weighted loss\n-   */\n-  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n-      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n-    Operand<T> loss;\n-    if (reduction == Reduction.NONE) {\n-      loss = weightedLoss;\n-    } else {\n-      loss =\n-          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n-      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n-        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n-      }\n-    }\n-    return loss;\n-  }\n-\n-  /**\n-   * Computes a safe mean of the losses.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n-   * @param numElements The number of measurable elements in <code>losses</code>.\n-   * @param <T> the data type of the losses\n-   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n-   *     zero, then zero is returned.\n-   */\n-  public static <T extends TNumber> Operand<T> safeMean(\n-      Ops tf, Operand<T> losses, long numElements) {\n-    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n-    return tf.math.divNoNan(\n-        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n-  }\n-\n-  /**\n-   * Gets a Constant integer array representing all the axes of the operand.\n-   *\n-   * @param tf the TensorFlow Ops\n-   * @param op the TensorFlow Ops\n-   * @param <T> the type of Operand\n-   * @return a Constant that represents all the axes of the operand.\n-   */\n-  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n-    int[] ranks = allAxis(op);\n-    return tf.constant(ranks);\n-  }\n-\n-  /**\n-   * Gets an integer array representing all the axes of the operand.\n-   *\n-   * @param op the Operand\n-   * @param <T> the type of Operand\n-   * @return the integer array representing all the axes of the operand.\n-   */\n-  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n-    int rank = op.asOutput().shape().numDimensions();\n-    int[] axes = new int[rank];\n-    for (int i = 0; i < rank; i++) {\n-      axes[i] = i;\n-    }\n-    return axes;\n-  }\n-}\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTQxOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502929418", "body": "Same comments as above for `predictions`.", "bodyText": "Same comments as above for predictions.", "bodyHTML": "<p dir=\"auto\">Same comments as above for <code>predictions</code>.</p>", "author": "deansher", "createdAt": "2020-10-11T15:21:24Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -204,7 +212,7 @@ public class LossesImpl {\n        */\n       labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n     }\n-    return new Tuple<>(labels, predictions);\n+    return new LossTuple<>(labels, predictions);\n   }\n \n   /**\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkzODIwNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502938204", "body": "To use `squeezeOrExpandDimensions` here, we might want to generalize that method so it doesn't think in terms of `predictions` when here we instead pass `loss`.", "bodyText": "To use squeezeOrExpandDimensions here, we might want to generalize that method so it doesn't think in terms of predictions when here we instead pass loss.", "bodyHTML": "<p dir=\"auto\">To use <code>squeezeOrExpandDimensions</code> here, we might want to generalize that method so it doesn't think in terms of <code>predictions</code> when here we instead pass <code>loss</code>.</p>", "author": "deansher", "createdAt": "2020-10-11T16:40:34Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "287c96e34eea177303716e6a2b72509c2c749333", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -223,7 +231,7 @@ public class LossesImpl {\n     if (sampleWeight == null) {\n       sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n     }\n-    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    LossTuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n     loss = result.getTarget();\n     sampleWeight = result.getSampleWeights();\n \n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..2ccb48ff 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -229,15 +253,15 @@ public class LossesImpl {\n       Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n     DataType<T> dataType = loss.asOutput().dataType();\n     if (sampleWeight == null) {\n-      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+      sampleWeight = cast(tf, tf.constant(1), dataType);\n     }\n     LossTuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n     loss = result.getTarget();\n     sampleWeight = result.getSampleWeights();\n \n-    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    Operand<T> weighted_losses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n     loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n-    return tf.dtypes.cast(loss, dataType);\n+    return cast(tf, loss, dataType);\n   }\n \n   /**\n", "next_change": {"commit": "02573b594ca552371b8f42fa9e53c019143e6931", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 2ccb48ff..92acafc9 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -259,8 +258,8 @@ public class LossesImpl {\n     loss = result.getTarget();\n     sampleWeight = result.getSampleWeights();\n \n-    Operand<T> weighted_losses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n-    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    Operand<T> weightedLosses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weightedLosses, reduction);\n     return cast(tf, loss, dataType);\n   }\n \n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MDM0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502940346", "body": "This is another case where the combination of `squeezeOrExpandDimensions` and broadcasting yields a complex relationship between shapes of `sampleWeight`, `loss`, and the return value. In particular, due to broadcasting in `mul`, in the case of `reduction` == `NONE` and a surprising shape of `sampleWeight`, the return value may have a very different shape than `loss`.", "bodyText": "This is another case where the combination of squeezeOrExpandDimensions and broadcasting yields a complex relationship between shapes of sampleWeight, loss, and the return value. In particular, due to broadcasting in mul, in the case of reduction == NONE and a surprising shape of sampleWeight, the return value may have a very different shape than loss.", "bodyHTML": "<p dir=\"auto\">This is another case where the combination of <code>squeezeOrExpandDimensions</code> and broadcasting yields a complex relationship between shapes of <code>sampleWeight</code>, <code>loss</code>, and the return value. In particular, due to broadcasting in <code>mul</code>, in the case of <code>reduction</code> == <code>NONE</code> and a surprising shape of <code>sampleWeight</code>, the return value may have a very different shape than <code>loss</code>.</p>", "author": "deansher", "createdAt": "2020-10-11T17:01:15Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..2ccb48ff 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -221,15 +253,15 @@ public class LossesImpl {\n       Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n     DataType<T> dataType = loss.asOutput().dataType();\n     if (sampleWeight == null) {\n-      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+      sampleWeight = cast(tf, tf.constant(1), dataType);\n     }\n-    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    LossTuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n     loss = result.getTarget();\n     sampleWeight = result.getSampleWeights();\n \n-    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    Operand<T> weighted_losses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n     loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n-    return tf.dtypes.cast(loss, dataType);\n+    return cast(tf, loss, dataType);\n   }\n \n   /**\n", "next_change": {"commit": "02573b594ca552371b8f42fa9e53c019143e6931", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 2ccb48ff..92acafc9 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -259,8 +258,8 @@ public class LossesImpl {\n     loss = result.getTarget();\n     sampleWeight = result.getSampleWeights();\n \n-    Operand<T> weighted_losses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n-    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    Operand<T> weightedLosses = tf.math.mul(loss, cast(tf, sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weightedLosses, reduction);\n     return cast(tf, loss, dataType);\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU2OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941569", "body": "`rank` could be `-1` at this point.", "bodyText": "rank could be -1 at this point.", "bodyHTML": "<p dir=\"auto\"><code>rank</code> could be <code>-1</code> at this point.</p>", "author": "deansher", "createdAt": "2020-10-11T17:12:44Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] axes = new int[rank];", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5ODAyNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504198026", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-13T19:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "249b65194bb055decf02d61f56378e7771e6d05f", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..9cc77b50 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -282,23 +289,15 @@ public class LossesImpl {\n    * @return a Constant that represents all the axes of the operand.\n    */\n   public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n-    int[] ranks = allAxis(op);\n-    return tf.constant(ranks);\n-  }\n-\n-  /**\n-   * Gets an integer array representing all the axes of the operand.\n-   *\n-   * @param op the Operand\n-   * @param <T> the type of Operand\n-   * @return the integer array representing all the axes of the operand.\n-   */\n-  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n     int rank = op.asOutput().shape().numDimensions();\n-    int[] axes = new int[rank];\n-    for (int i = 0; i < rank; i++) {\n-      axes[i] = i;\n+    if (rank != Shape.UNKNOWN_SIZE) {\n+      int[] axes = new int[rank];\n+      for (int i = 0; i < rank; i++) {\n+        axes[i] = i;\n+      }\n+      return tf.constant(axes);\n+    } else {\n+      return tf.range(tf.constant(0), tf.rank(op), tf.constant(1));\n     }\n-    return axes;\n   }\n }\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 9cc77b50..d77f513b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -300,4 +308,95 @@ public class LossesImpl {\n       return tf.range(tf.constant(0), tf.rank(op), tf.constant(1));\n     }\n   }\n+\n+  /**\n+   * Perform an inclusive range check on the values\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix A String prefix to include in the error message\n+   * @param values the values to check\n+   * @param minValue the minimum value\n+   * @param maxValue the maximum value\n+   * @param <T> the datatype for the values\n+   * @return the values possibly with control dependencies if the TensorFlow Ops represents a Graph\n+   *     Session\n+   * @throws IllegalArgumentException if the TensorFlow Ops represents an Eager Session\n+   */\n+  public static <T extends TNumber> Operand<T> rangeCheck(\n+      Ops tf, String prefix, Operand<T> values, Operand<T> minValue, Operand<T> maxValue) {\n+    Operand<TInt32> allDims = allAxes(tf, values);\n+    Operand<TBool> cond =\n+        tf.math.logicalAnd(\n+            tf.reduceAll(tf.math.greaterEqual(values, minValue), allDims),\n+            tf.reduceAll(tf.math.lessEqual(values, maxValue), allDims));\n+    // Graph and Eager mode need to be handled differently, control dependencies are not allowed in\n+    // Eager mode\n+    if (tf.scope().env().isGraph()) {\n+      AssertThat assertThat =\n+          tf.assertThat(\n+              cond,\n+              Arrays.asList(\n+                  tf.constant(prefix),\n+                  tf.constant(\": values out of range, \"),\n+                  tf.constant(\"minimum = \"),\n+                  minValue,\n+                  tf.constant(\", maximum = \"),\n+                  maxValue));\n+      Ops ltf =\n+          tf.withSubScope(\"rangeCheck\")\n+              .withControlDependencies(Collections.singletonList(assertThat));\n+      return ltf.identity(values);\n+    } else if (!cond.asOutput().data().getBoolean())\n+      throw new IllegalArgumentException(String.format(\"%s : values out of range\", prefix));\n+    else return values;\n+  }\n+\n+  /**\n+   * Checks to see if all the values are in the allowed values set. Running the operand in Graph\n+   * mode will throw {@link org.tensorflow.exceptions.TFInvalidArgumentException}, if at least one\n+   * value is not in the allowed values set. In Eager mode, this method will throw an {@link\n+   * IllegalArgumentException} if at least one value is not in the allowed values set.\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param prefix A String prefix to include in the error message\n+   * @param values the values to check\n+   * @param allowedValues the allowed values\n+   * @param <T> the data type for values and allowed values\n+   * @return the values possibly with control dependencies if the TensorFlow Ops represents a Graph\n+   *     Session\n+   * @throws IllegalArgumentException if the Session is in Eager mode and at least one value is not\n+   *     in the allowed values set\n+   */\n+  public static <T extends TNumber> Operand<T> valueCheck(\n+      Ops tf, String prefix, Operand<T> values, Operand<T> allowedValues) {\n+    Operand<T> flatValues =\n+        tf.reshape(values, tf.constant(Shape.of(values.asOutput().shape().size())));\n+    SetDiff1d<T, TInt32> diff = tf.setDiff1d(flatValues, allowedValues, TInt32.DTYPE);\n+    long diffSize = diff.out().asOutput().shape().size();\n+\n+    if (diffSize != Shape.UNKNOWN_SIZE) {\n+      if (diffSize != 0) { // at least 1 value in the diff did not match the allowed values.\n+        throw new IllegalArgumentException(String.format(\"%s : values not in value set,\", prefix));\n+      } else return values;\n+    } else { // use dynamic shape\n+      Operand<TBool> cond = tf.math.equal(tf.shape.size(tf.shape(diff.out())), tf.constant(0));\n+      // Graph and Eager mode need to be handled differently, control dependencies are not allowed\n+      // in Eager mode\n+      if (tf.scope().env().isGraph()) {\n+        AssertThat assertThat =\n+            tf.assertThat(\n+                cond,\n+                Arrays.asList(\n+                    tf.constant(prefix),\n+                    tf.constant(\": values not in value set, values = \"),\n+                    values));\n+        Ops ltf =\n+            tf.withSubScope(\"valueCheck\")\n+                .withControlDependencies(Collections.singletonList(assertThat));\n+        return ltf.identity(values);\n+      } else if (!cond.asOutput().data().getBoolean())\n+        throw new IllegalArgumentException(String.format(\"%s : values not in value set\", prefix));\n+      else return values;\n+    }\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTcyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502941721", "body": "`allAxes`?", "bodyText": "allAxes?", "bodyHTML": "<p dir=\"auto\"><code>allAxes</code>?</p>", "author": "deansher", "createdAt": "2020-10-11T17:14:06Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (predictionsRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple<>(labels, predictions, sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze or expand the sampleWeight based on the rank difference\n+   *\n+   * <p>If the rank difference is +1, squeeze the last dimension of sampleWeight, If the rank\n+   * difference is -1, expand the last dimension of sampleWeight. Otherwise, leave the shape of\n+   * sampleWeight as is.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeAdjustWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Collections.singletonList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  /**\n+   * Expand the last dimension of sampleWeight. if the rank difference is -1.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param sampleWeight the sample weights\n+   * @param rankDiff the difference in rank\n+   * @param <T> the data type for the Operands.\n+   * @return the adjusted sampleWeight\n+   */\n+  private static <T extends TNumber> Operand<T> maybeExpandWeights(\n+      Ops tf, Operand<T> sampleWeight, Operand<TInt32> rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static <T extends TNumber> Tuple<T> removeSqueezableDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1 && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1 && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple<>(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+\n+    // TODO Operand<TInt32> rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(predictionsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && Shape.isCompatible(labelsShape.size(-1), 1)) {\n+      /*\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Collections.singletonList(-1L)));\n+    }\n+    return new Tuple<>(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the weighted loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param loss the unweighted loss\n+   * @param reduction the type of reduction\n+   * @param sampleWeight the sample weight, if null then this defaults to one.\n+   * @param <T> the data type of the loss\n+   * @return the weighted loss\n+   */\n+  public static <T extends TNumber> Operand<T> computeWeightedLoss(\n+      Ops tf, Operand<T> loss, Reduction reduction, Operand<T> sampleWeight) {\n+    DataType<T> dataType = loss.asOutput().dataType();\n+    if (sampleWeight == null) {\n+      sampleWeight = tf.dtypes.cast(tf.constant(1), dataType);\n+    }\n+    Tuple<T> result = squeezeOrExpandDimensions(tf, null, loss, sampleWeight);\n+    loss = result.getTarget();\n+    sampleWeight = result.getSampleWeights();\n+\n+    Operand<T> weighted_losses = tf.math.mul(loss, tf.dtypes.cast(sampleWeight, dataType));\n+    loss = reduceWeightedLoss(tf, weighted_losses, reduction);\n+    return tf.dtypes.cast(loss, dataType);\n+  }\n+\n+  /**\n+   * Reduces the weighted loss based on the reduction type\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param weightedLoss the weighted loss\n+   * @param reduction the type of reduction\n+   * @param <T> the data type of the weighted loss\n+   * @return the reduced weighted loss\n+   */\n+  private static <T extends TNumber> Operand<T> reduceWeightedLoss(\n+      Ops tf, Operand<T> weightedLoss, Reduction reduction) {\n+    Operand<T> loss;\n+    if (reduction == Reduction.NONE) {\n+      loss = weightedLoss;\n+    } else {\n+      loss =\n+          tf.reduceSum(weightedLoss, allAxis(tf, weightedLoss), ReduceSum.keepDims(Boolean.FALSE));\n+      if (reduction == Reduction.AUTO || reduction == Reduction.SUM_OVER_BATCH_SIZE) {\n+        loss = safeMean(tf, loss, weightedLoss.asOutput().shape().size());\n+      }\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes a safe mean of the losses.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param losses </code>Operand</code> whose elements contain individual loss measurements.\n+   * @param numElements The number of measurable elements in <code>losses</code>.\n+   * @param <T> the data type of the losses\n+   * @return A scalar representing the mean of <code>losses</code>. If <code>numElements</code> is\n+   *     zero, then zero is returned.\n+   */\n+  public static <T extends TNumber> Operand<T> safeMean(\n+      Ops tf, Operand<T> losses, long numElements) {\n+    Operand<T> totalLoss = tf.reduceSum(losses, allAxis(tf, losses));\n+    return tf.math.divNoNan(\n+        totalLoss, tf.dtypes.cast(tf.constant(numElements), losses.asOutput().dataType()));\n+  }\n+\n+  /**\n+   * Gets a Constant integer array representing all the axes of the operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param op the TensorFlow Ops\n+   * @param <T> the type of Operand\n+   * @return a Constant that represents all the axes of the operand.\n+   */\n+  public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  /**\n+   * Gets an integer array representing all the axes of the operand.\n+   *\n+   * @param op the Operand\n+   * @param <T> the type of Operand\n+   * @return the integer array representing all the axes of the operand.\n+   */\n+  private static <T extends TNumber> int[] allAxis(Operand<T> op) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1Njc1NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502956755", "bodyText": "Changed name to allAxes", "author": "JimClarke5", "createdAt": "2020-10-11T19:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTcyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "249b65194bb055decf02d61f56378e7771e6d05f", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..9cc77b50 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -282,23 +289,15 @@ public class LossesImpl {\n    * @return a Constant that represents all the axes of the operand.\n    */\n   public static <T extends TNumber> Operand<TInt32> allAxis(Ops tf, Operand<T> op) {\n-    int[] ranks = allAxis(op);\n-    return tf.constant(ranks);\n-  }\n-\n-  /**\n-   * Gets an integer array representing all the axes of the operand.\n-   *\n-   * @param op the Operand\n-   * @param <T> the type of Operand\n-   * @return the integer array representing all the axes of the operand.\n-   */\n-  private static <T extends TNumber> int[] allAxis(Operand<T> op) {\n     int rank = op.asOutput().shape().numDimensions();\n-    int[] axes = new int[rank];\n-    for (int i = 0; i < rank; i++) {\n-      axes[i] = i;\n+    if (rank != Shape.UNKNOWN_SIZE) {\n+      int[] axes = new int[rank];\n+      for (int i = 0; i < rank; i++) {\n+        axes[i] = i;\n+      }\n+      return tf.constant(axes);\n+    } else {\n+      return tf.range(tf.constant(0), tf.rank(op), tf.constant(1));\n     }\n-    return axes;\n   }\n }\n", "next_change": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex 9cc77b50..d77f513b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -300,4 +308,95 @@ public class LossesImpl {\n       return tf.range(tf.constant(0), tf.rank(op), tf.constant(1));\n     }\n   }\n+\n+  /**\n+   * Perform an inclusive range check on the values\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix A String prefix to include in the error message\n+   * @param values the values to check\n+   * @param minValue the minimum value\n+   * @param maxValue the maximum value\n+   * @param <T> the datatype for the values\n+   * @return the values possibly with control dependencies if the TensorFlow Ops represents a Graph\n+   *     Session\n+   * @throws IllegalArgumentException if the TensorFlow Ops represents an Eager Session\n+   */\n+  public static <T extends TNumber> Operand<T> rangeCheck(\n+      Ops tf, String prefix, Operand<T> values, Operand<T> minValue, Operand<T> maxValue) {\n+    Operand<TInt32> allDims = allAxes(tf, values);\n+    Operand<TBool> cond =\n+        tf.math.logicalAnd(\n+            tf.reduceAll(tf.math.greaterEqual(values, minValue), allDims),\n+            tf.reduceAll(tf.math.lessEqual(values, maxValue), allDims));\n+    // Graph and Eager mode need to be handled differently, control dependencies are not allowed in\n+    // Eager mode\n+    if (tf.scope().env().isGraph()) {\n+      AssertThat assertThat =\n+          tf.assertThat(\n+              cond,\n+              Arrays.asList(\n+                  tf.constant(prefix),\n+                  tf.constant(\": values out of range, \"),\n+                  tf.constant(\"minimum = \"),\n+                  minValue,\n+                  tf.constant(\", maximum = \"),\n+                  maxValue));\n+      Ops ltf =\n+          tf.withSubScope(\"rangeCheck\")\n+              .withControlDependencies(Collections.singletonList(assertThat));\n+      return ltf.identity(values);\n+    } else if (!cond.asOutput().data().getBoolean())\n+      throw new IllegalArgumentException(String.format(\"%s : values out of range\", prefix));\n+    else return values;\n+  }\n+\n+  /**\n+   * Checks to see if all the values are in the allowed values set. Running the operand in Graph\n+   * mode will throw {@link org.tensorflow.exceptions.TFInvalidArgumentException}, if at least one\n+   * value is not in the allowed values set. In Eager mode, this method will throw an {@link\n+   * IllegalArgumentException} if at least one value is not in the allowed values set.\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param prefix A String prefix to include in the error message\n+   * @param values the values to check\n+   * @param allowedValues the allowed values\n+   * @param <T> the data type for values and allowed values\n+   * @return the values possibly with control dependencies if the TensorFlow Ops represents a Graph\n+   *     Session\n+   * @throws IllegalArgumentException if the Session is in Eager mode and at least one value is not\n+   *     in the allowed values set\n+   */\n+  public static <T extends TNumber> Operand<T> valueCheck(\n+      Ops tf, String prefix, Operand<T> values, Operand<T> allowedValues) {\n+    Operand<T> flatValues =\n+        tf.reshape(values, tf.constant(Shape.of(values.asOutput().shape().size())));\n+    SetDiff1d<T, TInt32> diff = tf.setDiff1d(flatValues, allowedValues, TInt32.DTYPE);\n+    long diffSize = diff.out().asOutput().shape().size();\n+\n+    if (diffSize != Shape.UNKNOWN_SIZE) {\n+      if (diffSize != 0) { // at least 1 value in the diff did not match the allowed values.\n+        throw new IllegalArgumentException(String.format(\"%s : values not in value set,\", prefix));\n+      } else return values;\n+    } else { // use dynamic shape\n+      Operand<TBool> cond = tf.math.equal(tf.shape.size(tf.shape(diff.out())), tf.constant(0));\n+      // Graph and Eager mode need to be handled differently, control dependencies are not allowed\n+      // in Eager mode\n+      if (tf.scope().env().isGraph()) {\n+        AssertThat assertThat =\n+            tf.assertThat(\n+                cond,\n+                Arrays.asList(\n+                    tf.constant(prefix),\n+                    tf.constant(\": values not in value set, values = \"),\n+                    values));\n+        Ops ltf =\n+            tf.withSubScope(\"valueCheck\")\n+                .withControlDependencies(Collections.singletonList(assertThat));\n+        return ltf.identity(values);\n+      } else if (!cond.asOutput().data().getBoolean())\n+        throw new IllegalArgumentException(String.format(\"%s : values not in value set\", prefix));\n+      else return values;\n+    }\n+  }\n }\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502943791", "body": "What should happen if `weightsRank` is `UNKNOWN`?", "bodyText": "What should happen if weightsRank is UNKNOWN?", "bodyHTML": "<p dir=\"auto\">What should happen if <code>weightsRank</code> is <code>UNKNOWN</code>?</p>", "author": "deansher", "createdAt": "2020-10-11T17:32:41Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1Njk5Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502956997", "bodyText": "It falls through and executes the last part of the method after the // Use dynamic rank. comment.", "author": "JimClarke5", "createdAt": "2020-10-11T19:40:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4MDM4Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503280383", "bodyText": ":-) Oh yeah, that.", "author": "deansher", "createdAt": "2020-10-12T13:01:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzc5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "642069c34d9e6b6c3df92cab4672c315029555de", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,70 +43,73 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return LossTuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+  public static <T extends TNumber> LossTuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    LossTuple<T> lossTuple = new LossTuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n-    if (sampleWeight == null) {\n-      return tuple;\n+    if (sampleWeights == null) { // nothing more to do.\n+      return lossTuple;\n     }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n+    Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n \n       if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n+        sampleWeights = tf.squeeze(sampleWeights);\n       } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+        sampleWeights = tf.expandDims(sampleWeights, tf.constant(-1L));\n       }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n     // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeights);\n     Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n+    sampleWeights =\n         tf.select(\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n+            sampleWeights,\n+            maybeAdjustWeights(tf, sampleWeights, rankDiff));\n+    return new LossTuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +108,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new LossTuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n   }\n \n   /**\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944132", "body": "Here we are working with the original `predictionsRank`, when we wanted to be working with the new rank.", "bodyText": "Here we are working with the original predictionsRank, when we wanted to be working with the new rank.", "bodyHTML": "<p dir=\"auto\">Here we are working with the original <code>predictionsRank</code>, when we wanted to be working with the new rank.</p>", "author": "deansher", "createdAt": "2020-10-11T17:35:45Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n+    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    long predictionsRank = predictionsShape.numDimensions();\n+\n+    if (labels != null) {\n+      Shape labelsShape = labels.asOutput().shape();\n+      long labelRank = labelsShape.numDimensions();\n+      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `label` and `prediction`.\n+        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+          // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n+          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        }\n+      } else { // use dynamic rank\n+        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple<>(labels, predictions, sampleWeight);\n+    }\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - predictionsRank == 1) {", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1NzgxNw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502957817", "bodyText": "This matches the original Python code, but when you think about it, the predictions rank would never change from UNKNOWN to KNOWN and vice versa in a static context.", "author": "JimClarke5", "createdAt": "2020-10-11T19:47:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4NTc1NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503285754", "bodyText": "I was thinking perhaps predictions changed rank through our squeezing it to match labels earlier in the method. But I think there's a more pernicious problem. Here's an elided version of some of the method's code. Notice that we may squeeze predictions, but we only store the result in tuple. If we then also work with sampleWeight, we neither reference the squeezed version of predictions nor return it.\n    if (labels != null) {\n        . . . \n        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n          tuple = removeSqueezableDimensions(tf, labels, predictions);\n        }\n      } else { // use dynamic rank\n        tuple = removeSqueezableDimensions(tf, labels, predictions);\n      }\n    }\n    . . .\n    if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n\n      if (weightsRank - predictionsRank == 1) {\n        sampleWeight = tf.squeeze(sampleWeight);\n        . . .\n      }\n      return new Tuple<>(labels, predictions, sampleWeight);\n    }", "author": "deansher", "createdAt": "2020-10-12T13:11:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5OTU5MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r504199591", "bodyText": "Yes, we should probably fetch the labels and predictions from tuple first. I'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-13T19:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDEzMg=="}], "type": "inlineReview", "revised_code": {"commit": "642069c34d9e6b6c3df92cab4672c315029555de", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,70 +43,73 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return LossTuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+  public static <T extends TNumber> LossTuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    LossTuple<T> lossTuple = new LossTuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n-    if (sampleWeight == null) {\n-      return tuple;\n+    if (sampleWeights == null) { // nothing more to do.\n+      return lossTuple;\n     }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n+    Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n \n       if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n+        sampleWeights = tf.squeeze(sampleWeights);\n       } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+        sampleWeights = tf.expandDims(sampleWeights, tf.constant(-1L));\n       }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n     // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeights);\n     Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n+    sampleWeights =\n         tf.select(\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n+            sampleWeights,\n+            maybeAdjustWeights(tf, sampleWeights, rankDiff));\n+    return new LossTuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +108,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new LossTuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n   }\n \n   /**\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502944276", "body": "This method has a myriad of complex cases, so I think it deserves its own direct unit test.", "bodyText": "This method has a myriad of complex cases, so I think it deserves its own direct unit test.", "bodyHTML": "<p dir=\"auto\">This method has a myriad of complex cases, so I think it deserves its own direct unit test.</p>", "author": "deansher", "createdAt": "2020-10-11T17:37:25Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+\n+public class LossesImpl {\n+\n+  /**\n+   * Squeeze or expand last dimension if needed with a sampleWeights of one.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank differs by 1 (using\n+   *       {@link #removeSqueezableDimensions}).\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight` if its rank differs by 1 from the new\n+   *       rank of <code>predictions`. If <code>sampleWeight` is scalar, it is kept scalar./li>\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>. Each of them possibly has the last\n+   *     dimension squeezed, <code>sampleWeight</code> could be extended by one dimension. If <code>sampleWeight</code>\n+   *     is null, (prediction, label) is returned.\n+   */\n+  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions) {\n+    return squeezeOrExpandDimensions(tf, labels, predictions, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed.\n+   *\n+   * <ol type=\"1\">\n+   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n+   *       `confusion_matrix.remove_squeezable_dimensions`). *\n+   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n+   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   * </ol>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n+   * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n+   *     </code>.\n+   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   *     prediction</code>.\n+   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     returned.\n+   */", "originalCommit": "17e96b5ab78ec7d4d87f24b0f8f97a54c3e9e882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1ODUxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r502958516", "bodyText": "I could not find direct test cases for this method in Python. It's defined in tensorflow/tensorflow/python/ops/losses/utils.py. You want to take a stab at it?", "author": "JimClarke5", "createdAt": "2020-10-11T19:54:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4NjQ5NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r503286495", "bodyText": ":-) Totally. I want to do some work on #92 first, so I'll open an issue for myself.", "author": "deansher", "createdAt": "2020-10-12T13:12:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDI3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "642069c34d9e6b6c3df92cab4672c315029555de", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex eb803256..e483a305 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -40,70 +43,73 @@ public class LossesImpl {\n    * Squeeze or expand last dimension if needed.\n    *\n    * <ol type=\"1\">\n-   *   <li>Squeezes last dim of `predictions` or `labels` if their rank differs by 1 (using *\n-   *       `confusion_matrix.remove_squeezable_dimensions`). *\n-   *   <li>Squeezes or expands last dim of `sampleWeight` if its rank differs by 1 from the new *\n-   *       rank of `predictions`. If `sampleWeight` is scalar, it is kept scalar./li> *\n+   *   <li>Squeezes last dim of <code>predictions</code> or <code>labels</code> if their rank do not\n+   *       differ by 1.\n+   *   <li>Squeezes or expands last dim of <code>sampleWeight</code> if its rank differs by 1 from\n+   *       the new rank of <code>predictions</code>. If <code>sampleWeight</code> is scalar, it is\n+   *       kept scalar.\n    * </ol>\n    *\n    * @param tf the TensorFlow Ops\n    * @param predictions Predicted values, a <code>Operand</code> of arbitrary dimensions.\n    * @param labels Optional label <code>Operand</code> whose dimensions match <code>prediction\n    *     </code>.\n-   * @param sampleWeight Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n+   * @param sampleWeights Optional sample weight(s) <code>Operand</code> whose dimensions match<code>\n    *     prediction</code>.\n-   * @return Tuple of <code>prediction</code>, <code>label</code> and <code>sampleWeight</code>.\n+   * @return LossTuple of <code>prediction<s/code>, <code>labels</code> and <code>sampleWeight</code>.\n    *     Each of them possibly has the last dimension squeezed, <code>sampleWeight</code> could be\n-   *     extended by one dimension. If <code>sampleWeight</code> is null, (prediction, label) is\n+   *     extended by one dimension. If <code>sampleWeight</code> is null, only the possibly shape modified <code>predictions</code> and <code>labels</code> are\n    *     returned.\n    */\n-  public static <T extends TNumber> Tuple<T> squeezeOrExpandDimensions(\n-      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeight) {\n-    Tuple<T> tuple = new Tuple<>(labels, predictions);\n+  public static <T extends TNumber> LossTuple<T> squeezeOrExpandDimensions(\n+      Ops tf, Operand<T> labels, Operand<T> predictions, Operand<T> sampleWeights) {\n+\n \n     Shape predictionsShape = predictions.asOutput().shape();\n     long predictionsRank = predictionsShape.numDimensions();\n \n+    // Default case when no modifications are made.\n+    LossTuple<T> lossTuple = new LossTuple<>(labels, predictions, sampleWeights);\n     if (labels != null) {\n       Shape labelsShape = labels.asOutput().shape();\n-      long labelRank = labelsShape.numDimensions();\n-      if (labelRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n-        // Use static rank for `label` and `prediction`.\n-        if (predictionsRank - labelRank != 1 || predictionsShape.size(-1) == 1) {\n+      long labelsRank = labelsShape.numDimensions();\n+      if (labelsRank != Shape.UNKNOWN_SIZE && predictionsRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for 'label' and 'prediction'.\n+        if (predictionsRank - labelsRank != 1 || predictionsShape.size(-1) == 1) {\n           // label, prediction = confusion_matrix.remove_squeezable_dimensions(label, prediction)\n-          tuple = removeSqueezableDimensions(tf, labels, predictions);\n+          lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n         }\n       } else { // use dynamic rank\n-        tuple = removeSqueezableDimensions(tf, labels, predictions);\n+        lossTuple = removeSqueezableDimensions(tf, labels, predictions);\n       }\n     }\n-    if (sampleWeight == null) {\n-      return tuple;\n+    if (sampleWeights == null) { // nothing more to do.\n+      return lossTuple;\n     }\n-    Shape weightsShape = sampleWeight.asOutput().shape();\n+    Shape weightsShape = sampleWeights.asOutput().shape();\n     long weightsRank = weightsShape.numDimensions();\n     if (weightsRank == 0) { // scalar\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n \n     if (predictionsRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n \n       if (weightsRank - predictionsRank == 1) {\n-        sampleWeight = tf.squeeze(sampleWeight);\n+        sampleWeights = tf.squeeze(sampleWeights);\n       } else if (predictionsRank - weightsRank == 1) {\n-        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+        sampleWeights = tf.expandDims(sampleWeights, tf.constant(-1L));\n       }\n-      return new Tuple<>(labels, predictions, sampleWeight);\n+      return new LossTuple<>(labels, predictions, sampleWeights);\n     }\n     // Use dynamic rank.\n-    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeight);\n+    Operand<TInt32> weightsRankTensor = tf.rank(sampleWeights);\n     Operand<TInt32> rankDiff = tf.math.sub(weightsRankTensor, tf.rank(predictions));\n-    sampleWeight =\n+    sampleWeights =\n         tf.select(\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n-            sampleWeight,\n-            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n-    return new Tuple<>(labels, predictions, sampleWeight);\n+            sampleWeights,\n+            maybeAdjustWeights(tf, sampleWeights, rankDiff));\n+    return new LossTuple<>(labels, predictions, sampleWeights);\n   }\n \n   /**\n", "next_change": {"commit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex e483a305..4a276d68 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -109,7 +108,7 @@ public class LossesImpl {\n             tf.math.equal(weightsRankTensor, tf.constant(0)),\n             sampleWeights,\n             maybeAdjustWeights(tf, sampleWeights, rankDiff));\n-    return new LossTuple<>(labels, predictions, sampleWeights);\n+    return new LossTuple<>(lossTuple.getLabels(), lossTuple.getTarget(), sampleWeights);\n   }\n \n   /**\n", "next_change": null}]}}]}}, {"oid": "642069c34d9e6b6c3df92cab4672c315029555de", "url": "https://github.com/tensorflow/java/commit/642069c34d9e6b6c3df92cab4672c315029555de", "message": "Repair JavaDOx", "committedDate": "2020-10-11T19:29:37Z", "type": "commit"}, {"oid": "249b65194bb055decf02d61f56378e7771e6d05f", "url": "https://github.com/tensorflow/java/commit/249b65194bb055decf02d61f56378e7771e6d05f", "message": "Fixed AllAxis to hanlde dynamic shape when static shape rank is unknown.", "committedDate": "2020-10-11T19:30:19Z", "type": "commit"}, {"oid": "794cfdca096223e521c8c45138fc872cc2a3ec75", "url": "https://github.com/tensorflow/java/commit/794cfdca096223e521c8c45138fc872cc2a3ec75", "message": "change method name allAxis to allAxes", "committedDate": "2020-10-11T19:36:18Z", "type": "commit"}, {"oid": "fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "url": "https://github.com/tensorflow/java/commit/fb26c59f40f45836c62f7e0421949cb5bd8e3e3c", "message": "change private method binaryCrossentropy to binaryCrossentropyHelper", "committedDate": "2020-10-13T17:12:40Z", "type": "commit"}, {"oid": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "url": "https://github.com/tensorflow/java/commit/928ef066f8d250b4ae41799eea40ab03fe3ecd23", "message": "Fixed squeezeOrExpandDimensions to make sure the updated labels, predictions and weights are returned in LossTuple", "committedDate": "2020-10-13T19:25:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDkzNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511490934", "body": "This javadoc can get out of date as it links to `Reduction#AUTO` rather than `REDUCTION_DEFAULT`. If `REDUCTION_DEFAULT` changed we'd need to update all the docs.", "bodyText": "This javadoc can get out of date as it links to Reduction#AUTO rather than REDUCTION_DEFAULT. If REDUCTION_DEFAULT changed we'd need to update all the docs.", "bodyHTML": "<p dir=\"auto\">This javadoc can get out of date as it links to <code>Reduction#AUTO</code> rather than <code>REDUCTION_DEFAULT</code>. If <code>REDUCTION_DEFAULT</code> changed we'd need to update all the docs.</p>", "author": "Craigacp", "createdAt": "2020-10-24T16:45:18Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NDY3Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511654677", "bodyText": "I fixed all instances across all classes  to use Loss#REDUCTION_DEFAULT  which is set to Reduction.AUTO in the Loss class.", "author": "JimClarke5", "createdAt": "2020-10-25T22:05:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDkzNA=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex a7491285..1550042d 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -63,7 +62,7 @@ public class CategoricalCrossentropy extends Loss {\n   /**\n    * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n    * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n-   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n    * #DEFAULT_AXIS}\n    *\n    * @param tf the TensorFlow Ops\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTMxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491316", "body": "Does `labelSmoothing = 1.0` mean the true label distribution is set to `1/n`? I'm not sure what \"squeezing the values towards 0.5\" means, because it would only be 0.5 in a binary problem.", "bodyText": "Does labelSmoothing = 1.0 mean the true label distribution is set to 1/n? I'm not sure what \"squeezing the values towards 0.5\" means, because it would only be 0.5 in a binary problem.", "bodyHTML": "<p dir=\"auto\">Does <code>labelSmoothing = 1.0</code> mean the true label distribution is set to <code>1/n</code>? I'm not sure what \"squeezing the values towards 0.5\" means, because it would only be 0.5 in a binary problem.</p>", "author": "Craigacp", "createdAt": "2020-10-24T16:49:17Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,219 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Reduction#AUTO}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NTYzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511655636", "bodyText": "Actually this is the comment for BinaryCrossentropy. It should be:\nFloat in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\nconfidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\nvalue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n\nI'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:14:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTMxNg=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex a7491285..1550042d 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -135,21 +134,20 @@ public class CategoricalCrossentropy extends Loss {\n \n   /**\n    * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n-   * a Loss Reduction of {@link Reduction#AUTO}, and a channel axis of {@link #DEFAULT_AXIS}\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n    *\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n     this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n   }\n \n   /**\n-   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Reduction#AUTO},\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n    * and a channel axis of {@link #DEFAULT_AXIS}\n    *\n    * @param tf the TensorFlow Ops\n", "next_change": {"commit": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 1550042d..3306d16b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -153,10 +170,9 @@ public class CategoricalCrossentropy extends Loss {\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n     this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n", "next_change": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -171,7 +171,7 @@ public class CategoricalCrossentropy extends Loss {\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    confidence on label values are relaxed. e.g. <code>labelSmoothing=0.2<code> means that we will use a\n    *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTU4MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511491581", "body": "What happens if multiple classes are set to `1.0`? Does it throw some exception, or compute a different function? If it's the former we should document that.", "bodyText": "What happens if multiple classes are set to 1.0? Does it throw some exception, or compute a different function? If it's the former we should document that.", "bodyHTML": "<p dir=\"auto\">What happens if multiple classes are set to <code>1.0</code>? Does it throw some exception, or compute a different function? If it's the former we should document that.</p>", "author": "Craigacp", "createdAt": "2020-10-24T16:51:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalHinge.java", "diffHunk": "@@ -0,0 +1,93 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the categorical hinge loss between labels and predictions.\n+ *\n+ * <p><code>loss = maximum(neg - pos + 1, 0)</code> where <code>neg=maximum((1-labels)*predictions)\n+ * </code> and <code>pos=sum(labels*predictions)</code>\n+ *\n+ * <p><code>labels</code> values are expected to be 0 or 1.</p>", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NjkzMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515166933", "bodyText": "It doesn't throw an exception and the Python test case uses multiple values set to one. There is a doc discrepancy between the documentation for CategorialHinge and the method for categorical_hinge wrt to expecting the values to be either 0 and 1, in both PythonTF and standalone Keras.", "author": "JimClarke5", "createdAt": "2020-10-30T15:07:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MTU4MQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjE0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511492140", "body": "Doc says KL when it should say logcosh.", "bodyText": "Doc says KL when it should say logcosh.", "bodyHTML": "<p dir=\"auto\">Doc says KL when it should say logcosh.</p>", "author": "Craigacp", "createdAt": "2020-10-24T16:58:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes Computes the logarithm of the hyperbolic cosine of the prediction error.\n+ *\n+ * <p><code>logcosh = log((exp(x) + exp(-x))/2)</code>, where <code>x</code> is the error <code>\n+ * predictions - y_true</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{1.f, 1.f}, {0.f, 0.f}});\n+ *    LogCosh logcosh = new LogCosh(tf);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.108\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.8f, 0.2f});\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions, sampleWeight);\n+ *    // produces 0.087f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces 0.217f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    LogCosh logcosh = new LogCosh(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = logcosh.call(labels, predictions);\n+ *    // produces [0.217f, 0f]\n+ * </pre>\n+ */\n+public class LogCosh extends Loss {\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name and a Loss\n+   * Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf) {\n+    this(tf, null, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using a Loss Reduction of {@link Reduction#AUTO}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public LogCosh(Ops tf, String name) {\n+    this(tf, name, Reduction.AUTO);\n+  }\n+\n+  /**\n+   * Creates a LogCosh Loss using {@link Class#getSimpleName()} as the loss name\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public LogCosh(Ops tf, Reduction reduction) {\n+    this(tf, null, reduction);\n+  }\n+\n+  /**\n+   * Creates a Kullback Leibler Divergence Loss", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1NjM0MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511656340", "bodyText": "Copy & paste error, I fixed it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:19:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MjE0MA=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java\nindex a0e99180..da6992ec 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/LogCosh.java\n", "chunk": "@@ -79,7 +79,7 @@ public class LogCosh extends Loss {\n   }\n \n   /**\n-   * Creates a Kullback Leibler Divergence Loss\n+   * Creates a LogCosh Loss\n    *\n    * @param tf the TensorFlow Ops\n    * @param name the name of the loss\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDIxOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494219", "body": "This mentions smoothing the labels towards 0.5, but I think it's really towards `1/n`.", "bodyText": "This mentions smoothing the labels towards 0.5, but I think it's really towards 1/n.", "bodyHTML": "<p dir=\"auto\">This mentions smoothing the labels towards 0.5, but I think it's really towards <code>1/n</code>.</p>", "author": "Craigacp", "createdAt": "2020-10-24T17:20:56Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1ODA0Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511658047", "bodyText": "Actually this documentation belongs to BinaryCrossentropy,\nHere is what it should be:\nFloat in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\nconfidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\nvalue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n\nI'll fix it.", "author": "JimClarke5", "createdAt": "2020-10-25T22:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDIxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -205,10 +208,9 @@ public class Losses {\n    * @param labels true targets\n    * @param predictions the predictions\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param axis the\n    * @param <T> the data type of the predictions and labels\n    * @return the categorical crossentropy loss.\n", "next_change": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..3b61b0f8 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -209,7 +219,7 @@ public class Losses {\n    * @param predictions the predictions\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     confidence on label values are relaxed. e.g. <code>labelSmoothing=0.2<code> means that we will use a\n    *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param axis the\n    * @param <T> the data type of the predictions and labels\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511494723", "body": "Is this properly backtracking? It looks like it's going to pull out the softmax output not the input.", "bodyText": "Is this properly backtracking? It looks like it's going to pull out the softmax output not the input.", "bodyHTML": "<p dir=\"auto\">Is this properly backtracking? It looks like it's going to pull out the softmax output not the input.</p>", "author": "Craigacp", "createdAt": "2020-10-24T17:26:37Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MDU2OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511660569", "bodyText": "This was a TODO and the output() what just  a holder until I figured out how to get the inputs to the\nSoftMax operation. If you don't have a suggestion, maybe remove this logic for now.\nHere is the logic from Python.\n    # When softmax activation function is used for output operation, we\n    # use logits from the softmax function directly to compute loss in order\n    # to prevent collapsing zero when training.\n    # See b/117284466\n    assert len(output.op.inputs) == 1\n    output = output.op.inputs[0]\n    return nn.softmax_cross_entropy_with_logits_v2(\n        labels=target, logits=output, axis=axis)", "author": "JimClarke5", "createdAt": "2020-10-25T22:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDYzOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244639", "bodyText": "I commented out this code and marked it as a TODO.", "author": "JimClarke5", "createdAt": "2020-10-26T20:23:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5NDcyMw=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -232,7 +234,9 @@ public class Losses {\n     if (fromLogits) {\n       return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n     }\n+    /* TODO\n     if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n       // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n       if (predictions.op().type().equals(\"Softmax\")) {\n         if (predictions.op().numOutputs() != 1)\n", "next_change": null}, {"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -241,6 +245,8 @@ public class Losses {\n         return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n       }\n     }\n+    */\n+\n     Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n     Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -247,8 +261,8 @@ public class Losses {\n     }\n     */\n \n-    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n-    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     predictions =\n         tf.math.div(\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -261,8 +257,8 @@ public class Losses {\n     }\n     */\n \n-    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n-    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf, tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf, tf.constant(EPSILON), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n     predictions =\n         tf.math.div(\n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511496862", "body": "The CosineSimilarity doesn't mention that the value can be positive, and doesn't seem to restrict the output of this function so it is non-positive.", "bodyText": "The CosineSimilarity doesn't mention that the value can be positive, and doesn't seem to restrict the output of this function so it is non-positive.", "bodyHTML": "<p dir=\"auto\">The CosineSimilarity doesn't mention that the value can be positive, and doesn't seem to restrict the output of this function so it is non-positive.</p>", "author": "Craigacp", "createdAt": "2020-10-24T17:49:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI1MTgyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512251821", "bodyText": "The JavaDoc on the class should have matched the JavaDoc on Losses.cosineSimilarity method which returns values between -1 and 1. I have fixed the JavaDoc on the class CosineSimilarity to match the method JavaDoc.", "author": "JimClarke5", "createdAt": "2020-10-26T20:36:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMyMDA4NQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512320085", "bodyText": "There is a discrepancy between the Python doc for CosineSimilarity  and the loss function cosine_similarity. Though the doc on CosineSimilarity indicates a value in the range of [-1 to  0], the actual method indicates a range of [-1 to  1].\nThere is nothing in the code between the two, that acts differently as CosineSimilarity just calls cosine_similarity.\nThe original Keras documentation on CosineSimilarity, agrees with the [-1  to  1] range. I am guessing the TensorFlow CosineSimilarity documentation is wrong.\nAnother weirdness creeps in in metrics.CosineSimilarity, which ends up calling another function called cosine_proximity. While the two different functions are in a similar category, they actually compute differently.\nWe'll address that issue in the metrics PR, as perhaps the metrics class should be renamed to CosineProximity.", "author": "JimClarke5", "createdAt": "2020-10-26T23:02:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM1ODE2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515358167", "bodyText": "We should probably open an issue on the main TF repo flagging the docs issue, but that can wait.", "author": "Craigacp", "createdAt": "2020-10-30T20:24:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NTEyOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522585129", "bodyText": "This javadoc is better, but I think it should mention that this function is inverted from the regular cosine similarity, as that's 1 when the values are most similar and -1 when they point in opposite directions. It makes sense that it is inverted because then you can minimise it sensibly, but it is confusing if you're just browsing through.", "author": "Craigacp", "createdAt": "2020-11-13T03:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5Njg2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -293,11 +309,15 @@ public class Losses {\n   /**\n    * Computes the cosine similarity loss between labels and predictions.\n    *\n-   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n-   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n-   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n-   * you try to maximize the proximity between predictions and targets. If either labels or\n-   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * <p>Note that it is a number between <code>-1</code> and <code>1</code>, which is different from\n+   * the mathematical definition of cosine similarity where <code>1</code> represents similar\n+   * vectors, and <code>0</code> represents dissimilar vectors. In this function, the numbers are\n+   * inverted in a range of <code>-1</code> to <code>1</code>. When it is a negative number between\n+   * <code>-1</code> and <code>0</code>, <code>0</code> indicates orthogonality and values closer to\n+   * <code>-1</code> indicate greater similarity. The values closer to <code>1</code> indicate\n+   * greater dissimilarity. This makes it usable as a loss function in a setting where you try to\n+   * maximize the proximity between predictions and targets. If either labels or predictions is a\n+   * zero vector, cosine similarity will be <code>0</code> regardless of the proximity between\n    * predictions and targets.\n    *\n    * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499143", "body": "But it didn't extract the logits, so won't this perform the wrong calculation?", "bodyText": "But it didn't extract the logits, so won't this perform the wrong calculation?", "bodyHTML": "<p dir=\"auto\">But it didn't extract the logits, so won't this perform the wrong calculation?</p>", "author": "Craigacp", "createdAt": "2020-10-24T18:14:31Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk0MDkzNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511940935", "bodyText": "Correct, I really need the inputs to the Softmax Operation. This is the same issue as mentioned before,\nbut I correctly marked these as TODO's", "author": "JimClarke5", "createdAt": "2020-10-26T13:01:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MjQ2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512072467", "bodyText": "Yeah, but I think it would be best to just remove all this and leave the TODO comment in, rather than half put it in and have it break in the common case.", "author": "Craigacp", "createdAt": "2020-10-26T15:56:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDQzMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244430", "bodyText": "I commented out this code and marked it as a TODO.", "author": "JimClarke5", "createdAt": "2020-10-26T20:22:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTE0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -495,8 +501,10 @@ public class Losses {\n     Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n     Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n \n+    /* TODO need ability to walk back inputs\n     if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n       // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n       if (predictions.op().type().equals(Softmax.OP_NAME)) {\n         // When softmax activation function is used for output operation, we\n         // use logits from the softmax function directly to compute loss in order\n", "next_change": null}, {"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -506,7 +514,9 @@ public class Losses {\n         // TODO output = output.op.inputs[0]\n         fromLogits = true;\n       }\n+\n     }\n+     */\n     if (!fromLogits) {\n \n       predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTQxMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499413", "body": "The doc is wrong here, it scales towards `1/n`.", "bodyText": "The doc is wrong here, it scales towards 1/n.", "bodyHTML": "<p dir=\"auto\">The doc is wrong here, it scales towards <code>1/n</code>.</p>", "author": "Craigacp", "createdAt": "2020-10-24T18:17:34Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,685 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Sigmoid;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = tf.dtypes.cast(labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels,dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), tf.dtypes.cast(tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        tf.dtypes.cast(tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work, cannot walk back, work around is only go back 1.\n+      // output = backtrackIdentity(output);\n+      if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+        if (output.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        output = output.op().output(0);\n+        return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      }\n+    }\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing correspond to\n+   *     heavier smoothing.\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> ops = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets,  values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = tf.dtypes.cast(tf.constant(delta), dataType);\n+    Operand<T> point5 = tf.dtypes.cast(tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = tf.dtypes.cast(tf.constant(-2), dataType);\n+    Operand<T> two = tf.dtypes.cast(tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = tf.dtypes.cast(tf.constant(EPSILON), dataType);\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = tf.dtypes.cast(labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank-1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = tf.dtypes.cast(labels, dataType);\n+    LossTuple<T> lossTuple = LossesImpl.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = tf.dtypes.cast(tf.constant(1), dataType);\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = tf.dtypes.cast(tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = tf.dtypes.cast(tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MTA4OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511661088", "bodyText": "Fixed the doc to:\n* @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n  *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n  *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>", "author": "JimClarke5", "createdAt": "2020-10-25T23:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTQxMw=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex cb6baa8c..b606cc04 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -604,10 +634,9 @@ public class Losses {\n    *\n    * @param tf the TensorFlow Ops\n    * @param labels true targets\n-   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n-   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n-   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n-   *     correspond to heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param <T> the data type of the labels\n    * @return the smoothed categorical labels\n    */\n", "next_change": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b606cc04..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -643,11 +657,11 @@ public class Losses {\n   private static <T extends TNumber> Operand<T> smoothLabelsCatX(\n       Ops tf, Operand<T> labels, float labelSmoothing) {\n     DataType<T> dataType = labels.asOutput().dataType();\n-    Operand<T> smoothing = tf.dtypes.cast(tf.constant(labelSmoothing), dataType);\n+    Operand<T> smoothing = cast(tf,  tf.constant(labelSmoothing), dataType);\n     Shape labelsShape = labels.asOutput().shape();\n     int numDims = labelsShape.numDimensions();\n-    Operand<T> numClasses = tf.dtypes.cast(tf.constant(labelsShape.size(numDims - 1)), dataType);\n-    Operand<T> oneMinusSmoothing = tf.dtypes.cast(tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> numClasses = cast(tf,  tf.constant(labelsShape.size(numDims - 1)), dataType);\n+    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n     return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), tf.math.div(smoothing, numClasses));\n   }\n \n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ff0b513c..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -654,14 +632,14 @@ public class Losses {\n    * @param <T> the data type of the labels\n    * @return the smoothed categorical labels\n    */\n-  private static <T extends TNumber> Operand<T> smoothLabelsCatX(\n+  private static <T extends TNumber> Operand<T> smoothCategoricalLabels(\n       Ops tf, Operand<T> labels, float labelSmoothing) {\n     DataType<T> dataType = labels.asOutput().dataType();\n-    Operand<T> smoothing = cast(tf,  tf.constant(labelSmoothing), dataType);\n+    Operand<T> smoothing = cast(tf, tf.constant(labelSmoothing), dataType);\n     Shape labelsShape = labels.asOutput().shape();\n     int numDims = labelsShape.numDimensions();\n-    Operand<T> numClasses = cast(tf,  tf.constant(labelsShape.size(numDims - 1)), dataType);\n-    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> numClasses = cast(tf, tf.constant(labelsShape.size(numDims - 1)), dataType);\n+    Operand<T> oneMinusSmoothing = cast(tf, tf.constant(1.f - labelSmoothing), dataType);\n     return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), tf.math.div(smoothing, numClasses));\n   }\n \n", "next_change": null}]}}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTk1NA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511499954", "body": "This links to `Reduction#AUTO` instead of `REDUCTION_DEFAULT`.", "bodyText": "This links to Reduction#AUTO instead of REDUCTION_DEFAULT.", "bodyHTML": "<p dir=\"auto\">This links to <code>Reduction#AUTO</code> instead of <code>REDUCTION_DEFAULT</code>.</p>", "author": "Craigacp", "createdAt": "2020-10-24T18:23:49Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+/**\n+ * Computes the crossentropy loss between labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. The labels are\n+ * expected to be provided as integers. If you want to provide labels using <code>one-hot</code>\n+ * representation, please use {@link CategoricalCrossentropy} loss. There should be <code># classes\n+ * </code> floating point values per feature for <code>predictions</code> and a single floating\n+ * point value per feature for <code>label</code>.\n+ *\n+ * <p>In the snippet below, there is a single floating point value per example for <code>labels\n+ * </code> and <code># classes</code> floating pointing values per example for <code>predictions\n+ * </code>. The shape of <code>labels</code> is <code>[batch_size]</code> and the shape of <code>\n+ * predictions</code> is <code>[batch_size, num_classes]</code>.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[] {1, 2});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 1.177f\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    SparseCategoricalCrossentropy sparseCCE = new SparseCategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = sparseCCE.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class SparseCategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final int AXIS_DEFAULT = -1;\n+  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n+\n+  private final boolean fromLogits;\n+  private final int axis;\n+\n+  /**\n+   * Creates a SparseCategoricalCrossentropy loss using {@link Class#getSimpleName()} as the loss\n+   * name, a Loss Reduction of {@link Reduction#AUTO}, and fromLogits={@link #FROM_LOGITS_DEFAULT}.", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY2MTExNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511661115", "bodyText": "Fixed", "author": "JimClarke5", "createdAt": "2020-10-25T23:00:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5OTk1NA=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java\nindex 7776e23e..7636cb89 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/SparseCategoricalCrossentropy.java\n", "chunk": "@@ -58,14 +58,13 @@ import org.tensorflow.types.family.TNumber;\n public class SparseCategoricalCrossentropy extends Loss {\n   public static final boolean FROM_LOGITS_DEFAULT = false;\n   public static final int AXIS_DEFAULT = -1;\n-  public static final Reduction REDUCTION_DEFAULT = Reduction.AUTO;\n \n   private final boolean fromLogits;\n   private final int axis;\n \n   /**\n    * Creates a SparseCategoricalCrossentropy loss using {@link Class#getSimpleName()} as the loss\n-   * name, a Loss Reduction of {@link Reduction#AUTO}, and fromLogits={@link #FROM_LOGITS_DEFAULT}.\n+   * name, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and fromLogits={@link #FROM_LOGITS_DEFAULT}.\n    *\n    * @param tf the TensorFlow Ops\n    */\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDc5Mw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r511500793", "body": "The last `f` is in caps. Is that intentional? It's not consistent throughout the file if so.", "bodyText": "The last f is in caps. Is that intentional? It's not consistent throughout the file if so.", "bodyHTML": "<p dir=\"auto\">The last <code>f</code> is in caps. Is that intentional? It's not consistent throughout the file if so.</p>", "author": "Craigacp", "createdAt": "2020-10-24T18:32:55Z", "path": "tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.junit.jupiter.api.Test;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.utils.TestSession;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+\n+public class CategoricalCrossentropyTest {\n+\n+  private final TestSession.Mode[] tfModes = {TestSession.Mode.EAGER, TestSession.Mode.GRAPH};\n+\n+  /** Test of call method, of class CategoricalCrossentropy. */\n+  @Test\n+  public void testAllCorrectUnweighted() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession testSession = TestSession.createTestSession(tfMode)) {\n+        Ops tf = testSession.getTF();\n+\n+        long[] trueArray = {\n+          1L, 0L, 0L,\n+          0L, 1L, 0L,\n+          0L, 0L, 1L\n+        };\n+        float[] predArray = {\n+          1.f, 0.f, 0.f,\n+          0.f, 1.f, 0.f,\n+          0.f, 0.f, 1.F", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI0NDA5OA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r512244098", "bodyText": "Changed all to F", "author": "JimClarke5", "createdAt": "2020-10-26T20:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUwMDc5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "2bc54dd821b01c368914efdae87e503c3a61d989", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java b/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\nindex f1bf8f0b..0e273fd6 100644\n--- a/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\n+++ b/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\n", "chunk": "@@ -26,30 +29,64 @@ public class CategoricalCrossentropyTest {\n           0L, 0L, 1L\n         };\n         float[] predArray = {\n-          1.f, 0.f, 0.f,\n-          0.f, 1.f, 0.f,\n-          0.f, 0.f, 1.F\n+          1.F, 0.F, 0.F,\n+          0.F, 1.F, 0.F,\n+          0.F, 0.F, 1.F\n         };\n         Operand<TInt64> yTrue = tf.reshape(tf.constant(trueArray), tf.constant(Shape.of(3, 3)));\n         Operand<TFloat32> yPred = tf.reshape(tf.constant(predArray), tf.constant(Shape.of(3, 3)));\n         CategoricalCrossentropy instance = new CategoricalCrossentropy(tf);\n         Operand<TFloat32> loss = instance.call(yTrue, yPred);\n-        float expected = 0f;\n+        float expected = 0F;\n         testSession.evaluate(expected, loss);\n \n         // Test with logits.\n         float[] logitsArray = {\n-          10.f, 0.f, 0.f,\n-          0.f, 10.f, 0.f,\n-          0.f, 0.f, 10.F\n+          10.F, 0.F, 0.F,\n+          0.F, 10.F, 0.F,\n+          0.F, 0.F, 10.F\n         };\n         yTrue = tf.reshape(tf.constant(trueArray), tf.constant(Shape.of(3, 3)));\n         Operand<TFloat32> logits =\n             tf.reshape(tf.constant(logitsArray), tf.constant(Shape.of(3, 3)));\n         instance = new CategoricalCrossentropy(tf, true);\n         loss = instance.call(yTrue, logits);\n-        testSession.setEpsilon(1e-3f);\n-        testSession.evaluate(0.0f, loss);\n+        testSession.setEpsilon(1e-3F);\n+        testSession.evaluate(0.0F, loss);\n+      }\n+  }\n+\n+  @Test\n+  public void testInvalidPredictionsRange() {\n+    for (TestSession.Mode tfMode : tfModes)\n+      try (TestSession testSession = TestSession.createTestSession(tfMode)) {\n+        Class catchClass =\n+                tfMode == TestSession.Mode.EAGER\n+                        ? IllegalArgumentException.class\n+                        : org.tensorflow.exceptions.TFInvalidArgumentException.class;\n+        assertThrows(\n+                catchClass,\n+                () -> {\n+                  Ops tf = testSession.getTF();\n+                  CategoricalCrossentropy instance = new CategoricalCrossentropy(tf);\n+                  float[] trueArray = {\n+                          1L, 0L, 0L,\n+                          0L, 1L, 0L,\n+                          0L, 0L, 1L\n+                  };\n+                  float[] predArray = {\n+                          -1.F, 0.F, 0.F,\n+                          0.F, 1.F, 0.F,\n+                          0.F, 0.F, 1.F\n+                  };\n+                  Operand<TFloat32> yTrue =\n+                          tf.reshape(tf.constant(trueArray), tf.constant(Shape.of(3, 3)));\n+                  Operand<TFloat32> yPred =\n+                          tf.reshape(tf.constant(predArray), tf.constant(Shape.of(2, 2)));\n+\n+                  Operand<TFloat32> loss = instance.call(yTrue, yPred);\n+                  testSession.run(loss);\n+                });\n       }\n   }\n \n", "next_change": {"commit": "c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java b/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\nindex 0e273fd6..36af5bd5 100644\n--- a/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\n+++ b/tensorflow-framework/src/test/java/org/tensorflow/framework/losses/CategoricalCrossentropyTest.java\n", "chunk": "@@ -60,33 +74,33 @@ public class CategoricalCrossentropyTest {\n   public void testInvalidPredictionsRange() {\n     for (TestSession.Mode tfMode : tfModes)\n       try (TestSession testSession = TestSession.createTestSession(tfMode)) {\n-        Class catchClass =\n-                tfMode == TestSession.Mode.EAGER\n-                        ? IllegalArgumentException.class\n-                        : org.tensorflow.exceptions.TFInvalidArgumentException.class;\n+        Class<? extends Throwable> catchClass =\n+            tfMode == TestSession.Mode.EAGER\n+                ? IllegalArgumentException.class\n+                : org.tensorflow.exceptions.TFInvalidArgumentException.class;\n         assertThrows(\n-                catchClass,\n-                () -> {\n-                  Ops tf = testSession.getTF();\n-                  CategoricalCrossentropy instance = new CategoricalCrossentropy(tf);\n-                  float[] trueArray = {\n-                          1L, 0L, 0L,\n-                          0L, 1L, 0L,\n-                          0L, 0L, 1L\n-                  };\n-                  float[] predArray = {\n-                          -1.F, 0.F, 0.F,\n-                          0.F, 1.F, 0.F,\n-                          0.F, 0.F, 1.F\n-                  };\n-                  Operand<TFloat32> yTrue =\n-                          tf.reshape(tf.constant(trueArray), tf.constant(Shape.of(3, 3)));\n-                  Operand<TFloat32> yPred =\n-                          tf.reshape(tf.constant(predArray), tf.constant(Shape.of(2, 2)));\n-\n-                  Operand<TFloat32> loss = instance.call(yTrue, yPred);\n-                  testSession.run(loss);\n-                });\n+            catchClass,\n+            () -> {\n+              Ops tf = testSession.getTF();\n+              CategoricalCrossentropy instance = new CategoricalCrossentropy(tf);\n+              float[] trueArray = {\n+                1L, 0L, 0L,\n+                0L, 1L, 0L,\n+                0L, 0L, 1L\n+              };\n+              float[] predArray = {\n+                -1.F, 0.F, 0.F,\n+                0.F, 1.F, 0.F,\n+                0.F, 0.F, 1.F\n+              };\n+              Operand<TFloat32> yTrue =\n+                  tf.reshape(tf.constant(trueArray), tf.constant(Shape.of(3, 3)));\n+              Operand<TFloat32> yPred =\n+                  tf.reshape(tf.constant(predArray), tf.constant(Shape.of(2, 2)));\n+\n+              Operand<TFloat32> loss = instance.call(yTrue, yPred);\n+              testSession.run(loss);\n+            });\n       }\n   }\n \n", "next_change": null}]}}]}}, {"oid": "2bc54dd821b01c368914efdae87e503c3a61d989", "url": "https://github.com/tensorflow/java/commit/2bc54dd821b01c368914efdae87e503c3a61d989", "message": "Fix JavaDoc,\nAdd in rangeCheck and valueCheck\nMisc fixes based on review", "committedDate": "2020-10-27T16:24:22Z", "type": "commit"}, {"oid": "951443b6cba9e42911ca2cfae05bee920d5ff229", "url": "https://github.com/tensorflow/java/commit/951443b6cba9e42911ca2cfae05bee920d5ff229", "message": "Fix unused imports and add @SuppressWarnings(\"unchecked\") for casts.", "committedDate": "2020-10-27T16:31:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513868067", "body": "What is the reason for having a seperate `Losses` class that implements all individual loss algorithms instead of implementing them directly in their respective class?\n\nI guess is to offer the choice to the user to choose an object-oriented API or a functional one? Then if we offer this, should we do the same for all other concepts in the framework?\n\nI wondering if we shouldn't take that decision for our users and only present a single API to accomplish a given task, for simplicity and consistency. I personally have a preference for the OO approach.", "bodyText": "What is the reason for having a seperate Losses class that implements all individual loss algorithms instead of implementing them directly in their respective class?\nI guess is to offer the choice to the user to choose an object-oriented API or a functional one? Then if we offer this, should we do the same for all other concepts in the framework?\nI wondering if we shouldn't take that decision for our users and only present a single API to accomplish a given task, for simplicity and consistency. I personally have a preference for the OO approach.", "bodyHTML": "<p dir=\"auto\">What is the reason for having a seperate <code>Losses</code> class that implements all individual loss algorithms instead of implementing them directly in their respective class?</p>\n<p dir=\"auto\">I guess is to offer the choice to the user to choose an object-oriented API or a functional one? Then if we offer this, should we do the same for all other concepts in the framework?</p>\n<p dir=\"auto\">I wondering if we shouldn't take that decision for our users and only present a single API to accomplish a given task, for simplicity and consistency. I personally have a preference for the OO approach.</p>", "author": "karllessard", "createdAt": "2020-10-29T01:44:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,711 @@\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesImpl;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+/** Built-in loss functions. */\n+public class Losses {", "originalCommit": "951443b6cba9e42911ca2cfae05bee920d5ff229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEwNTUyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515105523", "bodyText": "Metrics class will have to call these methods directly. The handling of the reduction is different in metrics from losses.", "author": "JimClarke5", "createdAt": "2020-10-30T13:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM4MzI5OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515383299", "bodyText": "Also, the methods in Losses, do not apply a reduction. Loss and Metric handle the reduction differently, so the raw loss needs to be fetched first. If we didn't have the loss methods , the the metric classes would have to duplicate that functionality. Also, I can only assume that Keras has this method split from the class for some reason. Of course, we could move the static method to its corresponding class and leave it static so that it can be still invoked independent of any reduction logic.\nIt is worth noting, that not all Metrics are based on Loss algorithms, such as AUC (Area under the curve).\nAlso, there is one class,\nCosineSimilarity, that uses a different algorithm between the Loss class (uses cosineSimilarity) as opposed to the corresponding Metric class (uses cosineProximity). While these two methods are in the same category, they do not compute the exact same result. I tried to reasearch why there is a difference, maybe @Craigacp can shed some light on this. One of the questions in Metrics is should the metric class be renamed to CosineProximity.", "author": "JimClarke5", "createdAt": "2020-10-30T21:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NzUwMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515397502", "bodyText": "That's because the losses version of cosine similarity isn't actually cosine similarity, it's the negative of it. Because the cosine of 0 is 1, but here when the two vectors are pointing in the same direction it emits -1. The metrics one actually computes cosine similarity.\nI assume it's to make the minimize function work consistently.", "author": "Craigacp", "createdAt": "2020-10-30T21:47:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg2ODA2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex b55550b9..ff0b513c 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -15,6 +30,8 @@ import org.tensorflow.types.TBool;\n import org.tensorflow.types.TInt64;\n import org.tensorflow.types.family.TNumber;\n \n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n /** Built-in loss functions. */\n public class Losses {\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r513870946", "body": "For me, a `*Impl` should be the implementation of an interface, this one looks more like a `LossesHelper` with all its static methods (and the class should probably be final).\n\nI did not went through the whole thing but it looks like these helpers could also be moved directly to `Loss` as protected methods?", "bodyText": "For me, a *Impl should be the implementation of an interface, this one looks more like a LossesHelper with all its static methods (and the class should probably be final).\nI did not went through the whole thing but it looks like these helpers could also be moved directly to Loss as protected methods?", "bodyHTML": "<p dir=\"auto\">For me, a <code>*Impl</code> should be the implementation of an interface, this one looks more like a <code>LossesHelper</code> with all its static methods (and the class should probably be final).</p>\n<p dir=\"auto\">I did not went through the whole thing but it looks like these helpers could also be moved directly to <code>Loss</code> as protected methods?</p>", "author": "karllessard", "createdAt": "2020-10-29T01:51:22Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,402 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.AssertThat;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.SetDiff1d;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * These are helper methods for Losses and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the Loss package.\n+ */\n+public class LossesImpl {", "originalCommit": "951443b6cba9e42911ca2cfae05bee920d5ff229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEwOTQxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515109416", "bodyText": "The split really comes for module visibility. Losses should be publicly accessible, while LossesImpl should be module private. Some LossesImpl methods may be used by metrics. Whether we call it LossesImpl of LossesHelper is a matter of preference. The current methods in LossesImpl should not be restricted to Loss classes as metrics classes may also make use of them, therefore protected is not the right semantic, .", "author": "JimClarke5", "createdAt": "2020-10-30T13:48:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyNDI0MQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r515224241", "bodyText": "It feels uncomfortable to me that we plan to use the LossesImpl methods from other parts of our framework while restricting them from public use. When a system's built-ins rely on privileged capabilities that aren't available to 3rd-party code, I think it is commonly a big problem for the system's extensibility. In this case, I do see room to argue that these methods aren't \"capabilities\", but are just \"implementation\" which can safely be hidden. But given that it is important to us to reuse them for our own metrics, I lean toward thinking of them as capabilities that we should expose.", "author": "deansher", "createdAt": "2020-10-30T16:30:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxNDA3Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r520014072", "bodyText": "There is a tight symmetry between Losses and Metrics as many (but not all) metrics rely on the methods in Losses.\nDon't think other packages will have this close of a relationship.", "author": "JimClarke5", "createdAt": "2020-11-09T18:06:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTcxMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105711", "bodyText": "Is there a potential use case justifying exposing these to the public? Seeing as they are utilities needed to implement Losses/Metrics.\nAgree with a rename to LossesHelper or LossesUtility to differentiate from interface implementation, however.", "author": "KartikChugh", "createdAt": "2020-11-11T04:31:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MDk0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nindex d77f513b..2ccb48ff 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n", "chunk": "@@ -16,6 +31,8 @@ import org.tensorflow.types.family.TNumber;\n import java.util.Arrays;\n import java.util.Collections;\n \n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n /**\n  * These are helper methods for Losses and will be module private when Java modularity is applied to\n  * TensorFlow Java. These methods should not be used outside of the Loss package.\n", "next_change": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\nsimilarity index 97%\nrename from tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nrename to tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\nindex 2ccb48ff..3dc1c66b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\n", "chunk": "@@ -34,10 +34,10 @@ import java.util.Collections;\n import static org.tensorflow.framework.utils.CastHelper.cast;\n \n /**\n- * These are helper methods for Losses and will be module private when Java modularity is applied to\n- * TensorFlow Java. These methods should not be used outside of the Loss package.\n+ * These are helper methods for Losses and Metrics and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the losses and metrics packages.\n  */\n-public class LossesImpl {\n+public class LossesHelper {\n \n   /**\n    * Squeeze or expand last dimension if needed with a sampleWeights of one.\n", "next_change": null}]}}]}}, {"oid": "ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "url": "https://github.com/tensorflow/java/commit/ebac9e84264db5b1ee101c6cd1b4966a77b9756f", "message": "Add copyright", "committedDate": "2020-10-29T17:54:49Z", "type": "commit"}, {"oid": "d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "url": "https://github.com/tensorflow/java/commit/d8f3254e7bf8e0eef7a8b715c805f9d378bc10ba", "message": "Add CastHelper and used that for all casts", "committedDate": "2020-10-29T18:04:39Z", "type": "commit"}, {"oid": "02573b594ca552371b8f42fa9e53c019143e6931", "url": "https://github.com/tensorflow/java/commit/02573b594ca552371b8f42fa9e53c019143e6931", "message": "Fix JavaDoc, change snake case to camel case.", "committedDate": "2020-11-09T18:18:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521105860", "body": "Class should have Javadoc description, no?", "bodyText": "Class should have Javadoc description, no?", "bodyHTML": "<p dir=\"auto\">Class should have Javadoc description, no?</p>", "author": "KartikChugh", "createdAt": "2020-11-11T04:32:02Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java", "diffHunk": "@@ -0,0 +1,303 @@\n+package org.tensorflow.framework.losses.impl;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.Reduction;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.family.TNumber;\n+\n+import java.util.Collections;\n+", "originalCommit": "928ef066f8d250b4ae41799eea40ab03fe3ecd23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTUwODAxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521508012", "bodyText": "I have changed the class name to LossesHelper.\nI don't understand your comment on Class JavaDoc. This is what I have in my copy.\n/**\n * These are helper methods for Losses and Metrics and will be module private when Java modularity is applied to\n * TensorFlow Java. These methods should not be used outside of the losses and metrics packages.\n */\n\nThe basic comment was put in a while a ago, and I just updated it to mention metrics.", "author": "JimClarke5", "createdAt": "2020-11-11T17:08:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU1NTE0Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521555142", "bodyText": "Rendering issue I think. Looks good, thanks.", "author": "KartikChugh", "createdAt": "2020-11-11T18:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTEwNTg2MA=="}], "type": "inlineReview", "revised_code": {"commit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\nsimilarity index 68%\nrename from tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\nrename to tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\nindex 4a276d68..3dc1c66b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesImpl.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/impl/LossesHelper.java\n", "chunk": "@@ -5,14 +20,24 @@ import org.tensorflow.Operand;\n import org.tensorflow.framework.losses.Reduction;\n import org.tensorflow.ndarray.Shape;\n import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.AssertThat;\n import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.SetDiff1d;\n import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.types.TBool;\n import org.tensorflow.types.TInt32;\n import org.tensorflow.types.family.TNumber;\n \n+import java.util.Arrays;\n import java.util.Collections;\n \n-public class LossesImpl {\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * These are helper methods for Losses and Metrics and will be module private when Java modularity is applied to\n+ * TensorFlow Java. These methods should not be used outside of the losses and metrics packages.\n+ */\n+public class LossesHelper {\n \n   /**\n    * Squeeze or expand last dimension if needed with a sampleWeights of one.\n", "next_change": null}]}}, {"oid": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "url": "https://github.com/tensorflow/java/commit/0bf49fe3203eb5f810ea09e0322fd36b6945856c", "message": "Change class LossesImpl to LossesHelper", "committedDate": "2020-11-11T17:01:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc5NDQyMw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r521794423", "body": "Can we remove this commented-out documentation? ", "bodyText": "Can we remove this commented-out documentation?", "bodyHTML": "<p dir=\"auto\">Can we remove this commented-out documentation?</p>", "author": "karllessard", "createdAt": "2020-11-12T03:20:01Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,728 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+  // private methods/**\n+  //   * Calculates the loss\n+  //   *\n+  //   * @param labels the truth values or labels\n+  //   * @param predictions the predictions\n+  //   * @param sampleWeights Optional SampleWeights acts as a coefficient for the loss. If a scalar\n+  // is\n+  //   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a\n+  // tensor\n+  //   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by\n+  // the\n+  //   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n+  //   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element\n+  // of\n+  //   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all\n+  // loss\n+  //   *     functions reduce by 1 dimension, usually axis=-1.)\n+  //   * @param <T> The data type of the predictions, sampleWeights and loss.\n+  //   * @param <U> The data type of the labels.\n+  //   * @return the loss\n+  //   *", "originalCommit": "0bf49fe3203eb5f810ea09e0322fd36b6945856c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE5MjQyOQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522192429", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-12T15:27:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc5NDQyMw=="}], "type": "inlineReview", "revised_code": {"commit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex ba641d19..0d1f5497 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -601,27 +601,6 @@ public class Losses {\n         tf.constant(-1));\n   }\n \n-  // private methods/**\n-  //   * Calculates the loss\n-  //   *\n-  //   * @param labels the truth values or labels\n-  //   * @param predictions the predictions\n-  //   * @param sampleWeights Optional SampleWeights acts as a coefficient for the loss. If a scalar\n-  // is\n-  //   *     provided, then the loss is simply scaled by the given value. If SampleWeights is a\n-  // tensor\n-  //   *     of size [batch_size], then the total loss for each sample of the batch is rescaled by\n-  // the\n-  //   *     corresponding element in the SampleWeights vector. If the shape of SampleWeights is\n-  //   *     [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element\n-  // of\n-  //   *     predictions is scaled by the corresponding value of SampleWeights. (Note on dN-1: all\n-  // loss\n-  //   *     functions reduce by 1 dimension, usually axis=-1.)\n-  //   * @param <T> The data type of the predictions, sampleWeights and loss.\n-  //   * @param <U> The data type of the labels.\n-  //   * @return the loss\n-  //   *\n \n   /**\n    * Smooths binary labels\n", "next_change": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 0d1f5497..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -601,7 +601,6 @@ public class Losses {\n         tf.constant(-1));\n   }\n \n-\n   /**\n    * Smooths binary labels\n    *\n", "next_change": null}]}}]}}, {"oid": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "url": "https://github.com/tensorflow/java/commit/0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "message": "Remove commented out JavaDoc", "committedDate": "2020-11-12T15:09:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522579190", "body": "We should open an issue to track inserting these cast checks into the optimizers for uniformity.", "bodyText": "We should open an issue to track inserting these cast checks into the optimizers for uniformity.", "bodyHTML": "<p dir=\"auto\">We should open an issue to track inserting these cast checks into the optimizers for uniformity.</p>", "author": "Craigacp", "createdAt": "2020-11-13T02:55:12Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/utils/CastHelper.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** A helper class for casting an Operand */\n+public class CastHelper {\n+\n+  /**\n+   * Casts an operand to the desired type.\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param value the value to be cast\n+   * @param requiredType the required data type\n+   * @param <T> the required data type\n+   * @param <U> the original data type of the value\n+   * @return the value cast to the required data type.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T extends TType, U extends TType> Operand<T> cast(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk3NjkxMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522976912", "bodyText": "I could do it in the #106  Learning Rate PR if that works.", "author": "JimClarke5", "createdAt": "2020-11-13T14:17:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAzNDIyMg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523034222", "bodyText": "No, let's not hold anything up for it, it's just something to clean up later.", "author": "Craigacp", "createdAt": "2020-11-13T15:45:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU3OTE5MA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MDkzNA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522580934", "body": "`label_smoothing` -> `labelSmoothing`, here and elsewhere in this file.", "bodyText": "label_smoothing -> labelSmoothing, here and elsewhere in this file.", "bodyHTML": "<p dir=\"auto\"><code>label_smoothing</code> -&gt; <code>labelSmoothing</code>, here and elsewhere in this file.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:01:28Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the cross-entropy loss between true labels and predicted labels.\n+ *\n+ * <p>Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For\n+ * each example, there should be a single floating-point value per prediction.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0.f, 1.f}, {0.f, 0.f}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.6f, 0.4f}, {0.4f, 0.6f}});\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 0.815\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {1.f, 0.f});\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.458f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces 1.630f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    BinaryCrossentropy bce = new BinaryCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = bce.call(labels, predictions);\n+ *    // produces [0.916f, 0.714f]\n+ * </pre>\n+ */\n+public class BinaryCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+\n+  /**\n+   * Creates a Binary Crossentropy Loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing and a\n+   * Loss Reduction of {@link Loss#REDUCTION_DEFAULT}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public BinaryCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using {@link Class#getSimpleName()} as the loss name, {@link\n+   * #FROM_LOGITS_DEFAULT} for fromLogits, and {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to the loss.\n+   */\n+  public BinaryCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT}, a reduction of {@link\n+   * Loss#REDUCTION_DEFAULT},\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using labelSmoothing of {@link #LABEL_SMOOTHING_DEFAULT} a\n+   * reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of the loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public BinaryCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT);\n+  }\n+\n+  /**\n+   * Creates a Binary Crossentropy loss using using {@link Class#getSimpleName()} as the loss name,\n+   * and a reduction of {@link Loss#REDUCTION_DEFAULT}.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk3NzUxNQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522977515", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MDkzNA=="}], "type": "inlineReview", "revised_code": {"commit": "914f16f4473512c8b5ef9df8ca43074b82d3edd0", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\nindex a2261705..56b06cce 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/BinaryCrossentropy.java\n", "chunk": "@@ -125,7 +125,7 @@ public class BinaryCrossentropy extends Loss {\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing A number in the range, [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n    *     compute the loss between the predicted labels and a smoothed version of the true labels,\n-   *     where the smoothing squeezes the labels towards 0.5. Larger values of label_smoothing\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n    *     correspond to heavier smoothing.\n    */\n   public BinaryCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQwMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581400", "body": "This one's still got the doc from BinaryCrossEntropy wrt `label_smoothing`. And it's snake_case.", "bodyText": "This one's still got the doc from BinaryCrossEntropy wrt label_smoothing. And it's snake_case.", "bodyHTML": "<p dir=\"auto\">This one's still got the doc from BinaryCrossEntropy wrt <code>label_smoothing</code>. And it's snake_case.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:03:20Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NDUyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522984528", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:29:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQwMA=="}], "type": "inlineReview", "revised_code": {"commit": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex e6665ddc..3306d16b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -169,10 +170,9 @@ public class CategoricalCrossentropy extends Loss {\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n     this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n", "next_change": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -171,7 +171,7 @@ public class CategoricalCrossentropy extends Loss {\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    confidence on label values are relaxed. e.g. <code>labelSmoothing=0.2<code> means that we will use a\n    *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQyOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581428", "body": "Incorrect doc.", "bodyText": "Incorrect doc.", "bodyHTML": "<p dir=\"auto\">Incorrect doc.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:03:30Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NDgzNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522984836", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:29:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQyOA=="}], "type": "inlineReview", "revised_code": {"commit": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex e6665ddc..3306d16b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -184,10 +184,9 @@ public class CategoricalCrossentropy extends Loss {\n    *\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param reduction Type of Reduction to apply to loss.\n    */\n   public CategoricalCrossentropy(\n", "next_change": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -185,8 +185,8 @@ public class CategoricalCrossentropy extends Loss {\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n-   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   *    confidence on label values are relaxed. e.g. <code>x=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param reduction Type of Reduction to apply to loss.\n    */\n   public CategoricalCrossentropy(\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQ3MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522581470", "body": "Incorrect doc.", "bodyText": "Incorrect doc.", "bodyHTML": "<p dir=\"auto\">Incorrect doc.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:03:39Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TNumber;\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/**\n+ * Computes the crossentropy loss between the labels and predictions.\n+ *\n+ * <p>Use this crossentropy loss function when there are two or more label classes. We expect labels\n+ * to be provided in a one_hot representation. If you want to provide labels as integers, please use\n+ * {@link SparseCategoricalCrossentropy} loss. There should be <code># classes</code> floating point\n+ * values per feature.\n+ *\n+ * <p>Standalone usage:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; labels =\n+ *        tf.constant(new float[][] {{0, 1, 0}, {0, 0, 1}});\n+ *    Operand&lt;TFloat32&gt; predictions =\n+ *        tf.constant(new float[][] {{0.05f, 0.95f, 0f}, {0.1f, 0.8f, 0.1f}});\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 1.177\n+ * </pre>\n+ *\n+ * <p>Calling with sample weight:\n+ *\n+ * <pre>\n+ *    Operand&lt;TFloat32&gt; sampleWeight = tf.constant(new float[] {0.3f, 0.7f});\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions, sampleWeight);\n+ *    // produces 0.814f\n+ * </pre>\n+ *\n+ * <p>Using <code>SUM</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce = new CategoricalCrossentropy(tf, Reduction.SUM);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces 2.354f\n+ * </pre>\n+ *\n+ * <p>Using <code>NONE</code> reduction type:\n+ *\n+ * <pre>\n+ *    CategoricalCrossentropy cce =\n+ *        new CategoricalCrossentropy(tf, Reduction.NONE);\n+ *    Operand&lt;TFloat32&gt; result = cce.call(labels, predictions);\n+ *    // produces [0.0513f, 2.303f]\n+ * </pre>\n+ */\n+public class CategoricalCrossentropy extends Loss {\n+  public static final boolean FROM_LOGITS_DEFAULT = false;\n+  public static final float LABEL_SMOOTHING_DEFAULT = 0.0f;\n+  public static final int DEFAULT_AXIS = -1;\n+\n+  private final boolean fromLogits;\n+  private final float labelSmoothing;\n+  private final int axis;\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and an axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public CategoricalCrossentropy(Ops tf) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #FROM_LOGITS_DEFAULT} for fromLogits,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, Reduction reduction) {\n+    this(tf, null, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss {@link #FROM_LOGITS_DEFAULT} for fromLogits, {@link\n+   * #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, Reduction reduction) {\n+    this(tf, name, FROM_LOGITS_DEFAULT, LABEL_SMOOTHING_DEFAULT, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * {@link #LABEL_SMOOTHING_DEFAULT} for labelSmoothing, a Loss Reduction of {@link\n+   * Loss#REDUCTION_DEFAULT}, and an axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits) {\n+    this(tf, null, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link #LABEL_SMOOTHING_DEFAULT} for\n+   * labelSmoothing, a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link\n+   * #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits) {\n+    this(tf, name, fromLogits, LABEL_SMOOTHING_DEFAULT, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name,\n+   * a Loss Reduction of {@link Loss#REDUCTION_DEFAULT}, and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   */\n+  public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing) {\n+    this(tf, null, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using a Loss Reduction of {@link Loss#REDUCTION_DEFAULT},\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   */\n+  public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n+    this(tf, name, fromLogits, labelSmoothing, REDUCTION_DEFAULT, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss using {@link Class#getSimpleName()} as the loss name\n+   * and a channel axis of {@link #DEFAULT_AXIS}\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n+   *     heavier smoothing.\n+   * @param reduction Type of Reduction to apply to loss.\n+   */\n+  public CategoricalCrossentropy(\n+      Ops tf, boolean fromLogits, float labelSmoothing, Reduction reduction) {\n+    this(tf, null, fromLogits, labelSmoothing, reduction, DEFAULT_AXIS);\n+  }\n+\n+  /**\n+   * Creates a categorical cross entropy Loss\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the name of this loss\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n+   *     loss between the predicted labels and a smoothed version of the true labels, where the\n+   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk4NjAxNg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522986016", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:31:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4MTQ3MA=="}], "type": "inlineReview", "revised_code": {"commit": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex e6665ddc..3306d16b 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -201,10 +200,9 @@ public class CategoricalCrossentropy extends Loss {\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param reduction Type of Reduction to apply to loss.\n    * @param axis The channels axis. <code>axis=-1</code> corresponds to data format `Channels Last'\n    *     and <code>axis=1</code> corresponds to data format 'Channels First'.\n", "next_change": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -201,7 +201,7 @@ public class CategoricalCrossentropy extends Loss {\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    confidence on label values are relaxed. e.g. <code>labelSmoothing=0.2<code> means that we will use a\n    *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param reduction Type of Reduction to apply to loss.\n    * @param axis The channels axis. <code>axis=-1</code> corresponds to data format `Channels Last'\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjU1OQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586559", "body": "I think this would be better called `smoothBinaryLabels` as it's not specific to the binary cross entropy as far as I can tell. But it's a private method so it's not too much of an issue.", "bodyText": "I think this would be better called smoothBinaryLabels as it's not specific to the binary cross entropy as far as I can tell. But it's a private method so it's not too much of an issue.", "bodyHTML": "<p dir=\"auto\">I think this would be better called <code>smoothBinaryLabels</code> as it's not specific to the binary cross entropy as far as I can tell. But it's a private method so it's not too much of an issue.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:11:08Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk5ODc4Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522998782", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:51:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjU1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 0d1f5497..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -614,11 +613,11 @@ public class Losses {\n    * @param <T> the data type of the labels\n    * @return the smoothed binary labels\n    */\n-  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+  private static <T extends TNumber> Operand<T> smoothBinaryLabels(\n       Ops tf, Operand<T> labels, float labelSmoothing) {\n     DataType<T> dataType = labels.asOutput().dataType();\n-    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n-    Operand<T> halfSmoothing = cast(tf,  tf.constant(0.5F * labelSmoothing), dataType);\n+    Operand<T> oneMinusSmoothing = cast(tf, tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = cast(tf, tf.constant(0.5F * labelSmoothing), dataType);\n     return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n   }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjY0Nw==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522586647", "body": "Similar comment to above, but `smoothCategoricalLabels`. Also I think the doc should explicitly state that it's smoothing the labels towards `1/n` where `n` is the number of classes.", "bodyText": "Similar comment to above, but smoothCategoricalLabels. Also I think the doc should explicitly state that it's smoothing the labels towards 1/n where n is the number of classes.", "bodyHTML": "<p dir=\"auto\">Similar comment to above, but <code>smoothCategoricalLabels</code>. Also I think the doc should explicitly state that it's smoothing the labels towards <code>1/n</code> where <code>n</code> is the number of classes.</p>", "author": "Craigacp", "createdAt": "2020-11-13T03:11:26Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.framework.losses;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.framework.losses.impl.LossTuple;\n+import org.tensorflow.framework.losses.impl.LossesHelper;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceAll;\n+import org.tensorflow.op.core.ReduceMax;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.math.Softplus;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+\n+import static org.tensorflow.framework.utils.CastHelper.cast;\n+\n+/** Built-in loss functions. */\n+public class Losses {\n+\n+  /** Default Fuzz factor. */\n+  public static final float EPSILON = 1e-7f;\n+\n+  /**\n+   * Calculates the mean absolute error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(abs(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsoluteError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(\n+        tf.math.abs(tf.math.sub(tLabels, predictions)), tf.constant(-1), Mean.keepDims(false));\n+  }\n+\n+  /**\n+   * Computes the mean squared error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(labels - predictions))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    Operand<T> tLabels = cast(tf,  labels, predictions.asOutput().dataType());\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    return tf.math.mean(tf.math.squaredDifference(predictions, tLabels), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Calculates the mean absolute percentage error between labels and predictions.\n+   *\n+   * <p><code>loss = 100 * reduceMean(abs((labels - predictions) / labels))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean absolute percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanAbsolutePercentageError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+    Operand<T> diff =\n+        tf.math.abs(\n+            tf.math.div(\n+                tf.math.sub(tLabels, predictions),\n+                tf.math.maximum(\n+                    tf.math.abs(tLabels), cast(tf,  tf.constant(EPSILON), dataType))));\n+    return tf.math.mul(\n+        cast(tf,  tf.constant(100), dataType), tf.math.mean(diff, tf.constant(-1)));\n+  }\n+\n+  /**\n+   * Calculates the mean squared logarithmic error between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(log(labels + 1) - log(predictions + 1)))</code>\n+   *\n+   * @param tf The TensorFlow Ops\n+   * @param labels the labels\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and result\n+   * @param <U> the data type of the labels\n+   * @return the mean squared logarithmic percentage error\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> meanSquaredLogarithmicError(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+\n+    Operand<T> firstLog = tf.math.log(tf.math.add(tf.math.maximum(predictions, epsilonConst), one));\n+    Operand<T> secondLog = tf.math.log(tf.math.add(tf.math.maximum(tLabels, epsilonConst), one));\n+\n+    return tf.math.mean(tf.math.squaredDifference(firstLog, secondLog), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the binary crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the binary crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> binaryCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, float labelSmoothing) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsBinaryX(tf, tLabels, labelSmoothing);\n+    }\n+    Operand<T> bce = binaryCrossentropyHelper(tf, tLabels, predictions, fromLogits);\n+    return tf.math.mean(bce, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the unreduced crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param target the target Operand\n+   * @param output the output, either logits or a probability distribution\n+   * @param fromLogits whether `output` is expected to be a logits tensor. By default, we consider\n+   *     that `output` encodes a probability distribution.\n+   * @param <T> the data type of the Operands\n+   * @return the binary crossentropy loss.\n+   */\n+  private static <T extends TNumber> Operand<T> binaryCrossentropyHelper(\n+      Ops tf, Operand<T> target, Operand<T> output, boolean fromLogits) {\n+    if (fromLogits)\n+      return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+\n+\n+    /* TODO - skip this loggic for now. It requires walking back the inputs which is not yet possible\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO - this does not work\n+      // TODO output = backtrackIdentity(output);\n+      // TODO if (output.op().type().equals(Sigmoid.OP_NAME)) {\n+      // TODO   if (output.op().numInputess() != 1)\n+      // TODO     throw new IllegalArgumentException(\"output can only have 1 output\");\n+      // TODO   output = output.op().inout(0);\n+       // TODO   return tf.nn.sigmoidCrossEntropyWithLogits(target, output);\n+      // TODO}\n+    }\n+    */\n+\n+    DataType<T> dataType = output.asOutput().dataType();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    return tf.math.neg(bce);\n+  }\n+\n+  /**\n+   * Computes the categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether to interpret predictions as a tensor of logit values\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *     confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *     value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param axis the\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical crossentropy loss.\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalCrossentropy(\n+      Ops tf,\n+      Operand<U> labels,\n+      Operand<T> predictions,\n+      boolean fromLogits,\n+      float labelSmoothing,\n+      int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> ops = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = ops.getTarget();\n+    tLabels = ops.getLabels();\n+\n+    if (labelSmoothing != 0.0f) {\n+      tLabels = smoothLabelsCatX(tf, tLabels, labelSmoothing);\n+    }\n+    if (fromLogits) {\n+      return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+    }\n+    /* TODO\n+    if (!(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (predictions.op().type().equals(\"Softmax\")) {\n+        if (predictions.op().numOutputs() != 1)\n+          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        predictions = predictions.op().output(0);\n+        return tf.nn.softmaxCrossEntropyWithLogits(tLabels, predictions, -1);\n+      }\n+    }\n+    */\n+\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    predictions =\n+        tf.math.div(\n+            predictions, tf.reduceSum(predictions, tf.constant(axis), ReduceSum.keepDims(true)));\n+    predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand<T> cce =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, tf.math.log(predictions)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(false));\n+    return tf.math.neg(cce);\n+  }\n+\n+  /**\n+   * Computes the categorical hinge loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be 0 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the categorical hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> categoricalHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    Operand<T> pos =\n+        tf.reduceSum(\n+            tf.math.mul(tLabels, predictions), tf.constant(-1), ReduceSum.keepDims(Boolean.FALSE));\n+    Operand<T> neg =\n+        tf.reduceMax(\n+            tf.math.mul(tf.math.sub(one, tLabels), predictions),\n+            tf.constant(-1),\n+            ReduceMax.keepDims(Boolean.FALSE));\n+    Operand<T> sub = tf.math.sub(neg, pos);\n+    Operand<T> add = tf.math.add(sub, one);\n+    return tf.math.maximum(zero, add);\n+  }\n+\n+  /**\n+   * Computes the cosine similarity loss between labels and predictions.\n+   *\n+   * <p>Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0\n+   * indicates orthogonality and values closer to -1 indicate greater similarity. The values closer\n+   * to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where\n+   * you try to maximize the proximity between predictions and targets. If either labels or\n+   * predictions is a zero vector, cosine similarity will be 0 regardless of the proximity between\n+   * predictions and targets.\n+   *\n+   * <p><code>loss = -sum(l2Norm(labels) * l2Norm(predictions))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param axis Axis along which to determine similarity.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the cosine similarity loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> cosineSimilarity(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    tLabels = l2Normalize(tf, tLabels, axis);\n+    predictions = l2Normalize(tf, predictions, axis);\n+    Operand<T> mathMul = tf.math.mul(tLabels, predictions);\n+    Operand<T> sum = tf.reduceSum(mathMul, tf.constant(axis), ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(sum);\n+  }\n+\n+  /**\n+   * Computes the hinge loss between labels and predictions\n+   *\n+   * <p><code>loss = reduceMean(maximum(1 - labels * predictions, 0))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> hinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+\n+    return tf.math.mean(\n+        tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Huber loss between labels and predictions.\n+   *\n+   * <p>For each value x in error = labels - predictions:\n+   *\n+   * <pre>\n+   *     loss = 0.5 * x^2                  if |x| &lt;= d\n+   *     loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d\n+   * </pre>\n+   *\n+   * <p>where d is delta.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param delta the point where the Huber loss function changes from quadratic to linear.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Huber loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> huber(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, float delta) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+\n+    Operand<T> error = tf.math.sub(predictions, tLabels);\n+    Operand<T> deltaConst = cast(tf,  tf.constant(delta), dataType);\n+    Operand<T> point5 = cast(tf,  tf.constant(0.5), dataType);\n+    Operand<T> absError = tf.math.abs(error);\n+    Operand<T> quadratic = tf.math.minimum(absError, deltaConst);\n+    Operand<T> linear = tf.math.sub(absError, quadratic);\n+    Operand<T> q2Point5 = tf.math.mul(point5, tf.math.mul(quadratic, quadratic));\n+    Operand<T> deltaLinear = tf.math.mul(deltaConst, linear);\n+    Operand<T> loss = tf.math.add(q2Point5, deltaLinear);\n+    return tf.math.mean(loss, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Kullback-Leibler divergence loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Kullback-Leibler divergence loss\n+   * @see <a href=\"https://en.wikipedia.org/wiki/Kullback?Leibler_divergence\">Kullback?Leibler\n+   *     divergence</a>\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> kullbackLeiblerDivergence(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    tLabels = tf.clipByValue(tLabels, epsilonConst, one);\n+    predictions = tf.clipByValue(predictions, epsilonConst, one);\n+    return tf.reduceSum(\n+        tf.math.mul(tLabels, tf.math.log(tf.math.div(tLabels, predictions))), tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the hyperbolic cosine loss between labels and predictions.\n+   *\n+   * <p><code>log(cosh(x))</code> is approximately equal to <code>(x ** 2) / 2</code> for small\n+   * <code>x</code> and to <code>abs(x) - log(2)</code> for large <code>x</code>. This means that\n+   * 'logCosh' works mostly like the mean squared error, but will not be so strongly affected by the\n+   * occasional wildly incorrect prediction.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the hyperbolic cosine divergence loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> logCosh(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> minusTwo = cast(tf,  tf.constant(-2), dataType);\n+    Operand<T> two = cast(tf,  tf.constant(2), dataType);\n+\n+    Operand<T> diff = tf.math.sub(predictions, tLabels);\n+    Softplus<T> softplus = tf.math.softplus(tf.math.mul(minusTwo, diff));\n+    Operand<T> logcosh = tf.math.sub(tf.math.add(diff, softplus), tf.math.log(two));\n+    return tf.math.mean(logcosh, tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the Poisson loss between labels and predictions.\n+   *\n+   * <p>The Poisson loss is the mean of the elements of the Tensor <code>\n+   * predictions - labels * log(predictions)</code>.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the Poisson loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> poisson(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+\n+    return tf.math.mean(\n+        tf.math.sub(\n+            predictions, tf.math.mul(tLabels, tf.math.log(tf.math.add(predictions, epsilonConst)))),\n+        tf.constant(-1));\n+  }\n+\n+  /**\n+   * Computes the sparse categorical crossentropy loss between labels and predictions.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param predictions the predictions\n+   * @param fromLogits Whether predictions is expected to be logits. By default, it is assumed that\n+   *     predictions encodes a probability distribution.\n+   * @param axis The dimension along which the entropy is computed.\n+   * @param <T> the data type of the predictions and labels\n+   * @return the sparse categorical crossentropy loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> sparseCategoricalCrossentropy(\n+      Ops tf, Operand<U> labels, Operand<T> predictions, boolean fromLogits, int axis) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> epsilonConst = cast(tf,  tf.constant(EPSILON), dataType);\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+\n+    /* TODO need ability to walk back inputs\n+    if (!fromLogits && !(predictions instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      /* TODO\n+      if (predictions.op().type().equals(Softmax.OP_NAME)) {\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO  if( output.op().numOutputs() != 1)\n+        //          throw new IllegalArgumentException(\"output can only have 1 output\");\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+\n+    }\n+     */\n+    if (!fromLogits) {\n+\n+      predictions = tf.clipByValue(predictions, epsilonConst, oneMinusEpsilonConst);\n+      predictions = tf.math.log(predictions);\n+    }\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    axis %= predictionsRank;\n+    if (axis < 0) {\n+      axis += predictionsRank;\n+    }\n+    if (axis != predictionsRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, predictionsRank);\n+      predictions = tf.linalg.transpose(predictions, tf.constant(axisNew));\n+    }\n+\n+    Operand<TInt64> iLabels = cast(tf,  labels, TInt64.DTYPE);\n+\n+    // Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    boolean updateShape = labelsRank != predictionsRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      Shape newShape = labelsShape.take(labelsRank - 1);\n+      iLabels = tf.reshape(iLabels, tf.constant(newShape)); // flatten one dimension\n+      predictions =\n+          tf.reshape(\n+              predictions,\n+              tf.constant(\n+                  new long[] {-1L, predictionsShape.size(predictionsShape.numDimensions() - 1)}));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Operand<T> loss = tf.nn.sparseSoftmaxCrossEntropyWithLogits(iLabels, predictions);\n+    if (updateShape && predictionsRank >= 3) {\n+      Shape newShape = predictionsShape.take(predictionsShape.numDimensions() - 1);\n+      loss = tf.reshape(loss, tf.constant(newShape));\n+    }\n+    return loss;\n+  }\n+\n+  /**\n+   * Computes the squared hinge loss between labels and predictions.\n+   *\n+   * <p><code>loss = reduceMean(square(maximum(1 - labels * predictions, 0)))</code>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets, values are expected to be -1 or 1. If binary (0 or 1) labels are *\n+   *     provided, they will be converted to -1 or 1.\n+   * @param predictions the predictions\n+   * @param <T> the data type of the predictions and labels\n+   * @return the squared hinge loss\n+   */\n+  public static <T extends TNumber, U extends TNumber> Operand<T> squaredHinge(\n+      Ops tf, Operand<U> labels, Operand<T> predictions) {\n+    DataType<T> dataType = predictions.asOutput().dataType();\n+    Operand<T> tLabels = cast(tf,  labels, dataType);\n+    LossTuple<T> lossTuple = LossesHelper.squeezeOrExpandDimensions(tf, tLabels, predictions, null);\n+    predictions = lossTuple.getTarget();\n+    tLabels = lossTuple.getLabels();\n+    Operand<T> one = cast(tf,  tf.constant(1), dataType);\n+    Operand<T> zero = cast(tf,  tf.constant(0), dataType);\n+\n+    tLabels = maybeConvertLabels(tf, tLabels);\n+    return tf.math.mean(\n+        tf.math.square(tf.math.maximum(tf.math.sub(one, tf.math.mul(tLabels, predictions)), zero)),\n+        tf.constant(-1));\n+  }\n+\n+\n+  /**\n+   * Smooths binary labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing A number in the range [0, 1]. When 0, no smoothing occurs. When &gt; 0,\n+   *     compute the loss between the predicted labels and a smoothed version of the true labels,\n+   *     where the smoothing squeezes the labels towards 0.5. Larger values of labelSmoothing\n+   *     correspond to heavier smoothing.\n+   * @param <T> the data type of the labels\n+   * @return the smoothed binary labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsBinaryX(\n+      Ops tf, Operand<T> labels, float labelSmoothing) {\n+    DataType<T> dataType = labels.asOutput().dataType();\n+    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> halfSmoothing = cast(tf,  tf.constant(0.5F * labelSmoothing), dataType);\n+    return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), halfSmoothing);\n+  }\n+\n+  /**\n+   * Smooths categorical labels\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels true targets\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   * @param <T> the data type of the labels\n+   * @return the smoothed categorical labels\n+   */\n+  private static <T extends TNumber> Operand<T> smoothLabelsCatX(", "originalCommit": "0eae9ee1b2ae6aaccf6c9216bbb90f8bcda0a9a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk5OTA2MA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r522999060", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T14:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU4NjY0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "b211937c946a67c6f3830e70bdccf97a54cd8051", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\nindex 0d1f5497..6b7c07d4 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/Losses.java\n", "chunk": "@@ -633,14 +632,14 @@ public class Losses {\n    * @param <T> the data type of the labels\n    * @return the smoothed categorical labels\n    */\n-  private static <T extends TNumber> Operand<T> smoothLabelsCatX(\n+  private static <T extends TNumber> Operand<T> smoothCategoricalLabels(\n       Ops tf, Operand<T> labels, float labelSmoothing) {\n     DataType<T> dataType = labels.asOutput().dataType();\n-    Operand<T> smoothing = cast(tf,  tf.constant(labelSmoothing), dataType);\n+    Operand<T> smoothing = cast(tf, tf.constant(labelSmoothing), dataType);\n     Shape labelsShape = labels.asOutput().shape();\n     int numDims = labelsShape.numDimensions();\n-    Operand<T> numClasses = cast(tf,  tf.constant(labelsShape.size(numDims - 1)), dataType);\n-    Operand<T> oneMinusSmoothing = cast(tf,  tf.constant(1.f - labelSmoothing), dataType);\n+    Operand<T> numClasses = cast(tf, tf.constant(labelsShape.size(numDims - 1)), dataType);\n+    Operand<T> oneMinusSmoothing = cast(tf, tf.constant(1.f - labelSmoothing), dataType);\n     return tf.math.add(tf.math.mul(labels, oneMinusSmoothing), tf.math.div(smoothing, numClasses));\n   }\n \n", "next_change": null}]}}, {"oid": "b211937c946a67c6f3830e70bdccf97a54cd8051", "url": "https://github.com/tensorflow/java/commit/b211937c946a67c6f3830e70bdccf97a54cd8051", "message": "Changed method name from smoothLabelsBinaryX to smoothBinaryLabels,\nsmoothLabelsCatX to smoothCategoricalLabels.\n\nAdded clarification oin JavaDoc for cosineSimilarity to describe the difference between the mathematical definition for cosine similarity and the loss definition.", "committedDate": "2020-11-13T14:56:15Z", "type": "commit"}, {"oid": "3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "url": "https://github.com/tensorflow/java/commit/3e0669e03b4c2a5bab5b4ffc0e2387dc0adccefb", "message": "Fixed JavaDoc for labelSmoothing", "committedDate": "2020-11-13T14:56:54Z", "type": "commit"}, {"oid": "914f16f4473512c8b5ef9df8ca43074b82d3edd0", "url": "https://github.com/tensorflow/java/commit/914f16f4473512c8b5ef9df8ca43074b82d3edd0", "message": "Fixed JavaDoc to change label_smoothing to labelSmoothing.", "committedDate": "2020-11-13T14:57:43Z", "type": "commit"}, {"oid": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "url": "https://github.com/tensorflow/java/commit/7eefbb7f197c731a7d304d055fd242d1acd9835f", "message": "Fix formatting", "committedDate": "2020-11-13T14:58:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzUxMA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047510", "body": "`alue` -> `value`", "bodyText": "alue -> value", "bodyHTML": "<p dir=\"auto\"><code>alue</code> -&gt; <code>value</code></p>", "author": "Craigacp", "createdAt": "2020-11-13T16:05:52Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -184,10 +184,9 @@ public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float la\n    *\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>", "originalCommit": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEzMzU5Mg==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523133592", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-11-13T18:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzUxMA=="}], "type": "inlineReview", "revised_code": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -185,8 +185,8 @@ public class CategoricalCrossentropy extends Loss {\n    * @param tf the TensorFlow Ops\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n-   *    alue of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n+   *    confidence on label values are relaxed. e.g. <code>x=0.2<code> means that we will use a\n+   *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    * @param reduction Type of Reduction to apply to loss.\n    */\n   public CategoricalCrossentropy(\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzgzOA==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523047838", "body": "`label_smoothing` -> `labelSmoothing`, and in the docs below.", "bodyText": "label_smoothing -> labelSmoothing, and in the docs below.", "bodyHTML": "<p dir=\"auto\"><code>label_smoothing</code> -&gt; <code>labelSmoothing</code>, and in the docs below.</p>", "author": "Craigacp", "createdAt": "2020-11-13T16:06:24Z", "path": "tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java", "diffHunk": "@@ -169,10 +170,9 @@ public CategoricalCrossentropy(Ops tf, boolean fromLogits, float labelSmoothing)\n    * @param tf the TensorFlow Ops\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n-   * @param labelSmoothing Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the\n-   *     loss between the predicted labels and a smoothed version of the true labels, where the\n-   *     smoothing squeezes the labels towards 0.5. Larger values of label_smoothing correspond to\n-   *     heavier smoothing.\n+   * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n+   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a", "originalCommit": "7eefbb7f197c731a7d304d055fd242d1acd9835f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEzNDIyMQ==", "url": "https://github.com/tensorflow/java/pull/129#discussion_r523134221", "bodyText": "Ugh, I thought I got them all. I just did a global replace across the package.", "author": "JimClarke5", "createdAt": "2020-11-13T18:10:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NzgzOA=="}], "type": "inlineReview", "revised_code": {"commit": "b87ad16118442643b845bb4e24a0145eea0056fb", "changed_code": [{"header": "diff --git a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\nindex 3306d16b..522446be 100644\n--- a/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n+++ b/tensorflow-framework/src/main/java/org/tensorflow/framework/losses/CategoricalCrossentropy.java\n", "chunk": "@@ -171,7 +171,7 @@ public class CategoricalCrossentropy extends Loss {\n    * @param name the name of this loss\n    * @param fromLogits Whether to interpret predictions as a tensor of logit values\n    * @param labelSmoothing Float in <code>[0, 1]</code>. When <code>&gt; 0</code>, label values are smoothed, meaning the\n-   *    confidence on label values are relaxed. e.g. <code>label_smoothing=0.2<code> means that we will use a\n+   *    confidence on label values are relaxed. e.g. <code>labelSmoothing=0.2<code> means that we will use a\n    *    value of </code>0.1<code> for label </code>0<code> and </code>0.9<code> for label </code>1<code>\n    */\n   public CategoricalCrossentropy(Ops tf, String name, boolean fromLogits, float labelSmoothing) {\n", "next_change": null}]}}, {"oid": "b87ad16118442643b845bb4e24a0145eea0056fb", "url": "https://github.com/tensorflow/java/commit/b87ad16118442643b845bb4e24a0145eea0056fb", "message": "replace label_smoothing with labelSmoothing.\nfix typo error in JavaDoc comment", "committedDate": "2020-11-13T18:11:21Z", "type": "commit"}, {"oid": "c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "url": "https://github.com/tensorflow/java/commit/c43cd21165c67d1972bc693a5d4a9ccdb49395eb", "message": "Add copyright to test cases", "committedDate": "2020-11-16T23:17:39Z", "type": "commit"}, {"oid": "4d9fd24b809fd4141e61ca504b32b251a993cf8c", "url": "https://github.com/tensorflow/java/commit/4d9fd24b809fd4141e61ca504b32b251a993cf8c", "message": "Fix copyright to attribute TensorFlow Authors.", "committedDate": "2020-11-16T23:36:54Z", "type": "commit"}, {"oid": "d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "url": "https://github.com/tensorflow/java/commit/d56d8d9dbfb8d1d8cfc4b829ea1e3b3bfe93478d", "message": "Fix typo on broadcast in JavaDoc", "committedDate": "2020-11-16T23:45:07Z", "type": "commit"}, {"oid": "744e32463c4aa8def4456fac4bcec53536a04fa4", "url": "https://github.com/tensorflow/java/commit/744e32463c4aa8def4456fac4bcec53536a04fa4", "message": "Fix typo on broadcast in JavaDoc", "committedDate": "2020-11-16T23:46:12Z", "type": "commit"}]}