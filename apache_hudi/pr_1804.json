{"pr_number": 1804, "pr_title": "[HUDI-960] Implementation of the HFile base and log file format.", "pr_author": "prashantwason", "pr_createdAt": "2020-07-06T21:28:13Z", "pr_url": "https://github.com/apache/hudi/pull/1804", "timeline": [{"oid": "1d972f2438671d360101f385e1f776015e9e7468", "url": "https://github.com/apache/hudi/commit/1d972f2438671d360101f385e1f776015e9e7468", "message": "[HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence, this base file format is well suited to applications like RFC-15, RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning", "committedDate": "2020-08-25T23:38:01Z", "type": "commit"}, {"oid": "aa11837b588b919b029e837ad945568d369e6d4b", "url": "https://github.com/apache/hudi/commit/aa11837b588b919b029e837ad945568d369e6d4b", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-26T01:08:46Z", "type": "commit"}, {"oid": "aa11837b588b919b029e837ad945568d369e6d4b", "url": "https://github.com/apache/hudi/commit/aa11837b588b919b029e837ad945568d369e6d4b", "message": "Code review feedback\n\n - Rebasing against master\n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize()", "committedDate": "2020-08-26T01:08:46Z", "type": "forcePushed"}, {"oid": "5a59b14de7af988f44ec0bf9e252202f2f2ae158", "url": "https://github.com/apache/hudi/commit/5a59b14de7af988f44ec0bf9e252202f2f2ae158", "message": "Some more improvements\n - Added three new configs for HoodieHFileConfig - prefetchBlocksOnOpen, cacheDataInL1, dropBehindCacheCompaction\n - Throw UnsupportedException in HFileReader.getRecordKeys()\n - Updated HoodieCopyOnWriteTable to create the correct merge handle (HoodieSortedMergeHandle for HFile and HoodieMergeHandle otherwise)", "committedDate": "2020-08-26T19:56:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTY5Mg==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477529692", "body": "Not sure if we can leak the type of base file to compactor. But did you think about having two overloaded methods here. So for parquet compaction path, iterator will be passed in, where as for hfile compaction, record map will be passed in. ", "bodyText": "Not sure if we can leak the type of base file to compactor. But did you think about having two overloaded methods here. So for parquet compaction path, iterator will be passed in, where as for hfile compaction, record map will be passed in.", "bodyHTML": "<p dir=\"auto\">Not sure if we can leak the type of base file to compactor. But did you think about having two overloaded methods here. So for parquet compaction path, iterator will be passed in, where as for hfile compaction, record map will be passed in.</p>", "author": "nsivabalan", "createdAt": "2020-08-26T19:11:41Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java", "diffHunk": "@@ -90,9 +91,10 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa\n    * Called by the compactor code path.\n    */\n   public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-      String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordIterator, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+      String partitionPath, String fileId, Map<String, HoodieRecord<T>> recordMap,", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4MDU0NA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478480544", "bodyText": "Ideally not. the more the compactor can function wihtout knowing the base file type specifics, the better", "author": "vinothchandar", "createdAt": "2020-08-27T14:53:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477545069", "body": "@vinothchandar : wrt you comment on having two diff configs. I see similar configs at other places too. like bloom index parallelism, we have one config per index type. Initially I thought we will have any one config which will be used by any index type that is being initialized. But I saw that every index has its own set of configs and don't share any. ", "bodyText": "@vinothchandar : wrt you comment on having two diff configs. I see similar configs at other places too. like bloom index parallelism, we have one config per index type. Initially I thought we will have any one config which will be used by any index type that is being initialized. But I saw that every index has its own set of configs and don't share any.", "bodyHTML": "<p dir=\"auto\"><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/vinothchandar/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vinothchandar\">@vinothchandar</a> : wrt you comment on having two diff configs. I see similar configs at other places too. like bloom index parallelism, we have one config per index type. Initially I thought we will have any one config which will be used by any index type that is being initialized. But I saw that every index has its own set of configs and don't share any.</p>", "author": "nsivabalan", "createdAt": "2020-08-26T19:41:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java", "diffHunk": "@@ -94,6 +100,16 @@ public Builder parquetPageSize(int pageSize) {\n       return this;\n     }\n \n+    public Builder hfileMaxFileSize(long maxFileSize) {\n+      props.setProperty(HFILE_FILE_MAX_BYTES, String.valueOf(maxFileSize));", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ3OTQ1Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478479457", "bodyText": "not following. sorry. are you suggesting having a single config or two?\nSo, we need to have a config per usage of HFile. so we can control the base file size for data, metadata, record index separately.\nWe cannot have a generic base.file.size or hfile.size config here, at this level IMO. cc @prashantwason", "author": "vinothchandar", "createdAt": "2020-08-27T14:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ5MDk4MA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478490980", "bodyText": "I see that you split this function into file specific functions. That doable but with more base file formats added, it may be cumbersome and verbose to keep adding the .limitXXXFileSize for specific formats.\n\n@prashantwason I think we need to eventually have a config \"per use\" of base file - data, metadata, index - since people may want to control them differently. So, in that sense, this has to kind of change.\nyes the change is backwards compatible to RDD clients (which I thought was okay, since its just uber. if you prefer to not have that, lmk. IMO, its about time, we cleaned these up, given we are moving to having way more base files/tables in the mix)", "author": "vinothchandar", "createdAt": "2020-08-27T15:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0NTA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r477549400", "body": "minor. why call this as sortedRecordsMap. I don't see any sorting actually", "bodyText": "minor. why call this as sortedRecordsMap. I don't see any sorting actually", "bodyHTML": "<p dir=\"auto\">minor. why call this as sortedRecordsMap. I don't see any sorting actually</p>", "author": "nsivabalan", "createdAt": "2020-08-26T19:49:14Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.storage.HoodieHFileReader;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.compress.Compression;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.hbase.util.Pair;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * HoodieHFileDataBlock contains a list of records stored inside an HFile format. It is used with the HFile\n+ * base file format.\n+ */\n+public class HoodieHFileDataBlock extends HoodieDataBlock {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileDataBlock.class);\n+  private static Compression.Algorithm compressionAlgorithm = Compression.Algorithm.GZ;\n+  private static int blockSize = 1 * 1024 * 1024;\n+\n+  public HoodieHFileDataBlock(@Nonnull Map<HeaderMetadataType, String> logBlockHeader,\n+       @Nonnull Map<HeaderMetadataType, String> logBlockFooter,\n+       @Nonnull Option<HoodieLogBlockContentLocation> blockContentLocation, @Nonnull Option<byte[]> content,\n+       FSDataInputStream inputStream, boolean readBlockLazily) {\n+    super(logBlockHeader, logBlockFooter, blockContentLocation, content, inputStream, readBlockLazily);\n+  }\n+\n+  public HoodieHFileDataBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,\n+       boolean readBlockLazily, long position, long blockSize, long blockEndpos, Schema readerSchema,\n+       Map<HeaderMetadataType, String> header, Map<HeaderMetadataType, String> footer) {\n+    super(content, inputStream, readBlockLazily,\n+          Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), readerSchema, header,\n+          footer);\n+  }\n+\n+  public HoodieHFileDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {\n+    super(records, header, new HashMap<>());\n+  }\n+\n+  @Override\n+  public HoodieLogBlockType getBlockType() {\n+    return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+  }\n+\n+  @Override\n+  protected byte[] serializeRecords() throws IOException {\n+    HFileContext context = new HFileContextBuilder().withBlockSize(blockSize).withCompression(compressionAlgorithm)\n+        .build();\n+    Configuration conf = new Configuration();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    FSDataOutputStream ostream = new FSDataOutputStream(baos, null);\n+\n+    HFile.Writer writer = HFile.getWriterFactory(conf, cacheConfig)\n+        .withOutputStream(ostream).withFileContext(context).create();\n+\n+    // Serialize records into bytes\n+    Map<String, byte[]> sortedRecordsMap = new TreeMap<>();", "originalCommit": "aa11837b588b919b029e837ad945568d369e6d4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4NjM1Nw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478486357", "bodyText": "its a tree map. its sorted/ordered", "author": "vinothchandar", "createdAt": "2020-08-27T15:00:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ4ODc4Mw==", "url": "https://github.com/apache/hudi/pull/1804#discussion_r478488783", "bodyText": "Does not matter where the sorting is performed. It will definitely be referable in the partitioner.\n\n@prashantwason for some reason I cannot find your comment overlaid here. (are you using the review feature?) . Anywasy, what I meant was just rdd.repartitionAndSort... for the AppendHandle path as well. There is no generic partitioner in Hudi, since the ones we have are all serving different purposes", "author": "vinothchandar", "createdAt": "2020-08-27T15:04:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU0OTQwMA=="}], "type": "inlineReview"}, {"oid": "cbad0b447df4e643701783a4c62f1ff988f96423", "url": "https://github.com/apache/hudi/commit/cbad0b447df4e643701783a4c62f1ff988f96423", "message": "Fixing checkstyle", "committedDate": "2020-08-31T14:17:16Z", "type": "commit"}]}