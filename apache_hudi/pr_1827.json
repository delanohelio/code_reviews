{"pr_number": 1827, "pr_title": "[HUDI-1089] Refactor hudi-client to support multi-engine", "pr_author": "wangxianghu", "pr_createdAt": "2020-07-14T11:56:30Z", "pr_url": "https://github.com/apache/hudi/pull/1827", "timeline": [{"oid": "9d7a51de66b0198df85c04d93d2e73ed5381714c", "url": "https://github.com/apache/hudi/commit/9d7a51de66b0198df85c04d93d2e73ed5381714c", "message": "Renaming some Abstract* classes as Hoodie* to improve readability", "committedDate": "2020-10-01T16:13:53Z", "type": "commit"}, {"oid": "9d7a51de66b0198df85c04d93d2e73ed5381714c", "url": "https://github.com/apache/hudi/commit/9d7a51de66b0198df85c04d93d2e73ed5381714c", "message": "Renaming some Abstract* classes as Hoodie* to improve readability", "committedDate": "2020-10-01T16:13:53Z", "type": "forcePushed"}, {"oid": "137da2b4ab862e16a48cfba84d56fc61ca762d69", "url": "https://github.com/apache/hudi/commit/137da2b4ab862e16a48cfba84d56fc61ca762d69", "message": "fix Unable to find driver bind address from spark config", "committedDate": "2020-10-01T18:21:52Z", "type": "commit"}, {"oid": "13795f5c1223424d44244e46acb0864b93ec403e", "url": "https://github.com/apache/hudi/commit/13795f5c1223424d44244e46acb0864b93ec403e", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine", "committedDate": "2020-07-15T15:22:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwMjIwNg==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r455702206", "body": "The new style or the old style, which one is right?", "bodyText": "The new style or the old style, which one is right?", "bodyHTML": "<p dir=\"auto\">The new style or the old style, which one is right?</p>", "author": "yanghua", "createdAt": "2020-07-16T10:58:14Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java", "diffHunk": "@@ -593,8 +592,8 @@ public String repairCompaction(\n     return output;\n   }\n \n-  private String getRenamesToBePrinted(List<RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n-      boolean headerOnly, String operation) {\n+  private String getRenamesToBePrinted(List<BaseCompactionAdminClient.RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n+                                       boolean headerOnly, String operation) {", "originalCommit": "13795f5c1223424d44244e46acb0864b93ec403e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzOTM4NQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r456239385", "bodyText": "Hi, @yanghua thanks for your review. I am not sure which one is right either, I will roll back these style issues just to keep as same as before.", "author": "wangxianghu", "createdAt": "2020-07-17T06:14:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwMjIwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwNTI1OA==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r455705258", "body": "Why do we need to change this class?", "bodyText": "Why do we need to change this class?", "bodyHTML": "<p dir=\"auto\">Why do we need to change this class?</p>", "author": "yanghua", "createdAt": "2020-07-16T11:04:11Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -40,7 +39,7 @@\n  * HoodieParquetWriter extends the ParquetWriter to help limit the size of underlying file. Provides a way to check if\n  * the current file can take more records with the <code>canWrite()</code>\n  */\n-public class HoodieParquetWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+public class HoodieParquetWriter<R extends IndexedRecord>", "originalCommit": "13795f5c1223424d44244e46acb0864b93ec403e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI0NzcyMQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r456247721", "bodyText": "Why do we need to change this class?\n\nThe Generic \"T\" is useless in this class, and it causes some generic problems in the abstraction, so I removed it.", "author": "wangxianghu", "createdAt": "2020-07-17T06:38:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwNTI1OA=="}], "type": "inlineReview"}, {"oid": "56dfec1d2da5b27397ee7821cd90aa3e81728f56", "url": "https://github.com/apache/hudi/commit/56dfec1d2da5b27397ee7821cd90aa3e81728f56", "message": "fix conflicts", "committedDate": "2020-07-30T15:31:03Z", "type": "forcePushed"}, {"oid": "958dac8b6c33b0ee46645c901f0ee0fafcd0f846", "url": "https://github.com/apache/hudi/commit/958dac8b6c33b0ee46645c901f0ee0fafcd0f846", "message": "rebase master WIP", "committedDate": "2020-08-23T03:58:42Z", "type": "forcePushed"}, {"oid": "009fae91da08a60624f054cebff04d5c59077251", "url": "https://github.com/apache/hudi/commit/009fae91da08a60624f054cebff04d5c59077251", "message": "abstract Helper class and write client", "committedDate": "2020-08-24T13:06:54Z", "type": "forcePushed"}, {"oid": "1c7d03a43097ffecf404dda29dd5aaa74fde95ca", "url": "https://github.com/apache/hudi/commit/1c7d03a43097ffecf404dda29dd5aaa74fde95ca", "message": "finish most source class", "committedDate": "2020-08-27T15:31:57Z", "type": "forcePushed"}, {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086", "url": "https://github.com/apache/hudi/commit/7aedc0bdc1565d47b044487aab04b659167bd086", "message": "resolve conflicts", "committedDate": "2020-09-07T14:23:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjMzOQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484516339", "body": "the MOR equivalent method got moved I guess", "bodyText": "the MOR equivalent method got moved I guess", "bodyHTML": "<p dir=\"auto\">the MOR equivalent method got moved I guess</p>", "author": "vinothchandar", "createdAt": "2020-09-07T16:33:44Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class RollbackUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(RollbackUtils.class);\n+\n+  static Map<HoodieLogBlock.HeaderMetadataType, String> generateHeader(String instantToRollback, String rollbackInstantTime) {\n+    // generate metadata\n+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>(3);\n+    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, rollbackInstantTime);\n+    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, instantToRollback);\n+    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n+        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n+    return header;\n+  }\n+\n+  /**\n+   * Helper to merge 2 rollback-stats for a given partition.\n+   *\n+   * @param stat1 HoodieRollbackStat\n+   * @param stat2 HoodieRollbackStat\n+   * @return Merged HoodieRollbackStat\n+   */\n+  static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRollbackStat stat2) {\n+    ValidationUtils.checkArgument(stat1.getPartitionPath().equals(stat2.getPartitionPath()));\n+    final List<String> successDeleteFiles = new ArrayList<>();\n+    final List<String> failedDeleteFiles = new ArrayList<>();\n+    final Map<FileStatus, Long> commandBlocksCount = new HashMap<>();\n+    final List<FileStatus> filesToRollback = new ArrayList<>();\n+    Option.ofNullable(stat1.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    Option.ofNullable(stat2.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n+  }\n+\n+  /**\n+   * Generate all rollback requests that needs rolling back this action without actually performing rollback for COW table type.\n+   * @param fs instance of {@link FileSystem} to use.\n+   * @param basePath base path of interest.\n+   * @param shouldAssumeDatePartitioning {@code true} if date partitioning should be assumed. {@code false} otherwise.\n+   * @return {@link List} of {@link ListingBasedRollbackRequest}s thus collected.\n+   */\n+  public static List<ListingBasedRollbackRequest> generateRollbackRequestsByListingCOW(FileSystem fs, String basePath, boolean shouldAssumeDatePartitioning) {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgyMzQ2NQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484823465", "bodyText": "the MOR equivalent method got moved I guess\n\nYes,  MOR equivalent method moved to SparkRollbackUtils.", "author": "wangxianghu", "createdAt": "2020-09-08T10:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjMzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjk5Ng==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484516996", "body": "might make sense to move the COMPACT_POOL_NAME also to the child class", "bodyText": "might make sense to move the COMPACT_POOL_NAME also to the child class", "bodyHTML": "<p dir=\"auto\">might make sense to move the COMPACT_POOL_NAME also to the child class</p>", "author": "vinothchandar", "createdAt": "2020-09-07T16:35:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/async/HoodieSparkAsyncCompactService.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.asyc.BaseAsyncCompactService;\n+import org.apache.hudi.client.AbstractHoodieWriteClient;\n+import org.apache.hudi.client.BaseCompactor;\n+import org.apache.hudi.client.HoodieSparkCompactor;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.IntStream;\n+\n+public class HoodieSparkAsyncCompactService extends BaseAsyncCompactService {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkAsyncCompactService.class);\n+\n+  private transient JavaSparkContext jssc;\n+  public HoodieSparkAsyncCompactService(HoodieEngineContext context, AbstractHoodieWriteClient client) {\n+    super(context, client);\n+    this.jssc = HoodieSparkEngineContext.getSparkContext(context);\n+  }\n+\n+  public HoodieSparkAsyncCompactService(HoodieEngineContext context, AbstractHoodieWriteClient client, boolean runInDaemonMode) {\n+    super(context, client, runInDaemonMode);\n+    this.jssc = HoodieSparkEngineContext.getSparkContext(context);\n+  }\n+\n+  @Override\n+  protected BaseCompactor createCompactor(AbstractHoodieWriteClient client) {\n+    return new HoodieSparkCompactor(client);\n+  }\n+\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> {\n+          Thread t = new Thread(r, \"async_compact_thread\");\n+          t.setDaemon(isRunInDaemonMode());\n+          return t;\n+        });\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDAxMw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484520013", "body": "wondering if this renaming will have any impact on deserializing older plans. cc @bvaradar to confirm ", "bodyText": "wondering if this renaming will have any impact on deserializing older plans. cc @bvaradar to confirm", "bodyHTML": "<p dir=\"auto\">wondering if this renaming will have any impact on deserializing older plans. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/bvaradar/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bvaradar\">@bvaradar</a> to confirm</p>", "author": "vinothchandar", "createdAt": "2020-09-07T16:46:49Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java", "diffHunk": "@@ -60,38 +60,38 @@ public static void main(String[] args) throws Exception {\n    */\n   public void run(JavaSparkContext jsc) throws Exception {\n     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath);\n-    try (CompactionAdminClient admin = new CompactionAdminClient(jsc, cfg.basePath)) {\n+    try (HoodieSparkCompactionAdminClient admin = new HoodieSparkCompactionAdminClient(new HoodieSparkEngineContext(jsc), cfg.basePath)) {\n       final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n       if (cfg.outputPath != null && fs.exists(new Path(cfg.outputPath))) {\n         throw new IllegalStateException(\"Output File Path already exists\");\n       }\n       switch (cfg.operation) {\n         case VALIDATE:\n-          List<ValidationOpResult> res =\n+          List<BaseCompactionAdminClient.ValidationOpResult> res =\n               admin.validateCompactionPlan(metaClient, cfg.compactionInstantTime, cfg.parallelism);\n           if (cfg.printOutput) {\n             printOperationResult(\"Result of Validation Operation :\", res);\n           }\n           serializeOperationResult(fs, res);\n           break;\n         case UNSCHEDULE_FILE:\n-          List<RenameOpResult> r = admin.unscheduleCompactionFileId(\n+          List<BaseCompactionAdminClient.RenameOpResult> r = admin.unscheduleCompactionFileId(\n               new HoodieFileGroupId(cfg.partitionPath, cfg.fileId), cfg.skipValidation, cfg.dryRun);\n           if (cfg.printOutput) {\n             System.out.println(r);\n           }\n           serializeOperationResult(fs, r);\n           break;\n         case UNSCHEDULE_PLAN:\n-          List<RenameOpResult> r2 = admin.unscheduleCompactionPlan(cfg.compactionInstantTime, cfg.skipValidation,\n+          List<BaseCompactionAdminClient.RenameOpResult> r2 = admin.unscheduleCompactionPlan(cfg.compactionInstantTime, cfg.skipValidation,", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDkyOQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484520929", "body": "rename: generateInputRecords", "bodyText": "rename: generateInputRecords", "bodyHTML": "<p dir=\"auto\">rename: generateInputRecords</p>", "author": "vinothchandar", "createdAt": "2020-09-07T16:50:14Z", "path": "hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java", "diffHunk": "@@ -43,18 +43,18 @@\n /**\n  * Spark Data frame based bootstrap input provider.\n  */\n-public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider {\n+public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider<JavaRDD<HoodieRecord>> {\n \n   private final transient SparkSession sparkSession;\n \n   public SparkParquetBootstrapDataProvider(TypedProperties props,\n-                                           JavaSparkContext jsc) {\n-    super(props, jsc);\n-    this.sparkSession = SparkSession.builder().config(jsc.getConf()).getOrCreate();\n+                                           HoodieSparkEngineContext context) {\n+    super(props, context);\n+    this.sparkSession = SparkSession.builder().config(context.getJavaSparkContext().getConf()).getOrCreate();\n   }\n \n   @Override\n-  public JavaRDD<HoodieRecord> generateInputRecordRDD(String tableName, String sourceBasePath,\n+  public JavaRDD<HoodieRecord> generateInputRecord(String tableName, String sourceBasePath,", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkyODExNw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484928117", "bodyText": "rename: generateInputRecords\n\ndone", "author": "wangxianghu", "createdAt": "2020-09-08T13:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODgxNQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484538815", "body": "why was this change required?", "bodyText": "why was this change required?", "bodyHTML": "<p dir=\"auto\">why was this change required?</p>", "author": "vinothchandar", "createdAt": "2020-09-07T18:15:06Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -81,7 +82,9 @@\n    */\n   public static SparkConf getSparkConfForTest(String appName) {\n     SparkConf sparkConf = new SparkConf().setAppName(appName)\n-        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").setMaster(\"local[8]\");\n+        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .set(\"spark.driver.host\",\"localhost\")", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTE2MQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931161", "bodyText": "why was this change required?\n\nI have rolled back this.\nThe unit test is not runnable in my local yesterday, but ok now... weird", "author": "wangxianghu", "createdAt": "2020-09-08T13:43:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5ODU1OQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484598559", "body": "need to ensure the ordering of closing resources is the same as before/", "bodyText": "need to ensure the ordering of closing resources is the same as before/", "bodyHTML": "<p dir=\"auto\">need to ensure the ordering of closing resources is the same as before/</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:01:41Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -716,32 +674,97 @@ private void rollbackPendingCommits() {\n    * @param compactionInstantTime Compaction Instant Time\n    * @return RDD of Write Status\n    */\n-  private JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n-    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n-    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n-      rollbackInflightCompaction(inflightInstant, table);\n-      table.getMetaClient().reloadActiveTimeline();\n-    }\n-    compactionTimer = metrics.getCompactionCtx();\n-    HoodieWriteMetadata compactionMetadata = table.compact(jsc, compactionInstantTime);\n-    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n-    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n-      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n-    }\n-    return statuses;\n-  }\n+  protected abstract O compact(String compactionInstantTime, boolean shouldComplete);\n \n   /**\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n-  private Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n+  protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n     Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {\n       // inline compaction should auto commit as the user is never given control\n       compact(compactionInstantTime, true);\n     });\n     return compactionInstantTimeOpt;\n   }\n+\n+  /**\n+   * Finalize Write operation.\n+   *\n+   * @param table       HoodieTable\n+   * @param instantTime Instant Time\n+   * @param stats       Hoodie Write Stat\n+   */\n+  protected void finalizeWrite(HoodieTable<T, I, K, O, P> table, String instantTime, List<HoodieWriteStat> stats) {\n+    try {\n+      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();\n+      table.finalizeWrite(context, instantTime, stats);\n+      if (finalizeCtx != null) {\n+        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));\n+        durationInMs.ifPresent(duration -> {\n+          LOG.info(\"Finalize write elapsed time (milliseconds): \" + duration);\n+          metrics.updateFinalizeWriteMetrics(duration, stats.size());\n+        });\n+      }\n+    } catch (HoodieIOException ioe) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + instantTime + \" due to finalize errors.\", ioe);\n+    }\n+  }\n+\n+  public HoodieMetrics getMetrics() {\n+    return metrics;\n+  }\n+\n+  public HoodieIndex<T, I, K, O, P> getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Get HoodieTable and init {@link Timer.Context}.\n+   *\n+   * @param operationType write operation type\n+   * @param instantTime   current inflight instant time\n+   * @return HoodieTable\n+   */\n+  protected abstract HoodieTable<T, I, K, O, P> getTableAndInitCtx(WriteOperationType operationType, String instantTime);\n+\n+  /**\n+   * Sets write schema from last instant since deletes may not have schema set in the config.\n+   */\n+  protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {\n+    try {\n+      HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();\n+      Option<HoodieInstant> lastInstant =\n+          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType()))\n+              .lastInstant();\n+      if (lastInstant.isPresent()) {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(\n+            activeTimeline.getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);\n+        if (commitMetadata.getExtraMetadata().containsKey(HoodieCommitMetadata.SCHEMA_KEY)) {\n+          config.setSchema(commitMetadata.getExtraMetadata().get(HoodieCommitMetadata.SCHEMA_KEY));\n+        } else {\n+          throw new HoodieIOException(\"Latest commit does not have any schema in commit metadata\");\n+        }\n+      } else {\n+        throw new HoodieIOException(\"Deletes issued without any prior commits\");\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"IOException thrown while reading last commit metadata\", e);\n+    }\n+  }\n+\n+  public abstract AsyncCleanerService startAsyncCleaningIfEnabled(AbstractHoodieWriteClient<T, I, K, O, P> client, String instantTime);\n+\n+  @Override\n+  public void close() {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTcwNw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931707", "bodyText": "need to ensure the ordering of closing resources is the same as before/\n\nYes, they are the same.", "author": "wangxianghu", "createdAt": "2020-09-08T13:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5ODU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5OTI3NQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484599275", "body": "Let's name this `SparkRDDWriteClient` ?", "bodyText": "Let's name this SparkRDDWriteClient ?", "bodyHTML": "<p dir=\"auto\">Let's name this <code>SparkRDDWriteClient</code> ?</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:05:55Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTMxNw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931317", "bodyText": "Let's name this SparkRDDWriteClient ?\n\ndone", "author": "wangxianghu", "createdAt": "2020-09-08T13:43:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5OTI3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDc0Mg==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484600742", "body": "why are we not hanging onto the returned object? ", "bodyText": "why are we not hanging onto the returned object?", "bodyHTML": "<p dir=\"auto\">why are we not hanging onto the returned object?</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:13:43Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,\n+    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkWriteClient.class);\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending, Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Register hudi classes for Kryo serialization.\n+   *\n+   * @param conf instance of SparkConf\n+   * @return SparkConf\n+   */\n+  public static SparkConf registerClasses(SparkConf conf) {\n+    conf.registerKryoClasses(new Class[]{HoodieWriteConfig.class, HoodieRecord.class, HoodieKey.class});\n+    return conf;\n+  }\n+\n+  @Override\n+  protected HoodieIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createIndex(HoodieWriteConfig writeConfig) {\n+    return HoodieSparkIndexFactory.createIndex(config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata) {\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, stats, extraMetadata);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieSparkTable.create(config, context);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> filterExists(JavaRDD<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieTable table = HoodieSparkTable.create(config, context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    JavaRDD<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);\n+    table.bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    startAsyncCleaningIfEnabled(this, instantTime);", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMzAxMw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484933013", "bodyText": "why are we not hanging onto the returned object?\n\nmy bad. done", "author": "wangxianghu", "createdAt": "2020-09-08T13:46:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDc0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDgwNw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484600807", "body": "same here and everywhere else. ", "bodyText": "same here and everywhere else.", "bodyHTML": "<p dir=\"auto\">same here and everywhere else.</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:14:01Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,\n+    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkWriteClient.class);\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending, Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Register hudi classes for Kryo serialization.\n+   *\n+   * @param conf instance of SparkConf\n+   * @return SparkConf\n+   */\n+  public static SparkConf registerClasses(SparkConf conf) {\n+    conf.registerKryoClasses(new Class[]{HoodieWriteConfig.class, HoodieRecord.class, HoodieKey.class});\n+    return conf;\n+  }\n+\n+  @Override\n+  protected HoodieIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createIndex(HoodieWriteConfig writeConfig) {\n+    return HoodieSparkIndexFactory.createIndex(config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata) {\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, stats, extraMetadata);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieSparkTable.create(config, context);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> filterExists(JavaRDD<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieTable table = HoodieSparkTable.create(config, context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    JavaRDD<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);\n+    table.bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT_PREPPED, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT_PREPPED);\n+    startAsyncCleaningIfEnabled(this, instantTime);", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMTYzMQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484601631", "body": "I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up. ", "bodyText": "I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up.", "bodyHTML": "<p dir=\"auto\">I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up.</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:18:10Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/embedded/SparkEmbeddedTimelineService.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.embedded;\n+\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+\n+/**\n+ * Spark implementation of Timeline Service.\n+ */\n+public class SparkEmbeddedTimelineService extends BaseEmbeddedTimelineService {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkEmbeddedTimelineService.class);\n+\n+  public SparkEmbeddedTimelineService(HoodieEngineContext context, FileSystemViewStorageConfig config) {\n+    super(context, config);\n+  }\n+\n+  @Override\n+  public void setHostAddrFromContext(HoodieEngineContext context) {\n+    SparkConf sparkConf = HoodieSparkEngineContext.getSparkContext(context).getConf();\n+    String hostAddr = sparkConf.get(\"spark.driver.host\", null);", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxNjI5NA==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485016294", "bodyText": "I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up.\n\ndone, add hoodie.embed.timeline.server.host to HoodieWriteConfig,  it can be obtained via method getEmbeddedServerHost()\nThis is not the same as before(acquired from sparkConf). users who enabled the embedded timeline service should config this hostaddr additionally.", "author": "wangxianghu", "createdAt": "2020-09-08T15:37:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMTYzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMjU2Mg==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484602562", "body": "note to self: make sure these methods are now in the base class", "bodyText": "note to self: make sure these methods are now in the base class", "bodyHTML": "<p dir=\"auto\">note to self: make sure these methods are now in the base class</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:23:12Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/HoodieSparkHBaseIndex.java", "diffHunk": "@@ -18,169 +18,60 @@\n \n package org.apache.hudi.index.hbase;\n \n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n import org.apache.hudi.common.model.HoodieKey;\n import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n import org.apache.hudi.common.util.collection.Pair;\n-import org.apache.hudi.config.HoodieHBaseIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.exception.HoodieDependentSystemUnavailableException;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.table.HoodieTable;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hbase.HBaseConfiguration;\n-import org.apache.hadoop.hbase.HRegionLocation;\n-import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.Connection;\n-import org.apache.hadoop.hbase.client.ConnectionFactory;\n-import org.apache.hadoop.hbase.client.Delete;\n-import org.apache.hadoop.hbase.client.Get;\n-import org.apache.hadoop.hbase.client.HTable;\n-import org.apache.hadoop.hbase.client.Mutation;\n-import org.apache.hadoop.hbase.client.Put;\n-import org.apache.hadoop.hbase.client.RegionLocator;\n-import org.apache.hadoop.hbase.client.Result;\n-import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.SparkConf;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.Function2;\n+import scala.Tuple2;\n \n import java.io.IOException;\n-import java.io.Serializable;\n import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n \n-import scala.Tuple2;\n+public class HoodieSparkHBaseIndex<T extends HoodieRecordPayload> extends BaseHoodieHBaseIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n \n-/**\n- * Hoodie Index implementation backed by HBase.\n- */\n-public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkHBaseIndex.class);\n \n   public static final String DEFAULT_SPARK_EXECUTOR_INSTANCES_CONFIG_NAME = \"spark.executor.instances\";\n   public static final String DEFAULT_SPARK_DYNAMIC_ALLOCATION_ENABLED_CONFIG_NAME = \"spark.dynamicAllocation.enabled\";\n   public static final String DEFAULT_SPARK_DYNAMIC_ALLOCATION_MAX_EXECUTORS_CONFIG_NAME =\n       \"spark.dynamicAllocation.maxExecutors\";\n \n-  private static final byte[] SYSTEM_COLUMN_FAMILY = Bytes.toBytes(\"_s\");\n-  private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n-  private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n-  private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n-\n-  private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n-  private static Connection hbaseConnection = null;\n-  private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n-  private int maxQpsPerRegionServer;\n-  /**\n-   * multiPutBatchSize will be computed and re-set in updateLocation if\n-   * {@link HoodieHBaseIndexConfig#HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP} is set to true.\n-   */\n-  private Integer multiPutBatchSize;\n-  private Integer numRegionServersForTable;\n-  private final String tableName;\n-  private HBasePutBatchSizeCalculator putBatchSizeCalculator;\n-\n-  public HBaseIndex(HoodieWriteConfig config) {\n+  public HoodieSparkHBaseIndex(HoodieWriteConfig config) {\n     super(config);\n-    this.tableName = config.getHbaseTableName();\n-    addShutDownHook();\n-    init(config);\n-  }\n-\n-  private void init(HoodieWriteConfig config) {\n-    this.multiPutBatchSize = config.getHbaseIndexGetBatchSize();\n-    this.qpsFraction = config.getHbaseIndexQPSFraction();\n-    this.maxQpsPerRegionServer = config.getHbaseIndexMaxQPSPerRegionServer();\n-    this.putBatchSizeCalculator = new HBasePutBatchSizeCalculator();\n-    this.hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-  }\n-\n-  public HBaseIndexQPSResourceAllocator createQPSResourceAllocator(HoodieWriteConfig config) {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzIzOA==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484603238", "body": "note to self: make sure these methods are in the base class now ", "bodyText": "note to self: make sure these methods are in the base class now", "bodyHTML": "<p dir=\"auto\">note to self: make sure these methods are in the base class now</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:26:16Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/simple/HoodieSparkGlobalSimpleIndex.java", "diffHunk": "@@ -71,43 +75,14 @@ public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n    * @return {@link JavaRDD} of records with record locations set\n    */\n   protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n-                                                         HoodieTable<T> hoodieTable) {\n+                                                         HoodieTable hoodieTable) {\n \n     JavaPairRDD<String, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n     JavaPairRDD<HoodieKey, HoodieRecordLocation> allRecordLocationsInTable = fetchAllRecordLocations(jsc, hoodieTable,\n         config.getGlobalSimpleIndexParallelism());\n     return getTaggedRecords(keyedInputRecordRDD, allRecordLocationsInTable);\n   }\n \n-  /**\n-   * Fetch record locations for passed in {@link HoodieKey}s.\n-   *\n-   * @param jsc         instance of {@link JavaSparkContext} to use\n-   * @param hoodieTable instance of {@link HoodieTable} of interest\n-   * @param parallelism parallelism to use\n-   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n-   */\n-  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchAllRecordLocations(JavaSparkContext jsc,\n-                                                                                 HoodieTable hoodieTable,\n-                                                                                 int parallelism) {\n-    List<Pair<String, HoodieBaseFile>> latestBaseFiles = getAllBaseFilesInTable(jsc, hoodieTable);\n-    return fetchRecordLocations(jsc, hoodieTable, parallelism, latestBaseFiles);\n-  }\n-\n-  /**\n-   * Load all files for all partitions as <Partition, filename> pair RDD.\n-   */\n-  protected List<Pair<String, HoodieBaseFile>> getAllBaseFilesInTable(final JavaSparkContext jsc, final HoodieTable hoodieTable) {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzgwMA==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484603800", "body": "at the MergeHandle level, we need not introduce any notion of RDDs. the `io` package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve ", "bodyText": "at the MergeHandle level, we need not introduce any notion of RDDs. the io package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve", "bodyHTML": "<p dir=\"auto\">at the MergeHandle level, we need not introduce any notion of RDDs. the <code>io</code> package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:29:10Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -54,9 +60,9 @@\n import java.util.Set;\n \n @SuppressWarnings(\"Duplicates\")\n-public class HoodieMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T> {\n+public class HoodieSparkMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjU4MQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485026581", "bodyText": "at the MergeHandle level, we need not introduce any notion of RDDs. the io package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve\n\nActually not yet. #1756 added support for rollbacks using marker files, and MarkerFiles is spark related.", "author": "wangxianghu", "createdAt": "2020-09-08T15:52:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDAxMQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604011", "body": "please refrain from moving methods around within the file. it makes life hard during review :( ", "bodyText": "please refrain from moving methods around within the file. it makes life hard during review :(", "bodyHTML": "<p dir=\"auto\">please refrain from moving methods around within the file. it makes life hard during review :(</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:30:08Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -71,34 +77,25 @@\n   protected boolean useWriterSchema;\n   private HoodieBaseFile baseFileToMerge;\n \n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     init(fileId, recordItr);\n     init(fileId, partitionPath, hoodieTable.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get());\n   }\n \n   /**\n    * Called by compactor code path.\n    */\n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-      Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n-      HoodieBaseFile dataFileToBeMerged, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n+                                HoodieBaseFile dataFileToBeMerged, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     this.keyToNewRecords = keyToNewRecords;\n     this.useWriterSchema = true;\n     init(fileId, this.partitionPath, dataFileToBeMerged);\n   }\n \n-  @Override", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNzU4Mw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485027583", "bodyText": "please refrain from moving methods around within the file. it makes life hard during review :(\n\nsorry for the inconvenient, let me see what I can do to avoid this :)", "author": "wangxianghu", "createdAt": "2020-09-08T15:54:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDI5Mg==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604292", "body": "same here. we need to make sure these factory methods don't have spark vs non-spark versions", "bodyText": "same here. we need to make sure these factory methods don't have spark vs non-spark versions", "bodyHTML": "<p dir=\"auto\">same here. we need to make sure these factory methods don't have spark vs non-spark versions</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:31:14Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkAppendHandleFactory.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * Factory to create {@link HoodieSparkAppendHandle}.\n+ */\n+public class SparkAppendHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwMDY4Nw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494000687", "bodyText": "same here. we need to make sure these factory methods don't have spark vs non-spark versions\n\ndone", "author": "wangxianghu", "createdAt": "2020-09-24T02:22:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDI5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDM2OQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604369", "body": "same. is there a way to not make these spark specific", "bodyText": "same. is there a way to not make these spark specific", "bodyHTML": "<p dir=\"auto\">same. is there a way to not make these spark specific</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:31:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkCreateHandleFactory.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+public class SparkCreateHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  @Override\n+  public HoodieSparkCreateHandle create(final HoodieWriteConfig hoodieConfig,", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMzc1NQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484933755", "bodyText": "same. is there a way to not make these spark specific\n\nI'll give a try", "author": "wangxianghu", "createdAt": "2020-09-08T13:47:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDM2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTI5MA==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484605290", "body": "we can actually try and keep this generic and just pass in what we need from `taggedRecords` to constructor instead of the entire thing ", "bodyText": "we can actually try and keep this generic and just pass in what we need from taggedRecords to constructor instead of the entire thing", "bodyHTML": "<p dir=\"auto\">we can actually try and keep this generic and just pass in what we need from <code>taggedRecords</code> to constructor instead of the entire thing</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:36:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkWorkloadProfile.java", "diffHunk": "@@ -22,49 +22,22 @@\n import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.util.Option;\n-\n import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n \n-import java.io.Serializable;\n-import java.util.HashMap;\n import java.util.Map;\n-import java.util.Set;\n-\n-import scala.Tuple2;\n \n /**\n- * Information about incoming records for upsert/insert obtained either via sampling or introspecting the data fully.\n- * <p>\n- * TODO(vc): Think about obtaining this directly from index.tagLocation\n+ * Spark implementation of {@link BaseWorkloadProfile}.\n+ * @param <T>\n  */\n-public class WorkloadProfile<T extends HoodieRecordPayload> implements Serializable {\n-\n-  /**\n-   * Input workload.\n-   */\n-  private final JavaRDD<HoodieRecord<T>> taggedRecords;\n-\n-  /**\n-   * Computed workload profile.\n-   */\n-  private final HashMap<String, WorkloadStat> partitionPathStatMap;\n-\n-  /**\n-   * Global workloadStat.\n-   */\n-  private final WorkloadStat globalStat;\n-\n-  public WorkloadProfile(JavaRDD<HoodieRecord<T>> taggedRecords) {\n-    this.taggedRecords = taggedRecords;\n-    this.partitionPathStatMap = new HashMap<>();\n-    this.globalStat = new WorkloadStat();\n-    buildProfile();\n+public class SparkWorkloadProfile<T extends HoodieRecordPayload> extends BaseWorkloadProfile<JavaRDD<HoodieRecord<T>>> {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA1ODI2MQ==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485058261", "bodyText": "we can actually try and keep this generic and just pass in what we need from taggedRecords to constructor instead of the entire thing\n\ndone", "author": "wangxianghu", "createdAt": "2020-09-08T16:43:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTUxNw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484605517", "body": "hmmm? why do we return null here", "bodyText": "hmmm? why do we return null here", "bodyHTML": "<p dir=\"auto\">hmmm? why do we return null here</p>", "author": "vinothchandar", "createdAt": "2020-09-08T01:37:30Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -77,34 +81,44 @@\n import org.apache.parquet.hadoop.ParquetReader;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n-public class BootstrapCommitActionExecutor<T extends HoodieRecordPayload<T>>\n-    extends BaseCommitActionExecutor<T, HoodieBootstrapWriteMetadata> {\n+public class SparkBootstrapCommitActionExecutor<T extends HoodieRecordPayload>\n+    extends BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>, HoodieBootstrapWriteMetadata> {\n \n-  private static final Logger LOG = LogManager.getLogger(BootstrapCommitActionExecutor.class);\n+  private static final Logger LOG = LogManager.getLogger(SparkBootstrapCommitActionExecutor.class);\n   protected String bootstrapSchema = null;\n   private transient FileSystem bootstrapSourceFileSystem;\n \n-  public BootstrapCommitActionExecutor(JavaSparkContext jsc, HoodieWriteConfig config, HoodieTable<?> table,\n-      Option<Map<String, String>> extraMetadata) {\n-    super(jsc, new HoodieWriteConfig.Builder().withProps(config.getProps())\n-        .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n-        .withBulkInsertParallelism(config.getBootstrapParallelism())\n-        .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n+  public SparkBootstrapCommitActionExecutor(HoodieSparkEngineContext context,\n+                                            HoodieWriteConfig config,\n+                                            HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> table,\n+                                            Option<Map<String, String>> extraMetadata) {\n+    super(context, new HoodieWriteConfig.Builder().withProps(config.getProps())\n+            .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n+            .withBulkInsertParallelism(config.getBootstrapParallelism())\n+            .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n         extraMetadata);\n     bootstrapSourceFileSystem = FSUtils.getFs(config.getBootstrapSourceBasePath(), hadoopConf);\n   }\n \n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {", "originalCommit": "7aedc0bdc1565d47b044487aab04b659167bd086", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDk0MDIwMw==", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484940203", "bodyText": "hmmm? why do we return null here\n\nBootstrapCommitActionExecutor dose not need this method actually, inherited from its parent class.", "author": "wangxianghu", "createdAt": "2020-09-08T13:55:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTUxNw=="}], "type": "inlineReview"}, {"oid": "01b4f4a433c49a83df2d6370effaa1356df1343d", "url": "https://github.com/apache/hudi/commit/01b4f4a433c49a83df2d6370effaa1356df1343d", "message": "add SparkHoodieIndex", "committedDate": "2020-09-08T15:33:16Z", "type": "forcePushed"}, {"oid": "54d352ac06b57da3116aefa00aec80cdd45ada5a", "url": "https://github.com/apache/hudi/commit/54d352ac06b57da3116aefa00aec80cdd45ada5a", "message": "trigger ci", "committedDate": "2020-09-09T14:57:00Z", "type": "forcePushed"}, {"oid": "23b9e3649bb337c5815b3d1234354667e0a47859", "url": "https://github.com/apache/hudi/commit/23b9e3649bb337c5815b3d1234354667e0a47859", "message": "trigger ci", "committedDate": "2020-09-10T13:53:02Z", "type": "forcePushed"}, {"oid": "23b9e3649bb337c5815b3d1234354667e0a47859", "url": "https://github.com/apache/hudi/commit/23b9e3649bb337c5815b3d1234354667e0a47859", "message": "trigger ci", "committedDate": "2020-09-10T13:53:02Z", "type": "forcePushed"}, {"oid": "23b9e3649bb337c5815b3d1234354667e0a47859", "url": "https://github.com/apache/hudi/commit/23b9e3649bb337c5815b3d1234354667e0a47859", "message": "trigger ci", "committedDate": "2020-09-10T13:53:02Z", "type": "forcePushed"}, {"oid": "b67ee14568ceb0bc0519594de42a20f6972ce2c0", "url": "https://github.com/apache/hudi/commit/b67ee14568ceb0bc0519594de42a20f6972ce2c0", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine", "committedDate": "2020-09-30T23:53:10Z", "type": "commit"}, {"oid": "cd5d75ec4eb8594f37b37f5cb78cdb0d0d138713", "url": "https://github.com/apache/hudi/commit/cd5d75ec4eb8594f37b37f5cb78cdb0d0d138713", "message": "Code Review Comments\n\n* Renaming HoodieSparkAsyncCompactService to SparkAsyncCompactService\n* Bug in SparkStreamingAsyncCompactService of not calling the super constructor with daemon mode.\n* Rename methods in HoodieEngineContext to setJobStatus() and setProperty()\n* Rename common packages to under client.common package\n* Bug in HoodieMergeHandle and compactor not getting the right merge memory", "committedDate": "2020-10-01T04:48:20Z", "type": "commit"}, {"oid": "6a79819a84ffb129574b218e4dd4c435f4e94b58", "url": "https://github.com/apache/hudi/commit/6a79819a84ffb129574b218e4dd4c435f4e94b58", "message": "More code review changes\n\n* Making HoodieSnapshotCopier/HoodieSnapshotExporter all use HoodieContext\n* More replacements of jsc.parallelize across hudi-spark-client\n* More replacements of jsc.setJobGroup across hudi-spark-client\n* Removing usages of HoodieIndex#fetchRecordLocation everywhere", "committedDate": "2020-10-01T06:52:52Z", "type": "commit"}, {"oid": "6a79819a84ffb129574b218e4dd4c435f4e94b58", "url": "https://github.com/apache/hudi/commit/6a79819a84ffb129574b218e4dd4c435f4e94b58", "message": "More code review changes\n\n* Making HoodieSnapshotCopier/HoodieSnapshotExporter all use HoodieContext\n* More replacements of jsc.parallelize across hudi-spark-client\n* More replacements of jsc.setJobGroup across hudi-spark-client\n* Removing usages of HoodieIndex#fetchRecordLocation everywhere", "committedDate": "2020-10-01T06:52:52Z", "type": "forcePushed"}, {"oid": "7e33f73f70a65be635357e6fddc3053ac5a00694", "url": "https://github.com/apache/hudi/commit/7e33f73f70a65be635357e6fddc3053ac5a00694", "message": "Dropping HoodieIndex#fetchRecordLocation API\n\n* Not used by any other major API\n* Removing `P` from the templatized list of parameters", "committedDate": "2020-10-01T08:34:51Z", "type": "commit"}]}