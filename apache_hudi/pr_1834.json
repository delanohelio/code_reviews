{"pr_number": 1834, "pr_title": "[HUDI-1013] Adding Bulk Insert V2 implementation", "pr_author": "nsivabalan", "pr_createdAt": "2020-07-15T13:32:12Z", "pr_url": "https://github.com/apache/hudi/pull/1834", "timeline": [{"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-11T11:18:57Z", "type": "commit"}, {"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance", "committedDate": "2020-08-11T11:18:57Z", "type": "forcePushed"}, {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "url": "https://github.com/apache/hudi/commit/5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-11T16:06:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NDg3Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468744877", "body": "note to self: need to understand this better and see if we can simplify", "bodyText": "note to self: need to understand this better and see if we can simplify", "bodyHTML": "<p dir=\"auto\">note to self: need to understand this better and see if we can simplify</p>", "author": "vinothchandar", "createdAt": "2020-08-11T17:26:32Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODMwNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468868307", "bodyText": "responded elsewhere. we could move this to getRecordKey(Row) and getPartitionPath(Row) if need be.", "author": "nsivabalan", "createdAt": "2020-08-11T21:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NDg3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468745957", "body": "as far as I can tell, this is private and set to null by default and not assigned anywhere else. so we will never pass `if (null != ..)` check. I think this should be if (null ==converterFn) if the intention was lazy initialization.", "bodyText": "as far as I can tell, this is private and set to null by default and not assigned anywhere else. so we will never pass if (null != ..) check. I think this should be if (null ==converterFn) if the intention was lazy initialization.", "bodyHTML": "<p dir=\"auto\">as far as I can tell, this is private and set to null by default and not assigned anywhere else. so we will never pass <code>if (null != ..)</code> check. I think this should be if (null ==converterFn) if the intention was lazy initialization.</p>", "author": "vinothchandar", "createdAt": "2020-08-11T17:28:23Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NTc1MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468885750", "bodyText": "hmmm, not sure on this. I will reconcile w/ Balaji on this.", "author": "nsivabalan", "createdAt": "2020-08-11T21:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMDgxMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468900813", "bodyText": "guess, this could be a bug. just now realizing, we don't have tests for this. we have tests for all built in key generators, but not for this. Will get it done by tonight. sorry to have missed.", "author": "nsivabalan", "createdAt": "2020-08-11T22:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468749640", "body": "got this when compiling\r\n\r\n```\r\nError:(433, 16) overloaded method value commit with alternatives:\r\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean <and>\r\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\r\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\r\n        client.commit(instantTime, writeStatuses,\r\n```", "bodyText": "got this when compiling\nError:(433, 16) overloaded method value commit with alternatives:\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean <and>\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\n        client.commit(instantTime, writeStatuses,", "bodyHTML": "<p dir=\"auto\">got this when compiling</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"Error:(433, 16) overloaded method value commit with alternatives:\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean &lt;and&gt;\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\n        client.commit(instantTime, writeStatuses,\n\"><pre><code>Error:(433, 16) overloaded method value commit with alternatives:\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean &lt;and&gt;\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\n        client.commit(instantTime, writeStatuses,\n</code></pre></div>", "author": "vinothchandar", "createdAt": "2020-08-11T17:34:45Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -95,20 +95,20 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n       Option<Map<String, String>> extraMetadata) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    return commit(instantTime, writeStatuses, extraMetadata, metaClient.getCommitActionType());\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStat(instantTime, stats, extraMetadata);\n   }\n \n-  private boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata, String actionType) {\n-\n+  // fixme(bulkinsertv2) this name is ughh\n+  public boolean commitStat(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1MDI0MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468750240", "bodyText": "Looks like we cannot avoid a new public API. so might as well rename", "author": "vinothchandar", "createdAt": "2020-08-11T17:35:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468755967", "body": "note to self : check if this is indeed correct. i was expecting us to do something like `row.setNullAt(i-5)` ", "bodyText": "note to self : check if this is indeed correct. i was expecting us to do something like row.setNullAt(i-5)", "bodyHTML": "<p dir=\"auto\">note to self : check if this is indeed correct. i was expecting us to do something like <code>row.setNullAt(i-5)</code></p>", "author": "vinothchandar", "createdAt": "2020-08-11T17:45:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg3OTE4MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468879180", "bodyText": "even I had the same doubt when I start reviewing this at first. thats why added some java docs for this class. row will have meta columns as well. just that meta columns will not be fetched from the row but from instance variables in this class. @bvaradar did some analysis before arriving at this", "author": "nsivabalan", "createdAt": "2020-08-11T21:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MDM5NA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468970394", "bodyText": "I think this is because row already has these metafields per se in the schema", "author": "vinothchandar", "createdAt": "2020-08-12T02:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2MTA4NQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468761085", "body": "rename to `getMetaColumnVal` ", "bodyText": "rename to getMetaColumnVal", "bodyHTML": "<p dir=\"auto\">rename to <code>getMetaColumnVal</code></p>", "author": "vinothchandar", "createdAt": "2020-08-11T17:54:22Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);\n+    }\n+  }\n+\n+  @Override\n+  public void update(int i, Object value) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = value.toString();\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = value.toString();\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = value.toString();\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = value.toString();\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = value.toString();\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.update(i, value);\n+    }\n+  }\n+\n+  private String getHoodieColumnVal(int ordinal) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468768261", "body": "should we be hardcoding these?", "bodyText": "should we be hardcoding these?", "bodyHTML": "<p dir=\"auto\">should we be hardcoding these?</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:06:51Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieRowParquetWriteSupport.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.HoodieDynamicBoundedBloomFilter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashMap;\n+\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MAX_RECORD_KEY_FOOTER;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MIN_RECORD_KEY_FOOTER;\n+\n+/**\n+ * Hoodie Write Support for directly writing Row to Parquet.\n+ */\n+public class HoodieRowParquetWriteSupport extends ParquetWriteSupport {\n+\n+  private Configuration hadoopConf;\n+  private BloomFilter bloomFilter;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieRowParquetWriteSupport(Configuration conf, StructType structType, BloomFilter bloomFilter) {\n+    super();\n+    Configuration hadoopConf = new Configuration(conf);\n+    hadoopConf.set(\"spark.sql.parquet.writeLegacyFormat\", \"false\");", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NTE5NQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468885195", "bodyText": "Nope. we need to fix this. The built in ParquetWriteSupport expects these two params to be set. I will double check once again to ensure this.", "author": "nsivabalan", "createdAt": "2020-08-11T21:51:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkzNDkyMg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468934922", "bodyText": "Check lines 94 to 104 https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala . Or was your ask just about hardcoding these configs.", "author": "nsivabalan", "createdAt": "2020-08-12T00:19:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MTQ4Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468981483", "bodyText": "yes. why we are hardcoding this. any ideas @bvaradar ?", "author": "vinothchandar", "createdAt": "2020-08-12T03:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MDYwMw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468770603", "body": "this is a misleading name. Need to rename this. its unclear if it refers to a hoodie dataset or a spark dataset framework.", "bodyText": "this is a misleading name. Need to rename this. its unclear if it refers to a hoodie dataset or a spark dataset framework.", "bodyHTML": "<p dir=\"auto\">this is a misleading name. Need to rename this. its unclear if it refers to a hoodie dataset or a spark dataset framework.</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:11:13Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieDatasetTestUtils.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+import static org.apache.hudi.common.testutils.FileSystemTestUtils.RANDOM;\n+\n+/**\n+ * Dataset test utils.\n+ */\n+public class HoodieDatasetTestUtils {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468771527", "body": "Need to understand why this is needed. so, we pick a different mode for the writer path I believe. We should use a config and not overload further if possible.", "bodyText": "Need to understand why this is needed. so, we pick a different mode for the writer path I believe. We should use a config and not overload further if possible.", "bodyHTML": "<p dir=\"auto\">Need to understand why this is needed. so, we pick a different mode for the writer path I believe. We should use a config and not overload further if possible.</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:12:50Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -35,6 +35,7 @@\n   // bulk insert\n   BULK_INSERT(\"bulk_insert\"),\n   BULK_INSERT_PREPPED(\"bulk_insert_prepped\"),\n+  BULK_INSERT_DATASET(\"bulk_insert_dataset\"),", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4NTg5Mg==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468985892", "bodyText": "removing this. it was easy enough.", "author": "vinothchandar", "createdAt": "2020-08-12T03:37:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MjY1OQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468772659", "body": "another general rule of thumb. we could always review our own diffs again before submitting to make sure whitespace changes are all intentional. cc @nsivabalan . ", "bodyText": "another general rule of thumb. we could always review our own diffs again before submitting to make sure whitespace changes are all intentional. cc @nsivabalan .", "bodyHTML": "<p dir=\"auto\">another general rule of thumb. we could always review our own diffs again before submitting to make sure whitespace changes are all intentional. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/nsivabalan/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nsivabalan\">@nsivabalan</a> .</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:14:46Z", "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -267,26 +258,26 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doDeleteOperation(HoodieWriteClient client, JavaRDD<HoodieKey> hoodieKeys,\n-                                                       String instantTime) {\n+      String instantTime) {\n     return client.delete(hoodieKeys, instantTime);\n   }\n \n   public static HoodieRecord createHoodieRecord(GenericRecord gr, Comparable orderingVal, HoodieKey hKey,\n-                                                String payloadClass) throws IOException {\n+      String payloadClass) throws IOException {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773381", "body": "is thre a way to avoid using positions and use names instead?", "bodyText": "is thre a way to avoid using positions and use names instead?", "bodyHTML": "<p dir=\"auto\">is thre a way to avoid using positions and use names instead?</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:15:58Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MTgxNw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468881817", "bodyText": "yes, you could do that. I vaguely remember running into some issues and then I went with positions. Don't remember exactly. Might have to code it up to check.", "author": "nsivabalan", "createdAt": "2020-08-11T21:43:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkzMjU3OA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468932578", "bodyText": "StructType is just the schema and for recordKey fields and partition paths, we parse the structType and store the chain of positions (if nested). don't think we get away without storing positions.", "author": "nsivabalan", "createdAt": "2020-08-12T00:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773991", "body": "Also, this being in `BuiltinKeyGenerator` and not `KeyGenerator` is a problem and will break all the custom key generators out there when they turn on row based writing, correct? should we move this up?", "bodyText": "Also, this being in BuiltinKeyGenerator and not KeyGenerator is a problem and will break all the custom key generators out there when they turn on row based writing, correct? should we move this up?", "bodyHTML": "<p dir=\"auto\">Also, this being in <code>BuiltinKeyGenerator</code> and not <code>KeyGenerator</code> is a problem and will break all the custom key generators out there when they turn on row based writing, correct? should we move this up?</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:17:09Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MjY1Mw==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468882653", "bodyText": "When I was doing the rebase, I saw getRecordKeyFieldNames in KeyGenerator was throwing UnsupportedOperationException. Hence went with the same for these methods too. Before rebase, we had this in KeyGenerator only. So didn't want to move this w/o consulting w/ you.", "author": "nsivabalan", "createdAt": "2020-08-11T21:45:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTQxOA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468901418", "bodyText": "on 2nd thought, yes, it makes sense to move this to KeyGenerator. and thats why we had the default impl of re-using getRecord(). So that all existing customers can still leverage bulk insert w/ dataset.", "author": "nsivabalan", "createdAt": "2020-08-11T22:32:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Njc1Ng==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468776756", "body": "might be good to assert this out in the constructor itself", "bodyText": "might be good to assert this out in the constructor itself", "bodyHTML": "<p dir=\"auto\">might be good to assert this out in the constructor itself</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:22:02Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java", "diffHunk": "@@ -55,21 +51,22 @@ public SimpleKeyGenerator(TypedProperties props, String partitionPathField) {\n \n   @Override\n   public String getRecordKey(GenericRecord record) {\n-    return KeyGenUtils.getRecordKey(record, recordKeyField);\n+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0));", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MzY2MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468883660", "bodyText": "We wanted to have the same behavior as getKey(). we don't throw exception in constructor if record key is not found. we throw only when getKey(GenericRecord record) is called.", "author": "nsivabalan", "createdAt": "2020-08-11T21:47:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Njc1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MzkwMQ==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468783901", "body": "is the .get(0) really fine?", "bodyText": "is the .get(0) really fine?", "bodyHTML": "<p dir=\"auto\">is the .get(0) really fine?</p>", "author": "vinothchandar", "createdAt": "2020-08-11T18:35:18Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -177,4 +191,26 @@ private long convertLongTimeToMillis(Long partitionVal) {\n     }\n     return MILLISECONDS.convert(partitionVal, timeUnit);\n   }\n+\n+  @Override\n+  public String getRecordKey(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), false);\n+  }\n+\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    Object fieldVal = null;\n+    Object partitionPathFieldVal =  RowKeyGeneratorHelper.getNestedFieldVal(row, getPartitionPathPositions().get(getPartitionPathFields().get(0)));", "originalCommit": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4NDM1MA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468884350", "bodyText": "yes, this extends from SimpleKeyGenerator. Also,  we have a special case for partition path if incase we don't find the field. couldn't find a better way to do it. position will return -1 and when parsing for actual Row, we will return DEFAULT_PARTITION_PATH.", "author": "nsivabalan", "createdAt": "2020-08-11T21:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MzkwMQ=="}], "type": "inlineReview"}, {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-12T15:23:44Z", "type": "commit"}, {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures", "committedDate": "2020-08-12T15:23:44Z", "type": "forcePushed"}, {"oid": "383a74578defdcb82a5dddde079e65899f6d60f0", "url": "https://github.com/apache/hudi/commit/383a74578defdcb82a5dddde079e65899f6d60f0", "message": "Some fixes to key generators and adding more tests for Row apis", "committedDate": "2020-08-12T16:06:11Z", "type": "commit"}, {"oid": "fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "url": "https://github.com/apache/hudi/commit/fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "message": "Cleaning up config placements, naming", "committedDate": "2020-08-12T20:39:33Z", "type": "commit"}, {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T05:16:14Z", "type": "commit"}, {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs", "committedDate": "2020-08-13T05:16:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTkyMjExOA==", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469922118", "body": "may I know where is the structType being used ? AvroConversionHelper.createConverterToAvro used row.Schema() and so we may not need it. probably we should rename this to boolean positionMapInitialized.", "bodyText": "may I know where is the structType being used ? AvroConversionHelper.createConverterToAvro used row.Schema() and so we may not need it. probably we should rename this to boolean positionMapInitialized.", "bodyHTML": "<p dir=\"auto\">may I know where is the structType being used ? AvroConversionHelper.createConverterToAvro used row.Schema() and so we may not need it. probably we should rename this to boolean positionMapInitialized.</p>", "author": "nsivabalan", "createdAt": "2020-08-13T12:42:55Z", "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -85,71 +84,40 @@ public final HoodieKey getKey(GenericRecord record) {\n     }).collect(Collectors.toList());\n   }\n \n-  @Override\n-  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n-    // parse simple feilds\n-    getRecordKeyFields().stream()\n-        .filter(f -> !(f.contains(\".\")))\n-        .forEach(f -> {\n-          if (structType.getFieldIndex(f).isDefined()) {\n-            recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n-          } else {\n-            throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n-          }\n-        });\n-    // parse nested fields\n-    getRecordKeyFields().stream()\n-        .filter(f -> f.contains(\".\"))\n-        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n-    // parse simple fields\n-    if (getPartitionPathFields() != null) {\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+  void buildFieldPositionMapIfNeeded(StructType structType) {\n+    if (this.structType == null) {\n+      // parse simple fields\n+      getRecordKeyFields().stream()\n+          .filter(f -> !(f.contains(\".\")))\n           .forEach(f -> {\n             if (structType.getFieldIndex(f).isDefined()) {\n-              partitionPathPositions.put(f,\n-                  Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n             } else {\n-              partitionPathPositions.put(f, Collections.singletonList(-1));\n+              throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n             }\n           });\n       // parse nested fields\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n-          .forEach(f -> partitionPathPositions.put(f,\n-              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n-    }\n-    this.structName = structName;\n-    this.structType = structType;\n-    this.recordNamespace = recordNamespace;\n-  }\n-\n-  /**\n-   * Fetch record key from {@link Row}.\n-   *\n-   * @param row instance of {@link Row} from which record key is requested.\n-   * @return the record key of interest from {@link Row}.\n-   */\n-  @Override\n-  public String getRecordKey(Row row) {\n-    if (null == converterFn) {\n-      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+      getRecordKeyFields().stream()\n+          .filter(f -> f.contains(\".\"))\n+          .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+      // parse simple fields\n+      if (getPartitionPathFields() != null) {\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+            .forEach(f -> {\n+              if (structType.getFieldIndex(f).isDefined()) {\n+                partitionPathPositions.put(f,\n+                    Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              } else {\n+                partitionPathPositions.put(f, Collections.singletonList(-1));\n+              }\n+            });\n+        // parse nested fields\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+            .forEach(f -> partitionPathPositions.put(f,\n+                RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+      }\n+      this.structType = structType;", "originalCommit": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}