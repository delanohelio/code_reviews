{"pr_number": 1360, "pr_title": "[HUDI-344][RFC-09] Hudi Dataset Snapshot Exporter", "pr_author": "OpenOpened", "pr_createdAt": "2020-02-27T04:23:51Z", "pr_url": "https://github.com/apache/hudi/pull/1360", "merge_commit": "44700d531a74f24762903df2729577a0d96e4ec0", "timeline": [{"oid": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "url": "https://github.com/apache/hudi/commit/e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "message": "first commit", "committedDate": "2020-02-27T04:16:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzUwNg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007506", "body": "```suggestion\r\n```", "bodyText": "Suggested change", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\"></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T06:50:00Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzY5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007699", "body": "I think we can omit the short param like `-opf` as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.", "bodyText": "I think we can omit the short param like -opf as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.", "bodyHTML": "<p dir=\"auto\">I think we can omit the short param like <code>-opf</code> as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.</p>", "author": "xushiyan", "createdAt": "2020-02-29T06:53:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE3NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008175", "body": "```suggestion\r\n    String sourceBasePath = null;\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String basePath = null;\n          \n          \n            \n                String sourceBasePath = null;", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">basePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">sourceBasePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:04:40Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE4MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008180", "body": "```suggestion\r\n    String targetBasePath = null;\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String outputPath = null;\n          \n          \n            \n                String targetBasePath = null;", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">outputPath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">targetBasePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:04:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODM2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008366", "body": "You may take the call but I would say without these local vars using the `cfg` variable directly to refer to those params looks quite readable", "bodyText": "You may take the call but I would say without these local vars using the cfg variable directly to refer to those params looks quite readable", "bodyHTML": "<p dir=\"auto\">You may take the call but I would say without these local vars using the <code>cfg</code> variable directly to refer to those params looks quite readable</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:08:47Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODczMQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008731", "body": "you can invoke `.parquet()` to be specific", "bodyText": "you can invoke .parquet() to be specific", "bodyHTML": "<p dir=\"auto\">you can invoke <code>.parquet()</code> to be specific</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:16:02Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);\n+        } else {\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTI4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009283", "body": "I think you'd also need to partition the output dir\r\n```suggestion\r\n              .write()\r\n              .partitionBy(outputPartitionField)\r\n```", "bodyText": "I think you'd also need to partition the output dir\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .write()\n          \n          \n            \n                          .write()\n          \n          \n            \n                          .partitionBy(outputPartitionField)", "bodyHTML": "<p dir=\"auto\">I think you'd also need to partition the output dir</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">              .write()</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">              .write()</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">              .partitionBy(outputPartitionField)</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:27:52Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009385", "body": "it would be good to have a `switch` to differentiate the cases and refactor each case to different methods for readability.\r\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.", "bodyText": "it would be good to have a switch to differentiate the cases and refactor each case to different methods for readability.\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.", "bodyHTML": "<p dir=\"auto\">it would be good to have a <code>switch</code> to differentiate the cases and refactor each case to different methods for readability.<br>\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:30:14Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA3MzYzNQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386073635", "bodyText": "Here, exporter not only supports the conversion of hudi data sets to parquet, but also supports all data types currently supported by spark, such as json or jdbc or avro conversion. The .format () method is the key. Of course, in Exporter, it is only divided into hudi type or not. If it is, then you can copy the relevant files to the specified directory. If not, you can read and convert it. I don't think \"switch cases\" are needed here.", "author": "OpenOpened", "createdAt": "2020-03-01T03:34:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5MzU3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093576", "bodyText": "@OpenOpened Understand that the format() can provide some flexibility there but i would argue against it for 2 reasons\n\nwe are providing this as a user-input argument to establish a contract with value passed to our API. We don't want to rely it on the internal logic where spark takes that and does the conversion. Imagine if user passes an invalid value like \"foo\" and then Spark API throws an error which will expose the internal implementation which user should not care about.\nWe want to be explicit on what the API supports and does not. switch makes it clear and the code more readable. And the default: case will be the ideal place to throw \"Unsupported\" exception to user.\nIn addition, the export() method is sort of lengthy. For better readability, I would suggest break it into multiple methods. (Handling different conversion type could be a good separation)", "author": "xushiyan", "createdAt": "2020-03-01T09:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI2MTQ5Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386261497", "bodyText": "If you use the swich cases code, it might look something like this:\nswich (uotput) { case \"hudi\":exportToHudi();break; case \"json\":exportToJson();break; ..... }\nFor the implementation part of the method, it is possible to just transform the format() parameters of spark, which is a bit redundant. We can use a compromise solution, using the Spark DataSource lookup method to verify the correctness of the user input format.", "author": "OpenOpened", "createdAt": "2020-03-02T08:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3OTMwOA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386279308", "bodyText": "I uploaded the latest code and gave a \"foo\" output format test case; -)", "author": "OpenOpened", "createdAt": "2020-03-02T09:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI4MDU2NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386280565", "bodyText": "In addition. DataSource lookup method and can handle the case when the user customizes the DataSrouce implementation.", "author": "OpenOpened", "createdAt": "2020-03-02T09:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4MzY2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386783660", "bodyText": "I got your concern. The major point I'd to emphasize here is we want to be explicit on what we support for this utility to the users.\nThe spark format() API is powerful and reduces the code; on the down side, it makes the code difficult to understand: we claim supporting parquet and hudi, but internally we rely on spark API support. I imagine whatever claimed in the configuration docs should be associated to the code for ease of understanding; that's what I intend the switch for. With fall-through switch cases, you can still use format() and achieve explicitness.\nThough saying that, I don't mind with your current approach and change the Exporter's docs saying support all Spark write format. A little bit worried about spark decoupling in future, but maybe not big issue for now.", "author": "xushiyan", "createdAt": "2020-03-03T03:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe76973..903b7ac6364 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}}, {"oid": "d6ffad986b20067b2708e212d00575345a039dff", "url": "https://github.com/apache/hudi/commit/d6ffad986b20067b2708e212d00575345a039dff", "message": "code optimize", "committedDate": "2020-03-01T03:38:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093867", "body": "When we export for non-hudi case, I think we should remove the `_hoodie_*` metadata columns.", "bodyText": "When we export for non-hudi case, I think we should remove the _hoodie_* metadata columns.", "bodyHTML": "<p dir=\"auto\">When we export for non-hudi case, I think we should remove the <code>_hoodie_*</code> metadata columns.</p>", "author": "xushiyan", "createdAt": "2020-03-01T09:56:17Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NDk1Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386094957", "bodyText": "With partitionBy() I think you left out the repartition() before write", "author": "xushiyan", "createdAt": "2020-03-01T10:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..0675765c8a4 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095272", "body": "```suggestion\r\n    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n          \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">SparkSession</span> spark <span class=\"pl-k\">=</span> <span class=\"pl-smi\">SparkSession</span><span class=\"pl-k\">.</span>builder()<span class=\"pl-k\">.</span>appName(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Hoodie-snapshot-exporter<span class=\"pl-pds\">\"</span></span>)<span class=\"pl-k x x-first\">.</span><span class=\"x\">master(</span><span class=\"pl-s\"><span class=\"pl-pds x\">\"</span><span class=\"x\">local[2]</span><span class=\"pl-pds x\">\"</span></span><span class=\"x x-last\">)</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">SparkSession</span> spark <span class=\"pl-k\">=</span> <span class=\"pl-smi\">SparkSession</span><span class=\"pl-k\">.</span>builder()<span class=\"pl-k\">.</span>appName(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Hoodie-snapshot-exporter<span class=\"pl-pds\">\"</span></span>)</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-03-01T10:17:50Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        } else {\n+          write.format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        }\n+      } else {\n+        // No transformation is needed for output format \"HUDI\", just copy the original files.\n+\n+        // Make sure the output directory is empty\n+        Path outputPath = new Path(cfg.targetOutputPath);\n+        if (fs.exists(outputPath)) {\n+          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n+        }\n+\n+        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+          // Only take latest version files <= latestCommit.\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+          // also need to copy over partition metadata\n+          Path partitionMetaFile =\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+          if (fs1.exists(partitionMetaFile)) {\n+            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+          }\n+\n+          return filePaths.iterator();\n+        }).foreach(tuple -> {\n+          String partition = tuple._1();\n+          Path sourceFilePath = new Path(tuple._2());\n+          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+\n+          if (!ifs.exists(toPartitionPath)) {\n+            ifs.mkdirs(toPartitionPath);\n+          }\n+          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+              ifs.getConf());\n+        });\n+\n+        // Also copy the .commit files\n+        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+        FileStatus[] commitFilesToCopy =\n+            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+                return true;\n+              } else {\n+                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                    HoodieTimeline.LESSER_OR_EQUAL);\n+              }\n+            });\n+        for (FileStatus commitStatus : commitFilesToCopy) {\n+          Path targetFilePath =\n+              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+          if (!fs.exists(targetFilePath.getParent())) {\n+            fs.mkdirs(targetFilePath.getParent());\n+          }\n+          if (fs.exists(targetFilePath)) {\n+            LOG.error(\n+                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+          }\n+          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+        }\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+  }\n+\n+  public static void main(String[] args) throws IOException {\n+    // Take input configs\n+    final Config cfg = new Config();\n+    new JCommander(cfg, null, args);\n+\n+    // Create a spark job to do the snapshot export\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1NzQ3OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386257479", "bodyText": "if remove master() method, spark test unable to work properly.\norg.apache.spark.SparkException: A master URL must be set in your configuration", "author": "OpenOpened", "createdAt": "2020-03-02T08:44:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc3OTM2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386779367", "bodyText": "ok but wouldn't this fail when running on production where we need to pass different master url?", "author": "xushiyan", "createdAt": "2020-03-03T03:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODMxOTAyOQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r388319029", "bodyText": "It's my fault. Thank you for pointing it out", "author": "OpenOpened", "createdAt": "2020-03-05T14:15:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac6364..230033c9e50 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -200,7 +220,7 @@ public class HoodieSnapshotExporter {\n     new JCommander(cfg, null, args);\n \n     // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n         .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n     LOG.info(\"Initializing spark job.\");\n \n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095388", "body": "This seems not used", "bodyText": "This seems not used", "bodyHTML": "<p dir=\"auto\">This seems not used</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:19:31Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);\n+      return Option.of(str + \", \\\"partition\\\": \\\"\" + record.getPartitionPath() + \"\\\"}\");\n+    } catch (IOException e) {\n+      return Option.empty();\n+    }\n+  }\n+\n+  public static List<String> convertToStringList(List<HoodieRecord> records) {\n+    return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1ODQ2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386258466", "bodyText": "DataSourceTestUtils actually exists under the hudi-spark test module, but we can't access it. I think we can move to the hudi-common package in the future to reuse the code as possible.", "author": "OpenOpened", "createdAt": "2020-03-02T08:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\nindex af5227ed99c..1a96b81a68e 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n", "chunk": "@@ -48,10 +47,4 @@ public class DataSourceTestUtils {\n     return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n         .collect(Collectors.toList());\n   }\n-\n-  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {\n-    return keys.stream()\n-        .map(hr -> \"{\\\"_row_key\\\":\\\"\" + hr.getRecordKey() + \"\\\",\\\"partition\\\":\\\"\" + hr.getPartitionPath() + \"\\\"}\")\n-        .collect(Collectors.toList());\n-  }\n }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095927", "body": "Any reason of not extending `HoodieCommonTestHarness` ?", "bodyText": "Any reason of not extending HoodieCommonTestHarness ?", "bodyHTML": "<p dir=\"auto\">Any reason of not extending <code>HoodieCommonTestHarness</code> ?</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:28:21Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Nzk5Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386277992", "bodyText": "There was little or no connection, so I chose not to inherit it. I have modified the latest code.", "author": "OpenOpened", "createdAt": "2020-03-02T09:28:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex efcfaed2118..c095f28558b 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -47,12 +48,11 @@ import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter {\n+public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n   private static String TEST_WRITE_TOKEN = \"1-0-1\";\n \n   private SparkSession spark = null;\n   private HoodieTestDataGenerator dataGen = null;\n-  private String basePath = null;\n   private String outputPath = null;\n   private String rootPath = null;\n   private FileSystem fs = null;\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjE4Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096182", "body": "This assumes what data column exists in `TestRawTripPayload`, which may easily break. I would suggest \r\n1. making this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and \r\n2. try to make the source data work for the case instead of wrangling with json string representation", "bodyText": "This assumes what data column exists in TestRawTripPayload, which may easily break. I would suggest\n\nmaking this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and\ntry to make the source data work for the case instead of wrangling with json string representation", "bodyHTML": "<p dir=\"auto\">This assumes what data column exists in <code>TestRawTripPayload</code>, which may easily break. I would suggest</p>\n<ol dir=\"auto\">\n<li>making this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and</li>\n<li>try to make the source data work for the case instead of wrangling with json string representation</li>\n</ol>", "author": "xushiyan", "createdAt": "2020-03-01T10:32:13Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096648", "body": "Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)\r\n", "bodyText": "Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)", "bodyHTML": "<p dir=\"auto\">Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:39:10Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {\n+  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n+\n+  private SparkSession spark = null;\n+  private HoodieTestDataGenerator dataGen = null;\n+  private String basePath = null;\n+  private String outputPath = null;\n+  private String rootPath = null;\n+  private FileSystem fs = null;\n+  private Map commonOpts;\n+  private HoodieSnapshotExporter.Config cfg;\n+  private JavaSparkContext jsc = null;\n+\n+  @Before\n+  public void initialize() throws IOException {\n+    spark = SparkSession.builder()\n+        .appName(\"Hoodie Datasource test\")\n+        .master(\"local[2]\")\n+        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .getOrCreate();\n+    jsc = new JavaSparkContext(spark.sparkContext());\n+    dataGen = new HoodieTestDataGenerator();\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    basePath = folder.getRoot().getAbsolutePath();\n+    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n+    commonOpts = new HashMap();\n+\n+    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n+    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n+    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n+    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n+\n+\n+    cfg = new HoodieSnapshotExporter.Config();\n+\n+    cfg.sourceBasePath = basePath;\n+    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n+    cfg.outputFormat = \"json\";\n+    cfg.outputPartitionField = \"partition\";\n+\n+  }\n+\n+  @After\n+  public void cleanup() throws Exception {\n+    if (spark != null) {\n+      spark.stop();\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotExporter() throws IOException {\n+    // Insert Operation\n+    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n+    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n+    inputDF.write().format(\"hudi\")\n+        .options(commonOpts)\n+        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n+        .mode(SaveMode.Overwrite)\n+        .save(basePath);\n+    long sourceCount = inputDF.count();\n+\n+    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n+    hoodieSnapshotExporter.export(spark, cfg);\n+\n+    long targetCount = spark.read().json(outputPath).count();\n+\n+    assertTrue(sourceCount == targetCount);\n+\n+    // Test snapshotPrefix\n+    long filterCount = inputDF.where(\"partition == '2015/03/16'\").count();\n+    cfg.snapshotPrefix = \"2015/03/16\";\n+    hoodieSnapshotExporter.export(spark, cfg);\n+    long targetFilterCount = spark.read().json(outputPath).count();\n+    assertTrue(filterCount == targetFilterCount);\n+\n+  }\n+\n+  // for testEmptySnapshotCopy\n+  public void init() throws IOException {\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n+    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n+    outputPath = rootPath + \"/output\";\n+\n+    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n+    fs = FSUtils.getFs(basePath, hadoopConf);\n+    HoodieTestUtils.init(hadoopConf, basePath);\n+  }\n+\n+  @Test\n+  public void testEmptySnapshotCopy() throws IOException {\n+    init();\n+    // There is no real data (only .hoodie directory)\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath)));\n+\n+    // Do the snapshot\n+    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n+    copier.snapshot(jsc, basePath, outputPath, true);\n+\n+    // Nothing changed; we just bail out\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n+  }\n+\n+  // TODO - uncomment this after fixing test failures\n+  // @Test", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Njk2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386276963", "bodyText": "I'm not quite sure what the comments here mean and what the implications might be. It may take officials to sort it out.", "author": "OpenOpened", "createdAt": "2020-03-02T09:26:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "e917358edc3c65252a2783b761c24a74b7aa04f3", "url": "https://github.com/apache/hudi/commit/e917358edc3c65252a2783b761c24a74b7aa04f3", "message": "code optimize", "committedDate": "2020-03-02T09:14:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786763", "body": "you need `startswith()` to be safe. There could be a column named \"total_hoodie_counts\"", "bodyText": "you need startswith() to be safe. There could be a column named \"total_hoodie_counts\"", "bodyHTML": "<p dir=\"auto\">you need <code>startswith()</code> to be safe. There could be a column named \"total_hoodie_counts\"</p>", "author": "xushiyan", "createdAt": "2020-03-03T03:55:53Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4ODA2MQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386788061", "bodyText": "This is optional but actually i'd even prefer having an immutable list of _hoodie_* reserved field names so we can be explicit on what we are removing while exporting.", "author": "xushiyan", "createdAt": "2020-03-03T04:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a4..230033c9e50 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +126,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": null}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc4Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786787", "body": "i think you need both `repartition()` and `partitionBy()`", "bodyText": "i think you need both repartition() and partitionBy()", "bodyHTML": "<p dir=\"auto\">i think you need both <code>repartition()</code> and <code>partitionBy()</code></p>", "author": "xushiyan", "createdAt": "2020-03-03T03:56:01Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n+        } else {\n+          write = reader.write();\n+        }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a4..230033c9e50 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +126,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": null}]}}, {"oid": "76133ce9788df7bc57406066811fb5e14d40a17c", "url": "https://github.com/apache/hudi/commit/76133ce9788df7bc57406066811fb5e14d40a17c", "message": "code optimize", "committedDate": "2020-03-05T14:13:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328760", "body": "the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to `Snapshot prefix or directory under the source Hudi dataset to be exported`? cc @OpenOpened @xushiyan ", "bodyText": "the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to Snapshot prefix or directory under the source Hudi dataset to be exported? cc @OpenOpened @xushiyan", "bodyHTML": "<p dir=\"auto\">the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to <code>Snapshot prefix or directory under the source Hudi dataset to be exported</code>? cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/OpenOpened/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/OpenOpened\">@OpenOpened</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/xushiyan/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xushiyan\">@xushiyan</a></p>", "author": "leesf", "createdAt": "2020-03-08T02:27:03Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTU4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365583", "bodyText": "@leesf thanks for catching this. I missed this one: this param is meant to segregate output not input. It was meant to be used in case for multiple exports with the same target path but wanted to be separated from each other (e.g., due to different export date). It actually overlaps with the target base path; users can simply change the target base paths to achieve this. So in conclusion we can just remove this param. @OpenOpened sounds good?", "author": "xushiyan", "createdAt": "2020-03-08T12:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0NDkxMg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389444912", "bodyText": "I don't think we can delete this parameter. We rely on the metadata file .hoodie metadata in the root directory of the datasource to find things like commitime, valid parquet files, etc. If you point directly to the folder that needs to be exported, such as ROOT/2015/03/16 for the test case, an exception will be thrown Hoodie table not found in path / tmp / junit4184871464195097137 / 2015/03/16 / .hoodie.  I agree with @leesf, modify the comment of --snapshot-perfix.", "author": "OpenOpened", "createdAt": "2020-03-09T03:02:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0OTk3Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389449973", "bodyText": "I see there're some gaps...this param snapshot-prefix is meant to let users export to a specific output directory. For example, --target-base-path=/mytable/ --snapshot-prefix=2020/03/03 and the output data will reside in /mytable/2020/03/03/. After removing --snapshot-prefix, the users will simply set --target-base-path=/mytable/2020/03/03/. This is a redundant param that should be removed from the RFC document too.\nIt is not meant to use on --source-base-path. Users will give the right base path to the hudi dataset to export like --source-base-path=/myhuditable/", "author": "xushiyan", "createdAt": "2020-03-09T03:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ1NTQ5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389455499", "bodyText": "Talked with @xushiyan  offline, the RFC9 means we only export whole hudi dataset, thus may not support exporting ROOT/2015/03/16, thus we could remove --snapshot-prefix and modify the code accordingly. If we need export specified partitions later, we could bring it back and may need more consideration(maybe not use contains, maybe equals or regular expression). WDYT @OpenOpened", "author": "leesf", "createdAt": "2020-03-09T04:08:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}], "type": "inlineReview", "revised_code": {"commit": "8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 230033c9e50..7d27b7b4101 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -71,7 +73,7 @@ public class HoodieSnapshotExporter {\n     @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n     String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the source Hudi dataset to be exported\")\n     String snapshotPrefix;\n \n     @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n", "next_change": {"commit": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7d27b7b4101..37a15aae641 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -73,9 +73,6 @@ public class HoodieSnapshotExporter {\n     @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n     String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the source Hudi dataset to be exported\")\n-    String snapshotPrefix;\n-\n     @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n", "next_change": null}]}}]}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328898", "body": "one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc @xushiyan @OpenOpened ", "bodyText": "one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc @xushiyan @OpenOpened", "bodyHTML": "<p dir=\"auto\">one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/xushiyan/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xushiyan\">@xushiyan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/OpenOpened/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/OpenOpened\">@OpenOpened</a></p>", "author": "leesf", "createdAt": "2020-03-08T02:30:26Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTA3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365076", "bodyText": "Per the RFC, we aim to just get the latest commit time.", "author": "xushiyan", "createdAt": "2020-03-08T12:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "url": "https://github.com/apache/hudi/commit/8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "message": "code optimize", "committedDate": "2020-03-09T03:08:10Z", "type": "commit"}, {"oid": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "url": "https://github.com/apache/hudi/commit/aa5e7798322ce0701776d8b44b608d601f61f0a7", "message": "remove --snapshot-prefix flag", "committedDate": "2020-03-09T05:58:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ4MjIwMw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389482203", "body": "nit: remove extra line? ", "bodyText": "nit: remove extra line?", "bodyHTML": "<p dir=\"auto\">nit: remove extra line?</p>", "author": "vinothchandar", "createdAt": "2020-03-09T06:33:49Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java", "diffHunk": "@@ -52,6 +52,7 @@\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n+", "originalCommit": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "45c0872ce680ad54312cfdeb4c8dd6f70525513c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 178a9075c3d..7d944d35591 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -52,7 +52,6 @@ import scala.Tuple2;\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n-\n public class HoodieSnapshotCopier implements Serializable {\n \n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotCopier.class);\n", "next_change": null}]}}, {"oid": "45c0872ce680ad54312cfdeb4c8dd6f70525513c", "url": "https://github.com/apache/hudi/commit/45c0872ce680ad54312cfdeb4c8dd6f70525513c", "message": "remove extra line", "committedDate": "2020-03-09T06:42:11Z", "type": "commit"}]}