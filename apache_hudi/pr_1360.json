{"pr_number": 1360, "pr_title": "[HUDI-344][RFC-09] Hudi Dataset Snapshot Exporter", "pr_author": "OpenOpened", "pr_createdAt": "2020-02-27T04:23:51Z", "pr_url": "https://github.com/apache/hudi/pull/1360", "merge_commit": "44700d531a74f24762903df2729577a0d96e4ec0", "timeline": [{"oid": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "url": "https://github.com/apache/hudi/commit/e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "message": "first commit", "committedDate": "2020-02-27T04:16:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzUwNg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007506", "body": "```suggestion\r\n```", "bodyText": "Suggested change", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\"></td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T06:50:00Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzY5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386007699", "body": "I think we can omit the short param like `-opf` as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.", "bodyText": "I think we can omit the short param like -opf as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.", "bodyHTML": "<p dir=\"auto\">I think we can omit the short param like <code>-opf</code> as it would create confusion due to the uncommonly recognized acronyms. The full names will work really well.</p>", "author": "xushiyan", "createdAt": "2020-02-29T06:53:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE3NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008175", "body": "```suggestion\r\n    String sourceBasePath = null;\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String basePath = null;\n          \n          \n            \n                String sourceBasePath = null;", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">basePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">sourceBasePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:04:40Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODE4MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008180", "body": "```suggestion\r\n    String targetBasePath = null;\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                String outputPath = null;\n          \n          \n            \n                String targetBasePath = null;", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">outputPath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">String</span> <span class=\"x x-first x-last\">targetBasePath</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>;</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:04:56Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODM2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008366", "body": "You may take the call but I would say without these local vars using the `cfg` variable directly to refer to those params looks quite readable", "bodyText": "You may take the call but I would say without these local vars using the cfg variable directly to refer to those params looks quite readable", "bodyHTML": "<p dir=\"auto\">You may take the call but I would say without these local vars using the <code>cfg</code> variable directly to refer to those params looks quite readable</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:08:47Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -60,33 +61,28 @@ public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n     String snapshotPrefix;\n \n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n   public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -89,8 +93,8 @@ public class HoodieSnapshotExporter {\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwODczMQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386008731", "body": "you can invoke `.parquet()` to be specific", "bodyText": "you can invoke .parquet() to be specific", "bodyHTML": "<p dir=\"auto\">you can invoke <code>.parquet()</code> to be specific</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:16:02Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);\n+        } else {\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .write()\n+              .format(outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(targetBasePath);", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTI4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009283", "body": "I think you'd also need to partition the output dir\r\n```suggestion\r\n              .write()\r\n              .partitionBy(outputPartitionField)\r\n```", "bodyText": "I think you'd also need to partition the output dir\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .write()\n          \n          \n            \n                          .write()\n          \n          \n            \n                          .partitionBy(outputPartitionField)", "bodyHTML": "<p dir=\"auto\">I think you'd also need to partition the output dir</p>\n  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">              .write()</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">              .write()</td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">              .partitionBy(outputPartitionField)</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-02-29T07:27:52Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+              .repartition(new Column(outputPartitionField))\n+              .write()", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386009385", "body": "it would be good to have a `switch` to differentiate the cases and refactor each case to different methods for readability.\r\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.", "bodyText": "it would be good to have a switch to differentiate the cases and refactor each case to different methods for readability.\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.", "bodyHTML": "<p dir=\"auto\">it would be good to have a <code>switch</code> to differentiate the cases and refactor each case to different methods for readability.<br>\nNote that though we only have 'hudi' and 'parquet' for now, there could be more cases in future when needs rise.</p>", "author": "xushiyan", "createdAt": "2020-02-29T07:30:14Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String basePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String outputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    String sourceBasePath = cfg.basePath;\n+    String targetBasePath = cfg.outputPath;\n+    String snapshotPrefix = cfg.snapshotPrefix;\n+    String outputFormat = cfg.outputFormat;\n+    String outputPartitionField = cfg.outputPartitionField;\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {", "originalCommit": "e98f2e1eb07b6a669f5fb3f35a37c6c3580e27bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA3MzYzNQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386073635", "bodyText": "Here, exporter not only supports the conversion of hudi data sets to parquet, but also supports all data types currently supported by spark, such as json or jdbc or avro conversion. The .format () method is the key. Of course, in Exporter, it is only divided into hudi type or not. If it is, then you can copy the relevant files to the specified directory. If not, you can read and convert it. I don't think \"switch cases\" are needed here.", "author": "OpenOpened", "createdAt": "2020-03-01T03:34:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5MzU3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093576", "bodyText": "@OpenOpened Understand that the format() can provide some flexibility there but i would argue against it for 2 reasons\n\nwe are providing this as a user-input argument to establish a contract with value passed to our API. We don't want to rely it on the internal logic where spark takes that and does the conversion. Imagine if user passes an invalid value like \"foo\" and then Spark API throws an error which will expose the internal implementation which user should not care about.\nWe want to be explicit on what the API supports and does not. switch makes it clear and the code more readable. And the default: case will be the ideal place to throw \"Unsupported\" exception to user.\nIn addition, the export() method is sort of lengthy. For better readability, I would suggest break it into multiple methods. (Handling different conversion type could be a good separation)", "author": "xushiyan", "createdAt": "2020-03-01T09:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI2MTQ5Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386261497", "bodyText": "If you use the swich cases code, it might look something like this:\nswich (uotput) { case \"hudi\":exportToHudi();break; case \"json\":exportToJson();break; ..... }\nFor the implementation part of the method, it is possible to just transform the format() parameters of spark, which is a bit redundant. We can use a compromise solution, using the Spark DataSource lookup method to verify the correctness of the user input format.", "author": "OpenOpened", "createdAt": "2020-03-02T08:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3OTMwOA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386279308", "bodyText": "I uploaded the latest code and gave a \"foo\" output format test case; -)", "author": "OpenOpened", "createdAt": "2020-03-02T09:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI4MDU2NQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386280565", "bodyText": "In addition. DataSource lookup method and can handle the case when the user customizes the DataSrouce implementation.", "author": "OpenOpened", "createdAt": "2020-03-02T09:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4MzY2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386783660", "bodyText": "I got your concern. The major point I'd to emphasize here is we want to be explicit on what we support for this utility to the users.\nThe spark format() API is powerful and reduces the code; on the down side, it makes the code difficult to understand: we claim supporting parquet and hudi, but internally we rely on spark API support. I imagine whatever claimed in the configuration docs should be associated to the code for ease of understanding; that's what I intend the switch for. With fall-through switch cases, you can still use format() and achieve explicitness.\nThough saying that, I don't mind with your current approach and change the Exporter's docs saying support all Spark write format. A little bit worried about spark decoupling in future, but maybe not big issue for now.", "author": "xushiyan", "createdAt": "2020-03-03T03:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwOTM4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "d6ffad986b20067b2708e212d00575345a039dff", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..903b7ac636 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -116,42 +112,40 @@ public class HoodieSnapshotExporter {\n         }\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n+          write.format(cfg.outputFormat)\n               .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+              .save(cfg.targetOutputPath);\n         }\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n \n         // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n+        Path outputPath = new Path(cfg.targetOutputPath);\n         if (fs.exists(outputPath)) {\n           LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n         }\n \n         jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n           // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n           List<Tuple2<String, String>> filePaths = new ArrayList<>();\n           dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n \n           // also need to copy over partition metadata\n           Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n           if (fs1.exists(partitionMetaFile)) {\n             filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n           }\n", "next_change": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 2e30fe7697..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -49,154 +53,155 @@ import scala.collection.JavaConversions;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ *\n+ * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n-\n public class HoodieSnapshotExporter {\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n   public static class Config implements Serializable {\n-    @Parameter(names = {\"--source-base-path\", \"-sbp\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String basePath = null;\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n \n-    @Parameter(names = {\"--target-base-path\", \"-tbp\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String outputPath = null;\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\", \"-sp\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n-    @Parameter(names = {\"--output-format\", \"-of\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n-    @Parameter(names = {\"--output-partition-field\", \"-opf\"}, description = \"A field to be used by Spark repartitioning\")\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n     String outputPartitionField;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    String sourceBasePath = cfg.basePath;\n-    String targetBasePath = cfg.outputPath;\n-    String snapshotPrefix = cfg.snapshotPrefix;\n-    String outputFormat = cfg.outputFormat;\n-    String outputPartitionField = cfg.outputPartitionField;\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n-    FileSystem fs = FSUtils.getFs(sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), sourceBasePath);\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n     final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n     // Get the latest commit\n     Option<HoodieInstant> latestCommit =\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n-      LOG.warn(\"No commits present. Nothing to snapshot\");\n-      return;\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, sourceBasePath, false);\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      if (!outputFormat.equalsIgnoreCase(\"hudi\")) {\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        if (!StringUtils.isNullOrEmpty(outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .repartition(new Column(outputPartitionField))\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-              .write()\n-              .format(outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(targetBasePath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(targetBasePath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(targetBasePath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(targetBasePath, partition);\n-          FileSystem ifs = FSUtils.getFs(targetBasePath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(targetBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"oid": "d6ffad986b20067b2708e212d00575345a039dff", "url": "https://github.com/apache/hudi/commit/d6ffad986b20067b2708e212d00575345a039dff", "message": "code optimize", "committedDate": "2020-03-01T03:38:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386093867", "body": "When we export for non-hudi case, I think we should remove the `_hoodie_*` metadata columns.", "bodyText": "When we export for non-hudi case, I think we should remove the _hoodie_* metadata columns.", "bodyHTML": "<p dir=\"auto\">When we export for non-hudi case, I think we should remove the <code>_hoodie_*</code> metadata columns.</p>", "author": "xushiyan", "createdAt": "2020-03-01T09:56:17Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NDk1Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386094957", "bodyText": "With partitionBy() I think you left out the repartition() before write", "author": "xushiyan", "createdAt": "2020-03-01T10:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5Mzg2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..0675765c8a 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -111,86 +115,102 @@ public class HoodieSnapshotExporter {\n           dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n         }\n       }\n-\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -100,97 +102,106 @@ public class HoodieSnapshotExporter {\n     if (partitions.size() > 0) {\n       List<String> dataFiles = new ArrayList<>();\n \n-      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n-        for (String partition : partitions) {\n-          if (partition.contains(cfg.snapshotPrefix)) {\n-            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-          }\n-        }\n-      } else {\n-        for (String partition : partitions) {\n-          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-        }\n+      for (String partition : partitions) {\n+        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n         // Do transformation\n-        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n-                .write();\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          // A field to do simple Spark repartitioning\n-          write.partitionBy(cfg.outputPartitionField)\n-              .format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n-          write.format(cfg.outputFormat)\n-              .mode(SaveMode.Overwrite)\n-              .save(cfg.targetOutputPath);\n+          write = reader.write();\n         }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n+        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+    return 0;\n+  }\n \n-        // Make sure the output directory is empty\n-        Path outputPath = new Path(cfg.targetOutputPath);\n-        if (fs.exists(outputPath)) {\n-          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-          fs.delete(new Path(cfg.targetOutputPath), true);\n-        }\n+  private void copySnapshot(JavaSparkContext jsc,\n+                            FileSystem fs,\n+                            Config cfg,\n+                            List<String> partitions,\n+                            List<String> dataFiles,\n+                            String latestCommitTimestamp,\n+                            SerializableConfiguration serConf) throws IOException {\n+    // Make sure the output directory is empty\n+    Path outputPath = new Path(cfg.targetOutputPath);\n+    if (fs.exists(outputPath)) {\n+      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+      fs.delete(new Path(cfg.targetOutputPath), true);\n+    }\n \n-        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n-          // Only take latest version files <= latestCommit.\n-          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n-          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n-\n-          // also need to copy over partition metadata\n-          Path partitionMetaFile =\n-              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-          if (fs1.exists(partitionMetaFile)) {\n-            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n-          }\n+    jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+      // Only take latest version files <= latestCommit.\n+      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+      // also need to copy over partition metadata\n+      Path partitionMetaFile =\n+          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+      if (fs1.exists(partitionMetaFile)) {\n+        filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+      }\n \n-          return filePaths.iterator();\n-        }).foreach(tuple -> {\n-          String partition = tuple._1();\n-          Path sourceFilePath = new Path(tuple._2());\n-          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      return filePaths.iterator();\n+    }).foreach(tuple -> {\n+      String partition = tuple._1();\n+      Path sourceFilePath = new Path(tuple._2());\n+      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-          if (!ifs.exists(toPartitionPath)) {\n-            ifs.mkdirs(toPartitionPath);\n+      if (!ifs.exists(toPartitionPath)) {\n+        ifs.mkdirs(toPartitionPath);\n+      }\n+      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          ifs.getConf());\n+    });\n+\n+    // Also copy the .commit files\n+    LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    FileStatus[] commitFilesToCopy =\n+        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+          if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+            return true;\n+          } else {\n+            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                HoodieTimeline.LESSER_OR_EQUAL);\n           }\n-          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-              ifs.getConf());\n         });\n-\n-        // Also copy the .commit files\n-        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-        FileStatus[] commitFilesToCopy =\n-            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n-              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n-                return true;\n-              } else {\n-                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n-                    HoodieTimeline.LESSER_OR_EQUAL);\n-              }\n-            });\n-        for (FileStatus commitStatus : commitFilesToCopy) {\n-          Path targetFilePath =\n-              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-          if (!fs.exists(targetFilePath.getParent())) {\n-            fs.mkdirs(targetFilePath.getParent());\n-          }\n-          if (fs.exists(targetFilePath)) {\n-            LOG.error(\n-                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n-          }\n-          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n-        }\n+    for (FileStatus commitStatus : commitFilesToCopy) {\n+      Path targetFilePath =\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+      if (!fs.exists(targetFilePath.getParent())) {\n+        fs.mkdirs(targetFilePath.getParent());\n       }\n-    } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      if (fs.exists(targetFilePath)) {\n+        LOG.error(\n+            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+      }\n+      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n     }\n   }\n \n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..b58b5d34b1 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,6 +206,14 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n+  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n+    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n+      fs.createNewFile(successTagPath);\n+    }\n+  }\n+\n   public static void main(String[] args) throws IOException {\n     // Take input configs\n     final Config cfg = new Config();\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex b58b5d34b1..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -171,63 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n-  private void createSuccessTag(FileSystem fs, String targetOutputPath) throws IOException {\n-    Path successTagPath = new Path(targetOutputPath + \"/_SUCCESS\");\n-    if (!fs.exists(successTagPath)) {\n-      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", targetOutputPath));\n-      fs.createNewFile(successTagPath);\n-    }\n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n   }\n \n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095272", "body": "```suggestion\r\n    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\r\n```", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n          \n          \n            \n                SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")", "bodyHTML": "  <div class=\"my-2 border rounded-1 js-suggested-changes-blob diff-view js-check-bidi\" id=\"\">\n    <div class=\"f6 p-2 lh-condensed border-bottom d-flex\">\n      <div class=\"flex-auto flex-items-center color-fg-muted\">\n        Suggested change\n        <span class=\"tooltipped tooltipped-multiline tooltipped-s\" aria-label=\"This code change can be committed by users with write permissions.\">\n          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-info hide-sm\">\n    <path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z\"></path>\n</svg>\n        </span>\n      </div>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper data file\" style=\"margin: 0; border: none; overflow-y: visible; overflow-x: auto;\">\n      <table class=\"d-table tab-size mb-0 width-full\" data-paste-markdown-skip=\"\">\n          <tbody><tr class=\"border-0\">\n            <td class=\"blob-num blob-num-deletion text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-deletion js-blob-code-deletion blob-code-marker-deletion\">    <span class=\"pl-smi\">SparkSession</span> spark <span class=\"pl-k\">=</span> <span class=\"pl-smi\">SparkSession</span><span class=\"pl-k\">.</span>builder()<span class=\"pl-k\">.</span>appName(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Hoodie-snapshot-exporter<span class=\"pl-pds\">\"</span></span>)<span class=\"pl-k x x-first\">.</span><span class=\"x\">master(</span><span class=\"pl-s\"><span class=\"pl-pds x\">\"</span><span class=\"x\">local[2]</span><span class=\"pl-pds x\">\"</span></span><span class=\"x x-last\">)</span></td>\n          </tr>\n          <tr class=\"border-0\">\n            <td class=\"blob-num blob-num-addition text-right border-0 px-2 py-1 lh-default\" data-line-number=\"\"></td>\n            <td class=\"border-0 px-2 py-1 blob-code-inner blob-code-addition js-blob-code-addition blob-code-marker-addition\">    <span class=\"pl-smi\">SparkSession</span> spark <span class=\"pl-k\">=</span> <span class=\"pl-smi\">SparkSession</span><span class=\"pl-k\">.</span>builder()<span class=\"pl-k\">.</span>appName(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Hoodie-snapshot-exporter<span class=\"pl-pds\">\"</span></span>)</td>\n          </tr>\n      </tbody></table>\n    </div>\n    <div class=\"js-apply-changes\"></div>\n  </div>\n", "author": "xushiyan", "createdAt": "2020-03-01T10:17:50Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.warn(\"No commits present. Nothing to snapshot\");\n+      return;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        DataFrameWriter<Row> write = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq())\n+                .write();\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          // A field to do simple Spark repartitioning\n+          write.partitionBy(cfg.outputPartitionField)\n+              .format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        } else {\n+          write.format(cfg.outputFormat)\n+              .mode(SaveMode.Overwrite)\n+              .save(cfg.targetOutputPath);\n+        }\n+      } else {\n+        // No transformation is needed for output format \"HUDI\", just copy the original files.\n+\n+        // Make sure the output directory is empty\n+        Path outputPath = new Path(cfg.targetOutputPath);\n+        if (fs.exists(outputPath)) {\n+          LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n+          fs.delete(new Path(cfg.targetOutputPath), true);\n+        }\n+\n+        jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n+          // Only take latest version files <= latestCommit.\n+          FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+          List<Tuple2<String, String>> filePaths = new ArrayList<>();\n+          dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+\n+          // also need to copy over partition metadata\n+          Path partitionMetaFile =\n+              new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n+          if (fs1.exists(partitionMetaFile)) {\n+            filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n+          }\n+\n+          return filePaths.iterator();\n+        }).foreach(tuple -> {\n+          String partition = tuple._1();\n+          Path sourceFilePath = new Path(tuple._2());\n+          Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+          FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+\n+          if (!ifs.exists(toPartitionPath)) {\n+            ifs.mkdirs(toPartitionPath);\n+          }\n+          FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+              ifs.getConf());\n+        });\n+\n+        // Also copy the .commit files\n+        LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+        FileStatus[] commitFilesToCopy =\n+            fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+              if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n+                return true;\n+              } else {\n+                String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+                return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+                    HoodieTimeline.LESSER_OR_EQUAL);\n+              }\n+            });\n+        for (FileStatus commitStatus : commitFilesToCopy) {\n+          Path targetFilePath =\n+              new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n+          if (!fs.exists(targetFilePath.getParent())) {\n+            fs.mkdirs(targetFilePath.getParent());\n+          }\n+          if (fs.exists(targetFilePath)) {\n+            LOG.error(\n+                String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+          }\n+          FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+        }\n+      }\n+    } else {\n+      LOG.info(\"The job has 0 partition to copy.\");\n+    }\n+  }\n+\n+  public static void main(String[] args) throws IOException {\n+    // Take input configs\n+    final Config cfg = new Config();\n+    new JCommander(cfg, null, args);\n+\n+    // Create a spark job to do the snapshot export\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1NzQ3OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386257479", "bodyText": "if remove master() method, spark test unable to work properly.\norg.apache.spark.SparkException: A master URL must be set in your configuration", "author": "OpenOpened", "createdAt": "2020-03-02T08:44:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc3OTM2Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386779367", "bodyText": "ok but wouldn't this fail when running on production where we need to pass different master url?", "author": "xushiyan", "createdAt": "2020-03-03T03:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODMxOTAyOQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r388319029", "bodyText": "It's my fault. Thank you for pointing it out", "author": "OpenOpened", "createdAt": "2020-03-05T14:15:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTI3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..230033c9e5 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -200,7 +220,7 @@ public class HoodieSnapshotExporter {\n     new JCommander(cfg, null, args);\n \n     // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n         .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n     LOG.info(\"Initializing spark job.\");\n \n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 903b7ac636..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -200,7 +211,7 @@ public class HoodieSnapshotExporter {\n     new JCommander(cfg, null, args);\n \n     // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\").master(\"local[2]\")\n+    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n         .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n     LOG.info(\"Initializing spark job.\");\n \n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -170,55 +211,62 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n-      FileSystem ifs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!ifs.exists(toPartitionPath)) {\n-        ifs.mkdirs(toPartitionPath);\n+      if (!fs.exists(toPartitionPath)) {\n+        fs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(ifs, sourceFilePath, ifs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          ifs.getConf());\n+      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n+          fs.getConf());\n     });\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n+    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n-            String commitTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n-            return HoodieTimeline.compareTimestamps(commitTime, latestCommitTimestamp,\n+            String instantTime = FSUtils.getCommitFromCommitFile(commitFilePath.getName());\n+            return HoodieTimeline.compareTimestamps(instantTime, latestCommitTimestamp,\n                 HoodieTimeline.LESSER_OR_EQUAL);\n           }\n         });\n     for (FileStatus commitStatus : commitFilesToCopy) {\n       Path targetFilePath =\n           new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!fs.exists(targetFilePath.getParent())) {\n-        fs.mkdirs(targetFilePath.getParent());\n+      if (!fileSystem.exists(targetFilePath.getParent())) {\n+        fileSystem.mkdirs(targetFilePath.getParent());\n       }\n-      if (fs.exists(targetFilePath)) {\n+      if (fileSystem.exists(targetFilePath)) {\n         LOG.error(\n             String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n       }\n-      FileUtil.copy(fs, commitStatus.getPath(), fs, targetFilePath, false, fs.getConf());\n+      FileUtil.copy(fileSystem, commitStatus.getPath(), fileSystem, targetFilePath, false, fileSystem.getConf());\n     }\n   }\n \n+  private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    return new HoodieTableFileSystemView(tableMetadata, tableMetadata\n+        .getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n   public static void main(String[] args) throws IOException {\n-    // Take input configs\n     final Config cfg = new Config();\n     new JCommander(cfg, null, args);\n \n-    // Create a spark job to do the snapshot export\n-    SparkSession spark = SparkSession.builder().appName(\"Hoodie-snapshot-exporter\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate();\n+    SparkConf sparkConf = new SparkConf().setAppName(\"Hoodie-snapshot-exporter\");\n+    sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);\n     LOG.info(\"Initializing spark job.\");\n \n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    // Stop the job\n-    spark.stop();\n+    try {\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n+    } finally {\n+      jsc.stop();\n+    }\n   }\n }\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095388", "body": "This seems not used", "bodyText": "This seems not used", "bodyHTML": "<p dir=\"auto\">This seems not used</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:19:31Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);\n+      return Option.of(str + \", \\\"partition\\\": \\\"\" + record.getPartitionPath() + \"\\\"}\");\n+    } catch (IOException e) {\n+      return Option.empty();\n+    }\n+  }\n+\n+  public static List<String> convertToStringList(List<HoodieRecord> records) {\n+    return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI1ODQ2Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386258466", "bodyText": "DataSourceTestUtils actually exists under the hudi-spark test module, but we can't access it. I think we can move to the hudi-common package in the future to reuse the code as possible.", "author": "OpenOpened", "createdAt": "2020-03-02T08:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTM4OA=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\nindex af5227ed99..1a96b81a68 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n", "chunk": "@@ -48,10 +47,4 @@ public class DataSourceTestUtils {\n     return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n         .collect(Collectors.toList());\n   }\n-\n-  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {\n-    return keys.stream()\n-        .map(hr -> \"{\\\"_row_key\\\":\\\"\" + hr.getRecordKey() + \"\\\",\\\"partition\\\":\\\"\" + hr.getPartitionPath() + \"\\\"}\")\n-        .collect(Collectors.toList());\n-  }\n }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\nindex af5227ed99..1a96b81a68 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n", "chunk": "@@ -48,10 +47,4 @@ public class DataSourceTestUtils {\n     return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n         .collect(Collectors.toList());\n   }\n-\n-  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {\n-    return keys.stream()\n-        .map(hr -> \"{\\\"_row_key\\\":\\\"\" + hr.getRecordKey() + \"\\\",\\\"partition\\\":\\\"\" + hr.getPartitionPath() + \"\\\"}\")\n-        .collect(Collectors.toList());\n-  }\n }\n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\ndeleted file mode 100644\nindex 1a96b81a68..0000000000\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n+++ /dev/null\n", "chunk": "@@ -1,50 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.utilities;\n-\n-import org.apache.hudi.common.TestRawTripPayload;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.util.Option;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Test utils for data source tests.\n- */\n-public class DataSourceTestUtils {\n-\n-  public static Option<String> convertToString(HoodieRecord record) {\n-    try {\n-      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n-      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n-      // Remove the last } bracket\n-      str = str.substring(0, str.length() - 1);\n-      return Option.of(str + \", \\\"partition\\\": \\\"\" + record.getPartitionPath() + \"\\\"}\");\n-    } catch (IOException e) {\n-      return Option.empty();\n-    }\n-  }\n-\n-  public static List<String> convertToStringList(List<HoodieRecord> records) {\n-    return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n-        .collect(Collectors.toList());\n-  }\n-}\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386095927", "body": "Any reason of not extending `HoodieCommonTestHarness` ?", "bodyText": "Any reason of not extending HoodieCommonTestHarness ?", "bodyHTML": "<p dir=\"auto\">Any reason of not extending <code>HoodieCommonTestHarness</code> ?</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:28:21Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Nzk5Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386277992", "bodyText": "There was little or no connection, so I chose not to inherit it. I have modified the latest code.", "author": "OpenOpened", "createdAt": "2020-03-02T09:28:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NTkyNw=="}], "type": "inlineReview", "revised_code": {"commit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex efcfaed211..c095f28558 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -47,12 +48,11 @@ import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter {\n+public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n   private static String TEST_WRITE_TOKEN = \"1-0-1\";\n \n   private SparkSession spark = null;\n   private HoodieTestDataGenerator dataGen = null;\n-  private String basePath = null;\n   private String outputPath = null;\n   private String rootPath = null;\n   private FileSystem fs = null;\n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex efcfaed211..920f1ed79e 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -47,12 +48,11 @@ import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter {\n+public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n   private static String TEST_WRITE_TOKEN = \"1-0-1\";\n \n   private SparkSession spark = null;\n   private HoodieTestDataGenerator dataGen = null;\n-  private String basePath = null;\n   private String outputPath = null;\n   private String rootPath = null;\n   private FileSystem fs = null;\n", "next_change": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex 920f1ed79e..f624247aa0 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -18,205 +18,171 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n-import org.apache.hudi.common.HoodieCommonTestHarness;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.SparkSession;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n \n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.SparkSession;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n \n-import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n+import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n-\n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n-\n-  @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n+@RunWith(Enclosed.class)\n+public class TestHoodieSnapshotExporter {\n+\n+  static class ExporterTestHarness extends HoodieClientTestHarness {\n+\n+    static final Logger LOG = LogManager.getLogger(ExporterTestHarness.class);\n+    static final int NUM_RECORDS = 100;\n+    static final String COMMIT_TIME = \"20200101000000\";\n+    static final String PARTITION_PATH = \"2020/01/01\";\n+    static final String TABLE_NAME = \"testing\";\n+    String sourcePath;\n+    String targetPath;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      initSparkContexts();\n+      initDFS();\n+      dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+\n+      // Initialize test data dirs\n+      sourcePath = dfsBasePath + \"/source/\";\n+      targetPath = dfsBasePath + \"/target/\";\n+      dfs.mkdirs(new Path(sourcePath));\n+      dfs.mkdirs(new Path(targetPath));\n+      HoodieTableMetaClient\n+          .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n+              HoodieAvroPayload.class.getName());\n+\n+      // Prepare data as source Hudi dataset\n+      HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n+      HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n+      hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n+      List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n+      JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n+      hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n+      hdfsWriteClient.close();\n+\n+      RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n+      while (itr.hasNext()) {\n+        LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n+      }\n+    }\n \n-  }\n+    @After\n+    public void tearDown() throws Exception {\n+      cleanupSparkContexts();\n+      cleanupDFS();\n+      cleanupTestDataGenerator();\n+    }\n \n-  @After\n-  public void cleanup() {\n-    if (spark != null) {\n-      spark.stop();\n+    private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n+      return HoodieWriteConfig.newBuilder()\n+          .withPath(basePath)\n+          .withEmbeddedTimelineServerEnabled(false)\n+          .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n+          .withParallelism(2, 2)\n+          .withBulkInsertParallelism(2)\n+          .forTable(TABLE_NAME)\n+          .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n+          .build();\n     }\n   }\n \n-  @Test\n-  public void testSnapshotExporter() throws IOException {\n-    // Insert Operation\n-    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n-    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n-    inputDF.write().format(\"hudi\")\n-        .options(commonOpts)\n-        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n-        .mode(SaveMode.Overwrite)\n-        .save(basePath);\n-    long sourceCount = inputDF.count();\n-\n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    long targetCount = spark.read().json(outputPath).count();\n-\n-    assertTrue(sourceCount == targetCount);\n-\n-    // Test Invalid OutputFormat\n-    cfg.outputFormat = \"foo\";\n-    int isError = hoodieSnapshotExporter.export(spark, cfg);\n-    assertTrue(isError == -1);\n-  }\n+  public static class TestHoodieSnapshotExporterForHudi extends ExporterTestHarness {\n+\n+    @Test\n+    public void testExportAsHudi() throws IOException {\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"hudi\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      // Check results\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.inflight\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.requested\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit.requested\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".inflight\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/hoodie.properties\")));\n+      String partition = targetPath + \"/\" + PARTITION_PATH;\n+      long numParquetFiles = Arrays.stream(dfs.listStatus(new Path(partition)))\n+          .filter(fileStatus -> fileStatus.getPath().toString().endsWith(\".parquet\"))\n+          .count();\n+      assertTrue(\"There should exist at least 1 parquet file.\", numParquetFiles >= 1);\n+      assertEquals(NUM_RECORDS, sqlContext.read().parquet(partition).count());\n+      assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n \n-  // for testEmptySnapshotCopy\n-  public void init() throws IOException {\n-    TemporaryFolder folder = new TemporaryFolder();\n-    folder.create();\n-    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n-    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n-    outputPath = rootPath + \"/output\";\n-\n-    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n-    fs = FSUtils.getFs(basePath, hadoopConf);\n-    HoodieTestUtils.init(hadoopConf, basePath);\n+    @Test\n+    public void testExportEmptyDataset() throws IOException {\n+      // delete all source data\n+      dfs.delete(new Path(sourcePath + \"/\" + PARTITION_PATH), true);\n+\n+      // export\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"hudi\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      // Check results\n+      assertEquals(\"Target path should be empty.\", 0, dfs.listStatus(new Path(targetPath)).length);\n+      assertFalse(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n   }\n \n-  @Test\n-  public void testEmptySnapshotCopy() throws IOException {\n-    init();\n-    // There is no real data (only .hoodie directory)\n-    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n-    assertFalse(fs.exists(new Path(outputPath)));\n-\n-    // Do the snapshot\n-    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n-    copier.snapshot(jsc, basePath, outputPath, true);\n+  @RunWith(Parameterized.class)\n+  public static class TestHoodieSnapshotExporterForNonHudi extends ExporterTestHarness {\n \n-    // Nothing changed; we just bail out\n-    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n-    assertFalse(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n-  }\n+    @Parameters\n+    public static Iterable<String[]> formats() {\n+      return Arrays.asList(new String[][] {{\"json\"}, {\"parquet\"}});\n+    }\n \n-  // TODO - uncomment this after fixing test failures\n-  // @Test\n-  public void testSnapshotCopy() throws Exception {\n-    // Generate some commits and corresponding parquets\n-    String commitTime1 = \"20160501010101\";\n-    String commitTime2 = \"20160502020601\";\n-    String commitTime3 = \"20160506030611\";\n-    new File(basePath + \"/.hoodie\").mkdirs();\n-    new File(basePath + \"/.hoodie/hoodie.properties\").createNewFile();\n-    // Only first two have commit files\n-    new File(basePath + \"/.hoodie/\" + commitTime1 + \".commit\").createNewFile();\n-    new File(basePath + \"/.hoodie/\" + commitTime2 + \".commit\").createNewFile();\n-    new File(basePath + \"/.hoodie/\" + commitTime3 + \".inflight\").createNewFile();\n-\n-    // Some parquet files\n-    new File(basePath + \"/2016/05/01/\").mkdirs();\n-    new File(basePath + \"/2016/05/02/\").mkdirs();\n-    new File(basePath + \"/2016/05/06/\").mkdirs();\n-    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{\"2016/05/01\", \"2016/05/02\", \"2016/05/06\"},\n-        basePath);\n-    // Make commit1\n-    File file11 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id11\"));\n-    file11.createNewFile();\n-    File file12 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id12\"));\n-    file12.createNewFile();\n-    File file13 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id13\"));\n-    file13.createNewFile();\n-\n-    // Make commit2\n-    File file21 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id21\"));\n-    file21.createNewFile();\n-    File file22 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id22\"));\n-    file22.createNewFile();\n-    File file23 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id23\"));\n-    file23.createNewFile();\n-\n-    // Make commit3\n-    File file31 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id31\"));\n-    file31.createNewFile();\n-    File file32 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id32\"));\n-    file32.createNewFile();\n-    File file33 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id33\"));\n-    file33.createNewFile();\n-\n-    // Do a snapshot copy\n-    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n-    copier.snapshot(jsc, basePath, outputPath, false);\n-\n-    // Check results\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file11.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file12.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file13.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file21.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file22.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file23.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file31.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file32.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file33.getName())));\n-\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime1 + \".commit\")));\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime2 + \".commit\")));\n-    assertFalse(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime3 + \".commit\")));\n-    assertFalse(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime3 + \".inflight\")));\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/hoodie.properties\")));\n-\n-    assertTrue(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n+    @Parameter\n+    public String format;\n+\n+    @Test\n+    public void testExportAsNonHudi() throws IOException {\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = format;\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(format).load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n   }\n }\n", "next_change": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex f624247aa0..6eb15a2d95 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -185,4 +196,85 @@ public class TestHoodieSnapshotExporter {\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n   }\n+\n+  public static class TestHoodieSnapshotExporterForRepartitioning extends ExporterTestHarness {\n+\n+    private static final String PARTITION_NAME = \"year\";\n+\n+    public static class UserDefinedPartitioner implements Partitioner {\n+\n+      @Override\n+      public DataFrameWriter<Row> partition(Dataset<Row> source) {\n+        return source\n+            .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n+            .repartition(new Column(PARTITION_NAME))\n+            .write()\n+            .partitionBy(PARTITION_NAME);\n+      }\n+    }\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"json\";\n+    }\n+\n+    @Test\n+    public void testExportWithPartitionField() throws IOException {\n+      // `driver` field is set in HoodieTestDataGenerator\n+      cfg.outputPartitionField = \"driver\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+      assertTrue(dfs.listStatus(new Path(targetPath)).length > 1);\n+    }\n+\n+    @Test\n+    public void testExportForUserDefinedPartitioner() throws IOException {\n+      cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, PARTITION_NAME, PARTITION_PATH))));\n+    }\n+  }\n+\n+  @RunWith(Parameterized.class)\n+  public static class TestHoodieSnapshotExporterInputValidation {\n+\n+    @Parameters\n+    public static Iterable<Object[]> data() {\n+      return Arrays.asList(new Object[][] {\n+          {\"json\", true}, {\"parquet\", true}, {\"hudi\", true},\n+          {\"JSON\", false}, {\"foo\", false}, {null, false}, {\"\", false}\n+      });\n+    }\n+\n+    @Parameter\n+    public String format;\n+    @Parameter(1)\n+    public boolean isValid;\n+\n+    @Test\n+    public void testValidateOutputFormat() {\n+      Throwable t = null;\n+      try {\n+        new OutputFormatValidator().validate(null, format);\n+      } catch (Exception e) {\n+        t = e;\n+      }\n+      if (isValid) {\n+        assertNull(t);\n+      } else {\n+        assertTrue(t instanceof ParameterException);\n+      }\n+    }\n+  }\n }\n", "next_change": {"commit": "06dae30297ea02ab122c9029a54f7927e8212039", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex 6eb15a2d95..fda192f962 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -238,43 +270,39 @@ public class TestHoodieSnapshotExporter {\n     @Test\n     public void testExportForUserDefinedPartitioner() throws IOException {\n       cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n-      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n \n       assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, PARTITION_NAME, PARTITION_PATH))));\n+      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, UserDefinedPartitioner.PARTITION_NAME, PARTITION_PATH))));\n     }\n   }\n \n-  @RunWith(Parameterized.class)\n-  public static class TestHoodieSnapshotExporterInputValidation {\n+  @Nested\n+  public class TestHoodieSnapshotExporterInputValidation {\n \n-    @Parameters\n-    public static Iterable<Object[]> data() {\n-      return Arrays.asList(new Object[][] {\n-          {\"json\", true}, {\"parquet\", true}, {\"hudi\", true},\n-          {\"JSON\", false}, {\"foo\", false}, {null, false}, {\"\", false}\n+    @ParameterizedTest\n+    @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n+    public void testValidateOutputFormat_withValidFormat(String format) {\n+      assertDoesNotThrow(() -> {\n+        new OutputFormatValidator().validate(null, format);\n       });\n     }\n \n-    @Parameter\n-    public String format;\n-    @Parameter(1)\n-    public boolean isValid;\n+    @ParameterizedTest\n+    @ValueSource(strings = {\"\", \"JSON\"})\n+    public void testValidateOutputFormat_withInvalidFormat(String format) {\n+      assertThrows(ParameterException.class, () -> {\n+        new OutputFormatValidator().validate(null, format);\n+      });\n+    }\n \n-    @Test\n-    public void testValidateOutputFormat() {\n-      Throwable t = null;\n-      try {\n+    @ParameterizedTest\n+    @NullSource\n+    public void testValidateOutputFormat_withNullFormat(String format) {\n+      assertThrows(ParameterException.class, () -> {\n         new OutputFormatValidator().validate(null, format);\n-      } catch (Exception e) {\n-        t = e;\n-      }\n-      if (isValid) {\n-        assertNull(t);\n-      } else {\n-        assertTrue(t instanceof ParameterException);\n-      }\n+      });\n     }\n   }\n }\n", "next_change": {"commit": "3b9a30528bd6a6369181702303f3384162b04a7f", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex fda192f962..ae74c1d04f 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -7,302 +7,51 @@\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n  */\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hudi.client.HoodieWriteClient;\n-import org.apache.hudi.common.HoodieClientTestHarness;\n-import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieAvroPayload;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieTableType;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.config.HoodieIndexConfig;\n-import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.index.HoodieIndex.IndexType;\n-import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n import org.apache.hudi.utilities.HoodieSnapshotExporter.OutputFormatValidator;\n-import org.apache.hudi.utilities.HoodieSnapshotExporter.Partitioner;\n-import org.apache.hudi.utilities.exception.HoodieSnapshotExporterException;\n \n import com.beust.jcommander.ParameterException;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.LocatedFileStatus;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.fs.RemoteIterator;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.sql.Column;\n-import org.apache.spark.sql.DataFrameWriter;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Nested;\n-import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.NullSource;\n import org.junit.jupiter.params.provider.ValueSource;\n \n-import java.io.IOException;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n-\n-public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n-\n-  static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n-  static final int NUM_RECORDS = 100;\n-  static final String COMMIT_TIME = \"20200101000000\";\n-  static final String PARTITION_PATH = \"2020\";\n-  static final String TABLE_NAME = \"testing\";\n-  String sourcePath;\n-  String targetPath;\n \n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    initSparkContexts();\n-    initDFS();\n-    dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+public class TestHoodieSnapshotExporter {\n \n-    // Initialize test data dirs\n-    sourcePath = dfsBasePath + \"/source/\";\n-    targetPath = dfsBasePath + \"/target/\";\n-    dfs.mkdirs(new Path(sourcePath));\n-    HoodieTableMetaClient\n-        .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n-            HoodieAvroPayload.class.getName());\n-\n-    // Prepare data as source Hudi dataset\n-    HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n-    HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n-    hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n-    List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n-    JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n-    hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n-    hdfsWriteClient.close();\n-\n-    RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n-    while (itr.hasNext()) {\n-      LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n-    }\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n+  public void testValidateOutputFormatWithValidFormat(String format) {\n+    assertDoesNotThrow(() -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n \n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupSparkContexts();\n-    cleanupDFS();\n-    cleanupTestDataGenerator();\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"\", \"JSON\"})\n+  public void testValidateOutputFormatWithInvalidFormat(String format) {\n+    assertThrows(ParameterException.class, () -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n \n-  private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n-    return HoodieWriteConfig.newBuilder()\n-        .withPath(basePath)\n-        .withEmbeddedTimelineServerEnabled(false)\n-        .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n-        .withParallelism(2, 2)\n-        .withBulkInsertParallelism(2)\n-        .forTable(TABLE_NAME)\n-        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n-        .build();\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForHudi {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = OutputFormatValidator.HUDI;\n-    }\n-\n-    @Test\n-    public void testExportAsHudi() throws IOException {\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      // Check results\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.inflight\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.requested\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit.requested\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".inflight\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/hoodie.properties\")));\n-      String partition = targetPath + \"/\" + PARTITION_PATH;\n-      long numParquetFiles = Arrays.stream(dfs.listStatus(new Path(partition)))\n-          .filter(fileStatus -> fileStatus.getPath().toString().endsWith(\".parquet\"))\n-          .count();\n-      assertTrue(numParquetFiles >= 1, \"There should exist at least 1 parquet file.\");\n-      assertEquals(NUM_RECORDS, sqlContext.read().parquet(partition).count());\n-      assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForEarlyAbort {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = OutputFormatValidator.HUDI;\n-    }\n-\n-    @Test\n-    public void testExportWhenTargetPathExists() throws IOException {\n-      // make target output path present\n-      dfs.mkdirs(new Path(targetPath));\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"The target output path already exists.\", thrown.getMessage());\n-    }\n-\n-    @Test\n-    public void testExportDatasetWithNoCommit() throws IOException {\n-      // delete commit files\n-      List<Path> commitFiles = Arrays.stream(dfs.listStatus(new Path(sourcePath + \"/.hoodie\")))\n-          .map(FileStatus::getPath)\n-          .filter(filePath -> filePath.getName().endsWith(\".commit\"))\n-          .collect(Collectors.toList());\n-      for (Path p : commitFiles) {\n-        dfs.delete(p, false);\n-      }\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"No commits present. Nothing to snapshot.\", thrown.getMessage());\n-    }\n-\n-    @Test\n-    public void testExportDatasetWithNoPartition() throws IOException {\n-      // delete all source data\n-      dfs.delete(new Path(sourcePath + \"/\" + PARTITION_PATH), true);\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"The source dataset has 0 partition to snapshot.\", thrown.getMessage());\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForNonHudi {\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"json\", \"parquet\"})\n-    public void testExportAsNonHudi(String format) throws IOException {\n-      HoodieSnapshotExporter.Config cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = format;\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(format).load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-    }\n-  }\n-\n-  public static class UserDefinedPartitioner implements Partitioner {\n-\n-    public static final String PARTITION_NAME = \"year\";\n-\n-    @Override\n-    public DataFrameWriter<Row> partition(Dataset<Row> source) {\n-      return source\n-          .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n-          .repartition(new Column(PARTITION_NAME))\n-          .write()\n-          .partitionBy(PARTITION_NAME);\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForRepartitioning {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = \"json\";\n-    }\n-\n-    @Test\n-    public void testExportWithPartitionField() throws IOException {\n-      // `driver` field is set in HoodieTestDataGenerator\n-      cfg.outputPartitionField = \"driver\";\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.listStatus(new Path(targetPath)).length > 1);\n-    }\n-\n-    @Test\n-    public void testExportForUserDefinedPartitioner() throws IOException {\n-      cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, UserDefinedPartitioner.PARTITION_NAME, PARTITION_PATH))));\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterInputValidation {\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n-    public void testValidateOutputFormat_withValidFormat(String format) {\n-      assertDoesNotThrow(() -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"\", \"JSON\"})\n-    public void testValidateOutputFormat_withInvalidFormat(String format) {\n-      assertThrows(ParameterException.class, () -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n-\n-    @ParameterizedTest\n-    @NullSource\n-    public void testValidateOutputFormat_withNullFormat(String format) {\n-      assertThrows(ParameterException.class, () -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n+  @ParameterizedTest\n+  @NullSource\n+  public void testValidateOutputFormatWithNullFormat(String format) {\n+    assertThrows(ParameterException.class, () -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n }\n", "next_change": null}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "06dae30297ea02ab122c9029a54f7927e8212039", "committedDate": "2020-04-28 23:38:16 +0800", "message": "[HUDI-810] Migrate ClientTestHarness to JUnit 5 (#1553)"}, {"oid": "506447fd4fde4cd922f7aa8f4e17a7f06666dc97", "committedDate": "2020-05-01 21:37:21 -0700", "message": "[HUDI-850] Avoid unnecessary listings in incremental cleaning mode (#1576)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "3b9a30528bd6a6369181702303f3384162b04a7f", "committedDate": "2020-07-05 16:44:31 -0700", "message": "[HUDI-996] Add functional test suite for hudi-utilities (#1746)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjE4Mg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096182", "body": "This assumes what data column exists in `TestRawTripPayload`, which may easily break. I would suggest \r\n1. making this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and \r\n2. try to make the source data work for the case instead of wrangling with json string representation", "bodyText": "This assumes what data column exists in TestRawTripPayload, which may easily break. I would suggest\n\nmaking this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and\ntry to make the source data work for the case instead of wrangling with json string representation", "bodyHTML": "<p dir=\"auto\">This assumes what data column exists in <code>TestRawTripPayload</code>, which may easily break. I would suggest</p>\n<ol dir=\"auto\">\n<li>making this a private test utils to the Exporter test class as it is very specific to it (not generic enough to be a standalone test utils) and</li>\n<li>try to make the source data work for the case instead of wrangling with json string representation</li>\n</ol>", "author": "xushiyan", "createdAt": "2020-03-01T10:32:13Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test utils for data source tests.\n+ */\n+public class DataSourceTestUtils {\n+\n+  public static Option<String> convertToString(HoodieRecord record) {\n+    try {\n+      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n+      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n+      // Remove the last } bracket\n+      str = str.substring(0, str.length() - 1);", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\ndeleted file mode 100644\nindex af5227ed99..0000000000\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java\n+++ /dev/null\n", "chunk": "@@ -1,57 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.utilities;\n-\n-import org.apache.hudi.common.TestRawTripPayload;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.util.Option;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Test utils for data source tests.\n- */\n-public class DataSourceTestUtils {\n-\n-  public static Option<String> convertToString(HoodieRecord record) {\n-    try {\n-      String str = ((TestRawTripPayload) record.getData()).getJsonData();\n-      str = \"{\" + str.substring(str.indexOf(\"\\\"timestamp\\\":\"));\n-      // Remove the last } bracket\n-      str = str.substring(0, str.length() - 1);\n-      return Option.of(str + \", \\\"partition\\\": \\\"\" + record.getPartitionPath() + \"\\\"}\");\n-    } catch (IOException e) {\n-      return Option.empty();\n-    }\n-  }\n-\n-  public static List<String> convertToStringList(List<HoodieRecord> records) {\n-    return records.stream().map(DataSourceTestUtils::convertToString).filter(Option::isPresent).map(Option::get)\n-        .collect(Collectors.toList());\n-  }\n-\n-  public static List<String> convertKeysToStringList(List<HoodieKey> keys) {\n-    return keys.stream()\n-        .map(hr -> \"{\\\"_row_key\\\":\\\"\" + hr.getRecordKey() + \"\\\",\\\"partition\\\":\\\"\" + hr.getPartitionPath() + \"\\\"}\")\n-        .collect(Collectors.toList());\n-  }\n-}\n", "next_change": null}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386096648", "body": "Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)\r\n", "bodyText": "Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)", "bodyHTML": "<p dir=\"auto\">Notice this is from original test for Snapshot copier, not sure about the historical issue for skipping it. I would slightly favor fixing it while you're at this stage so that we can claim the new \"Exporter\" is better tested than the old \"Copier\" :)</p>", "author": "xushiyan", "createdAt": "2020-03-01T10:39:10Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestHoodieSnapshotExporter {\n+  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n+\n+  private SparkSession spark = null;\n+  private HoodieTestDataGenerator dataGen = null;\n+  private String basePath = null;\n+  private String outputPath = null;\n+  private String rootPath = null;\n+  private FileSystem fs = null;\n+  private Map commonOpts;\n+  private HoodieSnapshotExporter.Config cfg;\n+  private JavaSparkContext jsc = null;\n+\n+  @Before\n+  public void initialize() throws IOException {\n+    spark = SparkSession.builder()\n+        .appName(\"Hoodie Datasource test\")\n+        .master(\"local[2]\")\n+        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .getOrCreate();\n+    jsc = new JavaSparkContext(spark.sparkContext());\n+    dataGen = new HoodieTestDataGenerator();\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    basePath = folder.getRoot().getAbsolutePath();\n+    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n+    commonOpts = new HashMap();\n+\n+    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n+    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n+    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n+    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n+    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n+\n+\n+    cfg = new HoodieSnapshotExporter.Config();\n+\n+    cfg.sourceBasePath = basePath;\n+    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n+    cfg.outputFormat = \"json\";\n+    cfg.outputPartitionField = \"partition\";\n+\n+  }\n+\n+  @After\n+  public void cleanup() throws Exception {\n+    if (spark != null) {\n+      spark.stop();\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotExporter() throws IOException {\n+    // Insert Operation\n+    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n+    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n+    inputDF.write().format(\"hudi\")\n+        .options(commonOpts)\n+        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n+        .mode(SaveMode.Overwrite)\n+        .save(basePath);\n+    long sourceCount = inputDF.count();\n+\n+    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n+    hoodieSnapshotExporter.export(spark, cfg);\n+\n+    long targetCount = spark.read().json(outputPath).count();\n+\n+    assertTrue(sourceCount == targetCount);\n+\n+    // Test snapshotPrefix\n+    long filterCount = inputDF.where(\"partition == '2015/03/16'\").count();\n+    cfg.snapshotPrefix = \"2015/03/16\";\n+    hoodieSnapshotExporter.export(spark, cfg);\n+    long targetFilterCount = spark.read().json(outputPath).count();\n+    assertTrue(filterCount == targetFilterCount);\n+\n+  }\n+\n+  // for testEmptySnapshotCopy\n+  public void init() throws IOException {\n+    TemporaryFolder folder = new TemporaryFolder();\n+    folder.create();\n+    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n+    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n+    outputPath = rootPath + \"/output\";\n+\n+    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n+    fs = FSUtils.getFs(basePath, hadoopConf);\n+    HoodieTestUtils.init(hadoopConf, basePath);\n+  }\n+\n+  @Test\n+  public void testEmptySnapshotCopy() throws IOException {\n+    init();\n+    // There is no real data (only .hoodie directory)\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath)));\n+\n+    // Do the snapshot\n+    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n+    copier.snapshot(jsc, basePath, outputPath, true);\n+\n+    // Nothing changed; we just bail out\n+    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n+    assertFalse(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n+  }\n+\n+  // TODO - uncomment this after fixing test failures\n+  // @Test", "originalCommit": "d6ffad986b20067b2708e212d00575345a039dff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjI3Njk2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386276963", "bodyText": "I'm not quite sure what the comments here mean and what the implications might be. It may take officials to sort it out.", "author": "OpenOpened", "createdAt": "2020-03-02T09:26:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjA5NjY0OA=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": {"commit": "14323cb10012bdbf80cbb838928af9301cb42ba0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex efcfaed211..f624247aa0 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -18,210 +18,171 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.SparkSession;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n \n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.SparkSession;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n \n-import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n+import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n+@RunWith(Enclosed.class)\n public class TestHoodieSnapshotExporter {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n-\n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String basePath = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n-\n-  @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    TemporaryFolder folder = new TemporaryFolder();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n-\n-  }\n \n-  @After\n-  public void cleanup() throws Exception {\n-    if (spark != null) {\n-      spark.stop();\n+  static class ExporterTestHarness extends HoodieClientTestHarness {\n+\n+    static final Logger LOG = LogManager.getLogger(ExporterTestHarness.class);\n+    static final int NUM_RECORDS = 100;\n+    static final String COMMIT_TIME = \"20200101000000\";\n+    static final String PARTITION_PATH = \"2020/01/01\";\n+    static final String TABLE_NAME = \"testing\";\n+    String sourcePath;\n+    String targetPath;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      initSparkContexts();\n+      initDFS();\n+      dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+\n+      // Initialize test data dirs\n+      sourcePath = dfsBasePath + \"/source/\";\n+      targetPath = dfsBasePath + \"/target/\";\n+      dfs.mkdirs(new Path(sourcePath));\n+      dfs.mkdirs(new Path(targetPath));\n+      HoodieTableMetaClient\n+          .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n+              HoodieAvroPayload.class.getName());\n+\n+      // Prepare data as source Hudi dataset\n+      HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n+      HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n+      hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n+      List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n+      JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n+      hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n+      hdfsWriteClient.close();\n+\n+      RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n+      while (itr.hasNext()) {\n+        LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n+      }\n     }\n-  }\n-\n-  @Test\n-  public void testSnapshotExporter() throws IOException {\n-    // Insert Operation\n-    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n-    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n-    inputDF.write().format(\"hudi\")\n-        .options(commonOpts)\n-        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n-        .mode(SaveMode.Overwrite)\n-        .save(basePath);\n-    long sourceCount = inputDF.count();\n-\n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    long targetCount = spark.read().json(outputPath).count();\n-\n-    assertTrue(sourceCount == targetCount);\n \n-    // Test snapshotPrefix\n-    long filterCount = inputDF.where(\"partition == '2015/03/16'\").count();\n-    cfg.snapshotPrefix = \"2015/03/16\";\n-    hoodieSnapshotExporter.export(spark, cfg);\n-    long targetFilterCount = spark.read().json(outputPath).count();\n-    assertTrue(filterCount == targetFilterCount);\n+    @After\n+    public void tearDown() throws Exception {\n+      cleanupSparkContexts();\n+      cleanupDFS();\n+      cleanupTestDataGenerator();\n+    }\n \n+    private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n+      return HoodieWriteConfig.newBuilder()\n+          .withPath(basePath)\n+          .withEmbeddedTimelineServerEnabled(false)\n+          .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n+          .withParallelism(2, 2)\n+          .withBulkInsertParallelism(2)\n+          .forTable(TABLE_NAME)\n+          .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n+          .build();\n+    }\n   }\n \n-  // for testEmptySnapshotCopy\n-  public void init() throws IOException {\n-    TemporaryFolder folder = new TemporaryFolder();\n-    folder.create();\n-    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n-    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n-    outputPath = rootPath + \"/output\";\n-\n-    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n-    fs = FSUtils.getFs(basePath, hadoopConf);\n-    HoodieTestUtils.init(hadoopConf, basePath);\n-  }\n+  public static class TestHoodieSnapshotExporterForHudi extends ExporterTestHarness {\n+\n+    @Test\n+    public void testExportAsHudi() throws IOException {\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"hudi\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      // Check results\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.inflight\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.requested\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit.requested\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".inflight\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/hoodie.properties\")));\n+      String partition = targetPath + \"/\" + PARTITION_PATH;\n+      long numParquetFiles = Arrays.stream(dfs.listStatus(new Path(partition)))\n+          .filter(fileStatus -> fileStatus.getPath().toString().endsWith(\".parquet\"))\n+          .count();\n+      assertTrue(\"There should exist at least 1 parquet file.\", numParquetFiles >= 1);\n+      assertEquals(NUM_RECORDS, sqlContext.read().parquet(partition).count());\n+      assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n \n-  @Test\n-  public void testEmptySnapshotCopy() throws IOException {\n-    init();\n-    // There is no real data (only .hoodie directory)\n-    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n-    assertFalse(fs.exists(new Path(outputPath)));\n+    @Test\n+    public void testExportEmptyDataset() throws IOException {\n+      // delete all source data\n+      dfs.delete(new Path(sourcePath + \"/\" + PARTITION_PATH), true);\n+\n+      // export\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"hudi\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      // Check results\n+      assertEquals(\"Target path should be empty.\", 0, dfs.listStatus(new Path(targetPath)).length);\n+      assertFalse(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n+  }\n \n-    // Do the snapshot\n-    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n-    copier.snapshot(jsc, basePath, outputPath, true);\n+  @RunWith(Parameterized.class)\n+  public static class TestHoodieSnapshotExporterForNonHudi extends ExporterTestHarness {\n \n-    // Nothing changed; we just bail out\n-    assertEquals(fs.listStatus(new Path(basePath)).length, 1);\n-    assertFalse(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n-  }\n+    @Parameters\n+    public static Iterable<String[]> formats() {\n+      return Arrays.asList(new String[][] {{\"json\"}, {\"parquet\"}});\n+    }\n \n-  // TODO - uncomment this after fixing test failures\n-  // @Test\n-  public void testSnapshotCopy() throws Exception {\n-    // Generate some commits and corresponding parquets\n-    String commitTime1 = \"20160501010101\";\n-    String commitTime2 = \"20160502020601\";\n-    String commitTime3 = \"20160506030611\";\n-    new File(basePath + \"/.hoodie\").mkdirs();\n-    new File(basePath + \"/.hoodie/hoodie.properties\").createNewFile();\n-    // Only first two have commit files\n-    new File(basePath + \"/.hoodie/\" + commitTime1 + \".commit\").createNewFile();\n-    new File(basePath + \"/.hoodie/\" + commitTime2 + \".commit\").createNewFile();\n-    new File(basePath + \"/.hoodie/\" + commitTime3 + \".inflight\").createNewFile();\n-\n-    // Some parquet files\n-    new File(basePath + \"/2016/05/01/\").mkdirs();\n-    new File(basePath + \"/2016/05/02/\").mkdirs();\n-    new File(basePath + \"/2016/05/06/\").mkdirs();\n-    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{\"2016/05/01\", \"2016/05/02\", \"2016/05/06\"},\n-        basePath);\n-    // Make commit1\n-    File file11 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id11\"));\n-    file11.createNewFile();\n-    File file12 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id12\"));\n-    file12.createNewFile();\n-    File file13 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime1, TEST_WRITE_TOKEN, \"id13\"));\n-    file13.createNewFile();\n-\n-    // Make commit2\n-    File file21 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id21\"));\n-    file21.createNewFile();\n-    File file22 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id22\"));\n-    file22.createNewFile();\n-    File file23 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime2, TEST_WRITE_TOKEN, \"id23\"));\n-    file23.createNewFile();\n-\n-    // Make commit3\n-    File file31 = new File(basePath + \"/2016/05/01/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id31\"));\n-    file31.createNewFile();\n-    File file32 = new File(basePath + \"/2016/05/02/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id32\"));\n-    file32.createNewFile();\n-    File file33 = new File(basePath + \"/2016/05/06/\" + FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, \"id33\"));\n-    file33.createNewFile();\n-\n-    // Do a snapshot copy\n-    HoodieSnapshotCopier copier = new HoodieSnapshotCopier();\n-    copier.snapshot(jsc, basePath, outputPath, false);\n-\n-    // Check results\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file11.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file12.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file13.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file21.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file22.getName())));\n-    assertTrue(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file23.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/01/\" + file31.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/02/\" + file32.getName())));\n-    assertFalse(fs.exists(new Path(outputPath + \"/2016/05/06/\" + file33.getName())));\n-\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime1 + \".commit\")));\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime2 + \".commit\")));\n-    assertFalse(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime3 + \".commit\")));\n-    assertFalse(fs.exists(new Path(outputPath + \"/.hoodie/\" + commitTime3 + \".inflight\")));\n-    assertTrue(fs.exists(new Path(outputPath + \"/.hoodie/hoodie.properties\")));\n-\n-    assertTrue(fs.exists(new Path(outputPath + \"/_SUCCESS\")));\n+    @Parameter\n+    public String format;\n+\n+    @Test\n+    public void testExportAsNonHudi() throws IOException {\n+      HoodieSnapshotExporter.Config cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = format;\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(format).load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+    }\n   }\n-\n }\n", "next_change": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex f624247aa0..6eb15a2d95 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -185,4 +196,85 @@ public class TestHoodieSnapshotExporter {\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n     }\n   }\n+\n+  public static class TestHoodieSnapshotExporterForRepartitioning extends ExporterTestHarness {\n+\n+    private static final String PARTITION_NAME = \"year\";\n+\n+    public static class UserDefinedPartitioner implements Partitioner {\n+\n+      @Override\n+      public DataFrameWriter<Row> partition(Dataset<Row> source) {\n+        return source\n+            .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n+            .repartition(new Column(PARTITION_NAME))\n+            .write()\n+            .partitionBy(PARTITION_NAME);\n+      }\n+    }\n+\n+    private HoodieSnapshotExporter.Config cfg;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+      super.setUp();\n+      cfg = new Config();\n+      cfg.sourceBasePath = sourcePath;\n+      cfg.targetOutputPath = targetPath;\n+      cfg.outputFormat = \"json\";\n+    }\n+\n+    @Test\n+    public void testExportWithPartitionField() throws IOException {\n+      // `driver` field is set in HoodieTestDataGenerator\n+      cfg.outputPartitionField = \"driver\";\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+      assertTrue(dfs.listStatus(new Path(targetPath)).length > 1);\n+    }\n+\n+    @Test\n+    public void testExportForUserDefinedPartitioner() throws IOException {\n+      cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n+      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+\n+      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n+      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n+      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, PARTITION_NAME, PARTITION_PATH))));\n+    }\n+  }\n+\n+  @RunWith(Parameterized.class)\n+  public static class TestHoodieSnapshotExporterInputValidation {\n+\n+    @Parameters\n+    public static Iterable<Object[]> data() {\n+      return Arrays.asList(new Object[][] {\n+          {\"json\", true}, {\"parquet\", true}, {\"hudi\", true},\n+          {\"JSON\", false}, {\"foo\", false}, {null, false}, {\"\", false}\n+      });\n+    }\n+\n+    @Parameter\n+    public String format;\n+    @Parameter(1)\n+    public boolean isValid;\n+\n+    @Test\n+    public void testValidateOutputFormat() {\n+      Throwable t = null;\n+      try {\n+        new OutputFormatValidator().validate(null, format);\n+      } catch (Exception e) {\n+        t = e;\n+      }\n+      if (isValid) {\n+        assertNull(t);\n+      } else {\n+        assertTrue(t instanceof ParameterException);\n+      }\n+    }\n+  }\n }\n", "next_change": {"commit": "06dae30297ea02ab122c9029a54f7927e8212039", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex 6eb15a2d95..fda192f962 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -238,43 +270,39 @@ public class TestHoodieSnapshotExporter {\n     @Test\n     public void testExportForUserDefinedPartitioner() throws IOException {\n       cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n-      new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+      new HoodieSnapshotExporter().export(jsc, cfg);\n \n       assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n       assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, PARTITION_NAME, PARTITION_PATH))));\n+      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, UserDefinedPartitioner.PARTITION_NAME, PARTITION_PATH))));\n     }\n   }\n \n-  @RunWith(Parameterized.class)\n-  public static class TestHoodieSnapshotExporterInputValidation {\n+  @Nested\n+  public class TestHoodieSnapshotExporterInputValidation {\n \n-    @Parameters\n-    public static Iterable<Object[]> data() {\n-      return Arrays.asList(new Object[][] {\n-          {\"json\", true}, {\"parquet\", true}, {\"hudi\", true},\n-          {\"JSON\", false}, {\"foo\", false}, {null, false}, {\"\", false}\n+    @ParameterizedTest\n+    @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n+    public void testValidateOutputFormat_withValidFormat(String format) {\n+      assertDoesNotThrow(() -> {\n+        new OutputFormatValidator().validate(null, format);\n       });\n     }\n \n-    @Parameter\n-    public String format;\n-    @Parameter(1)\n-    public boolean isValid;\n+    @ParameterizedTest\n+    @ValueSource(strings = {\"\", \"JSON\"})\n+    public void testValidateOutputFormat_withInvalidFormat(String format) {\n+      assertThrows(ParameterException.class, () -> {\n+        new OutputFormatValidator().validate(null, format);\n+      });\n+    }\n \n-    @Test\n-    public void testValidateOutputFormat() {\n-      Throwable t = null;\n-      try {\n+    @ParameterizedTest\n+    @NullSource\n+    public void testValidateOutputFormat_withNullFormat(String format) {\n+      assertThrows(ParameterException.class, () -> {\n         new OutputFormatValidator().validate(null, format);\n-      } catch (Exception e) {\n-        t = e;\n-      }\n-      if (isValid) {\n-        assertNull(t);\n-      } else {\n-        assertTrue(t instanceof ParameterException);\n-      }\n+      });\n     }\n   }\n }\n", "next_change": {"commit": "3b9a30528bd6a6369181702303f3384162b04a7f", "changed_code": [{"header": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\nindex fda192f962..ae74c1d04f 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java\n", "chunk": "@@ -7,302 +7,51 @@\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n  *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n  */\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hudi.client.HoodieWriteClient;\n-import org.apache.hudi.common.HoodieClientTestHarness;\n-import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieAvroPayload;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieTableType;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.config.HoodieIndexConfig;\n-import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.index.HoodieIndex.IndexType;\n-import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n import org.apache.hudi.utilities.HoodieSnapshotExporter.OutputFormatValidator;\n-import org.apache.hudi.utilities.HoodieSnapshotExporter.Partitioner;\n-import org.apache.hudi.utilities.exception.HoodieSnapshotExporterException;\n \n import com.beust.jcommander.ParameterException;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.LocatedFileStatus;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.fs.RemoteIterator;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.sql.Column;\n-import org.apache.spark.sql.DataFrameWriter;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Nested;\n-import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.NullSource;\n import org.junit.jupiter.params.provider.ValueSource;\n \n-import java.io.IOException;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n-\n-public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n-\n-  static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n-  static final int NUM_RECORDS = 100;\n-  static final String COMMIT_TIME = \"20200101000000\";\n-  static final String PARTITION_PATH = \"2020\";\n-  static final String TABLE_NAME = \"testing\";\n-  String sourcePath;\n-  String targetPath;\n \n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    initSparkContexts();\n-    initDFS();\n-    dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+public class TestHoodieSnapshotExporter {\n \n-    // Initialize test data dirs\n-    sourcePath = dfsBasePath + \"/source/\";\n-    targetPath = dfsBasePath + \"/target/\";\n-    dfs.mkdirs(new Path(sourcePath));\n-    HoodieTableMetaClient\n-        .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n-            HoodieAvroPayload.class.getName());\n-\n-    // Prepare data as source Hudi dataset\n-    HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n-    HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n-    hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n-    List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n-    JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n-    hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n-    hdfsWriteClient.close();\n-\n-    RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n-    while (itr.hasNext()) {\n-      LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n-    }\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n+  public void testValidateOutputFormatWithValidFormat(String format) {\n+    assertDoesNotThrow(() -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n \n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupSparkContexts();\n-    cleanupDFS();\n-    cleanupTestDataGenerator();\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"\", \"JSON\"})\n+  public void testValidateOutputFormatWithInvalidFormat(String format) {\n+    assertThrows(ParameterException.class, () -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n \n-  private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n-    return HoodieWriteConfig.newBuilder()\n-        .withPath(basePath)\n-        .withEmbeddedTimelineServerEnabled(false)\n-        .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n-        .withParallelism(2, 2)\n-        .withBulkInsertParallelism(2)\n-        .forTable(TABLE_NAME)\n-        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n-        .build();\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForHudi {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = OutputFormatValidator.HUDI;\n-    }\n-\n-    @Test\n-    public void testExportAsHudi() throws IOException {\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      // Check results\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.inflight\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".clean.requested\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".commit.requested\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/\" + COMMIT_TIME + \".inflight\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/.hoodie/hoodie.properties\")));\n-      String partition = targetPath + \"/\" + PARTITION_PATH;\n-      long numParquetFiles = Arrays.stream(dfs.listStatus(new Path(partition)))\n-          .filter(fileStatus -> fileStatus.getPath().toString().endsWith(\".parquet\"))\n-          .count();\n-      assertTrue(numParquetFiles >= 1, \"There should exist at least 1 parquet file.\");\n-      assertEquals(NUM_RECORDS, sqlContext.read().parquet(partition).count());\n-      assertTrue(dfs.exists(new Path(partition + \"/.hoodie_partition_metadata\")));\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForEarlyAbort {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = OutputFormatValidator.HUDI;\n-    }\n-\n-    @Test\n-    public void testExportWhenTargetPathExists() throws IOException {\n-      // make target output path present\n-      dfs.mkdirs(new Path(targetPath));\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"The target output path already exists.\", thrown.getMessage());\n-    }\n-\n-    @Test\n-    public void testExportDatasetWithNoCommit() throws IOException {\n-      // delete commit files\n-      List<Path> commitFiles = Arrays.stream(dfs.listStatus(new Path(sourcePath + \"/.hoodie\")))\n-          .map(FileStatus::getPath)\n-          .filter(filePath -> filePath.getName().endsWith(\".commit\"))\n-          .collect(Collectors.toList());\n-      for (Path p : commitFiles) {\n-        dfs.delete(p, false);\n-      }\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"No commits present. Nothing to snapshot.\", thrown.getMessage());\n-    }\n-\n-    @Test\n-    public void testExportDatasetWithNoPartition() throws IOException {\n-      // delete all source data\n-      dfs.delete(new Path(sourcePath + \"/\" + PARTITION_PATH), true);\n-\n-      // export\n-      final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {\n-        new HoodieSnapshotExporter().export(jsc, cfg);\n-      });\n-      assertEquals(\"The source dataset has 0 partition to snapshot.\", thrown.getMessage());\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForNonHudi {\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"json\", \"parquet\"})\n-    public void testExportAsNonHudi(String format) throws IOException {\n-      HoodieSnapshotExporter.Config cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = format;\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(format).load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-    }\n-  }\n-\n-  public static class UserDefinedPartitioner implements Partitioner {\n-\n-    public static final String PARTITION_NAME = \"year\";\n-\n-    @Override\n-    public DataFrameWriter<Row> partition(Dataset<Row> source) {\n-      return source\n-          .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n-          .repartition(new Column(PARTITION_NAME))\n-          .write()\n-          .partitionBy(PARTITION_NAME);\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterForRepartitioning {\n-\n-    private HoodieSnapshotExporter.Config cfg;\n-\n-    @BeforeEach\n-    public void setUp() throws Exception {\n-      cfg = new Config();\n-      cfg.sourceBasePath = sourcePath;\n-      cfg.targetOutputPath = targetPath;\n-      cfg.outputFormat = \"json\";\n-    }\n-\n-    @Test\n-    public void testExportWithPartitionField() throws IOException {\n-      // `driver` field is set in HoodieTestDataGenerator\n-      cfg.outputPartitionField = \"driver\";\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.listStatus(new Path(targetPath)).length > 1);\n-    }\n-\n-    @Test\n-    public void testExportForUserDefinedPartitioner() throws IOException {\n-      cfg.outputPartitioner = UserDefinedPartitioner.class.getName();\n-      new HoodieSnapshotExporter().export(jsc, cfg);\n-\n-      assertEquals(NUM_RECORDS, sqlContext.read().format(\"json\").load(targetPath).count());\n-      assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));\n-      assertTrue(dfs.exists(new Path(String.format(\"%s/%s=%s\", targetPath, UserDefinedPartitioner.PARTITION_NAME, PARTITION_PATH))));\n-    }\n-  }\n-\n-  @Nested\n-  public class TestHoodieSnapshotExporterInputValidation {\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"json\", \"parquet\", \"hudi\"})\n-    public void testValidateOutputFormat_withValidFormat(String format) {\n-      assertDoesNotThrow(() -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n-\n-    @ParameterizedTest\n-    @ValueSource(strings = {\"\", \"JSON\"})\n-    public void testValidateOutputFormat_withInvalidFormat(String format) {\n-      assertThrows(ParameterException.class, () -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n-\n-    @ParameterizedTest\n-    @NullSource\n-    public void testValidateOutputFormat_withNullFormat(String format) {\n-      assertThrows(ParameterException.class, () -> {\n-        new OutputFormatValidator().validate(null, format);\n-      });\n-    }\n+  @ParameterizedTest\n+  @NullSource\n+  public void testValidateOutputFormatWithNullFormat(String format) {\n+    assertThrows(ParameterException.class, () -> {\n+      new OutputFormatValidator().validate(null, format);\n+    });\n   }\n }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "06dae30297ea02ab122c9029a54f7927e8212039", "committedDate": "2020-04-28 23:38:16 +0800", "message": "[HUDI-810] Migrate ClientTestHarness to JUnit 5 (#1553)"}, {"oid": "506447fd4fde4cd922f7aa8f4e17a7f06666dc97", "committedDate": "2020-05-01 21:37:21 -0700", "message": "[HUDI-850] Avoid unnecessary listings in incremental cleaning mode (#1576)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "3b9a30528bd6a6369181702303f3384162b04a7f", "committedDate": "2020-07-05 16:44:31 -0700", "message": "[HUDI-996] Add functional test suite for hudi-utilities (#1746)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}]}, {"oid": "e917358edc3c65252a2783b761c24a74b7aa04f3", "url": "https://github.com/apache/hudi/commit/e917358edc3c65252a2783b761c24a74b7aa04f3", "message": "code optimize", "committedDate": "2020-03-02T09:14:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786763", "body": "you need `startswith()` to be safe. There could be a column named \"total_hoodie_counts\"", "bodyText": "you need startswith() to be safe. There could be a column named \"total_hoodie_counts\"", "bodyHTML": "<p dir=\"auto\">you need <code>startswith()</code> to be safe. There could be a column named \"total_hoodie_counts\"</p>", "author": "xushiyan", "createdAt": "2020-03-03T03:55:53Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4ODA2MQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386788061", "bodyText": "This is optional but actually i'd even prefer having an immutable list of _hoodie_* reserved field names so we can be explicit on what we are removing while exporting.", "author": "xushiyan", "createdAt": "2020-03-03T04:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a..230033c9e5 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +126,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +117,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..c39daa7d5e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -106,45 +136,44 @@ public class HoodieSnapshotExporter {\n         dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      try {\n-        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n-      } catch (Exception e) {\n-        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n-        return -1;\n-      }\n-      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n-        // Do transformation\n-        // A field to do simple Spark repartitioning\n-        DataFrameWriter<Row> write = null;\n-        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n-        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n-        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write().partitionBy(cfg.outputPartitionField);\n-        } else {\n-          write = reader.write();\n-        }\n-        write.format(cfg.outputFormat)\n-            .mode(SaveMode.Overwrite)\n-            .save(cfg.targetOutputPath);\n+      if (!cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n+        exportAsNonHudi(spark, cfg, dataFiles);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n         copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n       }\n+      createSuccessTag(fs, cfg.targetOutputPath);\n     } else {\n       LOG.info(\"The job has 0 partition to copy.\");\n     }\n-    return 0;\n+  }\n+\n+  private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFiles) {\n+    Partitioner defaultPartitioner = dataset -> {\n+      Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n+      return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n+          ? hoodieDroppedDataset.write()\n+          : hoodieDroppedDataset.repartition(new Column(cfg.outputPartitionField)).write().partitionBy(cfg.outputPartitionField);\n+    };\n+\n+    Partitioner partitioner = StringUtils.isNullOrEmpty(cfg.outputPartitioner)\n+        ? defaultPartitioner\n+        : ReflectionUtils.loadClass(cfg.outputPartitioner);\n+\n+    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    partitioner.partition(sourceDataset)\n+        .format(cfg.outputFormat)\n+        .mode(SaveMode.Overwrite)\n+        .save(cfg.targetOutputPath);\n   }\n \n   private void copySnapshot(JavaSparkContext jsc,\n-                            FileSystem fs,\n-                            Config cfg,\n-                            List<String> partitions,\n-                            List<String> dataFiles,\n-                            String latestCommitTimestamp,\n-                            SerializableConfiguration serConf) throws IOException {\n+      FileSystem fs,\n+      Config cfg,\n+      List<String> partitions,\n+      List<String> dataFiles,\n+      String latestCommitTimestamp,\n+      SerializableConfiguration serConf) throws IOException {\n     // Make sure the output directory is empty\n     Path outputPath = new Path(cfg.targetOutputPath);\n     if (fs.exists(outputPath)) {\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c39daa7d5e..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -160,37 +174,35 @@ public class HoodieSnapshotExporter {\n         ? defaultPartitioner\n         : ReflectionUtils.loadClass(cfg.outputPartitioner);\n \n-    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    Iterator<String> exportingFilePaths = jsc\n+        .parallelize(partitions, partitions.size())\n+        .flatMap(partition -> fsView\n+            .getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp)\n+            .map(HoodieBaseFile::getPath).iterator())\n+        .toLocalIterator();\n+\n+    Dataset<Row> sourceDataset = new SQLContext(jsc).read().parquet(JavaConversions.asScalaIterator(exportingFilePaths).toSeq());\n     partitioner.partition(sourceDataset)\n         .format(cfg.outputFormat)\n         .mode(SaveMode.Overwrite)\n         .save(cfg.targetOutputPath);\n   }\n \n-  private void copySnapshot(JavaSparkContext jsc,\n-      FileSystem fs,\n-      Config cfg,\n-      List<String> partitions,\n-      List<String> dataFiles,\n-      String latestCommitTimestamp,\n-      SerializableConfiguration serConf) throws IOException {\n-    // Make sure the output directory is empty\n-    Path outputPath = new Path(cfg.targetOutputPath);\n-    if (fs.exists(outputPath)) {\n-      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-      fs.delete(new Path(cfg.targetOutputPath), true);\n-    }\n-\n+  private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) throws IOException {\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n     jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n       // Only take latest version files <= latestCommit.\n-      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n       List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+      Stream<HoodieBaseFile> dataFiles = fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp);\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));\n \n       // also need to copy over partition metadata\n       Path partitionMetaFile =\n           new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-      if (fs1.exists(partitionMetaFile)) {\n+      FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      if (fs.exists(partitionMetaFile)) {\n         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n       }\n \n", "next_change": {"commit": "1f7add92916c37b05be270d9c75a9042134ec506", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7df630a11e..cf69dd2207 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,9 +213,10 @@ public class HoodieSnapshotExporter {\n       if (fs.exists(partitionMetaFile)) {\n         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n       }\n+      return filePaths.stream();\n+    }, partitions.size());\n \n-      return filePaths.iterator();\n-    }).foreach(tuple -> {\n+    context.foreach(files, tuple -> {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n", "next_change": {"commit": "bd9cceccb582ede88b989824241498e8c32d4f13", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex cf69dd2207..c69d0044ed 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -219,7 +219,7 @@ public class HoodieSnapshotExporter {\n     context.foreach(files, tuple -> {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n-      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      Path toPartitionPath = FSUtils.getPartitionPath(cfg.targetOutputPath, partition);\n       FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n       if (!fs.exists(toPartitionPath)) {\n", "next_change": {"commit": "be9b4195ea580b5f934af99be86d167e77749cf5", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c69d0044ed..187f66d073 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -220,20 +223,26 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = FSUtils.getPartitionPath(cfg.targetOutputPath, partition);\n-      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem executorSourceFs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      FileSystem executorOutputFs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!fs.exists(toPartitionPath)) {\n-        fs.mkdirs(toPartitionPath);\n+      if (!executorOutputFs.exists(toPartitionPath)) {\n+        executorOutputFs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          fs.getConf());\n+      FileUtil.copy(\n+          executorSourceFs,\n+          sourceFilePath,\n+          executorOutputFs,\n+          new Path(toPartitionPath, sourceFilePath.getName()),\n+          false,\n+          executorOutputFs.getConf());\n     }, files.size());\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem outputFs = FSUtils.getFs(cfg.targetOutputPath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n", "next_change": {"commit": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 187f66d073..ca0a495e31 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -235,14 +233,14 @@ public class HoodieSnapshotExporter {\n           executorOutputFs,\n           new Path(toPartitionPath, sourceFilePath.getName()),\n           false,\n+          false,\n           executorOutputFs.getConf());\n-    }, files.size());\n+    }, parallelism);\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-    FileSystem outputFs = FSUtils.getFs(cfg.targetOutputPath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), commitFilePath -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n", "next_change": null}, {"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 187f66d073..ca0a495e31 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -251,18 +249,24 @@ public class HoodieSnapshotExporter {\n             );\n           }\n         });\n-    for (FileStatus commitStatus : commitFilesToCopy) {\n+    context.foreach(Arrays.asList(commitFilesToCopy), commitFile -> {\n       Path targetFilePath =\n-          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!outputFs.exists(targetFilePath.getParent())) {\n-        outputFs.mkdirs(targetFilePath.getParent());\n-      }\n-      if (outputFs.exists(targetFilePath)) {\n-        LOG.error(\n-            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitFile.getPath().getName());\n+      FileSystem executorSourceFs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      FileSystem executorOutputFs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+\n+      if (!executorOutputFs.exists(targetFilePath.getParent())) {\n+        executorOutputFs.mkdirs(targetFilePath.getParent());\n       }\n-      FileUtil.copy(sourceFs, commitStatus.getPath(), outputFs, targetFilePath, false, outputFs.getConf());\n-    }\n+      FileUtil.copy(\n+          executorSourceFs,\n+          commitFile.getPath(),\n+          executorOutputFs,\n+          targetFilePath,\n+          false,\n+          false,\n+          executorOutputFs.getConf());\n+    }, parallelism);\n   }\n \n   private BaseFileOnlyView getBaseFileOnlyView(FileSystem sourceFs, Config cfg) {\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njc4Njc4Nw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r386786787", "body": "i think you need both `repartition()` and `partitionBy()`", "bodyText": "i think you need both repartition() and partitionBy()", "bodyHTML": "<p dir=\"auto\">i think you need both <code>repartition()</code> and <code>partitionBy()</code></p>", "author": "xushiyan", "createdAt": "2020-03-03T03:56:01Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+    LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n+        latestCommitTimestamp));\n+\n+    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+    if (partitions.size() > 0) {\n+      List<String> dataFiles = new ArrayList<>();\n+\n+      if (!StringUtils.isNullOrEmpty(cfg.snapshotPrefix)) {\n+        for (String partition : partitions) {\n+          if (partition.contains(cfg.snapshotPrefix)) {\n+            dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+          }\n+        }\n+      } else {\n+        for (String partition : partitions) {\n+          dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n+        }\n+      }\n+      try {\n+        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n+        return -1;\n+      }\n+      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n+        // Do transformation\n+        // A field to do simple Spark repartitioning\n+        DataFrameWriter<Row> write = null;\n+        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n+        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n+          write = reader.repartition(new Column(cfg.outputPartitionField))\n+              .write();\n+        } else {\n+          write = reader.write();\n+        }\n+        write.format(cfg.outputFormat)\n+            .mode(SaveMode.Overwrite)\n+            .save(cfg.targetOutputPath);", "originalCommit": "e917358edc3c65252a2783b761c24a74b7aa04f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76133ce9788df7bc57406066811fb5e14d40a17c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a..230033c9e5 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +126,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 0675765c8a..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -126,11 +117,11 @@ public class HoodieSnapshotExporter {\n         // A field to do simple Spark repartitioning\n         DataFrameWriter<Row> write = null;\n         Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.contains(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n+        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n         Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n         if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n           write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write();\n+              .write().partitionBy(cfg.outputPartitionField);\n         } else {\n           write = reader.write();\n         }\n", "next_change": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..c39daa7d5e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -106,45 +136,44 @@ public class HoodieSnapshotExporter {\n         dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n       }\n \n-      try {\n-        DataSource.lookupDataSource(cfg.outputFormat, spark.sessionState().conf());\n-      } catch (Exception e) {\n-        LOG.error(String.format(\"The %s output format is not supported! \", cfg.outputFormat));\n-        return -1;\n-      }\n-      if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {\n-        // Do transformation\n-        // A field to do simple Spark repartitioning\n-        DataFrameWriter<Row> write = null;\n-        Dataset<Row> original = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n-        List<Column> needColumns = Arrays.asList(original.columns()).stream().filter(col -> !col.startsWith(\"_hoodie_\")).map(col -> new Column(col)).collect(Collectors.toList());\n-        Dataset<Row> reader = original.select(JavaConversions.asScalaIterator(needColumns.iterator()).toSeq());\n-        if (!StringUtils.isNullOrEmpty(cfg.outputPartitionField)) {\n-          write = reader.repartition(new Column(cfg.outputPartitionField))\n-              .write().partitionBy(cfg.outputPartitionField);\n-        } else {\n-          write = reader.write();\n-        }\n-        write.format(cfg.outputFormat)\n-            .mode(SaveMode.Overwrite)\n-            .save(cfg.targetOutputPath);\n+      if (!cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n+        exportAsNonHudi(spark, cfg, dataFiles);\n       } else {\n         // No transformation is needed for output format \"HUDI\", just copy the original files.\n         copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n       }\n+      createSuccessTag(fs, cfg.targetOutputPath);\n     } else {\n       LOG.info(\"The job has 0 partition to copy.\");\n     }\n-    return 0;\n+  }\n+\n+  private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFiles) {\n+    Partitioner defaultPartitioner = dataset -> {\n+      Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n+      return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n+          ? hoodieDroppedDataset.write()\n+          : hoodieDroppedDataset.repartition(new Column(cfg.outputPartitionField)).write().partitionBy(cfg.outputPartitionField);\n+    };\n+\n+    Partitioner partitioner = StringUtils.isNullOrEmpty(cfg.outputPartitioner)\n+        ? defaultPartitioner\n+        : ReflectionUtils.loadClass(cfg.outputPartitioner);\n+\n+    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    partitioner.partition(sourceDataset)\n+        .format(cfg.outputFormat)\n+        .mode(SaveMode.Overwrite)\n+        .save(cfg.targetOutputPath);\n   }\n \n   private void copySnapshot(JavaSparkContext jsc,\n-                            FileSystem fs,\n-                            Config cfg,\n-                            List<String> partitions,\n-                            List<String> dataFiles,\n-                            String latestCommitTimestamp,\n-                            SerializableConfiguration serConf) throws IOException {\n+      FileSystem fs,\n+      Config cfg,\n+      List<String> partitions,\n+      List<String> dataFiles,\n+      String latestCommitTimestamp,\n+      SerializableConfiguration serConf) throws IOException {\n     // Make sure the output directory is empty\n     Path outputPath = new Path(cfg.targetOutputPath);\n     if (fs.exists(outputPath)) {\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c39daa7d5e..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -160,37 +174,35 @@ public class HoodieSnapshotExporter {\n         ? defaultPartitioner\n         : ReflectionUtils.loadClass(cfg.outputPartitioner);\n \n-    Dataset<Row> sourceDataset = spark.read().parquet(JavaConversions.asScalaIterator(dataFiles.iterator()).toSeq());\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    Iterator<String> exportingFilePaths = jsc\n+        .parallelize(partitions, partitions.size())\n+        .flatMap(partition -> fsView\n+            .getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp)\n+            .map(HoodieBaseFile::getPath).iterator())\n+        .toLocalIterator();\n+\n+    Dataset<Row> sourceDataset = new SQLContext(jsc).read().parquet(JavaConversions.asScalaIterator(exportingFilePaths).toSeq());\n     partitioner.partition(sourceDataset)\n         .format(cfg.outputFormat)\n         .mode(SaveMode.Overwrite)\n         .save(cfg.targetOutputPath);\n   }\n \n-  private void copySnapshot(JavaSparkContext jsc,\n-      FileSystem fs,\n-      Config cfg,\n-      List<String> partitions,\n-      List<String> dataFiles,\n-      String latestCommitTimestamp,\n-      SerializableConfiguration serConf) throws IOException {\n-    // Make sure the output directory is empty\n-    Path outputPath = new Path(cfg.targetOutputPath);\n-    if (fs.exists(outputPath)) {\n-      LOG.warn(String.format(\"The output path %s targetBasePath already exists, deleting\", outputPath));\n-      fs.delete(new Path(cfg.targetOutputPath), true);\n-    }\n-\n+  private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) throws IOException {\n+    final BaseFileOnlyView fsView = getBaseFileOnlyView(jsc, cfg);\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n     jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {\n       // Only take latest version files <= latestCommit.\n-      FileSystem fs1 = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n       List<Tuple2<String, String>> filePaths = new ArrayList<>();\n-      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile)));\n+      Stream<HoodieBaseFile> dataFiles = fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp);\n+      dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));\n \n       // also need to copy over partition metadata\n       Path partitionMetaFile =\n           new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);\n-      if (fs1.exists(partitionMetaFile)) {\n+      FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      if (fs.exists(partitionMetaFile)) {\n         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n       }\n \n", "next_change": {"commit": "1f7add92916c37b05be270d9c75a9042134ec506", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7df630a11e..cf69dd2207 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -205,9 +213,10 @@ public class HoodieSnapshotExporter {\n       if (fs.exists(partitionMetaFile)) {\n         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));\n       }\n+      return filePaths.stream();\n+    }, partitions.size());\n \n-      return filePaths.iterator();\n-    }).foreach(tuple -> {\n+    context.foreach(files, tuple -> {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n", "next_change": {"commit": "bd9cceccb582ede88b989824241498e8c32d4f13", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex cf69dd2207..c69d0044ed 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -219,7 +219,7 @@ public class HoodieSnapshotExporter {\n     context.foreach(files, tuple -> {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n-      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);\n+      Path toPartitionPath = FSUtils.getPartitionPath(cfg.targetOutputPath, partition);\n       FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n       if (!fs.exists(toPartitionPath)) {\n", "next_change": {"commit": "be9b4195ea580b5f934af99be86d167e77749cf5", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c69d0044ed..187f66d073 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -220,20 +223,26 @@ public class HoodieSnapshotExporter {\n       String partition = tuple._1();\n       Path sourceFilePath = new Path(tuple._2());\n       Path toPartitionPath = FSUtils.getPartitionPath(cfg.targetOutputPath, partition);\n-      FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+      FileSystem executorSourceFs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      FileSystem executorOutputFs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n \n-      if (!fs.exists(toPartitionPath)) {\n-        fs.mkdirs(toPartitionPath);\n+      if (!executorOutputFs.exists(toPartitionPath)) {\n+        executorOutputFs.mkdirs(toPartitionPath);\n       }\n-      FileUtil.copy(fs, sourceFilePath, fs, new Path(toPartitionPath, sourceFilePath.getName()), false,\n-          fs.getConf());\n+      FileUtil.copy(\n+          executorSourceFs,\n+          sourceFilePath,\n+          executorOutputFs,\n+          new Path(toPartitionPath, sourceFilePath.getName()),\n+          false,\n+          executorOutputFs.getConf());\n     }, files.size());\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-    final FileSystem fileSystem = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+    FileSystem outputFs = FSUtils.getFs(cfg.targetOutputPath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        fileSystem.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n", "next_change": {"commit": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 187f66d073..ca0a495e31 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -235,14 +233,14 @@ public class HoodieSnapshotExporter {\n           executorOutputFs,\n           new Path(toPartitionPath, sourceFilePath.getName()),\n           false,\n+          false,\n           executorOutputFs.getConf());\n-    }, files.size());\n+    }, parallelism);\n \n     // Also copy the .commit files\n     LOG.info(String.format(\"Copying .commit files which are no-late-than %s.\", latestCommitTimestamp));\n-    FileSystem outputFs = FSUtils.getFs(cfg.targetOutputPath, jsc.hadoopConfiguration());\n     FileStatus[] commitFilesToCopy =\n-        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), (commitFilePath) -> {\n+        sourceFs.listStatus(new Path(cfg.sourceBasePath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME), commitFilePath -> {\n           if (commitFilePath.getName().equals(HoodieTableConfig.HOODIE_PROPERTIES_FILE)) {\n             return true;\n           } else {\n", "next_change": null}, {"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 187f66d073..ca0a495e31 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -251,18 +249,24 @@ public class HoodieSnapshotExporter {\n             );\n           }\n         });\n-    for (FileStatus commitStatus : commitFilesToCopy) {\n+    context.foreach(Arrays.asList(commitFilesToCopy), commitFile -> {\n       Path targetFilePath =\n-          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitStatus.getPath().getName());\n-      if (!outputFs.exists(targetFilePath.getParent())) {\n-        outputFs.mkdirs(targetFilePath.getParent());\n-      }\n-      if (outputFs.exists(targetFilePath)) {\n-        LOG.error(\n-            String.format(\"The target output commit file (%s targetBasePath) already exists.\", targetFilePath));\n+          new Path(cfg.targetOutputPath + \"/\" + HoodieTableMetaClient.METAFOLDER_NAME + \"/\" + commitFile.getPath().getName());\n+      FileSystem executorSourceFs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());\n+      FileSystem executorOutputFs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());\n+\n+      if (!executorOutputFs.exists(targetFilePath.getParent())) {\n+        executorOutputFs.mkdirs(targetFilePath.getParent());\n       }\n-      FileUtil.copy(sourceFs, commitStatus.getPath(), outputFs, targetFilePath, false, outputFs.getConf());\n-    }\n+      FileUtil.copy(\n+          executorSourceFs,\n+          commitFile.getPath(),\n+          executorOutputFs,\n+          targetFilePath,\n+          false,\n+          false,\n+          executorOutputFs.getConf());\n+    }, parallelism);\n   }\n \n   private BaseFileOnlyView getBaseFileOnlyView(FileSystem sourceFs, Config cfg) {\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"oid": "76133ce9788df7bc57406066811fb5e14d40a17c", "url": "https://github.com/apache/hudi/commit/76133ce9788df7bc57406066811fb5e14d40a17c", "message": "code optimize", "committedDate": "2020-03-05T14:13:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328760", "body": "the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to `Snapshot prefix or directory under the source Hudi dataset to be exported`? cc @OpenOpened @xushiyan ", "bodyText": "the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to Snapshot prefix or directory under the source Hudi dataset to be exported? cc @OpenOpened @xushiyan", "bodyHTML": "<p dir=\"auto\">the description here, I feel a little bit wierd after reading the code below, it means only export source Hudi dataset which contains the snapshotPrefix? would be changed to <code>Snapshot prefix or directory under the source Hudi dataset to be exported</code>? cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/OpenOpened/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/OpenOpened\">@OpenOpened</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/xushiyan/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xushiyan\">@xushiyan</a></p>", "author": "leesf", "createdAt": "2020-03-08T02:27:03Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTU4Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365583", "bodyText": "@leesf thanks for catching this. I missed this one: this param is meant to segregate output not input. It was meant to be used in case for multiple exports with the same target path but wanted to be separated from each other (e.g., due to different export date). It actually overlaps with the target base path; users can simply change the target base paths to achieve this. So in conclusion we can just remove this param. @OpenOpened sounds good?", "author": "xushiyan", "createdAt": "2020-03-08T12:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0NDkxMg==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389444912", "bodyText": "I don't think we can delete this parameter. We rely on the metadata file .hoodie metadata in the root directory of the datasource to find things like commitime, valid parquet files, etc. If you point directly to the folder that needs to be exported, such as ROOT/2015/03/16 for the test case, an exception will be thrown Hoodie table not found in path / tmp / junit4184871464195097137 / 2015/03/16 / .hoodie.  I agree with @leesf, modify the comment of --snapshot-perfix.", "author": "OpenOpened", "createdAt": "2020-03-09T03:02:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ0OTk3Mw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389449973", "bodyText": "I see there're some gaps...this param snapshot-prefix is meant to let users export to a specific output directory. For example, --target-base-path=/mytable/ --snapshot-prefix=2020/03/03 and the output data will reside in /mytable/2020/03/03/. After removing --snapshot-prefix, the users will simply set --target-base-path=/mytable/2020/03/03/. This is a redundant param that should be removed from the RFC document too.\nIt is not meant to use on --source-base-path. Users will give the right base path to the hudi dataset to export like --source-base-path=/myhuditable/", "author": "xushiyan", "createdAt": "2020-03-09T03:32:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ1NTQ5OQ==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389455499", "bodyText": "Talked with @xushiyan  offline, the RFC9 means we only export whole hudi dataset, thus may not support exporting ROOT/2015/03/16, thus we could remove --snapshot-prefix and modify the code accordingly. If we need export specified partitions later, we could bring it back and may need more consideration(maybe not use contains, maybe equals or regular expression). WDYT @OpenOpened", "author": "leesf", "createdAt": "2020-03-09T04:08:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODc2MA=="}], "type": "inlineReview", "revised_code": {"commit": "8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 230033c9e5..7d27b7b410 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -71,7 +73,7 @@ public class HoodieSnapshotExporter {\n     @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n     String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the source Hudi dataset to be exported\")\n     String snapshotPrefix;\n \n     @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n", "next_change": {"commit": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7d27b7b410..37a15aae64 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -73,9 +73,6 @@ public class HoodieSnapshotExporter {\n     @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n     String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the source Hudi dataset to be exported\")\n-    String snapshotPrefix;\n-\n     @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 230033c9e5..f785d74304 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -71,9 +72,6 @@ public class HoodieSnapshotExporter {\n     @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n     String targetOutputPath = null;\n \n-    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n-    String snapshotPrefix;\n-\n     @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n     String outputFormat;\n \n", "next_change": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex f785d74304..c39daa7d5e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -45,41 +50,66 @@ import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.execution.datasources.DataSource;\n-\n-import scala.Tuple2;\n-import scala.collection.JavaConversions;\n \n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n  *\n  * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n public class HoodieSnapshotExporter {\n+\n+  @FunctionalInterface\n+  public interface Partitioner {\n+\n+    DataFrameWriter<Row> partition(Dataset<Row> source);\n+\n+  }\n+\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n+  public static class OutputFormatValidator implements IValueValidator<String> {\n+\n+    static final String HUDI = \"hudi\";\n+    static final List<String> FORMATS = ImmutableList.of(\"json\", \"parquet\", HUDI);\n+\n+    @Override\n+    public void validate(String name, String value) {\n+      if (value == null || !FORMATS.contains(value)) {\n+        throw new ParameterException(\n+            String.format(\"Invalid output format: value:%s: supported formats:%s\", value, FORMATS));\n+      }\n+    }\n+  }\n+\n   public static class Config implements Serializable {\n+\n     @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n-    String sourceBasePath = null;\n+    String sourceBasePath;\n \n-    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n-    String targetOutputPath = null;\n+    @Parameter(names = {\"--target-output-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath;\n \n-    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    @Parameter(names = {\"--output-format\"}, description = \"Output format for the exported dataset; accept these values: json|parquet|hudi\", required = true,\n+        validateValueWith = OutputFormatValidator.class)\n     String outputFormat;\n \n     @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n-    String outputPartitionField;\n+    String outputPartitionField = null;\n+\n+    @Parameter(names = {\"--output-partitioner\"}, description = \"A class to facilitate custom repartitioning\")\n+    String outputPartitioner = null;\n   }\n \n-  public int export(SparkSession spark, Config cfg) throws IOException {\n+  public void export(SparkSession spark, Config cfg) throws IOException {\n     JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n     FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c39daa7d5e..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -109,46 +113,56 @@ public class HoodieSnapshotExporter {\n     String outputPartitioner = null;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+  public void export(JavaSparkContext jsc, Config cfg) throws IOException {\n     FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n-    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n-    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n-        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n-    // Get the latest commit\n-    Option<HoodieInstant> latestCommit =\n-        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n-    if (!latestCommit.isPresent()) {\n-      LOG.error(\"No commits present. Nothing to snapshot\");\n-      return;\n+    if (outputPathExists(fs, cfg)) {\n+      throw new HoodieSnapshotExporterException(\"The target output path already exists.\");\n     }\n-    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+\n+    final String latestCommitTimestamp = getLatestCommitTimestamp(fs, cfg).<HoodieSnapshotExporterException>orElseThrow(() -> {\n+      throw new HoodieSnapshotExporterException(\"No commits present. Nothing to snapshot.\");\n+    });\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n-    if (partitions.size() > 0) {\n-      List<String> dataFiles = new ArrayList<>();\n-\n-      for (String partition : partitions) {\n-        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-      }\n+    final List<String> partitions = getPartitions(fs, cfg);\n+    if (partitions.isEmpty()) {\n+      throw new HoodieSnapshotExporterException(\"The source dataset has 0 partition to snapshot.\");\n+    }\n+    LOG.info(String.format(\"The job needs to export %d partitions.\", partitions.size()));\n \n-      if (!cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n-        exportAsNonHudi(spark, cfg, dataFiles);\n-      } else {\n-        // No transformation is needed for output format \"HUDI\", just copy the original files.\n-        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n-      }\n-      createSuccessTag(fs, cfg.targetOutputPath);\n+    if (cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n+      exportAsHudi(jsc, cfg, partitions, latestCommitTimestamp);\n     } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      exportAsNonHudi(jsc, cfg, partitions, latestCommitTimestamp);\n+    }\n+    createSuccessTag(fs, cfg);\n+  }\n+\n+  private boolean outputPathExists(FileSystem fs, Config cfg) throws IOException {\n+    return fs.exists(new Path(cfg.targetOutputPath));\n+  }\n+\n+  private Option<String> getLatestCommitTimestamp(FileSystem fs, Config cfg) {\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    Option<HoodieInstant> latestCommit = tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    return latestCommit.isPresent() ? Option.of(latestCommit.get().getTimestamp()) : Option.empty();\n+  }\n+\n+  private List<String> getPartitions(FileSystem fs, Config cfg) throws IOException {\n+    return FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+  }\n+\n+  private void createSuccessTag(FileSystem fs, Config cfg) throws IOException {\n+    Path successTagPath = new Path(cfg.targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", cfg.targetOutputPath));\n+      fs.createNewFile(successTagPath);\n     }\n   }\n \n-  private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFiles) {\n+  private void exportAsNonHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) {\n     Partitioner defaultPartitioner = dataset -> {\n       Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n       return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n", "next_change": {"commit": "be9b4195ea580b5f934af99be86d167e77749cf5", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7df630a11e..187f66d073 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -162,7 +166,8 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n-  private void exportAsNonHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) {\n+  private void exportAsNonHudi(JavaSparkContext jsc, FileSystem sourceFs,\n+                               Config cfg, List<String> partitions, String latestCommitTimestamp) {\n     Partitioner defaultPartitioner = dataset -> {\n       Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n       return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389328898", "body": "one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc @xushiyan @OpenOpened ", "bodyText": "one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc @xushiyan @OpenOpened", "bodyHTML": "<p dir=\"auto\">one minor question, would we support config commit time ? it is optional, and get lastest commit by default or get the specified commit time. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/xushiyan/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xushiyan\">@xushiyan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/OpenOpened/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/OpenOpened\">@OpenOpened</a></p>", "author": "leesf", "createdAt": "2020-03-08T02:30:26Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.TableFileSystemView;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.execution.datasources.DataSource;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n+ */\n+\n+public class HoodieSnapshotExporter {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--source-base-path\"}, description = \"Base path for the source Hudi dataset to be snapshotted\", required = true)\n+    String sourceBasePath = null;\n+\n+    @Parameter(names = {\"--target-base-path\"}, description = \"Base path for the target output files (snapshots)\", required = true)\n+    String targetOutputPath = null;\n+\n+    @Parameter(names = {\"--snapshot-prefix\"}, description = \"Snapshot prefix or directory under the target base path in order to segregate different snapshots\")\n+    String snapshotPrefix;\n+\n+    @Parameter(names = {\"--output-format\"}, description = \"e.g. Hudi or Parquet\", required = true)\n+    String outputFormat;\n+\n+    @Parameter(names = {\"--output-partition-field\"}, description = \"A field to be used by Spark repartitioning\")\n+    String outputPartitionField;\n+  }\n+\n+  public int export(SparkSession spark, Config cfg) throws IOException {\n+    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+    FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n+\n+    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n+    // Get the latest commit\n+    Option<HoodieInstant> latestCommit =\n+        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    if (!latestCommit.isPresent()) {\n+      LOG.error(\"No commits present. Nothing to snapshot\");\n+      return -1;\n+    }\n+    final String latestCommitTimestamp = latestCommit.get().getTimestamp();", "originalCommit": "76133ce9788df7bc57406066811fb5e14d40a17c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM2NTA3Ng==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389365076", "bodyText": "Per the RFC, we aim to just get the latest commit time.", "author": "xushiyan", "createdAt": "2020-03-08T12:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTMyODg5OA=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": {"commit": "779edc068865898049569da0fe750574f93a0dca", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 230033c9e5..c39daa7d5e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -94,7 +122,7 @@ public class HoodieSnapshotExporter {\n         tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n     if (!latestCommit.isPresent()) {\n       LOG.error(\"No commits present. Nothing to snapshot\");\n-      return -1;\n+      return;\n     }\n     final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n", "next_change": {"commit": "bc82e2be6cf080ab99092758368e91f509a2004c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex c39daa7d5e..7df630a11e 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -109,46 +113,56 @@ public class HoodieSnapshotExporter {\n     String outputPartitioner = null;\n   }\n \n-  public void export(SparkSession spark, Config cfg) throws IOException {\n-    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n+  public void export(JavaSparkContext jsc, Config cfg) throws IOException {\n     FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());\n \n-    final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());\n-    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n-    final TableFileSystemView.BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,\n-        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants());\n-    // Get the latest commit\n-    Option<HoodieInstant> latestCommit =\n-        tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n-    if (!latestCommit.isPresent()) {\n-      LOG.error(\"No commits present. Nothing to snapshot\");\n-      return;\n+    if (outputPathExists(fs, cfg)) {\n+      throw new HoodieSnapshotExporterException(\"The target output path already exists.\");\n     }\n-    final String latestCommitTimestamp = latestCommit.get().getTimestamp();\n+\n+    final String latestCommitTimestamp = getLatestCommitTimestamp(fs, cfg).<HoodieSnapshotExporterException>orElseThrow(() -> {\n+      throw new HoodieSnapshotExporterException(\"No commits present. Nothing to snapshot.\");\n+    });\n     LOG.info(String.format(\"Starting to snapshot latest version files which are also no-late-than %s.\",\n         latestCommitTimestamp));\n \n-    List<String> partitions = FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n-    if (partitions.size() > 0) {\n-      List<String> dataFiles = new ArrayList<>();\n-\n-      for (String partition : partitions) {\n-        dataFiles.addAll(fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp).map(f -> f.getPath()).collect(Collectors.toList()));\n-      }\n+    final List<String> partitions = getPartitions(fs, cfg);\n+    if (partitions.isEmpty()) {\n+      throw new HoodieSnapshotExporterException(\"The source dataset has 0 partition to snapshot.\");\n+    }\n+    LOG.info(String.format(\"The job needs to export %d partitions.\", partitions.size()));\n \n-      if (!cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n-        exportAsNonHudi(spark, cfg, dataFiles);\n-      } else {\n-        // No transformation is needed for output format \"HUDI\", just copy the original files.\n-        copySnapshot(jsc, fs, cfg, partitions, dataFiles, latestCommitTimestamp, serConf);\n-      }\n-      createSuccessTag(fs, cfg.targetOutputPath);\n+    if (cfg.outputFormat.equals(OutputFormatValidator.HUDI)) {\n+      exportAsHudi(jsc, cfg, partitions, latestCommitTimestamp);\n     } else {\n-      LOG.info(\"The job has 0 partition to copy.\");\n+      exportAsNonHudi(jsc, cfg, partitions, latestCommitTimestamp);\n+    }\n+    createSuccessTag(fs, cfg);\n+  }\n+\n+  private boolean outputPathExists(FileSystem fs, Config cfg) throws IOException {\n+    return fs.exists(new Path(cfg.targetOutputPath));\n+  }\n+\n+  private Option<String> getLatestCommitTimestamp(FileSystem fs, Config cfg) {\n+    final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs.getConf(), cfg.sourceBasePath);\n+    Option<HoodieInstant> latestCommit = tableMetadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+    return latestCommit.isPresent() ? Option.of(latestCommit.get().getTimestamp()) : Option.empty();\n+  }\n+\n+  private List<String> getPartitions(FileSystem fs, Config cfg) throws IOException {\n+    return FSUtils.getAllPartitionPaths(fs, cfg.sourceBasePath, false);\n+  }\n+\n+  private void createSuccessTag(FileSystem fs, Config cfg) throws IOException {\n+    Path successTagPath = new Path(cfg.targetOutputPath + \"/_SUCCESS\");\n+    if (!fs.exists(successTagPath)) {\n+      LOG.info(String.format(\"Creating _SUCCESS under target output path: %s\", cfg.targetOutputPath));\n+      fs.createNewFile(successTagPath);\n     }\n   }\n \n-  private void exportAsNonHudi(SparkSession spark, Config cfg, List<String> dataFiles) {\n+  private void exportAsNonHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) {\n     Partitioner defaultPartitioner = dataset -> {\n       Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n       return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n", "next_change": {"commit": "be9b4195ea580b5f934af99be86d167e77749cf5", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\nindex 7df630a11e..187f66d073 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java\n", "chunk": "@@ -162,7 +166,8 @@ public class HoodieSnapshotExporter {\n     }\n   }\n \n-  private void exportAsNonHudi(JavaSparkContext jsc, Config cfg, List<String> partitions, String latestCommitTimestamp) {\n+  private void exportAsNonHudi(JavaSparkContext jsc, FileSystem sourceFs,\n+                               Config cfg, List<String> partitions, String latestCommitTimestamp) {\n     Partitioner defaultPartitioner = dataset -> {\n       Dataset<Row> hoodieDroppedDataset = dataset.drop(JavaConversions.asScalaIterator(HoodieRecord.HOODIE_META_COLUMNS.iterator()).toSeq());\n       return StringUtils.isNullOrEmpty(cfg.outputPartitionField)\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0", "committedDate": "2020-03-15 20:24:30 +0800", "message": "[HUDI-344] Improve exporter tests (#1404)"}, {"oid": "779edc068865898049569da0fe750574f93a0dca", "committedDate": "2020-03-18 19:24:04 +0800", "message": "[HUDI-344] Add partitioner param to Exporter (#1405)"}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "bc82e2be6cf080ab99092758368e91f509a2004c", "committedDate": "2020-03-25 18:02:24 +0800", "message": "[HUDI-711] Refactor exporter main logic (#1436)"}, {"oid": "8c3001363d80b29733470221c192a72f541381c5", "committedDate": "2020-03-28 03:11:32 -0400", "message": "HUDI-479: Eliminate or Minimize use of Guava if possible (#1159)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "0d4848b68b625a17d05b38864a84a6cc71189bfa", "committedDate": "2020-05-13 15:37:03 -0700", "message": "[HUDI-811] Restructure test packages (#1607)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "43b9c1fa1caf97f6fb2baf68e350615541ea0a0c", "committedDate": "2021-06-23 17:04:25 +0800", "message": "[HUDI-1826] Add ORC support in HoodieSnapshotExporter (#3130)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "be9b4195ea580b5f934af99be86d167e77749cf5", "committedDate": "2022-09-27 12:21:19 -0700", "message": "[HUDI-4913] Fix HoodieSnapshotExporter for writing to a different S3 bucket or FS (#6785)"}, {"oid": "8d2ad715a5485c005aafd39a0ea1a274c858dd0b", "committedDate": "2022-11-22 16:47:11 +0530", "message": "[HUDI-712] Improve exporter file listing and copy perf (#7267)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}]}, {"oid": "8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "url": "https://github.com/apache/hudi/commit/8cd7d0b8e81cc19cdc4d8b3fb00bb5f7de074d73", "message": "code optimize", "committedDate": "2020-03-09T03:08:10Z", "type": "commit"}, {"oid": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "url": "https://github.com/apache/hudi/commit/aa5e7798322ce0701776d8b44b608d601f61f0a7", "message": "remove --snapshot-prefix flag", "committedDate": "2020-03-09T05:58:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ4MjIwMw==", "url": "https://github.com/apache/hudi/pull/1360#discussion_r389482203", "body": "nit: remove extra line? ", "bodyText": "nit: remove extra line?", "bodyHTML": "<p dir=\"auto\">nit: remove extra line?</p>", "author": "vinothchandar", "createdAt": "2020-03-09T06:33:49Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java", "diffHunk": "@@ -52,6 +52,7 @@\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n+", "originalCommit": "aa5e7798322ce0701776d8b44b608d601f61f0a7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "45c0872ce680ad54312cfdeb4c8dd6f70525513c", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 178a9075c3..7d944d3559 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -52,7 +52,6 @@ import scala.Tuple2;\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n-\n public class HoodieSnapshotCopier implements Serializable {\n \n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotCopier.class);\n", "next_change": null}]}, "revised_code_in_main": {"commit": "44700d531a74f24762903df2729577a0d96e4ec0", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 178a9075c3..7d944d3559 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -52,7 +52,6 @@ import scala.Tuple2;\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  */\n-\n public class HoodieSnapshotCopier implements Serializable {\n \n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotCopier.class);\n", "next_change": {"commit": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 7d944d3559..860e0ade71 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -51,6 +54,8 @@ import scala.Tuple2;\n \n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n+ *\n+ * @deprecated Use {@link HoodieSnapshotExporter} instead.\n  */\n public class HoodieSnapshotCopier implements Serializable {\n \n", "next_change": {"commit": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 860e0ade71..1f6cac0aac 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -52,6 +52,8 @@ import java.util.stream.Stream;\n \n import scala.Tuple2;\n \n+import static org.apache.hudi.utilities.UtilHelpers.buildSparkConf;\n+\n /**\n  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.\n  *\n", "next_change": {"commit": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "changed_code": [{"header": "diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\nindex 1f6cac0aac..5822f0c84f 100644\n--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java\n", "chunk": "@@ -61,7 +61,7 @@ import static org.apache.hudi.utilities.UtilHelpers.buildSparkConf;\n  */\n public class HoodieSnapshotCopier implements Serializable {\n \n-  private static final Logger LOG = LogManager.getLogger(HoodieSnapshotCopier.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(HoodieSnapshotCopier.class);\n \n   static class Config implements Serializable {\n \n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "44700d531a74f24762903df2729577a0d96e4ec0", "message": "Merge commit", "committedDate": null}, {"oid": "0241b21f771fd1b7438a103a7b49f913632d4b97", "committedDate": "2020-03-22 18:06:00 -0700", "message": "[HUDI-65] commitTime rename to instantTime (#1431)"}, {"oid": "e057c27603301d8b49e9b50b78a3ffce247b1059", "committedDate": "2020-03-29 10:58:49 -0700", "message": "[HUDI-744] Restructure hudi-common and clean up files under util packages (#1462)"}, {"oid": "fa36082554373dd4dce3e3d3159ab87300a4601d", "committedDate": "2020-03-30 11:46:52 +0800", "message": "[HUDI-746] Reduce build warnings < 10 (#1465)"}, {"oid": "c4b71622b90fc66f20f361d4c083b0a396572b75", "committedDate": "2020-04-30 09:19:39 -0700", "message": "[MINOR] Reorder HoodieTimeline#compareTimestamp arguments for better readability (#1575)"}, {"oid": "6c450957ced051de6231ad047bce22752210b786", "committedDate": "2020-05-26 09:23:34 -0700", "message": "[HUDI-690] Filter out inflight compaction in exporter (#1667)"}, {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7", "committedDate": "2020-07-19 10:29:25 -0700", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI. (#1289)"}, {"oid": "1f7add92916c37b05be270d9c75a9042134ec506", "committedDate": "2020-10-01 14:25:29 -0700", "message": "[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)"}, {"oid": "bd9cceccb582ede88b989824241498e8c32d4f13", "committedDate": "2020-12-10 10:19:19 +0800", "message": "[HUDI-1395] Fix partition path using FSUtils (#2312)"}, {"oid": "4e642268442782cdd7ad753981dd2571388cd189", "committedDate": "2021-01-04 07:59:47 -0800", "message": "[HUDI-1450] Use metadata table for listing in HoodieROTablePathFilter (apache#2326)"}, {"oid": "17df517b812c9a37dd64014f0d5c35a3cfac0c4e", "committedDate": "2021-01-07 11:34:06 -0800", "message": "[HUDI-1510] Move HoodieEngineContext and its dependencies to hudi-common (#2410)"}, {"oid": "7ce3ac778eb475bf23ffa31243dc0843ec7d089a", "committedDate": "2021-01-10 21:19:52 -0800", "message": "[HUDI-1479] Use HoodieEngineContext to parallelize fetching of partiton paths (#2417)"}, {"oid": "5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2", "committedDate": "2021-01-19 21:20:28 -0800", "message": "[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)"}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "57c8113ee1941615a03f0efc2e3d46b634e940eb", "committedDate": "2021-09-09 11:29:04 -0400", "message": "[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "a70355f44571036d7f99b3ca3cb240674bd1cf91", "committedDate": "2023-01-21 09:16:07 -0800", "message": "[HUDI-5579] Fixing Kryo registration to be properly wired into Spark sessions (#7702)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "68da21c92d4ac6538a8c491bc46d7c5f3abdd0c9", "committedDate": "2023-04-25 16:18:41 +0800", "message": "[MINOR] Fix typos in hudi-utilities module (#8470)"}]}, {"oid": "45c0872ce680ad54312cfdeb4c8dd6f70525513c", "url": "https://github.com/apache/hudi/commit/45c0872ce680ad54312cfdeb4c8dd6f70525513c", "message": "remove extra line", "committedDate": "2020-03-09T06:42:11Z", "type": "commit"}]}