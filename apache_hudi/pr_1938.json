{"pr_number": 1938, "pr_title": "[HUDI-920] Support Incremental query for MOR table", "pr_author": "garyli1019", "pr_createdAt": "2020-08-09T00:11:50Z", "pr_url": "https://github.com/apache/hudi/pull/1938", "timeline": [{"oid": "452be51154c5b4302469e9810b5569884d61c238", "url": "https://github.com/apache/hudi/commit/452be51154c5b4302469e9810b5569884d61c238", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2021-01-07T15:37:18Z", "type": "commit"}, {"oid": "452be51154c5b4302469e9810b5569884d61c238", "url": "https://github.com/apache/hudi/commit/452be51154c5b4302469e9810b5569884d61c238", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2021-01-07T15:37:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MTUzMw==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553481533", "body": "can we just have this in `hudi-spark` for now. thats the only module that needs to call this. ", "bodyText": "can we just have this in hudi-spark for now. thats the only module that needs to call this.", "bodyHTML": "<p dir=\"auto\">can we just have this in <code>hudi-spark</code> for now. thats the only module that needs to call this.</p>", "author": "vinothchandar", "createdAt": "2021-01-07T17:41:49Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI5MTc2MQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554291761", "bodyText": "This can be shared by other engines later. If we move this to spark we need to switch to scala code, then move it back later when supporting other engines. We could save some effort if we leave it here?", "author": "garyli1019", "createdAt": "2021-01-09T04:57:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MTUzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MjA3OA==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553482078", "body": "Can we redo this such that it can use the metadata table for obtaining the listing? You can see how this is done in HoodieParquetInputFormat. ", "bodyText": "Can we redo this such that it can use the metadata table for obtaining the listing? You can see how this is done in HoodieParquetInputFormat.", "bodyHTML": "<p dir=\"auto\">Can we redo this such that it can use the metadata table for obtaining the listing? You can see how this is done in HoodieParquetInputFormat.</p>", "author": "vinothchandar", "createdAt": "2021-01-07T17:42:52Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+      Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n+    // Extract files touched by these commits.\n+    // TODO This might need to be done in parallel like listStatus parallelism ?", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI4NzI2Mg==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554287262", "bodyText": "Are you referring to RFC-15 that not being landed yet? The current implementation of HoodieParquetInputFormat is listing all files of affected partitions and then do the filtering later.", "author": "garyli1019", "createdAt": "2021-01-09T04:05:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MjA3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4Mjc0Ng==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553482746", "body": "a little bit more clearer doc? does this method obtain all the file status that were affected by the list of commits to check?", "bodyText": "a little bit more clearer doc? does this method obtain all the file status that were affected by the list of commits to check?", "bodyHTML": "<p dir=\"auto\">a little bit more clearer doc? does this method obtain all the file status that were affected by the list of commits to check?</p>", "author": "vinothchandar", "createdAt": "2021-01-07T17:43:58Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4NDIwMQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553484201", "body": "you can just pick the latest such file now. it will have the latest log size using `getFileSizeInBytes()`. No need to do the addition here. ", "bodyText": "you can just pick the latest such file now. it will have the latest log size using getFileSizeInBytes(). No need to do the addition here.", "bodyHTML": "<p dir=\"auto\">you can just pick the latest such file now. it will have the latest log size using <code>getFileSizeInBytes()</code>. No need to do the addition here.</p>", "author": "vinothchandar", "createdAt": "2021-01-07T17:46:36Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+      Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n+    // Extract files touched by these commits.\n+    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n+    for (HoodieInstant commit: commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n+        if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n+          partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n+        }\n+        for (HoodieWriteStat stat : entry.getValue()) {\n+          String relativeFilePath = stat.getPath();\n+          Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n+          if (fullPath != null) {\n+            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n+              // If filesystem support Append. Update the FileStatus of log file if being appended.", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI5MTUwNQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554291505", "bodyText": "fixed.", "author": "garyli1019", "createdAt": "2021-01-09T04:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4NDIwMQ=="}], "type": "inlineReview"}, {"oid": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "url": "https://github.com/apache/hudi/commit/71299741c0c42a5def873d2d51dfd4e9bb5880cb", "message": "address comments", "committedDate": "2021-01-09T04:59:12Z", "type": "commit"}, {"oid": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "url": "https://github.com/apache/hudi/commit/71299741c0c42a5def873d2d51dfd4e9bb5880cb", "message": "address comments", "committedDate": "2021-01-09T04:59:12Z", "type": "forcePushed"}]}