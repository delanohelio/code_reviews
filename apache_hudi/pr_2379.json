{"pr_number": 2379, "pr_title": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "pr_author": "lw309637554", "pr_createdAt": "2020-12-25T12:35:48Z", "pr_url": "https://github.com/apache/hudi/pull/2379", "timeline": [{"oid": "76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "url": "https://github.com/apache/hudi/commit/76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "message": "[HUDI-1481]  add  structured streaming and delta streamer clustering unit test", "committedDate": "2020-12-22T23:33:46Z", "type": "commit"}, {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-27T10:49:16Z", "type": "commit"}, {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-27T10:49:16Z", "type": "forcePushed"}, {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399", "committedDate": "2020-12-28T06:52:17Z", "type": "commit"}, {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399", "committedDate": "2020-12-28T06:52:17Z", "type": "forcePushed"}, {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-31T05:06:52Z", "type": "commit"}, {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-31T05:06:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyNzgzMA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551627830", "body": "i think its better to create 'instantTime' instead of using user-specified instant time for scheduling. At least, we should validate instant time specified is greater than latest commit on the table. Otherwise, there can be unexpected side-effects.", "bodyText": "i think its better to create 'instantTime' instead of using user-specified instant time for scheduling. At least, we should validate instant time specified is greater than latest commit on the table. Otherwise, there can be unexpected side-effects.", "bodyHTML": "<p dir=\"auto\">i think its better to create 'instantTime' instead of using user-specified instant time for scheduling. At least, we should validate instant time specified is greater than latest commit on the table. Otherwise, there can be unexpected side-effects.</p>", "author": "satishkotha", "createdAt": "2021-01-04T23:25:42Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);\n+    return schema.toString();\n+  }\n+\n+  private int doCluster(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    JavaRDD<WriteStatus> writeResponse =\n+        (JavaRDD<WriteStatus>) client.cluster(cfg.clusteringInstantTime, true).getWriteStatuses();\n+    return UtilHelpers.handleErrors(jsc, cfg.clusteringInstantTime, writeResponse);\n+  }\n+\n+  private int doSchedule(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    return client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty()) ? 0 : -1;", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDM4MA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984380", "bodyText": "@satishkotha  make sense , have use client.scheduleClustering() to auto generate instantTime. Also have some questions for you. Why clustering instant time should greater than latest commit?  I see BaseScheduleCompactionActionExecutor.execute() have check  that should not have earliest write inflight instant time than compaction instant time. But BaseClusteringPlanActionExecutor.execute()  do not have similarity check. Thanks.", "author": "lw309637554", "createdAt": "2021-01-05T14:58:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyNzgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMDkyMA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551630920", "body": "This is probably minor. Looks like 'getTableAvroSchema' throws runtime exception if there are no completed commits. may be add a check to verify commit timeline is not empty before calling this method? This way we can throw meaningful error message: say \"Cannot run clustering without any completed commits\"", "bodyText": "This is probably minor. Looks like 'getTableAvroSchema' throws runtime exception if there are no completed commits. may be add a check to verify commit timeline is not empty before calling this method? This way we can throw meaningful error message: say \"Cannot run clustering without any completed commits\"", "bodyHTML": "<p dir=\"auto\">This is probably minor. Looks like 'getTableAvroSchema' throws runtime exception if there are no completed commits. may be add a check to verify commit timeline is not empty before calling this method? This way we can throw meaningful error message: say \"Cannot run clustering without any completed commits\"</p>", "author": "satishkotha", "createdAt": "2021-01-04T23:35:03Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDUzMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984532", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-05T14:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMDkyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMTEyNw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551631127", "body": "I'd suggest make this required only for executing clustering. For scheduling, we should auto-generate instant time to make it easy to run.", "bodyText": "I'd suggest make this required only for executing clustering. For scheduling, we should auto-generate instant time to make it easy to run.", "bodyHTML": "<p dir=\"auto\">I'd suggest make this required only for executing clustering. For scheduling, we should auto-generate instant time to make it easy to run.</p>", "author": "satishkotha", "createdAt": "2021-01-04T23:35:42Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDcyMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984722", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-05T14:59:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNTMzNQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551635335", "body": "I dont see any asserts related to clustering. Can we add basic validation that new files generated by clustering have same data as previous files? Let me know if I'm misreading.", "bodyText": "I dont see any asserts related to clustering. Can we add basic validation that new files generated by clustering have same data as previous files? Let me know if I'm misreading.", "bodyHTML": "<p dir=\"auto\">I dont see any asserts related to clustering. Can we add basic validation that new files generated by clustering have same data as previous files? Let me know if I'm misreading.</p>", "author": "satishkotha", "createdAt": "2021-01-04T23:44:12Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -682,6 +693,58 @@ public void testInlineClustering() throws Exception {\n     });\n   }\n \n+  private HoodieClusteringJob.Config buildHoodieClusteringUtilConfig(String basePath,\n+                                                                  String clusteringInstantTime, boolean runSchedule) {\n+    HoodieClusteringJob.Config config = new HoodieClusteringJob.Config();\n+    config.basePath = basePath;\n+    config.clusteringInstantTime = clusteringInstantTime;\n+    config.runSchedule = runSchedule;\n+    config.propsFilePath = dfsBasePath + \"/clusteringjob.properties\";\n+    return config;\n+  }\n+\n+  @Test\n+  public void testHoodieAsyncClusteringJob() throws Exception {\n+    String tableBasePath = dfsBasePath + \"/asyncClustering\";\n+    // Keep it higher than batch-size to test continuous mode\n+    int totalRecords = 3000;\n+\n+    // Initial bulk insert\n+    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n+    cfg.continuousMode = true;\n+    cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();\n+    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n+    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    cfg.configs.add(String.format(\"%s=true\", HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n+    deltaStreamerTestRunner(ds, cfg, (r) -> {\n+      TestHelpers.assertAtLeastNCommits(2, tableBasePath, dfs);\n+      // for not confiict with delta streamer commit, just add 3600s\n+      String clusterInstantTime = HoodieActiveTimeline.COMMIT_FORMATTER\n+          .format(new Date(System.currentTimeMillis() + 3600 * 1000));\n+      LOG.info(\"Cluster instant time \" + clusterInstantTime);\n+      HoodieClusteringJob.Config scheduleClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+          clusterInstantTime, true);\n+      HoodieClusteringJob scheduleClusteringJob = new HoodieClusteringJob(jsc, scheduleClusteringConfig);\n+      int scheduleClusteringResult = scheduleClusteringJob.cluster(scheduleClusteringConfig.retry);\n+      if (scheduleClusteringResult == 0) {\n+        LOG.info(\"Schedule clustering success, now cluster\");\n+        HoodieClusteringJob.Config clusterClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+            clusterInstantTime, false);\n+        HoodieClusteringJob clusterClusteringJob = new HoodieClusteringJob(jsc, clusterClusteringConfig);\n+        clusterClusteringJob.cluster(clusterClusteringConfig.retry);\n+        LOG.info(\"Cluster success\");\n+      } else {\n+        LOG.warn(\"Schedule clustering failed\");\n+      }\n+      HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n+      int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\n+      int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\n+      System.out.println(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\n+      return completeReplaceSize > 0;", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NjU2NQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551986565", "bodyText": "Because if always completeReplaceSize <= 0 the runner will throw time out exception.Now i add the assert for completeReplaceSize == 1. As the unit test mainly test async clustering schedule and cluster, just assert completeReplaceSize will be ok. For records check can cover in cluster unit test.", "author": "lw309637554", "createdAt": "2021-01-05T15:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNTMzNQ=="}], "type": "inlineReview"}, {"oid": "152ea1c63a7759168a20f1c50e6441c6246e9619", "url": "https://github.com/apache/hudi/commit/152ea1c63a7759168a20f1c50e6441c6246e9619", "message": "[HUDI-1498] Read clustering plan from requested file for inflight instant (#2389)", "committedDate": "2021-01-05T02:30:19Z", "type": "commit"}, {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-09T13:54:16Z", "type": "commit"}, {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-09T13:54:16Z", "type": "forcePushed"}]}