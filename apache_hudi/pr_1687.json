{"pr_number": 1687, "pr_title": " [HUDI-684] Introduced abstraction for writing and reading different types of base file formats.", "pr_author": "prashantwason", "pr_createdAt": "2020-05-29T21:40:18Z", "pr_url": "https://github.com/apache/hudi/pull/1687", "timeline": [{"oid": "b2a4c2e54b579e6c58ea9320f8b85625b577084e", "url": "https://github.com/apache/hudi/commit/b2a4c2e54b579e6c58ea9320f8b85625b577084e", "message": "[HUDI-684] Fixed merge conflict due to upstream changes.\n\nAdded extra unit tests for HFile Input format.", "committedDate": "2020-06-17T06:50:47Z", "type": "forcePushed"}, {"oid": "75667356003fe23b5fc4df8b37bcfd8aa256c61e", "url": "https://github.com/apache/hudi/commit/75667356003fe23b5fc4df8b37bcfd8aa256c61e", "message": "[HUDI-684] Fixed integration tests.", "committedDate": "2020-06-18T21:13:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTQ0Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261442", "body": "might as well expose getBaseFileExtension() in HoodieTable.", "bodyText": "might as well expose getBaseFileExtension() in HoodieTable.", "bodyHTML": "<p dir=\"auto\">might as well expose getBaseFileExtension() in HoodieTable.</p>", "author": "nsivabalan", "createdAt": "2020-06-21T22:11:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -115,7 +115,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n       String relativePath = new Path((partitionPath.isEmpty() ? \"\" : partitionPath + \"/\")\n-          + FSUtils.makeDataFileName(instantTime, writeToken, fileId)).toString();\n+          + FSUtils.makeDataFileName(instantTime, writeToken, fileId,\n+              hoodieTable.getBaseFileFormat().getFileExtension())).toString();", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261658", "body": "minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime. ", "bodyText": "minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime.", "bodyHTML": "<p dir=\"auto\">minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime.</p>", "author": "nsivabalan", "createdAt": "2020-06-21T22:14:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getStorageReader() throws IOException {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0NTM5MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443845391", "bodyText": "Yes, getNewStorageReader() would be more clear.", "author": "prashantwason", "createdAt": "2020-06-22T21:48:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443262086", "body": "similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.\r\n\r\n```\r\nHoodieFileWriter<IndexedRecord> getFileWriter(String instantTime, Path newFilePath){\r\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\r\n}\r\n```\r\n", "bodyText": "similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.\nHoodieFileWriter<IndexedRecord> getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}", "bodyHTML": "<p dir=\"auto\">similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"HoodieFileWriter&lt;IndexedRecord&gt; getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}\n\"><pre><code>HoodieFileWriter&lt;IndexedRecord&gt; getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}\n</code></pre></div>", "author": "nsivabalan", "createdAt": "2020-06-21T22:20:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -132,7 +133,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       // Create the writer for writing the new version file\n       storageWriter =\n-          HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+          HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NDAzMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443264032", "bodyText": "minor: similar to my suggestion in reader, try to see if we need to name this as getNewFileWriter()", "author": "nsivabalan", "createdAt": "2020-06-21T22:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0ODMzNQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443848335", "bodyText": "Sounds good.", "author": "prashantwason", "createdAt": "2020-06-22T21:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443265526", "body": "not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too. \r\n", "bodyText": "not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too.", "bodyHTML": "<p dir=\"auto\">not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too.</p>", "author": "nsivabalan", "createdAt": "2020-06-21T23:09:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -115,7 +115,11 @@ public CommitActionExecutor(JavaSparkContext jsc,\n   }\n \n   protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n-    return new HoodieMergeHandle<>(config, instantTime, (HoodieTable<T>)table, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    if (table.requireSortedRecords()) {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0OTYwMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443849600", "bodyText": "Your understanding it correct.\nI will take care of this in a separate PR with HFile implementation.", "author": "prashantwason", "createdAt": "2020-06-22T21:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg=="}], "type": "inlineReview"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "commit"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "forcePushed"}]}