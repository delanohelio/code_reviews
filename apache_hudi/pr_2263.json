{"pr_number": 2263, "pr_title": "[HUDI-1075] Implement simple clustering strategies to create and run ClusteringPlan", "pr_author": "satishkotha", "pr_createdAt": "2020-11-19T21:30:39Z", "pr_url": "https://github.com/apache/hudi/pull/2263", "timeline": [{"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "url": "https://github.com/apache/hudi/commit/f526c2c0eed8134aa513898ba48ef8ddf851272c", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan", "committedDate": "2020-12-09T06:14:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTMzNA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544859334", "body": "lets please add java docs for all these configs?", "bodyText": "lets please add java docs for all these configs?", "bodyHTML": "<p dir=\"auto\">lets please add java docs for all these configs?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T07:10:56Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String ASYNC_CLUSTERING_ENABLED = \"hoodie.clustering.enabled\";\n+  public static final String DEFAULT_ASYNC_CLUSTERING_ENABLED = \"false\";\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.strategy.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits an inline clustering will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzMwNQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553305", "bodyText": "Added", "author": "satishkotha", "createdAt": "2020-12-18T03:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjM2Nw==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544862367", "body": "make this more readable, by putting each param on a line like before?", "bodyText": "make this more readable, by putting each param on a line like before?", "bodyHTML": "<p dir=\"auto\">make this more readable, by putting each param on a line like before?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T07:18:02Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzM2NA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553364", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-12-18T03:40:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjM2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE0NQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545037145", "body": "should this be called `replaceTimer` as well? ", "bodyText": "should this be called replaceTimer as well?", "bodyHTML": "<p dir=\"auto\">should this be called <code>replaceTimer</code> as well?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:02:13Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java", "diffHunk": "@@ -48,6 +49,7 @@\n   private Timer deltaCommitTimer = null;\n   private Timer finalizeTimer = null;\n   private Timer compactionTimer = null;\n+  private Timer clusteringTimer = null;", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1Mzc4Mg==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553782", "bodyText": "This is specific to clustering operation and only used in WriteClient#cluster.\nWe could have a different timer for insertOverwrite and replace commands if needed. I like this approach because replace can mean multiple things. Let me know if you think common timer makes more sense.", "author": "satishkotha", "createdAt": "2020-12-18T03:42:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDk5MA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545044990", "body": "I think we should have 50 as the default value for this config and allow any value to passed in, as opposed to having this limit hard-coded.", "bodyText": "I think we should have 50 as the default value for this config and allow any value to passed in, as opposed to having this limit hard-coded.", "bodyHTML": "<p dir=\"auto\">I think we should have 50 as the default value for this config and allow any value to passed in, as opposed to having this limit hard-coded.</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:15:51Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzgwNw==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553807", "bodyText": "Fixed.", "author": "satishkotha", "createdAt": "2020-12-18T03:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTk3Ng==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545045976", "body": "why not try to do this in parallel using context.map() etc? it should improve performance as well", "bodyText": "why not try to do this in parallel using context.map() etc? it should improve performance as well", "bodyHTML": "<p dir=\"auto\">why not try to do this in parallel using context.map() etc? it should improve performance as well</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:17:34Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();\n+      if (maxClusteringGroups > MAX_CLUSTERING_GROUPS_STRATEGY) {\n+        LOG.warn(\"Reducing max clustering groups to \" + MAX_CLUSTERING_GROUPS_STRATEGY + \" for performance reasons\");\n+        maxClusteringGroups = MAX_CLUSTERING_GROUPS_STRATEGY;\n+      }\n+\n+      List<HoodieClusteringGroup> clusteringGroups = partitionPaths.stream()", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1Mzg1OQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553859", "bodyText": "Updated.", "author": "satishkotha", "createdAt": "2020-12-18T03:42:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTk3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NDc1OA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545054758", "body": "is this like a group number? Seems more like the number of output files produced by that group", "bodyText": "is this like a group number? Seems more like the number of output files produced by that group", "bodyHTML": "<p dir=\"auto\">is this like a group number? Seems more like the number of output files produced by that group</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:32:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))\n+        .sorted(Comparator.reverseOrder()).map(partitionPath -> partitionPath.replace(\"-\", \"/\"))\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  private int getNumberOfGroups(long groupSize, long targetFileSize) {", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDA0Mw==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554043", "bodyText": "yes, this is calculating number of FileGroups. I updated the name to be explicit", "author": "satishkotha", "createdAt": "2020-12-18T03:43:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NDc1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTA4OA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545055088", "body": "why do we need the /,- replace logic. It should correctly even without that? ", "bodyText": "why do we need the /,- replace logic. It should correctly even without that?", "bodyHTML": "<p dir=\"auto\">why do we need the /,- replace logic. It should correctly even without that?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:33:03Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDM3OA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554378", "bodyText": "Seems to work fine without replacing '/'.  So I removed it. Btw, compaction has this code, so I'm not sure if there are special cases where this replace is needed. It should be easy to add if we run into any issue. So I removed it.", "author": "satishkotha", "createdAt": "2020-12-18T03:44:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTA4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTExMg==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545059112", "body": "should the filtering for 2 happen at this level?  that seems like something a specific plan would do. \r\n1 makes sense to do at this level. ", "bodyText": "should the filtering for 2 happen at this level?  that seems like something a specific plan would do.\n1 makes sense to do at this level.", "bodyHTML": "<p dir=\"auto\">should the filtering for 2 happen at this level?  that seems like something a specific plan would do.<br>\n1 makes sense to do at this level.</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:39:58Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDQ5NA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554494", "bodyText": "Moved 2 to strategy specific implementation.", "author": "satishkotha", "createdAt": "2020-12-18T03:44:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTExMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060146", "body": "this is actually just a `ClusteringPlanStrategy` right? i.e it generates clustering plans. It has less to do with scheduling of clustering itself? ", "bodyText": "this is actually just a ClusteringPlanStrategy right? i.e it generates clustering plans. It has less to do with scheduling of clustering itself?", "bodyHTML": "<p dir=\"auto\">this is actually just a <code>ClusteringPlanStrategy</code> right? i.e it generates clustering plans. It has less to do with scheduling of clustering itself?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:41:44Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDM1Mg==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060352", "bodyText": "Should we rename this class and the configs?", "author": "vinothchandar", "createdAt": "2020-12-17T12:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDg0MQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554841", "bodyText": "I followed compaction naming pattern. But agree schedule has different meaning. I changed it to ClusteringPlanStrategy. WriteClient methods are still scheduleClustering(String instant), let me know if you think this needs to change also.", "author": "satishkotha", "createdAt": "2020-12-18T03:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MTMzMQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545061331", "body": "can we rename `numOutputGroups` to something that can be easily understood in the bulk insert context. If we leak some clustering terminology here, it becomes to harder to read this.", "bodyText": "can we rename numOutputGroups to something that can be easily understood in the bulk insert context. If we leak some clustering terminology here, it becomes to harder to read this.", "bodyHTML": "<p dir=\"auto\">can we rename <code>numOutputGroups</code> to something that can be easily understood in the bulk insert context. If we leak some clustering terminology here, it becomes to harder to read this.</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:43:40Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,21 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, update index and return result.\n+   */\n   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,\n                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,\n                                                     Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner);\n+\n+  /**\n+   * Only write input records. Does not change timeline/index. Return information about new files created.\n+   */\n+  public abstract O bulkInsert(I inputRecords, String instantTime,\n+                               HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n+                               boolean performDedupe,\n+                               Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,\n+                               boolean addMetadataFields,\n+                               int numOutputGroups);", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDk4OQ==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554989", "bodyText": "Looks like I already changed subclasses, but overlooked base class. Thanks for catching this.", "author": "satishkotha", "createdAt": "2020-12-18T03:46:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MTMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MjM0NA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545062344", "body": "are these cleanups strictly needed for this PR? if you have tested them already, its okay. but generally, separating these in a different refactor PR is preferrable. ", "bodyText": "are these cleanups strictly needed for this PR? if you have tested them already, its okay. but generally, separating these in a different refactor PR is preferrable.", "bodyHTML": "<p dir=\"auto\">are these cleanups strictly needed for this PR? if you have tested them already, its okay. but generally, separating these in a different refactor PR is preferrable.</p>", "author": "vinothchandar", "createdAt": "2020-12-17T12:45:20Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java", "diffHunk": "@@ -40,21 +36,6 @@\n public class LogFileSizeBasedCompactionStrategy extends BoundedIOCompactionStrategy\n     implements Comparator<HoodieCompactionOperation> {\n \n-  private static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILE_SIZE\";", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTM3MA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555370", "bodyText": "I refactored it based on other review feedback. There are unit tests for this. So I am reasonably confident theres no errors. I'll keep such changes for another PR next time. Thanks", "author": "satishkotha", "createdAt": "2020-12-18T03:48:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MjM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzMTY3Ng==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545131676", "body": "place each parameter on its own line?", "bodyText": "place each parameter on its own line?", "bodyHTML": "<p dir=\"auto\">place each parameter on its own line?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T14:28:33Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTM5Nw==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555397", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-12-18T03:48:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzMTY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzOTA1MA==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545139050", "body": "this is very metric specific. could we stick it somewhere else closer to actual usage in hudi-client-common? may be rename the class to something like `FileSliceMetricUtils` ?", "bodyText": "this is very metric specific. could we stick it somewhere else closer to actual usage in hudi-client-common? may be rename the class to something like FileSliceMetricUtils ?", "bodyHTML": "<p dir=\"auto\">this is very metric specific. could we stick it somewhere else closer to actual usage in hudi-client-common? may be rename the class to something like <code>FileSliceMetricUtils</code> ?</p>", "author": "vinothchandar", "createdAt": "2020-12-17T14:38:32Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/FileSliceUtils.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * A utility class for numeric.\n+ */\n+public class FileSliceUtils {\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static void addFileSliceCommonMetrics(List<FileSlice> fileSlices, Map<String, Double> metrics, long defaultBaseFileSize) {", "originalCommit": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTUxNw==", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555517", "bodyText": "Moved it as suggested.", "author": "satishkotha", "createdAt": "2020-12-18T03:48:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzOTA1MA=="}], "type": "inlineReview"}, {"oid": "6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "url": "https://github.com/apache/hudi/commit/6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan", "committedDate": "2020-12-22T01:34:15Z", "type": "commit"}, {"oid": "6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "url": "https://github.com/apache/hudi/commit/6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan", "committedDate": "2020-12-22T01:34:15Z", "type": "forcePushed"}]}