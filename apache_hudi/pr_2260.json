{"pr_number": 2260, "pr_title": "[HUDI-1381] Schedule compaction based on time elapsed", "pr_author": "Karl-WangSK", "pr_createdAt": "2020-11-18T08:05:50Z", "pr_url": "https://github.com/apache/hudi/pull/2260", "merge_commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "timeline": [{"oid": "679310a69fdb607d084bae429e03c528b5d60159", "url": "https://github.com/apache/hudi/commit/679310a69fdb607d084bae429e03c528b5d60159", "message": "update", "committedDate": "2020-11-18T08:01:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjI3NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042274", "body": "Missing doc about `initialTime `.", "bodyText": "Missing doc about initialTime .", "bodyHTML": "<p dir=\"auto\">Missing doc about <code>initialTime </code>.</p>", "author": "yanghua", "createdAt": "2020-11-28T13:49:35Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -315,7 +315,8 @@ public HoodieActiveTimeline getActiveTimeline() {\n    */\n   public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,\n                                                                   String instantTime,\n-                                                                  Option<Map<String, String>> extraMetadata);\n+                                                                  Option<Map<String, String>> extraMetadata,\n+                                                                  String initialTime);", "originalCommit": "6dac8043125b788234a2184f423ecc9d21c5757a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b593f1062931a4d017ae8bd7dd42e47a8873a39f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\nindex e5ab4b8940..d56e6e7dd4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n", "chunk": "@@ -315,8 +353,7 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem\n    */\n   public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,\n                                                                   String instantTime,\n-                                                                  Option<Map<String, String>> extraMetadata,\n-                                                                  String initialTime);\n+                                                                  Option<Map<String, String>> extraMetadata);\n \n   /**\n    * Run Compaction on the table. Compaction arranges the data so that it is optimized for data access.\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\nindex e5ab4b8940..74ffdfc653 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n", "chunk": "@@ -315,8 +348,7 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem\n    */\n   public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,\n                                                                   String instantTime,\n-                                                                  Option<Map<String, String>> extraMetadata,\n-                                                                  String initialTime);\n+                                                                  Option<Map<String, String>> extraMetadata);\n \n   /**\n    * Run Compaction on the table. Compaction arranges the data so that it is optimized for data access.\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\nindex 74ffdfc653..135eb8be86 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n", "chunk": "@@ -353,12 +365,11 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem\n   /**\n    * Run Compaction on the table. Compaction arranges the data so that it is optimized for data access.\n    *\n-   * @param context HoodieEngineContext\n+   * @param context               HoodieEngineContext\n    * @param compactionInstantTime Instant Time\n    */\n   public abstract HoodieWriteMetadata<O> compact(HoodieEngineContext context,\n-                                              String compactionInstantTime);\n-\n+                                                 String compactionInstantTime);\n \n   /**\n    * Schedule clustering for the instant time.\n", "next_change": {"commit": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\nindex 135eb8be86..4a6f4ae1f4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n", "chunk": "@@ -371,6 +410,31 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem\n   public abstract HoodieWriteMetadata<O> compact(HoodieEngineContext context,\n                                                  String compactionInstantTime);\n \n+  /**\n+   * Schedule log compaction for the instant time.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling log compaction\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return\n+   */\n+  public Option<HoodieCompactionPlan> scheduleLogCompaction(HoodieEngineContext context,\n+                                                            String instantTime,\n+                                                            Option<Map<String, String>> extraMetadata) {\n+    throw new UnsupportedOperationException(\"Log compaction is not supported for this table type\");\n+  }\n+\n+  /**\n+   * Run Log Compaction on the table. Log Compaction arranges the data so that it is optimized for data access.\n+   *\n+   * @param context               HoodieEngineContext\n+   * @param logCompactionInstantTime Instant Time\n+   */\n+  public HoodieWriteMetadata<O> logCompact(HoodieEngineContext context,\n+                                           String logCompactionInstantTime) {\n+    throw new UnsupportedOperationException(\"Log compaction is not supported for this table type\");\n+  }\n+\n   /**\n    * Schedule clustering for the instant time.\n    *\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "3b36cb805d066a3112e3a355ef502dbe4b2c1824", "committedDate": "2021-03-15 13:42:57 -0700", "message": "[HUDI-1552] Improve performance of key lookups from base file in Metadata Table. (#2494)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "8869b3b4184bbec4502e2e3f6fde0ea9260cf0b0", "committedDate": "2021-05-14 15:43:37 +0800", "message": "[HUDI-1902] Clean the corrupted files generated by FlinkMergeAndReplaceHandle (#2949)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "b8dad628e584e0acfa8ef6ba0056f7cb6efafad0", "committedDate": "2021-09-16 11:16:06 -0400", "message": "[HUDI-2422] Adding rollback plan and rollback requested instant (#3651)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "f14d4e65e7edab58aa86c495e4bd4caa79086d6b", "committedDate": "2021-10-11 13:58:33 -0400", "message": "[HUDI-2540] Fixed wrong validation for metadataTableEnabled in HoodieTable (#3781)"}, {"oid": "cff384d23f464692b792abc4b4a7ca6731ed8067", "committedDate": "2021-10-13 18:44:43 -0400", "message": "[HUDI-2552] Fixing some test failures to unblock broken CI master (#3793)"}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}, {"oid": "c9d641cc30ba5b1e65aa87b2c6d48a3bdd788564", "committedDate": "2021-10-23 10:07:09 -0400", "message": "[HUDI-2468] Metadata table support for rolling back the first commit (#3843)"}, {"oid": "0223c442ec9a746834d1b2f2582c5267b692823a", "committedDate": "2021-10-28 04:16:00 -0400", "message": "[HUDI-2502] Refactor index in hudi-client module (#3778)"}, {"oid": "29574af239ae4596034a17999484ed069ec7123f", "committedDate": "2021-10-29 12:14:39 -0400", "message": "[HUDI-2573] Fixing double locking with multi-writers (#3827)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "6d109c6de5078560abab96f96dc2a9bfe9ed8e57", "committedDate": "2021-11-08 22:12:32 -0500", "message": "[HUDI-2595] Fixing metadata table updates such that only regular writes from data table can trigger table services in metadata table (#3900)"}, {"oid": "04eb5fdc651bce3b34f17c1828879b18f1b1b6f8", "committedDate": "2021-11-17 10:06:55 +0530", "message": "[HUDI-2753] Ensure list based rollback strategy is used for restore (#3983)"}, {"oid": "5755ff25a49aa495e418acde7ef8d246e16b4973", "committedDate": "2021-11-26 10:02:15 -0800", "message": "[HUDI-2814] Addressing issues w/ Z-order Layout Optimization (#4060)"}, {"oid": "2c7656c35faf625184f0356ca0693be6a70206c6", "committedDate": "2021-11-26 23:19:26 -0800", "message": "[HUDI-2475] [HUDI-2862] Metadata table creation and avoid bootstrapping race for write client & add locking for upgrade (#4114)"}, {"oid": "2d864f75247fd2feff6509235c7df84ec8707738", "committedDate": "2021-12-10 14:56:09 -0800", "message": "[HUDI-2814] Make Z-index more generic Column-Stats Index (#4106)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "b8601a9f58ae4dc5099cc7f828df7fd8d0aeb798", "committedDate": "2022-02-03 20:24:04 -0800", "message": "[HUDI-2656] Generalize HoodieIndex for flexible record data type (#3893)"}, {"oid": "773b3179834d6e0d2912a7a9d4b3b3873d0c2a21", "committedDate": "2022-02-07 06:28:13 -0800", "message": "[HUDI-2941] Show _hoodie_operation in spark sql results (#4649)"}, {"oid": "e7ec3a82dc274b8d683d74e59ee7bf35d7827ce0", "committedDate": "2022-02-10 08:06:23 -0500", "message": "[HUDI-2432] Adding restore.requested instant and restore plan for restore action (#4605)"}, {"oid": "c77b2591d057fdc1c014a80284024e19254e3606", "committedDate": "2022-02-26 08:02:12 -0500", "message": "[HUDI-2439] Remove SparkBoundedInMemoryExecutor (#4860)"}, {"oid": "10d866f083248d37923d694165e362a21f3d529d", "committedDate": "2022-03-02 15:14:44 +0800", "message": "[HUDI-3315] RFC-35 Part-1 Support bucket index in Flink writer (#4679)"}, {"oid": "e7bb0413af889c3a595e9fb3ea41145da992d8c9", "committedDate": "2022-03-11 18:40:13 -0500", "message": "[HUDI-3556] Re-use rollback instant for rolling back of clustering and compaction if rollback failed mid-way (#4971)"}, {"oid": "7446ff95a7b099d8016b7ca48fdeed9b290d710f", "committedDate": "2022-03-17 04:17:56 -0700", "message": "[HUDI-2439] Replace RDD with HoodieData in HoodieSparkTable and commit executors (#4856)"}, {"oid": "e5a2baeed0861e332000cb0fe3e3f263e82c15fc", "committedDate": "2022-03-29 14:44:47 -0400", "message": "[HUDI-3549] Removing dependency on \"spark-avro\"  (#4955)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "020786a5f9d25bf140decf24d65e07dd738e4f9d", "committedDate": "2022-04-02 16:31:06 +0530", "message": "[HUDI-3451] Delete metadata table when the write client disables MDT (#5186)"}, {"oid": "c34eb07598adfd975fa1b9f5235dfc48a7a247bd", "committedDate": "2022-04-03 16:12:14 +0530", "message": "[MINOR] Reuse deleteMetadataTable for disabling metadata table (#5217)"}, {"oid": "b28f0d6ceb7750075be82b7bd4160a4475801159", "committedDate": "2022-04-04 08:08:20 -0700", "message": "[HUDI-3290] Different file formats for the partition metadata file. (#5179)"}, {"oid": "898be6174a62e8079cdc33648e73787829ee6963", "committedDate": "2022-04-05 23:06:52 -0400", "message": "[HUDI-3782] Fixing table config when any of the index is disabled (#5222)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "52e63b39d6189beb3b381944ed553bb0052b12c9", "committedDate": "2022-05-13 21:01:15 -0400", "message": "[HUDI-4097] add table info to jobStatus (#5529)"}, {"oid": "676d5cefe0294f495db04ec6476afdc00cbf0dd2", "committedDate": "2022-05-24 13:07:55 +0800", "message": "[HUDI-4138] Fix the concurrency modification of hoodie table config for flink (#5660)"}, {"oid": "d4f0326b4bbbabefc5c75617b2b5d6b8bf55fe11", "committedDate": "2022-06-20 14:29:21 +0800", "message": "[HUDI-4275] Refactor rollback inflight instant for clustering/compaction to reuse some code (#5894)"}, {"oid": "23c9c5c2962e1ac053cb4395fa148b9015272990", "committedDate": "2022-07-05 07:50:17 -0700", "message": "[HUDI-3836] Improve the way of fetching metadata partitions from table (#5286)"}, {"oid": "05606708fabdd3b6414cce04802ac85617976d04", "committedDate": "2022-07-14 16:00:08 +0800", "message": "[HUDI-4393] Add marker file for target file when flink merge handle rolls over (#6103)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "4f6f15c3c761621eaaa1b3b52e0c2841626afe53", "committedDate": "2022-10-25 11:24:32 +0800", "message": "[HUDI-5049] Supports dropPartition for Flink catalog (#6991)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "4e05ca817194c60ea42d665de76f62bc80125cdb", "committedDate": "2022-12-12 10:28:22 -0500", "message": "[HUDI-5078] Fixing isTableService for replace commits (#7037)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "9a0e1ce4887891a4281503fb0049ac32dd785850", "committedDate": "2023-01-11 10:00:35 -0500", "message": "[HUDI-5349] Clean up partially failed restore (#7605)"}, {"oid": "146f39d49e53eeef40bddd657b9ab11e217f3bc3", "committedDate": "2023-01-24 13:58:11 +0530", "message": "[HUDI-5593] Fixing deadlocks due to async cleaner awaiting for lock while main thread is acquired the lock and awaiting for async cleaner to finish (#7739)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "e25381c696693024d1b837f67d742fcbc32240bc", "committedDate": "2023-02-12 17:00:10 +0530", "message": "[HUDI-5764] Rollback delta commits from `HoodieIndexer` lazily in metadata table (#7921)"}, {"oid": "5af78c665bd8261269c612832a7d3fefe2957217", "committedDate": "2023-02-27 10:28:57 +0800", "message": "[HUDI-5845] Remove usage of deprecated getTableAvroSchemaWithoutMetadataFields. (#8032)"}, {"oid": "18d528f33d8b1dd7a836e5543ddf36e0a9c95ad1", "committedDate": "2023-03-04 12:00:25 +0800", "message": "[HUDI-5736] Common de-coupling column drop flag and schema validation flag (#7895)"}, {"oid": "0d6f656e5bb8a8acf75e7d7e8c3485e3748e06be", "committedDate": "2023-03-14 11:57:45 -0700", "message": "[HUDI-5927] Improve parallelism of deleting invalid files (#8172)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "398918c66e7f8fbeea4738cf20731cb9765ee0dc", "committedDate": "2023-05-15 22:21:21 +0530", "message": "[MINOR] Optimizing schema validation in Metadata table (#7619)"}, {"oid": "b28eeafea68fd1c35dddbd39fe7bdb995a1ad965", "committedDate": "2023-05-24 14:00:35 +0800", "message": "[MINOR] Check the return value from delete during rollback and finalize to ensure the files actually got deleted (#8606)"}, {"oid": "c859ea4cd23bc4ae56fca4914f34bbf5858bfde5", "committedDate": "2023-05-25 11:24:40 +0800", "message": "Revert \"[MINOR] Check the return value from delete during rollback and finalize to ensure the files actually got deleted (#8606)\" (#8803)"}, {"oid": "df64d647fa32b33aa9bcf8b147ab72993cf7ccb1", "committedDate": "2023-05-26 09:21:32 +0800", "message": "[HUDI-1517] create marker file for every log file (#4913)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjU1MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042551", "body": "break the second `with` to a new line?", "bodyText": "break the second with to a new line?", "bodyHTML": "<p dir=\"auto\">break the second <code>with</code> to a new line?</p>", "author": "yanghua", "createdAt": "2020-11-28T13:52:31Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -39,17 +39,18 @@\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)", "originalCommit": "6dac8043125b788234a2184f423ecc9d21c5757a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b593f1062931a4d017ae8bd7dd42e47a8873a39f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 2828588566..066a9656f2 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -39,18 +39,17 @@ import static org.junit.jupiter.api.Assertions.assertEquals;\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)\n-                        .withMaxDeltaTimeBeforeCompaction(maxDeltaTime).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n         .build();\n   }\n \n   @Test\n   public void testCompactionIsNotScheduledEarly() throws Exception {\n     // Given: make two commits\n-    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60);\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(\"000\", 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n", "next_change": {"commit": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 066a9656f2..81e91af09b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -39,21 +38,25 @@ import static org.junit.jupiter.api.Assertions.assertEquals;\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true)\n+            .withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)\n+            .withMaxDeltaTimeBeforeCompaction(maxDeltaTime)\n+            .withMaxDeltaTimeBeforeCompactionEnabled(true).build())\n         .build();\n   }\n \n   @Test\n   public void testCompactionIsNotScheduledEarly() throws Exception {\n     // Given: make two commits\n-    HoodieWriteConfig cfg = getConfigForInlineCompaction(3);\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n-      List<HoodieRecord> records = dataGen.generateInserts(\"000\", 100);\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n-      runNextDeltaCommits(writeClient, readClient, Arrays.asList(\"000\", \"001\"), records, cfg, true, new ArrayList<>());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n \n       // Then: ensure no compaction is executedm since there are only 2 delta commits\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 2828588566..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -39,22 +38,25 @@ import static org.junit.jupiter.api.Assertions.assertEquals;\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime, CompactionTriggerStrategy inlineCompactionType) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)\n-                        .withMaxDeltaTimeBeforeCompaction(maxDeltaTime).build())\n+            .withInlineCompaction(true)\n+            .withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)\n+            .withMaxDeltaSecondsBeforeCompaction(maxDeltaTime)\n+            .withInlineCompactionTriggerStrategy(inlineCompactionType).build())\n         .build();\n   }\n \n   @Test\n   public void testCompactionIsNotScheduledEarly() throws Exception {\n     // Given: make two commits\n-    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60);\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n-      List<HoodieRecord> records = dataGen.generateInserts(\"000\", 100);\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n-      runNextDeltaCommits(writeClient, readClient, Arrays.asList(\"000\", \"001\"), records, cfg, true, new ArrayList<>());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n \n       // Then: ensure no compaction is executedm since there are only 2 delta commits\n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -57,7 +57,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 2 delta commits\n       assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -60,7 +60,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 2 delta commits\n-      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": {"commit": "6aba00e84fade0b800e2d73c2f16be948af48d54", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..310ff4fe8a 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -59,7 +63,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n-      // Then: ensure no compaction is executedm since there are only 2 delta commits\n+      // Then: ensure no compaction is executed since there are only 2 delta commits\n       assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a", "url": "https://github.com/apache/hudi/commit/6dac8043125b788234a2184f423ecc9d21c5757a", "message": "support flink", "committedDate": "2020-11-18T15:28:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTE1OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548791159", "body": "How about mark this `initialTime` as lastCompaction time, then we can gei it from timeline. \r\nthen, when we get a new commit, we can check the interval between these two timestamps to decide whether execute compact or not.\r\nin this way:\r\n1. there is no need to update it additionally\r\n2. It would be more accurate.(Time elapsed since the last compact)", "bodyText": "How about mark this initialTime as lastCompaction time, then we can gei it from timeline.\nthen, when we get a new commit, we can check the interval between these two timestamps to decide whether execute compact or not.\nin this way:\n\nthere is no need to update it additionally\nIt would be more accurate.(Time elapsed since the last compact)", "bodyHTML": "<p dir=\"auto\">How about mark this <code>initialTime</code> as lastCompaction time, then we can gei it from timeline.<br>\nthen, when we get a new commit, we can check the interval between these two timestamps to decide whether execute compact or not.<br>\nin this way:</p>\n<ol dir=\"auto\">\n<li>there is no need to update it additionally</li>\n<li>It would be more accurate.(Time elapsed since the last compact)</li>\n</ol>", "author": "wangxianghu", "createdAt": "2020-12-25T03:36:01Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -129,6 +130,7 @@ public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n     this.index = createIndex(writeConfig);\n+    this.initialTime = HoodieActiveTimeline.createNewInstantTime();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk1MDE4OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548950189", "bodyText": "yes. right", "author": "Karl-WangSK", "createdAt": "2020-12-26T06:29:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTE1OQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTc5Mw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548791793", "body": "Do you mean one day?  might be too long, who about an hour", "bodyText": "Do you mean one day?  might be too long, who about an hour", "bodyHTML": "<p dir=\"auto\">Do you mean one day?  might be too long, who about an hour</p>", "author": "wangxianghu", "createdAt": "2020-12-25T03:41:01Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -109,6 +110,7 @@\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n+  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60 * 24);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODg4Nzg5OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548887898", "bodyText": "ok", "author": "Karl-WangSK", "createdAt": "2020-12-25T16:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTc5Mw=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njc3MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548796771", "body": "how about triggering a compaction when any one of the conditions is met?", "bodyText": "how about triggering a compaction when any one of the conditions is met?", "bodyHTML": "<p dir=\"auto\">how about triggering a compaction when any one of the conditions is met?</p>", "author": "wangxianghu", "createdAt": "2020-12-25T04:19:44Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -65,10 +66,12 @@ protected HoodieCompactionPlan scheduleCompaction() {\n \n     int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n         .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && timeCompaction(instantTime, initialTime, deltaCommitsSinceLastCompaction)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODg4Njc1Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548886752", "bodyText": "yes. So two conditions in if means they won't trigger compact.\nReturn true means imcompactable.", "author": "Karl-WangSK", "createdAt": "2020-12-25T15:47:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njc3MQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njg5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548796896", "body": "The timestamp is of `yyyyMMddHHmmss` format, we can not simply sum them by `+`", "bodyText": "The timestamp is of yyyyMMddHHmmss format, we can not simply sum them by +", "bodyHTML": "<p dir=\"auto\">The timestamp is of <code>yyyyMMddHHmmss</code> format, we can not simply sum them by <code>+</code></p>", "author": "wangxianghu", "createdAt": "2020-12-25T04:20:58Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -85,4 +88,15 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  protected boolean timeCompaction(String instantTime, String initialTime, int deltaCommitsSinceLastCompaction) {\n+    if (Long.parseLong(initialTime) + config.getInlineCompactDeltaElapsedTimeMax() > Long.parseLong(instantTime)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk4MDcwOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548980709", "bodyText": "revised", "author": "Karl-WangSK", "createdAt": "2020-12-26T13:01:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njg5Ng=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODk0NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r549538945", "body": "we'd better throw an 'HoodieCompactionException' here", "bodyText": "we'd better throw an 'HoodieCompactionException' here", "bodyHTML": "<p dir=\"auto\">we'd better throw an 'HoodieCompactionException' here</p>", "author": "wangxianghu", "createdAt": "2020-12-29T01:44:51Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -90,4 +99,13 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  public Long parseTs(String time) {\n+    Long timestamp = null;\n+    try {\n+      timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n+    } catch (ParseException e) {\n+      e.printStackTrace();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTM1Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r549539352", "bodyText": "ok", "author": "Karl-WangSK", "createdAt": "2020-12-29T01:47:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODk0NQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU0NzI1NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552547254", "body": "Can we define a variable to make the code more readable?", "bodyText": "Can we define a variable to make the code more readable?", "bodyHTML": "<p dir=\"auto\">Can we define a variable to make the code more readable?</p>", "author": "yanghua", "createdAt": "2021-01-06T12:14:09Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && parseTs(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseTs(instantTime)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU0ODcxOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552548719", "body": "IMO, we can use `String.format(...)` to make the log message more readable, right?", "bodyText": "IMO, we can use String.format(...) to make the log message more readable, right?", "bodyHTML": "<p dir=\"auto\">IMO, we can use <code>String.format(...)</code> to make the log message more readable, right?</p>", "author": "yanghua", "createdAt": "2021-01-06T12:15:45Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && parseTs(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseTs(instantTime)) {\n       LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n           + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+          + config.getInlineCompactDeltaCommitMax() + \". Or \" + config.getInlineCompactDeltaElapsedTimeMax()\n+              + \"ms elapsed time need since last compaction \" + lastCompactionTs);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU5MDQyMQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552590421", "bodyText": "ok", "author": "Karl-WangSK", "createdAt": "2021-01-06T13:05:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU0ODcxOQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1MTQwOA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552551408", "body": "Please give it a better name?", "bodyText": "Please give it a better name?", "bodyHTML": "<p dir=\"auto\">Please give it a better name?</p>", "author": "yanghua", "createdAt": "2021-01-06T12:18:51Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -90,4 +99,13 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  public Long parseTs(String time) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1MzAwMg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552553002", "body": "Can we break the second `withXXX ` into a new line?", "bodyText": "Can we break the second withXXX  into a new line?", "bodyHTML": "<p dir=\"auto\">Can we break the second <code>withXXX </code> into a new line?</p>", "author": "yanghua", "createdAt": "2021-01-06T12:20:45Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -39,21 +38,23 @@\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTQ0NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555444", "body": "Replacing `For` to `BasedOn` or `Via` sounds better?", "bodyText": "Replacing For to BasedOn or Via sounds better?", "bodyHTML": "<p dir=\"auto\">Replacing <code>For</code> to <code>BasedOn</code> or <code>Via</code> sounds better?</p>", "author": "yanghua", "createdAt": "2021-01-06T12:23:53Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -62,9 +63,9 @@ public void testCompactionIsNotScheduledEarly() throws Exception {\n   }\n \n   @Test\n-  public void testSuccessfulCompaction() throws Exception {\n+  public void testSuccessfulCompactionForNumCommits() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTY1OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555659", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-06T12:24:09Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +86,94 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionForTime() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTkzNw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555937", "body": "whitespace between `5,10`", "bodyText": "whitespace between 5,10", "bodyHTML": "<p dir=\"auto\">whitespace between <code>5,10</code></p>", "author": "yanghua", "createdAt": "2021-01-06T12:24:32Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +86,94 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionForTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5,10);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjcwNjg4MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552706881", "body": "Want to make sure: if we need to match the two conditions at the same time? Or they are two different choices? Can users only choose one of them?", "bodyText": "Want to make sure: if we need to match the two conditions at the same time? Or they are two different choices? Can users only choose one of them?", "bodyHTML": "<p dir=\"auto\">Want to make sure: if we need to match the two conditions at the same time? Or they are two different choices? Can users only choose one of them?</p>", "author": "yanghua", "createdAt": "2021-01-06T15:18:48Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzA3NTUwMg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553075502", "bodyText": "Currently, compaction will be triggered once reach one of the conditions.\nCan users only choose one of them?\nNo, but they can set time or commitsNum to Infinity", "author": "Karl-WangSK", "createdAt": "2021-01-07T02:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjcwNjg4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzI3NTkzMg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553275932", "bodyText": "IMO, provicing an option and let user choose a compaction strategy sounds more reasonable. wdyt?", "author": "yanghua", "createdAt": "2021-01-07T11:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjcwNjg4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQxMTY5MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553411690", "bodyText": "yes. added!", "author": "Karl-WangSK", "createdAt": "2021-01-07T15:47:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjcwNjg4MQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": null, "commits_in_main": null}, {"oid": "b593f1062931a4d017ae8bd7dd42e47a8873a39f", "url": "https://github.com/apache/hudi/commit/b593f1062931a4d017ae8bd7dd42e47a8873a39f", "message": "[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)", "committedDate": "2021-01-06T15:07:24Z", "type": "forcePushed"}, {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "url": "https://github.com/apache/hudi/commit/ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "message": "update", "committedDate": "2021-01-07T06:27:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553699091", "body": "Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).\r\n\r\nbesides, we got 4 conditions here:\r\n1. compact with commit num only;\r\n2. compact with time elapsed only;\r\n3. compact when both commit num and time elapsed meet requirements\uff1b\r\n4. compact when one of them is met\r\n\r\nWDYT @Karl-WangSK  cc @yanghua ", "bodyText": "Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).\nbesides, we got 4 conditions here:\n\ncompact with commit num only;\ncompact with time elapsed only;\ncompact when both commit num and time elapsed meet requirements\uff1b\ncompact when one of them is met\n\nWDYT @Karl-WangSK  cc @yanghua", "bodyHTML": "<p dir=\"auto\">Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).</p>\n<p dir=\"auto\">besides, we got 4 conditions here:</p>\n<ol dir=\"auto\">\n<li>compact with commit num only;</li>\n<li>compact with time elapsed only;</li>\n<li>compact when both commit num and time elapsed meet requirements\uff1b</li>\n<li>compact when one of them is met</li>\n</ol>\n<p dir=\"auto\">WDYT <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/Karl-WangSK/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Karl-WangSK\">@Karl-WangSK</a>  cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/yanghua/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanghua\">@yanghua</a></p>", "author": "wangxianghu", "createdAt": "2021-01-08T02:04:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,32 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n+    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n+    boolean compactable;\n+    if (numCommitEnabled && !timeEnabled) {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n+    } else if (!numCommitEnabled && timeEnabled) {\n+      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    } else {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    }", "originalCommit": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MTc1NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559351754", "bodyText": "if we call this function. it means we turn ASYNC_COMPACT_ENABLE_OPT_KEY on. Do we still need to check first?\n\nThe switch of ASYNC_COMPACT_ENABLE_OPT_KEY  must be the pre-condition.", "author": "yanghua", "createdAt": "2021-01-18T07:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "d087f09d4da2b2339413799f4eb13a091faacb9a", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b101442ded..6a601011c8 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -74,37 +74,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n       deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n     // judge if we need to compact according to num delta commits and time elapsed\n-    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n-    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n-    boolean compactable;\n-    if (numCommitEnabled && !timeEnabled) {\n-      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n-    } else if (!numCommitEnabled && timeEnabled) {\n-      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n-    } else {\n-      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n-          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n-    }\n+    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);\n     if (compactable) {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-              + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", deltaCommitsSinceLastCompaction,\n-          lastCompactionTs, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), lastCompactionTs));\n-      return new HoodieCompactionPlan();\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n     }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+    LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n+            + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", deltaCommitsSinceLastCompaction,\n+        lastCompactionTs, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), lastCompactionTs));\n+    return new HoodieCompactionPlan();\n+  }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+  public boolean getCompactType(Integer deltaCommitsSinceLastCompaction, String lastCompactionTs) {\n+    switch (config.getInlineCompactType()) {\n+      case COMMIT_NUM:\n+        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction;\n+      case TIME_ELAPSED:\n+        return  parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+      case NUM_OR_TIME:\n+        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction\n+            || parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+      case NUM_AND_TIME:\n+        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction\n+            && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+      default:\n+        throw new HoodieCompactionException(\"Compact type unspecified, set \" + config.getInlineCompactType());\n     }\n   }\n \n", "next_change": {"commit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 6a601011c8..67db2dceda 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -91,27 +80,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n       }\n     }\n \n-    LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-            + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", deltaCommitsSinceLastCompaction,\n-        lastCompactionTs, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), lastCompactionTs));\n     return new HoodieCompactionPlan();\n   }\n \n-  public boolean getCompactType(Integer deltaCommitsSinceLastCompaction, String lastCompactionTs) {\n-    switch (config.getInlineCompactType()) {\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+    Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n+    if (lastCompaction.isPresent()) {\n+      lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+    }\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n+\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n       case COMMIT_NUM:\n-        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction;\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n       case TIME_ELAPSED:\n-        return  parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n       case NUM_OR_TIME:\n-        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction\n-            || parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n       case NUM_AND_TIME:\n-        return config.getInlineCompactDeltaCommitMax() <= deltaCommitsSinceLastCompaction\n-            && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Compact type unspecified, set \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n+\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n+                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n+              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n     }\n+    return compactable;\n   }\n \n   public Long parseToTimestamp(String time) {\n", "next_change": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -105,41 +105,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n-        break;\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n+              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n+              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_OR_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_AND_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n     }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n-    }\n-    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -157,7 +157,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n         }\n         return compactable;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n     }\n   }\n \n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b7d4d58334..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b101442ded..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -61,54 +61,86 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n-    String lastCompactionTs;\n-    int deltaCommitsSinceLastCompaction;\n+\n+    String latestInstantTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    // judge if we need to compact according to num delta commits and time elapsed\n-    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n-    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n-    boolean compactable;\n-    if (numCommitEnabled && !timeEnabled) {\n-      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n-    } else if (!numCommitEnabled && timeEnabled) {\n-      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n-    } else {\n-      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n-          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n-    }\n-    if (compactable) {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-              + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", deltaCommitsSinceLastCompaction,\n-          lastCompactionTs, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), lastCompactionTs));\n-      return new HoodieCompactionPlan();\n-    }\n-\n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n+  }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"oid": "d087f09d4da2b2339413799f4eb13a091faacb9a", "url": "https://github.com/apache/hudi/commit/d087f09d4da2b2339413799f4eb13a091faacb9a", "message": "update", "committedDate": "2021-01-10T06:53:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM5MTA4MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r556391080", "body": "how about extract all these logic to one method `needCompact(Table table, CompactType compactType )`, and init proper vars when need.", "bodyText": "how about extract all these logic to one method needCompact(Table table, CompactType compactType ), and init proper vars when need.", "bodyHTML": "<p dir=\"auto\">how about extract all these logic to one method <code>needCompact(Table table, CompactType compactType )</code>, and init proper vars when need.</p>", "author": "wangxianghu", "createdAt": "2021-01-13T09:48:29Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,34 +63,64 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);", "originalCommit": "d087f09d4da2b2339413799f4eb13a091faacb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 6a601011c8..67db2dceda 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -61,20 +62,8 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n-    Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n-    String lastCompactionTs;\n-    int deltaCommitsSinceLastCompaction;\n-    if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    }\n     // judge if we need to compact according to num delta commits and time elapsed\n-    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    boolean compactable = needCompact(config.getInlineCompactType());\n     if (compactable) {\n       LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n       HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -63,7 +63,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     // judge if we need to compact according to num delta commits and time elapsed\n-    boolean compactable = needCompact(config.getInlineCompactType());\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n     if (compactable) {\n       LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n       HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 6a601011c8..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -61,20 +61,8 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n-    Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n-    String lastCompactionTs;\n-    int deltaCommitsSinceLastCompaction;\n-    if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n-      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    }\n     // judge if we need to compact according to num delta commits and time elapsed\n-    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n     if (compactable) {\n       LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n       HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -19,53 +19,94 @@\n package org.apache.hudi.table.action.compact;\n \n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n-import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.common.engine.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieFileGroupId;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n import org.apache.hudi.common.table.view.SyncableFileSystemView;\n import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieCompactionException;\n+import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.api.java.JavaRDD;\n \n import java.io.IOException;\n import java.text.ParseException;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.stream.Collectors;\n \n-@SuppressWarnings(\"checkstyle:LineLength\")\n-public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload> extends\n-    BaseScheduleCompactionActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+public class ScheduleCompactionActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCompactionPlan>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(ScheduleCompactionActionExecutor.class);\n \n-  private static final Logger LOG = LogManager.getLogger(SparkScheduleCompactionActionExecutor.class);\n+  private final Option<Map<String, String>> extraMetadata;\n+  private final HoodieCompactor compactor;\n \n-  public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n-                                               HoodieWriteConfig config,\n-                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                               String instantTime,\n-                                               Option<Map<String, String>> extraMetadata) {\n-    super(context, config, table, instantTime, extraMetadata);\n+  public ScheduleCompactionActionExecutor(HoodieEngineContext context,\n+                                          HoodieWriteConfig config,\n+                                          HoodieTable<T, I, K, O> table,\n+                                          String instantTime,\n+                                          Option<Map<String, String>> extraMetadata,\n+                                          HoodieCompactor compactor) {\n+    super(context, config, table, instantTime);\n+    this.extraMetadata = extraMetadata;\n+    this.compactor = compactor;\n   }\n \n   @Override\n-  protected HoodieCompactionPlan scheduleCompaction() {\n+  public Option<HoodieCompactionPlan> execute() {\n+    if (!config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()\n+        && !config.getFailedWritesCleanPolicy().isLazy()) {\n+      // if there are inflight writes, their instantTime must not be less than that of compaction instant time\n+      table.getActiveTimeline().getCommitsTimeline().filterPendingExcludingCompaction().firstInstant()\n+          .ifPresent(earliestInflight -> ValidationUtils.checkArgument(\n+              HoodieTimeline.compareTimestamps(earliestInflight.getTimestamp(), HoodieTimeline.GREATER_THAN, instantTime),\n+              \"Earliest write inflight instant time must be later than compaction time. Earliest :\" + earliestInflight\n+                  + \", Compaction scheduled at \" + instantTime));\n+      // Committed and pending compaction instants should have strictly lower timestamps\n+      List<HoodieInstant> conflictingInstants = table.getActiveTimeline()\n+          .getWriteTimeline().filterCompletedAndCompactionInstants().getInstants()\n+          .filter(instant -> HoodieTimeline.compareTimestamps(\n+              instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))\n+          .collect(Collectors.toList());\n+      ValidationUtils.checkArgument(conflictingInstants.isEmpty(),\n+          \"Following instants have timestamps >= compactionInstant (\" + instantTime + \") Instants :\"\n+              + conflictingInstants);\n+    }\n+\n+    HoodieCompactionPlan plan = scheduleCompaction();\n+    if (plan != null && (plan.getOperations() != null) && (!plan.getOperations().isEmpty())) {\n+      extraMetadata.ifPresent(plan::setExtraMetadata);\n+      HoodieInstant compactionInstant =\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instantTime);\n+      try {\n+        table.getActiveTimeline().saveToCompactionRequested(compactionInstant,\n+            TimelineMetadataUtils.serializeCompactionPlan(plan));\n+      } catch (IOException ioe) {\n+        throw new HoodieIOException(\"Exception scheduling compaction\", ioe);\n+      }\n+      return Option.of(plan);\n+    }\n+    return Option.empty();\n+  }\n+\n+  private HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     // judge if we need to compact according to num delta commits and time elapsed\n     boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n     if (compactable) {\n       LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n       try {\n         SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n         Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f", "url": "https://github.com/apache/hudi/commit/7d0453e72c7f9136e16ffaf928e09906917f745f", "message": "update", "committedDate": "2021-01-14T03:32:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDAxNw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350017", "body": "Please revert this change.", "bodyText": "Please revert this change.", "bodyHTML": "<p dir=\"auto\">Please revert this change.</p>", "author": "yanghua", "createdAt": "2021-01-18T06:59:09Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -18,6 +18,7 @@\n \n package org.apache.hudi.config;\n \n+import org.apache.hadoop.hbase.io.compress.Compression;", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fbd87b4915f52d3010edeebe66b966855e1abfdc", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..ea52beced6 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -18,7 +18,6 @@\n \n package org.apache.hudi.config;\n \n-import org.apache.hadoop.hbase.io.compress.Compression;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.bootstrap.BootstrapMode;\n import org.apache.hudi.common.config.DefaultHoodieConfig;\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..e3c1ef6819 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -18,13 +18,12 @@\n \n package org.apache.hudi.config;\n \n-import org.apache.hadoop.hbase.io.compress.Compression;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.bootstrap.BootstrapMode;\n import org.apache.hudi.common.config.DefaultHoodieConfig;\n import org.apache.hudi.common.config.HoodieMetadataConfig;\n+import org.apache.hudi.common.engine.EngineType;\n import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n-import org.apache.hudi.client.common.EngineType;\n import org.apache.hudi.common.model.HoodieCleaningPolicy;\n import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex e3c1ef6819..24ba109546 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -24,6 +24,7 @@ import org.apache.hudi.common.config.DefaultHoodieConfig;\n import org.apache.hudi.common.config.HoodieMetadataConfig;\n import org.apache.hudi.common.engine.EngineType;\n import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;\n import org.apache.hudi.common.model.HoodieCleaningPolicy;\n import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n", "next_change": {"commit": "74241947c123c860a1b0344f25cef316440a70d6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 24ba109546..944cd02306 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -18,18 +18,23 @@\n \n package org.apache.hudi.config;\n \n+import org.apache.hadoop.hbase.io.compress.Compression;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.bootstrap.BootstrapMode;\n+import org.apache.hudi.client.transaction.ConflictResolutionStrategy;\n import org.apache.hudi.common.config.DefaultHoodieConfig;\n import org.apache.hudi.common.config.HoodieMetadataConfig;\n+import org.apache.hudi.common.config.LockConfiguration;\n import org.apache.hudi.common.engine.EngineType;\n import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;\n import org.apache.hudi.common.model.HoodieCleaningPolicy;\n import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.common.model.WriteConcurrencyMode;\n import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n import org.apache.hudi.execution.bulkinsert.BulkInsertSortMode;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n", "next_change": {"commit": "7261f0850727aea611cd34e1bb07d684b44534f6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 944cd02306..cf5ac5ce9f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -37,7 +37,7 @@ import org.apache.hudi.common.util.ReflectionUtils;\n import org.apache.hudi.common.util.ValidationUtils;\n import org.apache.hudi.execution.bulkinsert.BulkInsertSortMode;\n import org.apache.hudi.index.HoodieIndex;\n-import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorType;\n import org.apache.hudi.metrics.MetricsReporterType;\n import org.apache.hudi.metrics.datadog.DatadogHttpClient.ApiSite;\n import org.apache.hudi.table.action.compact.CompactionTriggerStrategy;\n", "next_change": {"commit": "b8fe5b91d599418cd908d833fd63edc7f362c548", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex cf5ac5ce9f..9e89e0ef7f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -42,6 +42,7 @@ import org.apache.hudi.metrics.MetricsReporterType;\n import org.apache.hudi.metrics.datadog.DatadogHttpClient.ApiSite;\n import org.apache.hudi.table.action.compact.CompactionTriggerStrategy;\n import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;\n+import org.apache.orc.CompressionKind;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n \n import javax.annotation.concurrent.Immutable;\n", "next_change": {"commit": "75040ee9e5caa0783009b6ef529d6605e82d4135", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 9e89e0ef7f..357e3f574e 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -42,140 +42,302 @@ import org.apache.hudi.metrics.MetricsReporterType;\n import org.apache.hudi.metrics.datadog.DatadogHttpClient.ApiSite;\n import org.apache.hudi.table.action.compact.CompactionTriggerStrategy;\n import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;\n+\n+import org.apache.hadoop.hbase.io.compress.Compression;\n import org.apache.orc.CompressionKind;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n \n import javax.annotation.concurrent.Immutable;\n+\n import java.io.File;\n import java.io.FileReader;\n import java.io.IOException;\n import java.io.InputStream;\n import java.util.Arrays;\n import java.util.List;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Properties;\n import java.util.function.Supplier;\n import java.util.stream.Collectors;\n \n-import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\n-import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\n-\n /**\n  * Class storing configs for the HoodieWriteClient.\n  */\n @Immutable\n-public class HoodieWriteConfig extends DefaultHoodieConfig {\n+public class HoodieWriteConfig extends HoodieConfig {\n \n   private static final long serialVersionUID = 0L;\n \n-  public static final String TABLE_NAME = \"hoodie.table.name\";\n-  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n-  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n-  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-\n-  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n-  public static final String KEYGENERATOR_TYPE_PROP = \"hoodie.datasource.write.keygenerator.type\";\n-  public static final String DEFAULT_KEYGENERATOR_TYPE = KeyGeneratorType.SIMPLE.name();\n-\n-  public static final String DEFAULT_ROLLBACK_USING_MARKERS = \"false\";\n-  public static final String ROLLBACK_USING_MARKERS = \"hoodie.rollback.using.markers\";\n-  public static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n-  public static final String BASE_PATH_PROP = \"hoodie.base.path\";\n-  public static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n-  public static final String AVRO_SCHEMA_VALIDATE = \"hoodie.avro.schema.validate\";\n-  public static final String DEFAULT_AVRO_SCHEMA_VALIDATE = \"false\";\n-  public static final String DEFAULT_PARALLELISM = \"1500\";\n-  public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n-  public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n-  public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n-  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulkinsert.schema.ddl\";\n-  public static final String UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\";\n-  public static final String DELETE_PARALLELISM = \"hoodie.delete.shuffle.parallelism\";\n-  public static final String DEFAULT_ROLLBACK_PARALLELISM = \"100\";\n-  public static final String ROLLBACK_PARALLELISM = \"hoodie.rollback.parallelism\";\n-  public static final String WRITE_BUFFER_LIMIT_BYTES = \"hoodie.write.buffer.limit.bytes\";\n-  public static final String DEFAULT_WRITE_BUFFER_LIMIT_BYTES = String.valueOf(4 * 1024 * 1024);\n-  public static final String COMBINE_BEFORE_INSERT_PROP = \"hoodie.combine.before.insert\";\n-  public static final String DEFAULT_COMBINE_BEFORE_INSERT = \"false\";\n-  public static final String COMBINE_BEFORE_UPSERT_PROP = \"hoodie.combine.before.upsert\";\n-  public static final String DEFAULT_COMBINE_BEFORE_UPSERT = \"true\";\n-  public static final String COMBINE_BEFORE_DELETE_PROP = \"hoodie.combine.before.delete\";\n-  public static final String DEFAULT_COMBINE_BEFORE_DELETE = \"true\";\n-  public static final String WRITE_STATUS_STORAGE_LEVEL = \"hoodie.write.status.storage.level\";\n-  public static final String DEFAULT_WRITE_STATUS_STORAGE_LEVEL = \"MEMORY_AND_DISK_SER\";\n-  public static final String HOODIE_AUTO_COMMIT_PROP = \"hoodie.auto.commit\";\n-  public static final String DEFAULT_HOODIE_AUTO_COMMIT = \"true\";\n-\n-  public static final String HOODIE_WRITE_STATUS_CLASS_PROP = \"hoodie.writestatus.class\";\n-  public static final String DEFAULT_HOODIE_WRITE_STATUS_CLASS = WriteStatus.class.getName();\n-  public static final String FINALIZE_WRITE_PARALLELISM = \"hoodie.finalize.write.parallelism\";\n-  public static final String DEFAULT_FINALIZE_WRITE_PARALLELISM = DEFAULT_PARALLELISM;\n-  public static final String MARKERS_DELETE_PARALLELISM = \"hoodie.markers.delete.parallelism\";\n-  public static final String DEFAULT_MARKERS_DELETE_PARALLELISM = \"100\";\n-  public static final String BULKINSERT_SORT_MODE = \"hoodie.bulkinsert.sort.mode\";\n-  public static final String DEFAULT_BULKINSERT_SORT_MODE = BulkInsertSortMode.GLOBAL_SORT\n-      .toString();\n-\n-  public static final String EMBEDDED_TIMELINE_SERVER_ENABLED = \"hoodie.embed.timeline.server\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ENABLED = \"true\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = \"hoodie.embed.timeline.server.reuse.enabled\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = \"false\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_PORT = \"hoodie.embed.timeline.server.port\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_PORT = \"0\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_THREADS = \"hoodie.embed.timeline.server.threads\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_THREADS = \"-1\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT = \"hoodie.embed.timeline.server.gzip\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_COMPRESS_OUTPUT = \"true\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_USE_ASYNC = \"hoodie.embed.timeline.server.async\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ASYNC = \"false\";\n-\n-  public static final String FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP = \"hoodie.fail.on.timeline.archiving\";\n-  public static final String DEFAULT_FAIL_ON_TIMELINE_ARCHIVING_ENABLED = \"true\";\n-  // time between successive attempts to ensure written data's metadata is consistent on storage\n-  public static final String INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP =\n-      \"hoodie.consistency.check.initial_interval_ms\";\n-  public static long DEFAULT_INITIAL_CONSISTENCY_CHECK_INTERVAL_MS = 2000L;\n-\n-  // max interval time\n-  public static final String MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP = \"hoodie.consistency.check.max_interval_ms\";\n-  public static long DEFAULT_MAX_CONSISTENCY_CHECK_INTERVAL_MS = 300000L;\n-\n-  // maximum number of checks, for consistency of written data. Will wait upto 256 Secs\n-  public static final String MAX_CONSISTENCY_CHECKS_PROP = \"hoodie.consistency.check.max_checks\";\n-  public static int DEFAULT_MAX_CONSISTENCY_CHECKS = 7;\n-\n-  // Data validation check performed during merges before actual commits\n-  private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n-  private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n-\n-  // Allow duplicates with inserts while merging with existing records\n-  private static final String MERGE_ALLOW_DUPLICATE_ON_INSERTS = \"hoodie.merge.allow.duplicate.on.inserts\";\n-  private static final String DEFAULT_MERGE_ALLOW_DUPLICATE_ON_INSERTS = \"false\";\n-\n-  public static final String CLIENT_HEARTBEAT_INTERVAL_IN_MS_PROP = \"hoodie.client.heartbeat.interval_in_ms\";\n-  public static final Integer DEFAULT_CLIENT_HEARTBEAT_INTERVAL_IN_MS = 60 * 1000;\n-\n-  public static final String CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES_PROP = \"hoodie.client.heartbeat.tolerable.misses\";\n-  public static final Integer DEFAULT_CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES = 2;\n-  // Enable different concurrency support\n-  public static final String WRITE_CONCURRENCY_MODE_PROP =\n-      \"hoodie.write.concurrency.mode\";\n-  public static final String DEFAULT_WRITE_CONCURRENCY_MODE = WriteConcurrencyMode.SINGLE_WRITER.name();\n-\n-  // Comma separated metadata key prefixes to override from latest commit during overlapping commits via multi writing\n-  public static final String WRITE_META_KEY_PREFIXES_PROP =\n-      \"hoodie.write.meta.key.prefixes\";\n-  public static final String DEFAULT_WRITE_META_KEY_PREFIXES = \"\";\n+  public static final ConfigProperty<String> TABLE_NAME = ConfigProperty\n+      .key(\"hoodie.table.name\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Table name that will be used for registering with metastores like HMS. Needs to be same across runs.\");\n+\n+  public static final ConfigProperty<String> PRECOMBINE_FIELD_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.precombine.field\")\n+      .defaultValue(\"ts\")\n+      .withDocumentation(\"Field used in preCombining before actual write. When two records have the same key value, \"\n+          + \"we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)\");\n+\n+  public static final ConfigProperty<String> WRITE_PAYLOAD_CLASS = ConfigProperty\n+      .key(\"hoodie.datasource.write.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. \"\n+          + \"This will render any value set for PRECOMBINE_FIELD_OPT_VAL in-effective\");\n+\n+  public static final ConfigProperty<String> KEYGENERATOR_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.keygenerator.class\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Key generator class, that implements `org.apache.hudi.keygen.KeyGenerator` \"\n+          + \"extract a key out of incoming records.\");\n+\n+  public static final ConfigProperty<String> KEYGENERATOR_TYPE_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.keygenerator.type\")\n+      .defaultValue(KeyGeneratorType.SIMPLE.name())\n+      .withDocumentation(\"Easily configure one the built-in key generators, instead of specifying the key generator class.\"\n+          + \"Currently supports SIMPLE, COMPLEX, TIMESTAMP, CUSTOM, NON_PARTITION, GLOBAL_DELETE\");\n+\n+  public static final ConfigProperty<String> ROLLBACK_USING_MARKERS = ConfigProperty\n+      .key(\"hoodie.rollback.using.markers\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Enables a more efficient mechanism for rollbacks based on the marker files generated \"\n+          + \"during the writes. Turned off by default.\");\n+\n+  public static final ConfigProperty<String> TIMELINE_LAYOUT_VERSION = ConfigProperty\n+      .key(\"hoodie.timeline.layout.version\")\n+      .noDefaultValue()\n+      .sinceVersion(\"0.5.1\")\n+      .withDocumentation(\"Controls the layout of the timeline. Version 0 relied on renames, Version 1 (default) models \"\n+          + \"the timeline as an immutable log relying only on atomic writes for object storage.\");\n+\n+  public static final ConfigProperty<String> BASE_PATH_PROP = ConfigProperty\n+      .key(\"hoodie.base.path\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Base path on lake storage, under which all the table data is stored. \"\n+          + \"Always prefix it explicitly with the storage scheme (e.g hdfs://, s3:// etc). \"\n+          + \"Hudi stores all the main meta-data about commits, savepoints, cleaning audit logs \"\n+          + \"etc in .hoodie directory under this base path directory.\");\n+\n+  public static final ConfigProperty<String> AVRO_SCHEMA = ConfigProperty\n+      .key(\"hoodie.avro.schema\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Schema string representing the current write schema of the table. Hudi passes this to \"\n+          + \"implementations of HoodieRecordPayload to convert incoming records to avro. This is also used as the write schema \"\n+          + \"evolving records during an update.\");\n+\n+  public static final ConfigProperty<String> AVRO_SCHEMA_VALIDATE = ConfigProperty\n+      .key(\"hoodie.avro.schema.validate\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Validate the schema used for the write against the latest schema, for backwards compatibility.\");\n+\n+  public static final ConfigProperty<String> INSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.insert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Parallelism for inserting records into the table. Inserts can shuffle data before writing to tune file sizes and optimize the storage layout.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.bulkinsert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"For large initial imports using bulk_insert operation, controls the parallelism to use for sort modes or custom partitioning done\"\n+          + \"before writing records to the table.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = ConfigProperty\n+      .key(\"hoodie.bulkinsert.user.defined.partitioner.class\")\n+      .noDefaultValue()\n+      .withDocumentation(\"If specified, this class will be used to re-partition records before they are bulk inserted. This can be used to sort, pack, cluster data\"\n+          + \" optimally for common query patterns.\");\n+\n+  public static final ConfigProperty<String> UPSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.upsert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Parallelism to use for upsert operation on the table. Upserts can shuffle data to perform index lookups, file sizing, bin packing records optimally\"\n+          + \"into file groups.\");\n+\n+  public static final ConfigProperty<String> DELETE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.delete.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Parallelism used for \u201cdelete\u201d operation. Delete operations also performs shuffles, similar to upsert operation.\");\n+\n+  public static final ConfigProperty<String> ROLLBACK_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.rollback.parallelism\")\n+      .defaultValue(\"100\")\n+      .withDocumentation(\"Parallelism for rollback of commits. Rollbacks perform delete of files or logging delete blocks to file groups on storage in parallel.\");\n+\n+  public static final ConfigProperty<String> WRITE_BUFFER_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.write.buffer.limit.bytes\")\n+      .defaultValue(String.valueOf(4 * 1024 * 1024))\n+      .withDocumentation(\"Size of in-memory buffer used for parallelizing network reads and lake storage writes.\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_INSERT_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.insert\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When inserted records share same key, controls whether they should be first combined (i.e de-duplicated) before\"\n+          + \" writing to storage.\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_UPSERT_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.upsert\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"When upserted records share same key, controls whether they should be first combined (i.e de-duplicated) before\"\n+          + \" writing to storage. This should be turned off only if you are absolutely certain that there are no duplicates incoming, \"\n+          + \" otherwise it can lead to duplicate keys and violate the uniqueness guarantees.\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_DELETE_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.delete\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"During delete operations, controls whether we should combine deletes (and potentially also upserts) before \"\n+          + \" writing to storage.\");\n+\n+  public static final ConfigProperty<String> WRITE_STATUS_STORAGE_LEVEL = ConfigProperty\n+      .key(\"hoodie.write.status.storage.level\")\n+      .defaultValue(\"MEMORY_AND_DISK_SER\")\n+      .withDocumentation(\"Write status objects hold metadata about a write (stats, errors), that is not yet committed to storage. \"\n+          + \"This controls the how that information is cached for inspection by clients. We rarely expect this to be changed.\");\n+\n+  public static final ConfigProperty<String> HOODIE_AUTO_COMMIT_PROP = ConfigProperty\n+      .key(\"hoodie.auto.commit\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Controls whether a write operation should auto commit. This can be turned off to perform inspection\"\n+          + \" of the uncommitted write before deciding to commit.\");\n+\n+  public static final ConfigProperty<String> HOODIE_WRITE_STATUS_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.writestatus.class\")\n+      .defaultValue(WriteStatus.class.getName())\n+      .withDocumentation(\"Subclass of \" + WriteStatus.class.getName() + \" to be used to collect information about a write. Can be \"\n+          + \"overridden to collection additional metrics/statistics about the data if needed.\");\n+\n+  public static final ConfigProperty<String> FINALIZE_WRITE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.finalize.write.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Parallelism for the write finalization internal operation, which involves removing any partially written \"\n+          + \"files from lake storage, before committing the write. Reduce this value, if the high number of tasks incur delays for smaller tables \"\n+          + \"or low latency writes.\");\n+\n+  public static final ConfigProperty<String> MARKERS_DELETE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.markers.delete.parallelism\")\n+      .defaultValue(\"100\")\n+      .withDocumentation(\"Determines the parallelism for deleting marker files, which are used to track all files (valid or invalid/partial) written during \"\n+          + \"a write operation. Increase this value if delays are observed, with large batch writes.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_SORT_MODE = ConfigProperty\n+      .key(\"hoodie.bulkinsert.sort.mode\")\n+      .defaultValue(BulkInsertSortMode.GLOBAL_SORT.toString())\n+      .withDocumentation(\"Sorting modes to use for sorting records for bulk insert. This is user when user \"\n+          + BULKINSERT_USER_DEFINED_PARTITIONER_CLASS.key() + \"is not configured. Available values are - \"\n+          + \"GLOBAL_SORT: this ensures best file sizes, with lowest memory overhead at cost of sorting. \"\n+          + \"PARTITION_SORT: Strikes a balance by only sorting within a partition, still keeping the memory overhead of writing \"\n+          + \"lowest and best effort file sizing. \"\n+          + \"NONE: No sorting. Fastest and matches `spark.write.parquet()` in terms of number of files, overheads\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_ENABLED = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"When true, spins up an instance of the timeline server (meta server that serves cached file listings, statistics),\"\n+          + \"running on each writer's driver process, accepting requests during the write from executors.\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.reuse.enabled\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Controls whether the timeline server instance should be cached and reused across the JVM (across task lifecycles)\"\n+          + \"to avoid startup costs. This should rarely be changed.\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_PORT = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.port\")\n+      .defaultValue(\"0\")\n+      .withDocumentation(\"Port at which the timeline server listens for requests. When running embedded in each writer, it picks \"\n+          + \"a free port and communicates to all the executors. This should rarely be changed.\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_THREADS = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.threads\")\n+      .defaultValue(\"-1\")\n+      .withDocumentation(\"Number of threads to serve requests in the timeline server. By default, auto configured based on the number of underlying cores.\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.gzip\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Controls whether gzip compression is used, for large responses from the timeline server, to improve latency.\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_USE_ASYNC = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Controls whether or not, the requests to the timeline server are processed in asynchronous fashion, \"\n+          + \"potentially improving throughput.\");\n+\n+  public static final ConfigProperty<String> FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.fail.on.timeline.archiving\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Timeline archiving removes older instants from the timeline, after each write operation, to minimize metadata overhead. \"\n+          + \"Controls whether or not, the write should be failed as well, if such archiving fails.\");\n+\n+  public static final ConfigProperty<Long> INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.initial_interval_ms\")\n+      .defaultValue(2000L)\n+      .withDocumentation(\"Initial time between successive attempts to ensure written data's metadata is consistent on storage. Grows with exponential\"\n+          + \" backoff after the initial value.\");\n+\n+  public static final ConfigProperty<Long> MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.max_interval_ms\")\n+      .defaultValue(300000L)\n+      .withDocumentation(\"Max time to wait between successive attempts at performing consistency checks\");\n+\n+  public static final ConfigProperty<Integer> MAX_CONSISTENCY_CHECKS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.max_checks\")\n+      .defaultValue(7)\n+      .withDocumentation(\"Maximum number of checks, for consistency of written data.\");\n+\n+  public static final ConfigProperty<String> MERGE_DATA_VALIDATION_CHECK_ENABLED = ConfigProperty\n+      .key(\"hoodie.merge.data.validation.enabled\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When enabled, data validation checks are performed during merges to ensure expected \"\n+          + \"number of records after merge operation.\");\n+\n+  public static final ConfigProperty<String> MERGE_ALLOW_DUPLICATE_ON_INSERTS = ConfigProperty\n+      .key(\"hoodie.merge.allow.duplicate.on.inserts\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When enabled, we allow duplicate keys even if inserts are routed to merge with an existing file (for ensuring file sizing).\"\n+          + \" This is only relevant for insert operation, since upsert, delete operations will ensure unique key constraints are maintained.\");\n+\n+  public static final ConfigProperty<ExternalSpillableMap.DiskMapType> SPILLABLE_DISK_MAP_TYPE = ConfigProperty\n+      .key(\"hoodie.spillable.diskmap.type\")\n+      .defaultValue(ExternalSpillableMap.DiskMapType.BITCASK)\n+      .withDocumentation(\"When handling input data that cannot be held in memory, to merge with a file on storage, a spillable diskmap is employed.  \"\n+          + \"By default, we use a persistent hashmap based loosely on bitcask, that offers O(1) inserts, lookups. \"\n+          + \"Change this to `ROCKS_DB` to prefer using rocksDB, for handling the spill.\");\n+\n+  public static final ConfigProperty<Integer> CLIENT_HEARTBEAT_INTERVAL_IN_MS_PROP = ConfigProperty\n+      .key(\"hoodie.client.heartbeat.interval_in_ms\")\n+      .defaultValue(60 * 1000)\n+      .withDocumentation(\"Writers perform heartbeats to indicate liveness. Controls how often (in ms), such heartbeats are registered to lake storage.\");\n+\n+  public static final ConfigProperty<Integer> CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES_PROP = ConfigProperty\n+      .key(\"hoodie.client.heartbeat.tolerable.misses\")\n+      .defaultValue(2)\n+      .withDocumentation(\"Number of heartbeat misses, before a writer is deemed not alive and all pending writes are aborted.\");\n+\n+  public static final ConfigProperty<String> WRITE_CONCURRENCY_MODE_PROP = ConfigProperty\n+      .key(\"hoodie.write.concurrency.mode\")\n+      .defaultValue(WriteConcurrencyMode.SINGLE_WRITER.name())\n+      .withDocumentation(\"Enable different concurrency modes. Options are \"\n+          + \"SINGLE_WRITER: Only one active writer to the table. Maximizes throughput\"\n+          + \"OPTIMISTIC_CONCURRENCY_CONTROL: Multiple writers can operate on the table and exactly one of them succeed \"\n+          + \"if a conflict (writes affect the same file group) is detected.\");\n+\n+  public static final ConfigProperty<String> WRITE_META_KEY_PREFIXES_PROP = ConfigProperty\n+      .key(\"hoodie.write.meta.key.prefixes\")\n+      .defaultValue(\"\")\n+      .withDocumentation(\"Comma separated metadata key prefixes to override from latest commit \"\n+          + \"during overlapping commits via multi writing\");\n \n   /**\n-   * The specified write schema. In most case, we do not need set this parameter,\n-   * but for the case the write schema is not equal to the specified table schema, we can\n-   * specify the write schema by this parameter.\n-   *\n-   * Currently the MergeIntoHoodieTableCommand use this to specify the write schema.\n+   * Currently the  use this to specify the write schema.\n    */\n-  public static final String WRITE_SCHEMA_PROP = \"hoodie.write.schema\";\n+  public static final ConfigProperty<String> WRITE_SCHEMA_PROP = ConfigProperty\n+      .key(\"hoodie.write.schema\")\n+      .noDefaultValue()\n+      .withDocumentation(\"The specified write schema. In most case, we do not need set this parameter,\"\n+          + \" but for the case the write schema is not equal to the specified table schema, we can\"\n+          + \" specify the write schema by this parameter. Used by MergeIntoHoodieTableCommand\");\n \n   /**\n    * HUDI-858 : There are users who had been directly using RDD APIs and have relied on a behavior in 0.4.x to allow\n", "next_change": {"commit": "b6124ff85a107ab170430947a24bc71df8612f1c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 357e3f574e..b82d10b0ef 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -323,21 +438,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n           + \"OPTIMISTIC_CONCURRENCY_CONTROL: Multiple writers can operate on the table and exactly one of them succeed \"\n           + \"if a conflict (writes affect the same file group) is detected.\");\n \n-  public static final ConfigProperty<String> WRITE_META_KEY_PREFIXES_PROP = ConfigProperty\n-      .key(\"hoodie.write.meta.key.prefixes\")\n-      .defaultValue(\"\")\n-      .withDocumentation(\"Comma separated metadata key prefixes to override from latest commit \"\n-          + \"during overlapping commits via multi writing\");\n-\n-  /**\n-   * Currently the  use this to specify the write schema.\n-   */\n-  public static final ConfigProperty<String> WRITE_SCHEMA_PROP = ConfigProperty\n+  public static final ConfigProperty<String> WRITE_SCHEMA_OVERRIDE = ConfigProperty\n       .key(\"hoodie.write.schema\")\n       .noDefaultValue()\n-      .withDocumentation(\"The specified write schema. In most case, we do not need set this parameter,\"\n-          + \" but for the case the write schema is not equal to the specified table schema, we can\"\n-          + \" specify the write schema by this parameter. Used by MergeIntoHoodieTableCommand\");\n+      .withDocumentation(\"Config allowing to override writer's schema. This might be necessary in \"\n+          + \"cases when writer's schema derived from the incoming dataset might actually be different from \"\n+          + \"the schema we actually want to use when writing. This, for ex, could be the case for\"\n+          + \"'partial-update' use-cases (like `MERGE INTO` Spark SQL statement for ex) where only \"\n+          + \"a projection of the incoming dataset might be used to update the records in the existing table, \"\n+          + \"prompting us to override the writer's schema\");\n \n   /**\n    * HUDI-858 : There are users who had been directly using RDD APIs and have relied on a behavior in 0.4.x to allow\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "77ba561a6bacbf9e7dc9c1582eb068f7ad800cd9", "committedDate": "2021-02-23 20:56:58 -0500", "message": "[HUDI-1347] Fix Hbase index to make rollback synchronous (via config) (#2188)"}, {"oid": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "committedDate": "2021-03-02 21:58:41 -0800", "message": "[HUDI-1553] Configuration and metrics for the TimelineService. (#2495)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "7fed7352bd506e20e5316bb0b3ed9e5c1e9c76df", "committedDate": "2021-05-27 13:38:33 +0800", "message": "[HUDI-1865] Make embedded time line service singleton (#2899)"}, {"oid": "f760ec543ec9ea23b7d4c9f61c76a283bd737f27", "committedDate": "2021-06-07 23:24:32 -0700", "message": "[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)"}, {"oid": "7261f0850727aea611cd34e1bb07d684b44534f6", "committedDate": "2021-06-08 09:26:10 -0400", "message": "[HUDI-1929] Support configure KeyGenerator by type (#2993)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "b376cefc3e899aa28992796925708746561d6087", "committedDate": "2021-07-01 18:48:59 +0800", "message": "[MINOR] Add Documentation to KEYGENERATOR_TYPE_PROP (#3196)"}, {"oid": "62a1ad8b3a2a3c1dabba0a4622117636920b6c13", "committedDate": "2021-07-03 20:27:37 +0800", "message": "[HUDI-1930] Bootstrap support configure KeyGenerator by type (#3170)"}, {"oid": "a4dcbb5c5a2a94e4f69524194d8777d082af31ab", "committedDate": "2021-07-05 23:03:41 -0700", "message": "[HUDI-2028] Implement RockDbBasedMap as an alternate to DiskBasedMap in ExternalSpillableMap (#3194)"}, {"oid": "ea9e5d0e8b7557ef82631ac173d67f15bad13690", "committedDate": "2021-07-07 11:15:25 -0400", "message": "[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)"}, {"oid": "5804ad8e32ae05758ebc5e47f5d4fb4db371ab52", "committedDate": "2021-07-11 14:43:38 -0400", "message": "[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)"}, {"oid": "b0089b894ad12da11fbd6a0fb08508c7adee68e6", "committedDate": "2021-07-13 00:24:40 -0400", "message": "[MINOR] Fix EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION config (#3250)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "d024439764ceeca6366cb33689b729a1c69a6272", "committedDate": "2021-07-14 22:57:38 -0400", "message": "[HUDI-2029] Implement compression for DiskBasedMap in Spillable Map (#3128)"}, {"oid": "38cd74b56328d154c004f04f1335784529d8e93d", "committedDate": "2021-07-16 12:24:41 +0800", "message": "[MINOR] Allow users to choose ORC as base file format in Spark SQL (#3279)"}, {"oid": "d5026e9a24850bdcce9f6df3686bf2235d7d01c4", "committedDate": "2021-07-19 20:43:48 -0400", "message": "[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3", "committedDate": "2021-07-26 17:21:04 -0400", "message": "[HUDI-2176, 2178, 2179] Adding virtual key support to COW table (#3306)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "bbadac7de1bb57300ca7e796ebb401fdbb66a0f8", "committedDate": "2021-07-28 21:30:18 -0700", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write (#2296)"}, {"oid": "826a04d1425f47fdd80c293569a359021d1b6586", "committedDate": "2021-08-03 12:07:45 -0700", "message": "[HUDI-2072] Add pre-commit validator framework (#3153)"}, {"oid": "91bb0d13184c57ec08f02db3337e734bc20739c4", "committedDate": "2021-08-03 17:50:30 -0700", "message": "[HUDI-2255] Refactor Datasource options (#3373)"}, {"oid": "70b6bd485f8d1ef9a9b15999edda2472d0b4d65a", "committedDate": "2021-08-06 22:53:08 -0400", "message": "[HUDI-1468] Support custom clustering strategies and preserve commit metadata as part of clustering (#3419)"}, {"oid": "b4441abcf74951ec0ce28593b96baa84456a97d3", "committedDate": "2021-08-09 10:10:15 -0700", "message": "[HUDI-2194] Skip the latest N partitions when choosing partitions to create ClusteringPlan (#3300)"}, {"oid": "21db6d7a84d4a83ec98c110e92ff9c92d05dd530", "committedDate": "2021-08-10 20:23:23 +0800", "message": "[HUDI-1771] Propagate CDC format for hoodie (#3285)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "76bc686a77a485544c9e75cfefa59fa021470a0c", "committedDate": "2021-08-12 15:45:57 -0700", "message": "[HUDI-1292] Created a config to enable/disable syncing of metadata table. (#3427)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "642b1b671de8c6a35ae7858c9b03d3dff70889dd", "committedDate": "2021-08-13 19:29:22 -0400", "message": "[HUDI-2151]  Flipping defaults (#3452)"}, {"oid": "9056c68744a3f31ac2625e004ec6e155d2e86be9", "committedDate": "2021-08-14 08:18:49 -0400", "message": "[HUDI-2305] Add MARKERS.type and fix marker-based rollback (#3472)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "e39d0a2f2852ef51c524e5b16a1cecb099674eed", "committedDate": "2021-08-20 02:42:59 -0700", "message": "Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)"}, {"oid": "de94787a85b272f79181dff73907b0f20855ee78", "committedDate": "2021-08-24 21:45:17 +0800", "message": "[HUDI-2345] Hoodie columns sort partitioner for bulk insert (#3523)"}, {"oid": "21fd6edfe7721c674b40877fbbdbac71b36bf782", "committedDate": "2021-09-02 11:14:09 +0800", "message": "[HUDI-2384] Change log file size config to long (#3577)"}, {"oid": "e528dd798ab8ce6e4d444d2d771c107c503e8f25", "committedDate": "2021-09-10 18:20:26 -0700", "message": "[HUDI-2394] Implement Kafka Sink Protocol for Hudi for Ingesting Immutable Data (#3592)"}, {"oid": "2791fb9a964b39ef9aaec83eafd080013186b2eb", "committedDate": "2021-09-16 15:08:10 +0800", "message": "[HUDI-2423] Separate some config logic from HoodieMetricsConfig into HoodieMetricsGraphiteConfig HoodieMetricsJmxConfig (#3652)"}, {"oid": "61d009608899bc70c1372d5cb00a2f35e188c30c", "committedDate": "2021-09-17 19:39:55 +0800", "message": "[HUDI-2434] Make periodSeconds of GraphiteReporter configurable (#3667)"}, {"oid": "06c2cc2c8b1ad88bb4c9bbdb496053a079767e9b", "committedDate": "2021-09-24 13:33:34 +0800", "message": "[HUDI-2385] Make parquet dictionary encoding configurable (#3578)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "08c35a55b3133ddaead0581c9129e88a869421a1", "committedDate": "2021-11-05 13:03:41 -0400", "message": "[HUDI-2526] Make spark.sql.parquet.writeLegacyFormat configurable (#3917)"}, {"oid": "dfe3b84715e8fecfa96ef615c217f5eaf0da94e8", "committedDate": "2021-11-09 17:37:59 -0500", "message": "[HUDI-2579] Make deltastreamer checkpoint state merging more explicit (#3820)"}, {"oid": "4f217fe718b0b4e9656c2a45f7b89cb5df15a4f2", "committedDate": "2021-11-12 07:29:37 -0500", "message": "[HUDI-2151] Part1 Setting default parallelism to 200 for some of write configs (#3948)"}, {"oid": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "committedDate": "2021-11-13 09:12:33 +0800", "message": "[HUDI-2697] Minor changes about hbase index config. (#3927)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "ce7d2333078e4e1f16de1bce6d448c5eef1e4111", "committedDate": "2021-11-17 11:51:28 +0530", "message": "[HUDI-2151] Part3 Enabling marker based rollback as default rollback strategy (#3950)"}, {"oid": "2d3f2a3275ba615245fcabda96b8282cb86940ad", "committedDate": "2021-11-17 14:43:00 -0500", "message": "[HUDI-2734] Setting default metadata enable as false for Java (#4003)"}, {"oid": "3bdab01a498d605faede833af2d88cd8ec9237a0", "committedDate": "2021-11-22 19:19:59 -0500", "message": "[HUDI-2550] Expand File-Group candidates list for appending for MOR tables (#3986)"}, {"oid": "e22150fe15da2985b20077d2e0734fcd46b85a6f", "committedDate": "2021-11-23 07:29:03 +0530", "message": "[HUDI-1937] Rollback unfinished replace commit to allow updates (#3869)"}, {"oid": "ca9bfa2a4000575dbaa379c91898786f040a9917", "committedDate": "2021-11-23 14:23:28 +0530", "message": "[HUDI-2332] Add clustering and compaction in Kafka Connect Sink (#3857)"}, {"oid": "435ea1543c034194d7ca0b589b7b043fc49c07ac", "committedDate": "2021-11-24 18:26:40 -0500", "message": "[HUDI-2793] Fixing deltastreamer checkpoint fetch/copy over (#4034)"}, {"oid": "88067f57a23575aae3c371a7c7871e4207ca3bea", "committedDate": "2021-11-25 19:17:38 +0800", "message": "[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)"}, {"oid": "e0125a7911d77afd4a82a49caaccaa3c10df0377", "committedDate": "2021-11-25 13:33:16 -0800", "message": "[HUDI-2801] Add Amazon CloudWatch metrics reporter (#4081)"}, {"oid": "d1e83e4ba0b881f9410f0ae9f3799c967b6891cb", "committedDate": "2021-11-26 16:41:05 -0500", "message": "[HUDI-2767] Enabling timeline-server-based marker as default (#4112)"}, {"oid": "24380c20606d63c7c129cb45a73f786f223e7d39", "committedDate": "2021-11-30 17:47:16 -0800", "message": "Revert \"[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)\" (#4169)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "9797fdfbb27ca8f5f06875ad958b597becc27a8d", "committedDate": "2021-12-10 19:42:20 -0800", "message": "[HUDI-2974] Make the prefix for metrics name configurable (#4274)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "2444f40a4be5bbf0bf210dee5690267a9a1e35c8", "committedDate": "2021-12-31 11:07:52 +0530", "message": "[HUDI-3095] abstract partition filter logic to enable code reuse (#4454)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "827549949c4ac472fdc528a35ae421b20e2cc83a", "committedDate": "2022-01-08 10:22:44 -0500", "message": "[HUDI-2909] Handle logical type in TimestampBasedKeyGenerator (#4203)"}, {"oid": "251d4eb3b64704b9dd51bf6f6ecb5bf47089b745", "committedDate": "2022-01-10 08:40:24 +0530", "message": "[HUDI-3030] InProcessLockPovider as default when any async servcies enabled with no lock provider override (#4406)"}, {"oid": "9fe28e56b49c7bf68ae2d83bfe89755314aa793b", "committedDate": "2022-01-11 23:23:55 -0800", "message": "[HUDI-3045] New clustering regex match config to choose partitions when building clustering plan (#4346)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "14d08bb64c4bea20a692b3d3bced5cc9800cd541", "committedDate": "2022-01-20 15:34:56 +0400", "message": "[MINOR] Fix typo in the doc of BULK_INSERT_SORT_MODE (#4652)"}, {"oid": "bc7882cbe924ce8000f4a738b8673fe7a5cf69fb", "committedDate": "2022-01-24 16:53:54 -0500", "message": "[HUDI-2872][HUDI-2646] Refactoring layout optimization (clustering) flow to support linear ordering (#4606)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "538ec44fa8a23926b584c3bcdd24feb9894d4c51", "committedDate": "2022-02-15 09:49:53 -0500", "message": "[HUDI-2931] Add config to disable table services (#4777)"}, {"oid": "359fbfde798b50edc06ee1d0520efcd971a289bc", "committedDate": "2022-02-20 15:31:31 -0500", "message": "[HUDI-2648] Retry FileSystem action instead of failed directly. (#3887)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "92cdc5987a2b5a6faecec96224f545ab49ee6ef2", "committedDate": "2022-02-25 11:30:10 -0500", "message": "[HUDI-3515] Making rdd unpersist optional at the end of writes (#4898)"}, {"oid": "62f534d00228653059c4fed944d444632bc07091", "committedDate": "2022-03-04 09:33:16 +0800", "message": "[HUDI-3445] Support Clustering Command Based on Call Procedure Command for Spark SQL (#4901)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "f0bcee3c014cf59bdad3eaf8212d94a589073f0b", "committedDate": "2022-03-07 13:42:03 -0500", "message": "[HUDI-3561] Avoid including whole `MultipleSparkJobExecutionStrategy` object into the closure for Spark to serialize (#4954)"}, {"oid": "29040762fa511f89e678dc15ca5ae7f9f097fb8a", "committedDate": "2022-03-07 17:01:49 -0500", "message": "[HUDI-3576] Configuring timeline refreshes based on latest commit (#4973)"}, {"oid": "575bc6346825796e091a12be5a53b04980f82637", "committedDate": "2022-03-08 10:39:04 -0500", "message": "[HUDI-3356][HUDI-3203] HoodieData for metadata index records; BloomFilter construction from index based on the type param (#4848)"}, {"oid": "034addaef5834eff09cfd9ac5cc2656df95ca0e8", "committedDate": "2022-03-09 21:45:25 -0500", "message": "[HUDI-3396] Make sure `BaseFileOnlyViewRelation` only reads projected columns (#4818)"}, {"oid": "95e6e538109af9fe60aa99219e4aa1d7ce9511e2", "committedDate": "2022-03-17 01:25:04 -0400", "message": "[HUDI-3404] Automatically adjust write configs based on metadata table and write concurrency mode (#4975)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "5f570ea151d0212ab1bb2d1f5693035626b76d31", "committedDate": "2022-03-21 22:56:31 -0400", "message": "[HUDI-2883] Refactor hive sync tool / config to use reflection and standardize configs (#4175)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "fb45fc9cb9581abc40922ddcbee21dfc016d4edc", "committedDate": "2022-04-01 20:14:07 -0700", "message": "[HUDI-3773] Fix parallelism used for metadata table bloom filter index (#5209)"}, {"oid": "84064a9b081c246f306855ae125f0dae5eb8f6d0", "committedDate": "2022-04-02 23:44:10 -0700", "message": "[HUDI-3772] Fixing auto adjustment of lock configs for deltastreamer (#5207)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "3e97c88c4ff9cbeb312805daf6d52da5f1bcb0bd", "committedDate": "2022-04-09 15:30:11 -0400", "message": "[HUDI-3807] Add a new config to control the use of metadata index in HoodieBloomIndex (#5268)"}, {"oid": "bab691692e31ae3e432bb6cf4c90436fba408a2c", "committedDate": "2022-04-13 17:33:26 -0400", "message": "[HUDI-3686] Fix inline and async table service check in HoodieWriteConfig (#5307)"}, {"oid": "4e928a6fe1ddd7e126ab155bb1c03f8630bb873d", "committedDate": "2022-04-28 15:18:56 -0700", "message": "[HUDI-3943] Some description fixes for 0.10.1 docs (#5447)"}, {"oid": "f492c52ee4d2f3d6dfbbf574833f837322166fbd", "committedDate": "2022-04-29 16:21:52 -0700", "message": "[HUDI-3862] Fix default configurations of HoodieHBaseIndexConfig (#5308)"}, {"oid": "6e16e719cd614329018cd34a7c57d342fe2fa376", "committedDate": "2022-05-14 07:37:31 -0400", "message": "[HUDI-3980] Suport kerberos hbase index (#5464)"}, {"oid": "61030d8e7a5a05e215efed672267ac163b0cbcf6", "committedDate": "2022-05-16 11:07:01 +0800", "message": "[HUDI-3123] consistent hashing index: basic write path (upsert/insert) (#4480)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "cf837b49008fd351a3f89beb5e4e5c17c30b9a3c", "committedDate": "2022-05-25 19:38:56 +0530", "message": "[HUDI-3193] Decouple hudi-aws from hudi-client-common (#5666)"}, {"oid": "7f8630cc57fbb9d29e8dc7ca87b582264da073fd", "committedDate": "2022-06-02 09:48:48 +0800", "message": "[HUDI-4167] Remove the timeline refresh with initializing hoodie table (#5716)"}, {"oid": "4f6fc726d0d3d2dd427210228bbb36cf18893a92", "committedDate": "2022-06-06 10:21:00 -0700", "message": "[HUDI-4140] Fixing hive style partitioning and default partition with bulk insert row writer with SimpleKeyGen and virtual keys (#5664)"}, {"oid": "35afdb4316d496bbb37ebb9e1598d84bd8a4000d", "committedDate": "2022-06-07 16:30:46 -0700", "message": "[HUDI-4178] Addressing performance regressions in Spark DataSourceV2 Integration (#5737)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "da28e38fe3d25e3ab212dc02312f0ae395371072", "committedDate": "2022-07-23 14:37:04 -0500", "message": "[HUDI-4071] Make NONE sort mode as default for bulk insert (#6195)"}, {"oid": "a0ffd05b7773c4c83714a60dcaa79332ee3aada3", "committedDate": "2022-07-23 16:10:53 -0700", "message": "[HUDI-4448] Remove the latest commit refresh for timeline server (#6179)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "e5faf2cc8470f83323b34305a135461c7e43d14e", "committedDate": "2022-07-26 18:09:17 +0800", "message": "[HUDI-4210] Create custom hbase index to solve data skew issue on hbase regions (#5797)"}, {"oid": "cdaec5a8da060157eb7426b5b70419c5f8868e04", "committedDate": "2022-07-27 14:47:49 -0700", "message": "[HUDI-4186] Support Hudi with Spark 3.3.0 (#5943)"}, {"oid": "767c196631240aeda5a8ef4603c17f05a407d8f7", "committedDate": "2022-08-06 18:19:29 -0400", "message": "[HUDI-4303] Adding 4 to 5 upgrade handler to check for old deprecated \"default\" partition value (#6248)"}, {"oid": "6badae46f0f3e743f0502bd34e124cf6cfabcec0", "committedDate": "2022-09-12 12:05:40 +0530", "message": "[HUDI-3558] Consistent bucket index: bucket resizing (split&merge) & concurrent write during resizing (#4958)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5e624698f78f4707d62c7f26b044a69a250aae43", "committedDate": "2022-09-22 09:17:09 -0400", "message": "[HUDI-4363] Support Clustering row writer to improve performance (#6046)"}, {"oid": "efe553b327bc025d242afa37221a740dca9b1ea6", "committedDate": "2022-09-23 18:36:48 +0800", "message": "[HUDI-4897] Refactor the merge handle in CDC mode (#6740)"}, {"oid": "58fe71d2b316a83b30ce4b80a36dd7beed001e58", "committedDate": "2022-09-29 01:37:46 -0400", "message": "[HUDI-4722] Added locking metrics for Hudi (#6502)"}, {"oid": "f3d4ce919d4909f9533255ee2a9a0450c8e44c73", "committedDate": "2022-10-01 18:21:23 +0800", "message": "[HUDI-4916] Implement change log feed for Flink (#6840)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "a5434b6b4d9bef9eea29bf33f08e7f13753057a9", "committedDate": "2022-11-02 20:02:18 -0400", "message": "[HUDI-3963] Use Lock-Free Message Queue Disruptor Improving Hoodie Writing Efficiency  (#5416)"}, {"oid": "6b73c814f931f8dcdec500a4364354ac9488ce68", "committedDate": "2022-11-17 01:54:54 +0800", "message": "[HUDI-5209] Fixing `QueueBasedExecutor` in Spark bundles (missing Disruptor as dep) (#7188)"}, {"oid": "91e0db57b94c479a73e4f78d571b50c1e5ea541f", "committedDate": "2022-11-23 09:04:20 +0800", "message": "[MINOR] Use direct marker for spark engine when timeline server is disabled (#7272)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "1cdbf68d4ee641b9e7eb0129a26e0f969b37d8ca", "committedDate": "2022-11-28 22:48:06 -0500", "message": "[HUDI-5242] Do not fail Meta sync in Deltastreamer when inline table service fails (#7243)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "8d13a7e383c30ec1421b68dd052fbd33f438bc3d", "committedDate": "2022-12-15 09:18:54 -0800", "message": "[HUDI-5023] Consuming records from Iterator directly instead of using inner message queue (#7174)"}, {"oid": "16d33ba3cb953435660566ba2ec63a45204a7814", "committedDate": "2023-01-15 21:46:12 -0800", "message": "[HUDI-3654] Add new module `hudi-metaserver` (#5064)"}, {"oid": "c9bc03ed8681ab64eea2520f5511464915389c51", "committedDate": "2023-01-17 07:24:16 -0800", "message": "[HUDI-4148] Add client for Hudi table service manager (TSM) (#6732)"}, {"oid": "ec5022b4fdd94c106bf243038aadf781f31b5be9", "committedDate": "2023-01-18 09:30:52 -0800", "message": "[MINOR] Unify naming for record merger (#7660)"}, {"oid": "c18d6153e105ac34fb410c60ce8a153327931782", "committedDate": "2023-01-23 10:03:19 -0800", "message": "[HUDI-1575] Early Conflict Detection For Multi-writer (#6133)"}, {"oid": "2fc20c186b77017f7f1c6f6abe8559a9e8cfe578", "committedDate": "2023-01-24 20:04:55 +0530", "message": "[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)"}, {"oid": "c95abd3213f4806f535c6d7cb8b346616c3368fb", "committedDate": "2023-01-25 19:01:33 +0530", "message": "Revert \"[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)\" (#7747)"}, {"oid": "7e35874c7ba68dfa32b3b27ece35112cc434a7c6", "committedDate": "2023-01-25 14:34:18 -0800", "message": "[HUDI-5617] Rename configs for async conflict detector for clarity (#7750)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3a08bdc3f971b3534e8fb6f34772340cfdf055a9", "committedDate": "2023-01-25 19:29:42 -0800", "message": "[HUDI-5363] Removing default value for shuffle parallelism configs (#7723)"}, {"oid": "ff590c6d72c523b41c0790087053fd3933564ac8", "committedDate": "2023-01-27 18:56:32 -0800", "message": "[HUDI-5023] Switching default Write Executor type to `SIMPLE` (#7476)"}, {"oid": "c21eca564c6426413cbdc9e83bc40ad7c59c7e5d", "committedDate": "2023-01-28 13:23:30 -0500", "message": "[HUDI-5626] Rename CDC logging mode options (#7760)"}, {"oid": "2c56aa4ce994714a57402399c1bec99579926f66", "committedDate": "2023-01-28 14:03:02 -0600", "message": "[HUDI-5631] Improve defaults of early conflict detection configs (#7770)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "88d8e5e96d5f5ee553b8405e32c79388e3ed3c09", "committedDate": "2023-01-29 14:57:05 -0800", "message": "[MINOR] Cleaning up recently introduced configs (#7772)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "7064c380506814964dd85773e2ee7b7f187b88c3", "committedDate": "2023-02-01 11:19:45 -0800", "message": "[MINOR] Restoring existing behavior for `DeltaStreamer` Incremental Source (#7810)"}, {"oid": "ef3a17e3d97428a6f0d6e7ac888747d65a8792c5", "committedDate": "2023-02-04 15:14:52 +0800", "message": "[HUDI-5692] SpillableMapBasePath should be lazily loaded (#7837)"}, {"oid": "ff832f4d86091b71af42cc46c2aa209d80396899", "committedDate": "2023-02-04 17:39:58 +0800", "message": "[MINOR] Validate configs for OCC early conflict detection (#7848)"}, {"oid": "0c9465f2ab6cf6472df3046c681b72290e6034bd", "committedDate": "2023-02-05 00:28:20 -0800", "message": "[MINOR] Improve configuration configs (#7855)"}, {"oid": "53fca761be1db224dc384238a7d2853ff2a1d227", "committedDate": "2023-02-07 19:59:06 -0500", "message": "[MINOR] fixed docs for WRITE_EXECUTOR_TYPE (#7880)"}, {"oid": "be92be657a348954cc21062ca24e8a10caea17ee", "committedDate": "2023-02-20 10:05:09 +0800", "message": "[HUDI-5786] Add a new config to specific spark write rdd storage level (#7941)"}, {"oid": "0c84482aa915611db17f4505974c876240a8101c", "committedDate": "2023-02-20 20:00:31 +0530", "message": "[HUDI-5774] Fix prometheus configs for metadata table and support metric labels (#7933)"}, {"oid": "d705dcc4188223fbd824f36a5d211abeda7b1f23", "committedDate": "2023-02-24 10:23:25 +0800", "message": "[HUDI-5173] Skip if there is only one file in clusteringGroup  (#7159)"}, {"oid": "e131505a4f0bfef6e8cca04e102b3ea1c6d308e2", "committedDate": "2023-02-27 12:26:50 +0530", "message": "[HUDI-5838] Mask sensitive info while printing hudi properties in DeltaStreamer  (#8027)"}, {"oid": "c7cebf445f4dc6895cc343a396b0ea1871b362e7", "committedDate": "2023-02-27 23:57:08 -0500", "message": "[HUDI-5843] multiwriter deltastreamer checkpoints (#8043)"}, {"oid": "81e6e854883a94d41ae5b7187c608a8ddbc7bf35", "committedDate": "2023-03-03 21:06:43 +0530", "message": "[HUDI-5847] Add support for multiple metric reporters and metric labels (#8041)"}, {"oid": "a8312a9b8c39f4baabf753974fa092c4767abb72", "committedDate": "2023-03-08 14:46:01 +0800", "message": "[HUDI-5887] Distinguish the single writer enabling metadata table and multi-writer use cases for lock guard (#8111)"}, {"oid": "38e4078d23b00d0acdd02a28eec08560d175cdc9", "committedDate": "2023-03-13 21:30:19 -0700", "message": "[MINOR] Ignoring warn msg for timeline server for metadata table (#8168)"}, {"oid": "e51b4575cb7642eb61bcc02d95c99466dd3e8eda", "committedDate": "2023-03-17 15:17:24 -0700", "message": "[HUDI-5920] Improve documentation of parallelism configs (#8157)"}, {"oid": "cb1395a820febc4bb13c544369ca94a55fff0a29", "committedDate": "2023-03-30 11:07:29 -0700", "message": "[HUDI-5893] Mark advanced configs (#8295)"}, {"oid": "c53d9fbe019a43f31b3eb7556ff109d71287cf6c", "committedDate": "2023-03-31 13:30:34 -0700", "message": "[HUDI-5900] Clean up unused metadata configs (#8125)"}, {"oid": "6fd885fb3dc5c66caf6a1775fa87b4f7212056d8", "committedDate": "2023-03-31 21:21:28 -0700", "message": "[HUDI-5740] Refactor Deltastreamer and schema providers to use HoodieConfig/ConfigProperty (#8152)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "f9f110695fc69f2d7085648a6610888bb10ad8e4", "committedDate": "2023-04-19 04:09:48 -0400", "message": "[HUDI-6056] Validate archival configs alignment with cleaner configs with policy based on hours (#8422)"}, {"oid": "44a0c29560db2d85e3b0d963beed55225596baff", "committedDate": "2023-04-21 09:50:34 +0800", "message": "[HUDI-6100] Fixed overflow in setting log block size by making it long everywhere (#8495)"}, {"oid": "5a5c4863452197def89390beb0ab584eb08aabb6", "committedDate": "2023-04-21 23:06:46 -0700", "message": "[HUDI-5934] Remove archival configs for metadata table (#8319)"}, {"oid": "9a9dd3a82d3e69f1d5eebe46c79c8fd0dc2355db", "committedDate": "2023-04-23 16:37:48 +0800", "message": "[HUDI-6123] Auto adjust lock configs for single writer (#8542)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "e04dc0951cf21122f0d3dd4f673b87b663253109", "committedDate": "2023-05-04 04:05:13 -0700", "message": "[HUDI-5315] Use sample writes to estimate record size (#8390)"}, {"oid": "cabcb2bf2cddedeb3a34047af3935b27cfdfb858", "committedDate": "2023-05-05 06:28:14 -0700", "message": "[HUDI-5968] Fix global index duplicate and handle custom payload when update partition (#8490)"}, {"oid": "66e838c8f18b8ca50a3839f4768da5edfed0c416", "committedDate": "2023-05-06 15:09:03 +0800", "message": "[MINOR] Remove redundant advanced config marking (#8600)"}, {"oid": "6cdc1f583ef8d1281d2171c42fc982e2715f08f8", "committedDate": "2023-05-08 07:29:57 -0700", "message": "[HUDI-5895] Remove bootstrap key generator configs (#8557)"}, {"oid": "a5bd50c067f2a82be2470d4649f2be3007404c40", "committedDate": "2023-05-23 15:54:42 +0800", "message": "[MINOR] Disable schema validation in master (#8781)"}, {"oid": "9d58ee4b1f1fce213ee3f4ff478eb003da943924", "committedDate": "2023-05-24 20:09:54 +0800", "message": "[HUDI-5994] Bucket index supports bulk insert row writer (#8776)"}, {"oid": "59786113fae88382f03412c7c79f7a827bf9393f", "committedDate": "2023-05-26 12:06:32 +0530", "message": "[HUDI-5998] Speed up reads from bootstrapped tables in spark (#8303)"}, {"oid": "41e1e9a4fda2e399f4d50222e81bdfa713bbce23", "committedDate": "2023-05-30 14:58:33 -0700", "message": "[MINOR] Ensure metrics prefix does not contain any dot. (#8599)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc3Mw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350773", "body": "please revert this change.", "bodyText": "please revert this change.", "bodyHTML": "<p dir=\"auto\">please revert this change.</p>", "author": "yanghua", "createdAt": "2021-01-18T07:01:16Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  ", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4259478b945ea0440e0c28c56dc6290ee2df4442", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..ccf53a56a4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ccf53a56a4..b1155e3bc9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+\n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "a318c91498d1899107f886b73c3404cd5f50258f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex b1155e3bc9..251ea2305f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..e3c1ef6819 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +485,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex e3c1ef6819..4e493e4432 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -485,7 +515,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+\n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4e493e4432..1783535e82 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -372,251 +562,251 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n    * compaction properties.\n    */\n   public HoodieCleaningPolicy getCleanerPolicy() {\n-    return HoodieCleaningPolicy.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n+    return HoodieCleaningPolicy.valueOf(getString(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n   }\n \n   public int getCleanerFileVersionsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP);\n   }\n \n   public int getCleanerCommitsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP);\n   }\n \n   public int getMaxCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getMinCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getParquetSmallFileLimit() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES));\n+    return getInt(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES);\n   }\n \n   public double getRecordSizeEstimationThreshold() {\n-    return Double.parseDouble(props.getProperty(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP));\n+    return getDouble(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP);\n   }\n \n   public int getCopyOnWriteInsertSplitSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE);\n   }\n \n   public int getCopyOnWriteRecordSizeEstimate() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE);\n   }\n \n   public boolean shouldAutoTuneInsertSplits() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS));\n+    return getBoolean(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS);\n   }\n \n   public int getCleanerParallelism() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_PARALLELISM));\n+    return getInt(HoodieCompactionConfig.CLEANER_PARALLELISM);\n   }\n \n   public boolean isAutoClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.AUTO_CLEAN_PROP);\n   }\n \n   public boolean isAsyncClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.ASYNC_CLEAN_PROP);\n   }\n \n   public boolean incrementalCleanerModeEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE);\n   }\n \n-  public boolean isInlineCompaction() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_PROP));\n+  public boolean inlineCompactionEnabled() {\n+    return getBoolean(HoodieCompactionConfig.INLINE_COMPACT_PROP);\n   }\n \n   public CompactionTriggerStrategy getInlineCompactTriggerStrategy() {\n-    return CompactionTriggerStrategy.valueOf(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n+    return CompactionTriggerStrategy.valueOf(getString(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n   }\n \n   public int getInlineCompactDeltaCommitMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP);\n   }\n \n   public int getInlineCompactDeltaSecondsMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP);\n   }\n \n   public CompactionStrategy getCompactionStrategy() {\n-    return ReflectionUtils.loadClass(props.getProperty(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n+    return ReflectionUtils.loadClass(getString(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n   }\n \n   public Long getTargetIOPerCompactionInMB() {\n-    return Long.parseLong(props.getProperty(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP));\n+    return getLong(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP);\n   }\n \n   public Boolean getCompactionLazyBlockReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP);\n   }\n \n   public Boolean getCompactionReverseLogReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP);\n   }\n \n-  public boolean isInlineClustering() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n+  public boolean inlineClusteringEnabled() {\n+    return getBoolean(HoodieClusteringConfig.INLINE_CLUSTERING_PROP);\n   }\n \n   public boolean isAsyncClusteringEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    return getBoolean(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY);\n   }\n \n   public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClusteringEnabled();\n+    return inlineClusteringEnabled() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP));\n+    return getInt(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP);\n   }\n \n   public String getPayloadClass() {\n-    return props.getProperty(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n+    return getString(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n   }\n \n   public int getTargetPartitionsPerDayBasedCompaction() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP));\n+    return getInt(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP);\n   }\n \n   public int getCommitArchivalBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP));\n+    return getInt(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP);\n   }\n \n   public Boolean shouldCleanBootstrapBaseFile() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED);\n   }\n \n   public String getClusteringUpdatesStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n+    return getString(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n   }\n \n   public HoodieFailedWritesCleaningPolicy getFailedWritesCleanPolicy() {\n     return HoodieFailedWritesCleaningPolicy\n-        .valueOf(props.getProperty(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n+        .valueOf(getString(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n   }\n \n   /**\n    * Clustering properties.\n    */\n   public String getClusteringPlanStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n   }\n \n   public String getClusteringExecutionStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n   }\n \n   public long getClusteringMaxBytesInGroup() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP);\n   }\n \n   public long getClusteringSmallFileLimit() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT);\n   }\n \n   public int getClusteringMaxNumGroups() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS);\n   }\n \n   public long getClusteringTargetFileMaxBytes() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES);\n   }\n \n   public int getTargetPartitionsForClustering() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS);\n   }\n \n   public String getClusteringSortColumns() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n+    return getString(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n \n   /**\n    * index properties.\n    */\n   public HoodieIndex.IndexType getIndexType() {\n-    return HoodieIndex.IndexType.valueOf(props.getProperty(HoodieIndexConfig.INDEX_TYPE_PROP));\n+    return HoodieIndex.IndexType.valueOf(getString(HoodieIndexConfig.INDEX_TYPE_PROP));\n   }\n \n   public String getIndexClass() {\n-    return props.getProperty(HoodieIndexConfig.INDEX_CLASS_PROP);\n+    return getString(HoodieIndexConfig.INDEX_CLASS_PROP);\n   }\n \n   public int getBloomFilterNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES));\n+    return getInt(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES);\n   }\n \n   public double getBloomFilterFPP() {\n-    return Double.parseDouble(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_FPP));\n+    return getDouble(HoodieIndexConfig.BLOOM_FILTER_FPP);\n   }\n \n   public String getHbaseZkQuorum() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n   }\n \n   public int getHbaseZkPort() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP);\n   }\n \n   public String getHBaseZkZnodeParent() {\n-    return props.getProperty(HoodieIndexConfig.HBASE_ZK_ZNODEPARENT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_ZNODEPARENT);\n   }\n \n   public String getHbaseTableName() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n   }\n \n   public int getHbaseIndexGetBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHBaseIndexRollbackSync() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC);\n   }\n \n   public int getHbaseIndexPutBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHbaseIndexPutBatchSizeAutoCompute() {\n-    return Boolean.valueOf(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP);\n   }\n \n   public String getHBaseQPSResourceAllocatorClass() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n+    return getString(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n   }\n \n   public String getHBaseQPSZKnodePath() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n   }\n \n   public String getHBaseZkZnodeSessionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n   }\n \n   public String getHBaseZkZnodeConnectionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n   }\n \n   public boolean getHBaseIndexShouldComputeQPSDynamically() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY));\n+    return getBoolean(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY);\n   }\n \n   public int getHBaseIndexDesiredPutsTime() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS));\n+    return getInt(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS);\n   }\n \n   public String getBloomFilterType() {\n-    return props.getProperty(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n+    return getString(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n   }\n \n   public int getDynamicBloomFilterMaxNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES));\n+    return getInt(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES);\n   }\n \n   /**\n", "next_change": {"commit": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 1783535e82..4cbbbbc950 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -815,15 +947,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -947,15 +1294,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": null}, {"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -963,11 +1310,11 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * Hoodie jobs to an Hbase Region Server\n    */\n   public int getHbaseIndexMaxQPSPerRegionServer() {\n-    return getInt(HoodieHBaseIndexConfig.HBASE_MAX_QPS_PER_REGION_SERVER);\n+    return getInt(HoodieHBaseIndexConfig.MAX_QPS_PER_REGION_SERVER);\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_UPDATE_PARTITION_PATH);\n+    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 448ce9f7b4..eb3df38428 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1314,7 +1357,7 @@ public class HoodieWriteConfig extends HoodieConfig {\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n+    return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "e5faf2cc8470f83323b34305a135461c7e43d14e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex eb3df38428..1650a79ee9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1360,6 +1553,10 @@ public class HoodieWriteConfig extends HoodieConfig {\n     return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n+  public int getHBaseIndexRegionCount() {\n+    return getInt(HoodieHBaseIndexConfig.BUCKET_NUMBER);\n+  }\n+\n   public int getBloomIndexParallelism() {\n     return getInt(HoodieIndexConfig.BLOOM_INDEX_PARALLELISM);\n   }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "77ba561a6bacbf9e7dc9c1582eb068f7ad800cd9", "committedDate": "2021-02-23 20:56:58 -0500", "message": "[HUDI-1347] Fix Hbase index to make rollback synchronous (via config) (#2188)"}, {"oid": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "committedDate": "2021-03-02 21:58:41 -0800", "message": "[HUDI-1553] Configuration and metrics for the TimelineService. (#2495)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "7fed7352bd506e20e5316bb0b3ed9e5c1e9c76df", "committedDate": "2021-05-27 13:38:33 +0800", "message": "[HUDI-1865] Make embedded time line service singleton (#2899)"}, {"oid": "f760ec543ec9ea23b7d4c9f61c76a283bd737f27", "committedDate": "2021-06-07 23:24:32 -0700", "message": "[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)"}, {"oid": "7261f0850727aea611cd34e1bb07d684b44534f6", "committedDate": "2021-06-08 09:26:10 -0400", "message": "[HUDI-1929] Support configure KeyGenerator by type (#2993)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "b376cefc3e899aa28992796925708746561d6087", "committedDate": "2021-07-01 18:48:59 +0800", "message": "[MINOR] Add Documentation to KEYGENERATOR_TYPE_PROP (#3196)"}, {"oid": "62a1ad8b3a2a3c1dabba0a4622117636920b6c13", "committedDate": "2021-07-03 20:27:37 +0800", "message": "[HUDI-1930] Bootstrap support configure KeyGenerator by type (#3170)"}, {"oid": "a4dcbb5c5a2a94e4f69524194d8777d082af31ab", "committedDate": "2021-07-05 23:03:41 -0700", "message": "[HUDI-2028] Implement RockDbBasedMap as an alternate to DiskBasedMap in ExternalSpillableMap (#3194)"}, {"oid": "ea9e5d0e8b7557ef82631ac173d67f15bad13690", "committedDate": "2021-07-07 11:15:25 -0400", "message": "[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)"}, {"oid": "5804ad8e32ae05758ebc5e47f5d4fb4db371ab52", "committedDate": "2021-07-11 14:43:38 -0400", "message": "[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)"}, {"oid": "b0089b894ad12da11fbd6a0fb08508c7adee68e6", "committedDate": "2021-07-13 00:24:40 -0400", "message": "[MINOR] Fix EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION config (#3250)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "d024439764ceeca6366cb33689b729a1c69a6272", "committedDate": "2021-07-14 22:57:38 -0400", "message": "[HUDI-2029] Implement compression for DiskBasedMap in Spillable Map (#3128)"}, {"oid": "38cd74b56328d154c004f04f1335784529d8e93d", "committedDate": "2021-07-16 12:24:41 +0800", "message": "[MINOR] Allow users to choose ORC as base file format in Spark SQL (#3279)"}, {"oid": "d5026e9a24850bdcce9f6df3686bf2235d7d01c4", "committedDate": "2021-07-19 20:43:48 -0400", "message": "[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3", "committedDate": "2021-07-26 17:21:04 -0400", "message": "[HUDI-2176, 2178, 2179] Adding virtual key support to COW table (#3306)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "bbadac7de1bb57300ca7e796ebb401fdbb66a0f8", "committedDate": "2021-07-28 21:30:18 -0700", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write (#2296)"}, {"oid": "826a04d1425f47fdd80c293569a359021d1b6586", "committedDate": "2021-08-03 12:07:45 -0700", "message": "[HUDI-2072] Add pre-commit validator framework (#3153)"}, {"oid": "91bb0d13184c57ec08f02db3337e734bc20739c4", "committedDate": "2021-08-03 17:50:30 -0700", "message": "[HUDI-2255] Refactor Datasource options (#3373)"}, {"oid": "70b6bd485f8d1ef9a9b15999edda2472d0b4d65a", "committedDate": "2021-08-06 22:53:08 -0400", "message": "[HUDI-1468] Support custom clustering strategies and preserve commit metadata as part of clustering (#3419)"}, {"oid": "b4441abcf74951ec0ce28593b96baa84456a97d3", "committedDate": "2021-08-09 10:10:15 -0700", "message": "[HUDI-2194] Skip the latest N partitions when choosing partitions to create ClusteringPlan (#3300)"}, {"oid": "21db6d7a84d4a83ec98c110e92ff9c92d05dd530", "committedDate": "2021-08-10 20:23:23 +0800", "message": "[HUDI-1771] Propagate CDC format for hoodie (#3285)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "76bc686a77a485544c9e75cfefa59fa021470a0c", "committedDate": "2021-08-12 15:45:57 -0700", "message": "[HUDI-1292] Created a config to enable/disable syncing of metadata table. (#3427)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "642b1b671de8c6a35ae7858c9b03d3dff70889dd", "committedDate": "2021-08-13 19:29:22 -0400", "message": "[HUDI-2151]  Flipping defaults (#3452)"}, {"oid": "9056c68744a3f31ac2625e004ec6e155d2e86be9", "committedDate": "2021-08-14 08:18:49 -0400", "message": "[HUDI-2305] Add MARKERS.type and fix marker-based rollback (#3472)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "e39d0a2f2852ef51c524e5b16a1cecb099674eed", "committedDate": "2021-08-20 02:42:59 -0700", "message": "Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)"}, {"oid": "de94787a85b272f79181dff73907b0f20855ee78", "committedDate": "2021-08-24 21:45:17 +0800", "message": "[HUDI-2345] Hoodie columns sort partitioner for bulk insert (#3523)"}, {"oid": "21fd6edfe7721c674b40877fbbdbac71b36bf782", "committedDate": "2021-09-02 11:14:09 +0800", "message": "[HUDI-2384] Change log file size config to long (#3577)"}, {"oid": "e528dd798ab8ce6e4d444d2d771c107c503e8f25", "committedDate": "2021-09-10 18:20:26 -0700", "message": "[HUDI-2394] Implement Kafka Sink Protocol for Hudi for Ingesting Immutable Data (#3592)"}, {"oid": "2791fb9a964b39ef9aaec83eafd080013186b2eb", "committedDate": "2021-09-16 15:08:10 +0800", "message": "[HUDI-2423] Separate some config logic from HoodieMetricsConfig into HoodieMetricsGraphiteConfig HoodieMetricsJmxConfig (#3652)"}, {"oid": "61d009608899bc70c1372d5cb00a2f35e188c30c", "committedDate": "2021-09-17 19:39:55 +0800", "message": "[HUDI-2434] Make periodSeconds of GraphiteReporter configurable (#3667)"}, {"oid": "06c2cc2c8b1ad88bb4c9bbdb496053a079767e9b", "committedDate": "2021-09-24 13:33:34 +0800", "message": "[HUDI-2385] Make parquet dictionary encoding configurable (#3578)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "08c35a55b3133ddaead0581c9129e88a869421a1", "committedDate": "2021-11-05 13:03:41 -0400", "message": "[HUDI-2526] Make spark.sql.parquet.writeLegacyFormat configurable (#3917)"}, {"oid": "dfe3b84715e8fecfa96ef615c217f5eaf0da94e8", "committedDate": "2021-11-09 17:37:59 -0500", "message": "[HUDI-2579] Make deltastreamer checkpoint state merging more explicit (#3820)"}, {"oid": "4f217fe718b0b4e9656c2a45f7b89cb5df15a4f2", "committedDate": "2021-11-12 07:29:37 -0500", "message": "[HUDI-2151] Part1 Setting default parallelism to 200 for some of write configs (#3948)"}, {"oid": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "committedDate": "2021-11-13 09:12:33 +0800", "message": "[HUDI-2697] Minor changes about hbase index config. (#3927)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "ce7d2333078e4e1f16de1bce6d448c5eef1e4111", "committedDate": "2021-11-17 11:51:28 +0530", "message": "[HUDI-2151] Part3 Enabling marker based rollback as default rollback strategy (#3950)"}, {"oid": "2d3f2a3275ba615245fcabda96b8282cb86940ad", "committedDate": "2021-11-17 14:43:00 -0500", "message": "[HUDI-2734] Setting default metadata enable as false for Java (#4003)"}, {"oid": "3bdab01a498d605faede833af2d88cd8ec9237a0", "committedDate": "2021-11-22 19:19:59 -0500", "message": "[HUDI-2550] Expand File-Group candidates list for appending for MOR tables (#3986)"}, {"oid": "e22150fe15da2985b20077d2e0734fcd46b85a6f", "committedDate": "2021-11-23 07:29:03 +0530", "message": "[HUDI-1937] Rollback unfinished replace commit to allow updates (#3869)"}, {"oid": "ca9bfa2a4000575dbaa379c91898786f040a9917", "committedDate": "2021-11-23 14:23:28 +0530", "message": "[HUDI-2332] Add clustering and compaction in Kafka Connect Sink (#3857)"}, {"oid": "435ea1543c034194d7ca0b589b7b043fc49c07ac", "committedDate": "2021-11-24 18:26:40 -0500", "message": "[HUDI-2793] Fixing deltastreamer checkpoint fetch/copy over (#4034)"}, {"oid": "88067f57a23575aae3c371a7c7871e4207ca3bea", "committedDate": "2021-11-25 19:17:38 +0800", "message": "[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)"}, {"oid": "e0125a7911d77afd4a82a49caaccaa3c10df0377", "committedDate": "2021-11-25 13:33:16 -0800", "message": "[HUDI-2801] Add Amazon CloudWatch metrics reporter (#4081)"}, {"oid": "d1e83e4ba0b881f9410f0ae9f3799c967b6891cb", "committedDate": "2021-11-26 16:41:05 -0500", "message": "[HUDI-2767] Enabling timeline-server-based marker as default (#4112)"}, {"oid": "24380c20606d63c7c129cb45a73f786f223e7d39", "committedDate": "2021-11-30 17:47:16 -0800", "message": "Revert \"[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)\" (#4169)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "9797fdfbb27ca8f5f06875ad958b597becc27a8d", "committedDate": "2021-12-10 19:42:20 -0800", "message": "[HUDI-2974] Make the prefix for metrics name configurable (#4274)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "2444f40a4be5bbf0bf210dee5690267a9a1e35c8", "committedDate": "2021-12-31 11:07:52 +0530", "message": "[HUDI-3095] abstract partition filter logic to enable code reuse (#4454)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "827549949c4ac472fdc528a35ae421b20e2cc83a", "committedDate": "2022-01-08 10:22:44 -0500", "message": "[HUDI-2909] Handle logical type in TimestampBasedKeyGenerator (#4203)"}, {"oid": "251d4eb3b64704b9dd51bf6f6ecb5bf47089b745", "committedDate": "2022-01-10 08:40:24 +0530", "message": "[HUDI-3030] InProcessLockPovider as default when any async servcies enabled with no lock provider override (#4406)"}, {"oid": "9fe28e56b49c7bf68ae2d83bfe89755314aa793b", "committedDate": "2022-01-11 23:23:55 -0800", "message": "[HUDI-3045] New clustering regex match config to choose partitions when building clustering plan (#4346)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "14d08bb64c4bea20a692b3d3bced5cc9800cd541", "committedDate": "2022-01-20 15:34:56 +0400", "message": "[MINOR] Fix typo in the doc of BULK_INSERT_SORT_MODE (#4652)"}, {"oid": "bc7882cbe924ce8000f4a738b8673fe7a5cf69fb", "committedDate": "2022-01-24 16:53:54 -0500", "message": "[HUDI-2872][HUDI-2646] Refactoring layout optimization (clustering) flow to support linear ordering (#4606)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "538ec44fa8a23926b584c3bcdd24feb9894d4c51", "committedDate": "2022-02-15 09:49:53 -0500", "message": "[HUDI-2931] Add config to disable table services (#4777)"}, {"oid": "359fbfde798b50edc06ee1d0520efcd971a289bc", "committedDate": "2022-02-20 15:31:31 -0500", "message": "[HUDI-2648] Retry FileSystem action instead of failed directly. (#3887)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "92cdc5987a2b5a6faecec96224f545ab49ee6ef2", "committedDate": "2022-02-25 11:30:10 -0500", "message": "[HUDI-3515] Making rdd unpersist optional at the end of writes (#4898)"}, {"oid": "62f534d00228653059c4fed944d444632bc07091", "committedDate": "2022-03-04 09:33:16 +0800", "message": "[HUDI-3445] Support Clustering Command Based on Call Procedure Command for Spark SQL (#4901)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "f0bcee3c014cf59bdad3eaf8212d94a589073f0b", "committedDate": "2022-03-07 13:42:03 -0500", "message": "[HUDI-3561] Avoid including whole `MultipleSparkJobExecutionStrategy` object into the closure for Spark to serialize (#4954)"}, {"oid": "29040762fa511f89e678dc15ca5ae7f9f097fb8a", "committedDate": "2022-03-07 17:01:49 -0500", "message": "[HUDI-3576] Configuring timeline refreshes based on latest commit (#4973)"}, {"oid": "575bc6346825796e091a12be5a53b04980f82637", "committedDate": "2022-03-08 10:39:04 -0500", "message": "[HUDI-3356][HUDI-3203] HoodieData for metadata index records; BloomFilter construction from index based on the type param (#4848)"}, {"oid": "034addaef5834eff09cfd9ac5cc2656df95ca0e8", "committedDate": "2022-03-09 21:45:25 -0500", "message": "[HUDI-3396] Make sure `BaseFileOnlyViewRelation` only reads projected columns (#4818)"}, {"oid": "95e6e538109af9fe60aa99219e4aa1d7ce9511e2", "committedDate": "2022-03-17 01:25:04 -0400", "message": "[HUDI-3404] Automatically adjust write configs based on metadata table and write concurrency mode (#4975)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "5f570ea151d0212ab1bb2d1f5693035626b76d31", "committedDate": "2022-03-21 22:56:31 -0400", "message": "[HUDI-2883] Refactor hive sync tool / config to use reflection and standardize configs (#4175)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "fb45fc9cb9581abc40922ddcbee21dfc016d4edc", "committedDate": "2022-04-01 20:14:07 -0700", "message": "[HUDI-3773] Fix parallelism used for metadata table bloom filter index (#5209)"}, {"oid": "84064a9b081c246f306855ae125f0dae5eb8f6d0", "committedDate": "2022-04-02 23:44:10 -0700", "message": "[HUDI-3772] Fixing auto adjustment of lock configs for deltastreamer (#5207)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "3e97c88c4ff9cbeb312805daf6d52da5f1bcb0bd", "committedDate": "2022-04-09 15:30:11 -0400", "message": "[HUDI-3807] Add a new config to control the use of metadata index in HoodieBloomIndex (#5268)"}, {"oid": "bab691692e31ae3e432bb6cf4c90436fba408a2c", "committedDate": "2022-04-13 17:33:26 -0400", "message": "[HUDI-3686] Fix inline and async table service check in HoodieWriteConfig (#5307)"}, {"oid": "4e928a6fe1ddd7e126ab155bb1c03f8630bb873d", "committedDate": "2022-04-28 15:18:56 -0700", "message": "[HUDI-3943] Some description fixes for 0.10.1 docs (#5447)"}, {"oid": "f492c52ee4d2f3d6dfbbf574833f837322166fbd", "committedDate": "2022-04-29 16:21:52 -0700", "message": "[HUDI-3862] Fix default configurations of HoodieHBaseIndexConfig (#5308)"}, {"oid": "6e16e719cd614329018cd34a7c57d342fe2fa376", "committedDate": "2022-05-14 07:37:31 -0400", "message": "[HUDI-3980] Suport kerberos hbase index (#5464)"}, {"oid": "61030d8e7a5a05e215efed672267ac163b0cbcf6", "committedDate": "2022-05-16 11:07:01 +0800", "message": "[HUDI-3123] consistent hashing index: basic write path (upsert/insert) (#4480)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "cf837b49008fd351a3f89beb5e4e5c17c30b9a3c", "committedDate": "2022-05-25 19:38:56 +0530", "message": "[HUDI-3193] Decouple hudi-aws from hudi-client-common (#5666)"}, {"oid": "7f8630cc57fbb9d29e8dc7ca87b582264da073fd", "committedDate": "2022-06-02 09:48:48 +0800", "message": "[HUDI-4167] Remove the timeline refresh with initializing hoodie table (#5716)"}, {"oid": "4f6fc726d0d3d2dd427210228bbb36cf18893a92", "committedDate": "2022-06-06 10:21:00 -0700", "message": "[HUDI-4140] Fixing hive style partitioning and default partition with bulk insert row writer with SimpleKeyGen and virtual keys (#5664)"}, {"oid": "35afdb4316d496bbb37ebb9e1598d84bd8a4000d", "committedDate": "2022-06-07 16:30:46 -0700", "message": "[HUDI-4178] Addressing performance regressions in Spark DataSourceV2 Integration (#5737)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "da28e38fe3d25e3ab212dc02312f0ae395371072", "committedDate": "2022-07-23 14:37:04 -0500", "message": "[HUDI-4071] Make NONE sort mode as default for bulk insert (#6195)"}, {"oid": "a0ffd05b7773c4c83714a60dcaa79332ee3aada3", "committedDate": "2022-07-23 16:10:53 -0700", "message": "[HUDI-4448] Remove the latest commit refresh for timeline server (#6179)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "e5faf2cc8470f83323b34305a135461c7e43d14e", "committedDate": "2022-07-26 18:09:17 +0800", "message": "[HUDI-4210] Create custom hbase index to solve data skew issue on hbase regions (#5797)"}, {"oid": "cdaec5a8da060157eb7426b5b70419c5f8868e04", "committedDate": "2022-07-27 14:47:49 -0700", "message": "[HUDI-4186] Support Hudi with Spark 3.3.0 (#5943)"}, {"oid": "767c196631240aeda5a8ef4603c17f05a407d8f7", "committedDate": "2022-08-06 18:19:29 -0400", "message": "[HUDI-4303] Adding 4 to 5 upgrade handler to check for old deprecated \"default\" partition value (#6248)"}, {"oid": "6badae46f0f3e743f0502bd34e124cf6cfabcec0", "committedDate": "2022-09-12 12:05:40 +0530", "message": "[HUDI-3558] Consistent bucket index: bucket resizing (split&merge) & concurrent write during resizing (#4958)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5e624698f78f4707d62c7f26b044a69a250aae43", "committedDate": "2022-09-22 09:17:09 -0400", "message": "[HUDI-4363] Support Clustering row writer to improve performance (#6046)"}, {"oid": "efe553b327bc025d242afa37221a740dca9b1ea6", "committedDate": "2022-09-23 18:36:48 +0800", "message": "[HUDI-4897] Refactor the merge handle in CDC mode (#6740)"}, {"oid": "58fe71d2b316a83b30ce4b80a36dd7beed001e58", "committedDate": "2022-09-29 01:37:46 -0400", "message": "[HUDI-4722] Added locking metrics for Hudi (#6502)"}, {"oid": "f3d4ce919d4909f9533255ee2a9a0450c8e44c73", "committedDate": "2022-10-01 18:21:23 +0800", "message": "[HUDI-4916] Implement change log feed for Flink (#6840)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "a5434b6b4d9bef9eea29bf33f08e7f13753057a9", "committedDate": "2022-11-02 20:02:18 -0400", "message": "[HUDI-3963] Use Lock-Free Message Queue Disruptor Improving Hoodie Writing Efficiency  (#5416)"}, {"oid": "6b73c814f931f8dcdec500a4364354ac9488ce68", "committedDate": "2022-11-17 01:54:54 +0800", "message": "[HUDI-5209] Fixing `QueueBasedExecutor` in Spark bundles (missing Disruptor as dep) (#7188)"}, {"oid": "91e0db57b94c479a73e4f78d571b50c1e5ea541f", "committedDate": "2022-11-23 09:04:20 +0800", "message": "[MINOR] Use direct marker for spark engine when timeline server is disabled (#7272)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "1cdbf68d4ee641b9e7eb0129a26e0f969b37d8ca", "committedDate": "2022-11-28 22:48:06 -0500", "message": "[HUDI-5242] Do not fail Meta sync in Deltastreamer when inline table service fails (#7243)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "8d13a7e383c30ec1421b68dd052fbd33f438bc3d", "committedDate": "2022-12-15 09:18:54 -0800", "message": "[HUDI-5023] Consuming records from Iterator directly instead of using inner message queue (#7174)"}, {"oid": "16d33ba3cb953435660566ba2ec63a45204a7814", "committedDate": "2023-01-15 21:46:12 -0800", "message": "[HUDI-3654] Add new module `hudi-metaserver` (#5064)"}, {"oid": "c9bc03ed8681ab64eea2520f5511464915389c51", "committedDate": "2023-01-17 07:24:16 -0800", "message": "[HUDI-4148] Add client for Hudi table service manager (TSM) (#6732)"}, {"oid": "ec5022b4fdd94c106bf243038aadf781f31b5be9", "committedDate": "2023-01-18 09:30:52 -0800", "message": "[MINOR] Unify naming for record merger (#7660)"}, {"oid": "c18d6153e105ac34fb410c60ce8a153327931782", "committedDate": "2023-01-23 10:03:19 -0800", "message": "[HUDI-1575] Early Conflict Detection For Multi-writer (#6133)"}, {"oid": "2fc20c186b77017f7f1c6f6abe8559a9e8cfe578", "committedDate": "2023-01-24 20:04:55 +0530", "message": "[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)"}, {"oid": "c95abd3213f4806f535c6d7cb8b346616c3368fb", "committedDate": "2023-01-25 19:01:33 +0530", "message": "Revert \"[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)\" (#7747)"}, {"oid": "7e35874c7ba68dfa32b3b27ece35112cc434a7c6", "committedDate": "2023-01-25 14:34:18 -0800", "message": "[HUDI-5617] Rename configs for async conflict detector for clarity (#7750)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3a08bdc3f971b3534e8fb6f34772340cfdf055a9", "committedDate": "2023-01-25 19:29:42 -0800", "message": "[HUDI-5363] Removing default value for shuffle parallelism configs (#7723)"}, {"oid": "ff590c6d72c523b41c0790087053fd3933564ac8", "committedDate": "2023-01-27 18:56:32 -0800", "message": "[HUDI-5023] Switching default Write Executor type to `SIMPLE` (#7476)"}, {"oid": "c21eca564c6426413cbdc9e83bc40ad7c59c7e5d", "committedDate": "2023-01-28 13:23:30 -0500", "message": "[HUDI-5626] Rename CDC logging mode options (#7760)"}, {"oid": "2c56aa4ce994714a57402399c1bec99579926f66", "committedDate": "2023-01-28 14:03:02 -0600", "message": "[HUDI-5631] Improve defaults of early conflict detection configs (#7770)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "88d8e5e96d5f5ee553b8405e32c79388e3ed3c09", "committedDate": "2023-01-29 14:57:05 -0800", "message": "[MINOR] Cleaning up recently introduced configs (#7772)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "7064c380506814964dd85773e2ee7b7f187b88c3", "committedDate": "2023-02-01 11:19:45 -0800", "message": "[MINOR] Restoring existing behavior for `DeltaStreamer` Incremental Source (#7810)"}, {"oid": "ef3a17e3d97428a6f0d6e7ac888747d65a8792c5", "committedDate": "2023-02-04 15:14:52 +0800", "message": "[HUDI-5692] SpillableMapBasePath should be lazily loaded (#7837)"}, {"oid": "ff832f4d86091b71af42cc46c2aa209d80396899", "committedDate": "2023-02-04 17:39:58 +0800", "message": "[MINOR] Validate configs for OCC early conflict detection (#7848)"}, {"oid": "0c9465f2ab6cf6472df3046c681b72290e6034bd", "committedDate": "2023-02-05 00:28:20 -0800", "message": "[MINOR] Improve configuration configs (#7855)"}, {"oid": "53fca761be1db224dc384238a7d2853ff2a1d227", "committedDate": "2023-02-07 19:59:06 -0500", "message": "[MINOR] fixed docs for WRITE_EXECUTOR_TYPE (#7880)"}, {"oid": "be92be657a348954cc21062ca24e8a10caea17ee", "committedDate": "2023-02-20 10:05:09 +0800", "message": "[HUDI-5786] Add a new config to specific spark write rdd storage level (#7941)"}, {"oid": "0c84482aa915611db17f4505974c876240a8101c", "committedDate": "2023-02-20 20:00:31 +0530", "message": "[HUDI-5774] Fix prometheus configs for metadata table and support metric labels (#7933)"}, {"oid": "d705dcc4188223fbd824f36a5d211abeda7b1f23", "committedDate": "2023-02-24 10:23:25 +0800", "message": "[HUDI-5173] Skip if there is only one file in clusteringGroup  (#7159)"}, {"oid": "e131505a4f0bfef6e8cca04e102b3ea1c6d308e2", "committedDate": "2023-02-27 12:26:50 +0530", "message": "[HUDI-5838] Mask sensitive info while printing hudi properties in DeltaStreamer  (#8027)"}, {"oid": "c7cebf445f4dc6895cc343a396b0ea1871b362e7", "committedDate": "2023-02-27 23:57:08 -0500", "message": "[HUDI-5843] multiwriter deltastreamer checkpoints (#8043)"}, {"oid": "81e6e854883a94d41ae5b7187c608a8ddbc7bf35", "committedDate": "2023-03-03 21:06:43 +0530", "message": "[HUDI-5847] Add support for multiple metric reporters and metric labels (#8041)"}, {"oid": "a8312a9b8c39f4baabf753974fa092c4767abb72", "committedDate": "2023-03-08 14:46:01 +0800", "message": "[HUDI-5887] Distinguish the single writer enabling metadata table and multi-writer use cases for lock guard (#8111)"}, {"oid": "38e4078d23b00d0acdd02a28eec08560d175cdc9", "committedDate": "2023-03-13 21:30:19 -0700", "message": "[MINOR] Ignoring warn msg for timeline server for metadata table (#8168)"}, {"oid": "e51b4575cb7642eb61bcc02d95c99466dd3e8eda", "committedDate": "2023-03-17 15:17:24 -0700", "message": "[HUDI-5920] Improve documentation of parallelism configs (#8157)"}, {"oid": "cb1395a820febc4bb13c544369ca94a55fff0a29", "committedDate": "2023-03-30 11:07:29 -0700", "message": "[HUDI-5893] Mark advanced configs (#8295)"}, {"oid": "c53d9fbe019a43f31b3eb7556ff109d71287cf6c", "committedDate": "2023-03-31 13:30:34 -0700", "message": "[HUDI-5900] Clean up unused metadata configs (#8125)"}, {"oid": "6fd885fb3dc5c66caf6a1775fa87b4f7212056d8", "committedDate": "2023-03-31 21:21:28 -0700", "message": "[HUDI-5740] Refactor Deltastreamer and schema providers to use HoodieConfig/ConfigProperty (#8152)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "f9f110695fc69f2d7085648a6610888bb10ad8e4", "committedDate": "2023-04-19 04:09:48 -0400", "message": "[HUDI-6056] Validate archival configs alignment with cleaner configs with policy based on hours (#8422)"}, {"oid": "44a0c29560db2d85e3b0d963beed55225596baff", "committedDate": "2023-04-21 09:50:34 +0800", "message": "[HUDI-6100] Fixed overflow in setting log block size by making it long everywhere (#8495)"}, {"oid": "5a5c4863452197def89390beb0ab584eb08aabb6", "committedDate": "2023-04-21 23:06:46 -0700", "message": "[HUDI-5934] Remove archival configs for metadata table (#8319)"}, {"oid": "9a9dd3a82d3e69f1d5eebe46c79c8fd0dc2355db", "committedDate": "2023-04-23 16:37:48 +0800", "message": "[HUDI-6123] Auto adjust lock configs for single writer (#8542)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "e04dc0951cf21122f0d3dd4f673b87b663253109", "committedDate": "2023-05-04 04:05:13 -0700", "message": "[HUDI-5315] Use sample writes to estimate record size (#8390)"}, {"oid": "cabcb2bf2cddedeb3a34047af3935b27cfdfb858", "committedDate": "2023-05-05 06:28:14 -0700", "message": "[HUDI-5968] Fix global index duplicate and handle custom payload when update partition (#8490)"}, {"oid": "66e838c8f18b8ca50a3839f4768da5edfed0c416", "committedDate": "2023-05-06 15:09:03 +0800", "message": "[MINOR] Remove redundant advanced config marking (#8600)"}, {"oid": "6cdc1f583ef8d1281d2171c42fc982e2715f08f8", "committedDate": "2023-05-08 07:29:57 -0700", "message": "[HUDI-5895] Remove bootstrap key generator configs (#8557)"}, {"oid": "a5bd50c067f2a82be2470d4649f2be3007404c40", "committedDate": "2023-05-23 15:54:42 +0800", "message": "[MINOR] Disable schema validation in master (#8781)"}, {"oid": "9d58ee4b1f1fce213ee3f4ff478eb003da943924", "committedDate": "2023-05-24 20:09:54 +0800", "message": "[HUDI-5994] Bucket index supports bulk insert row writer (#8776)"}, {"oid": "59786113fae88382f03412c7c79f7a827bf9393f", "committedDate": "2023-05-26 12:06:32 +0530", "message": "[HUDI-5998] Speed up reads from bootstrapped tables in spark (#8303)"}, {"oid": "41e1e9a4fda2e399f4d50222e81bdfa713bbce23", "committedDate": "2023-05-30 14:58:33 -0700", "message": "[MINOR] Ensure metrics prefix does not contain any dot. (#8599)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc5OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350798", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-18T07:01:22Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  ", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4259478b945ea0440e0c28c56dc6290ee2df4442", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..ccf53a56a4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ccf53a56a4..b1155e3bc9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+\n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "a318c91498d1899107f886b73c3404cd5f50258f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex b1155e3bc9..251ea2305f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 96e60c49d9..e3c1ef6819 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +501,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex e3c1ef6819..4e493e4432 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -501,7 +531,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+\n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4e493e4432..1783535e82 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -372,251 +562,251 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n    * compaction properties.\n    */\n   public HoodieCleaningPolicy getCleanerPolicy() {\n-    return HoodieCleaningPolicy.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n+    return HoodieCleaningPolicy.valueOf(getString(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n   }\n \n   public int getCleanerFileVersionsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP);\n   }\n \n   public int getCleanerCommitsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP);\n   }\n \n   public int getMaxCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getMinCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getParquetSmallFileLimit() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES));\n+    return getInt(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES);\n   }\n \n   public double getRecordSizeEstimationThreshold() {\n-    return Double.parseDouble(props.getProperty(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP));\n+    return getDouble(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP);\n   }\n \n   public int getCopyOnWriteInsertSplitSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE);\n   }\n \n   public int getCopyOnWriteRecordSizeEstimate() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE);\n   }\n \n   public boolean shouldAutoTuneInsertSplits() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS));\n+    return getBoolean(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS);\n   }\n \n   public int getCleanerParallelism() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_PARALLELISM));\n+    return getInt(HoodieCompactionConfig.CLEANER_PARALLELISM);\n   }\n \n   public boolean isAutoClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.AUTO_CLEAN_PROP);\n   }\n \n   public boolean isAsyncClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.ASYNC_CLEAN_PROP);\n   }\n \n   public boolean incrementalCleanerModeEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE);\n   }\n \n-  public boolean isInlineCompaction() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_PROP));\n+  public boolean inlineCompactionEnabled() {\n+    return getBoolean(HoodieCompactionConfig.INLINE_COMPACT_PROP);\n   }\n \n   public CompactionTriggerStrategy getInlineCompactTriggerStrategy() {\n-    return CompactionTriggerStrategy.valueOf(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n+    return CompactionTriggerStrategy.valueOf(getString(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n   }\n \n   public int getInlineCompactDeltaCommitMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP);\n   }\n \n   public int getInlineCompactDeltaSecondsMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP);\n   }\n \n   public CompactionStrategy getCompactionStrategy() {\n-    return ReflectionUtils.loadClass(props.getProperty(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n+    return ReflectionUtils.loadClass(getString(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n   }\n \n   public Long getTargetIOPerCompactionInMB() {\n-    return Long.parseLong(props.getProperty(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP));\n+    return getLong(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP);\n   }\n \n   public Boolean getCompactionLazyBlockReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP);\n   }\n \n   public Boolean getCompactionReverseLogReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP);\n   }\n \n-  public boolean isInlineClustering() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n+  public boolean inlineClusteringEnabled() {\n+    return getBoolean(HoodieClusteringConfig.INLINE_CLUSTERING_PROP);\n   }\n \n   public boolean isAsyncClusteringEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    return getBoolean(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY);\n   }\n \n   public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClusteringEnabled();\n+    return inlineClusteringEnabled() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP));\n+    return getInt(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP);\n   }\n \n   public String getPayloadClass() {\n-    return props.getProperty(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n+    return getString(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n   }\n \n   public int getTargetPartitionsPerDayBasedCompaction() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP));\n+    return getInt(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP);\n   }\n \n   public int getCommitArchivalBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP));\n+    return getInt(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP);\n   }\n \n   public Boolean shouldCleanBootstrapBaseFile() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED);\n   }\n \n   public String getClusteringUpdatesStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n+    return getString(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n   }\n \n   public HoodieFailedWritesCleaningPolicy getFailedWritesCleanPolicy() {\n     return HoodieFailedWritesCleaningPolicy\n-        .valueOf(props.getProperty(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n+        .valueOf(getString(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n   }\n \n   /**\n    * Clustering properties.\n    */\n   public String getClusteringPlanStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n   }\n \n   public String getClusteringExecutionStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n   }\n \n   public long getClusteringMaxBytesInGroup() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP);\n   }\n \n   public long getClusteringSmallFileLimit() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT);\n   }\n \n   public int getClusteringMaxNumGroups() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS);\n   }\n \n   public long getClusteringTargetFileMaxBytes() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES);\n   }\n \n   public int getTargetPartitionsForClustering() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS);\n   }\n \n   public String getClusteringSortColumns() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n+    return getString(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n \n   /**\n    * index properties.\n    */\n   public HoodieIndex.IndexType getIndexType() {\n-    return HoodieIndex.IndexType.valueOf(props.getProperty(HoodieIndexConfig.INDEX_TYPE_PROP));\n+    return HoodieIndex.IndexType.valueOf(getString(HoodieIndexConfig.INDEX_TYPE_PROP));\n   }\n \n   public String getIndexClass() {\n-    return props.getProperty(HoodieIndexConfig.INDEX_CLASS_PROP);\n+    return getString(HoodieIndexConfig.INDEX_CLASS_PROP);\n   }\n \n   public int getBloomFilterNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES));\n+    return getInt(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES);\n   }\n \n   public double getBloomFilterFPP() {\n-    return Double.parseDouble(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_FPP));\n+    return getDouble(HoodieIndexConfig.BLOOM_FILTER_FPP);\n   }\n \n   public String getHbaseZkQuorum() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n   }\n \n   public int getHbaseZkPort() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP);\n   }\n \n   public String getHBaseZkZnodeParent() {\n-    return props.getProperty(HoodieIndexConfig.HBASE_ZK_ZNODEPARENT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_ZNODEPARENT);\n   }\n \n   public String getHbaseTableName() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n   }\n \n   public int getHbaseIndexGetBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHBaseIndexRollbackSync() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC);\n   }\n \n   public int getHbaseIndexPutBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHbaseIndexPutBatchSizeAutoCompute() {\n-    return Boolean.valueOf(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP);\n   }\n \n   public String getHBaseQPSResourceAllocatorClass() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n+    return getString(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n   }\n \n   public String getHBaseQPSZKnodePath() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n   }\n \n   public String getHBaseZkZnodeSessionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n   }\n \n   public String getHBaseZkZnodeConnectionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n   }\n \n   public boolean getHBaseIndexShouldComputeQPSDynamically() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY));\n+    return getBoolean(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY);\n   }\n \n   public int getHBaseIndexDesiredPutsTime() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS));\n+    return getInt(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS);\n   }\n \n   public String getBloomFilterType() {\n-    return props.getProperty(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n+    return getString(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n   }\n \n   public int getDynamicBloomFilterMaxNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES));\n+    return getInt(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES);\n   }\n \n   /**\n", "next_change": {"commit": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 1783535e82..4cbbbbc950 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -815,15 +947,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -947,15 +1294,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": null}, {"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -963,11 +1310,11 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * Hoodie jobs to an Hbase Region Server\n    */\n   public int getHbaseIndexMaxQPSPerRegionServer() {\n-    return getInt(HoodieHBaseIndexConfig.HBASE_MAX_QPS_PER_REGION_SERVER);\n+    return getInt(HoodieHBaseIndexConfig.MAX_QPS_PER_REGION_SERVER);\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_UPDATE_PARTITION_PATH);\n+    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 448ce9f7b4..eb3df38428 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1314,7 +1357,7 @@ public class HoodieWriteConfig extends HoodieConfig {\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n+    return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "e5faf2cc8470f83323b34305a135461c7e43d14e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex eb3df38428..1650a79ee9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1360,6 +1553,10 @@ public class HoodieWriteConfig extends HoodieConfig {\n     return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n+  public int getHBaseIndexRegionCount() {\n+    return getInt(HoodieHBaseIndexConfig.BUCKET_NUMBER);\n+  }\n+\n   public int getBloomIndexParallelism() {\n     return getInt(HoodieIndexConfig.BLOOM_INDEX_PARALLELISM);\n   }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "77ba561a6bacbf9e7dc9c1582eb068f7ad800cd9", "committedDate": "2021-02-23 20:56:58 -0500", "message": "[HUDI-1347] Fix Hbase index to make rollback synchronous (via config) (#2188)"}, {"oid": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "committedDate": "2021-03-02 21:58:41 -0800", "message": "[HUDI-1553] Configuration and metrics for the TimelineService. (#2495)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "7fed7352bd506e20e5316bb0b3ed9e5c1e9c76df", "committedDate": "2021-05-27 13:38:33 +0800", "message": "[HUDI-1865] Make embedded time line service singleton (#2899)"}, {"oid": "f760ec543ec9ea23b7d4c9f61c76a283bd737f27", "committedDate": "2021-06-07 23:24:32 -0700", "message": "[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)"}, {"oid": "7261f0850727aea611cd34e1bb07d684b44534f6", "committedDate": "2021-06-08 09:26:10 -0400", "message": "[HUDI-1929] Support configure KeyGenerator by type (#2993)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "b376cefc3e899aa28992796925708746561d6087", "committedDate": "2021-07-01 18:48:59 +0800", "message": "[MINOR] Add Documentation to KEYGENERATOR_TYPE_PROP (#3196)"}, {"oid": "62a1ad8b3a2a3c1dabba0a4622117636920b6c13", "committedDate": "2021-07-03 20:27:37 +0800", "message": "[HUDI-1930] Bootstrap support configure KeyGenerator by type (#3170)"}, {"oid": "a4dcbb5c5a2a94e4f69524194d8777d082af31ab", "committedDate": "2021-07-05 23:03:41 -0700", "message": "[HUDI-2028] Implement RockDbBasedMap as an alternate to DiskBasedMap in ExternalSpillableMap (#3194)"}, {"oid": "ea9e5d0e8b7557ef82631ac173d67f15bad13690", "committedDate": "2021-07-07 11:15:25 -0400", "message": "[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)"}, {"oid": "5804ad8e32ae05758ebc5e47f5d4fb4db371ab52", "committedDate": "2021-07-11 14:43:38 -0400", "message": "[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)"}, {"oid": "b0089b894ad12da11fbd6a0fb08508c7adee68e6", "committedDate": "2021-07-13 00:24:40 -0400", "message": "[MINOR] Fix EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION config (#3250)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "d024439764ceeca6366cb33689b729a1c69a6272", "committedDate": "2021-07-14 22:57:38 -0400", "message": "[HUDI-2029] Implement compression for DiskBasedMap in Spillable Map (#3128)"}, {"oid": "38cd74b56328d154c004f04f1335784529d8e93d", "committedDate": "2021-07-16 12:24:41 +0800", "message": "[MINOR] Allow users to choose ORC as base file format in Spark SQL (#3279)"}, {"oid": "d5026e9a24850bdcce9f6df3686bf2235d7d01c4", "committedDate": "2021-07-19 20:43:48 -0400", "message": "[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3", "committedDate": "2021-07-26 17:21:04 -0400", "message": "[HUDI-2176, 2178, 2179] Adding virtual key support to COW table (#3306)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "bbadac7de1bb57300ca7e796ebb401fdbb66a0f8", "committedDate": "2021-07-28 21:30:18 -0700", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write (#2296)"}, {"oid": "826a04d1425f47fdd80c293569a359021d1b6586", "committedDate": "2021-08-03 12:07:45 -0700", "message": "[HUDI-2072] Add pre-commit validator framework (#3153)"}, {"oid": "91bb0d13184c57ec08f02db3337e734bc20739c4", "committedDate": "2021-08-03 17:50:30 -0700", "message": "[HUDI-2255] Refactor Datasource options (#3373)"}, {"oid": "70b6bd485f8d1ef9a9b15999edda2472d0b4d65a", "committedDate": "2021-08-06 22:53:08 -0400", "message": "[HUDI-1468] Support custom clustering strategies and preserve commit metadata as part of clustering (#3419)"}, {"oid": "b4441abcf74951ec0ce28593b96baa84456a97d3", "committedDate": "2021-08-09 10:10:15 -0700", "message": "[HUDI-2194] Skip the latest N partitions when choosing partitions to create ClusteringPlan (#3300)"}, {"oid": "21db6d7a84d4a83ec98c110e92ff9c92d05dd530", "committedDate": "2021-08-10 20:23:23 +0800", "message": "[HUDI-1771] Propagate CDC format for hoodie (#3285)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "76bc686a77a485544c9e75cfefa59fa021470a0c", "committedDate": "2021-08-12 15:45:57 -0700", "message": "[HUDI-1292] Created a config to enable/disable syncing of metadata table. (#3427)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "642b1b671de8c6a35ae7858c9b03d3dff70889dd", "committedDate": "2021-08-13 19:29:22 -0400", "message": "[HUDI-2151]  Flipping defaults (#3452)"}, {"oid": "9056c68744a3f31ac2625e004ec6e155d2e86be9", "committedDate": "2021-08-14 08:18:49 -0400", "message": "[HUDI-2305] Add MARKERS.type and fix marker-based rollback (#3472)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "e39d0a2f2852ef51c524e5b16a1cecb099674eed", "committedDate": "2021-08-20 02:42:59 -0700", "message": "Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)"}, {"oid": "de94787a85b272f79181dff73907b0f20855ee78", "committedDate": "2021-08-24 21:45:17 +0800", "message": "[HUDI-2345] Hoodie columns sort partitioner for bulk insert (#3523)"}, {"oid": "21fd6edfe7721c674b40877fbbdbac71b36bf782", "committedDate": "2021-09-02 11:14:09 +0800", "message": "[HUDI-2384] Change log file size config to long (#3577)"}, {"oid": "e528dd798ab8ce6e4d444d2d771c107c503e8f25", "committedDate": "2021-09-10 18:20:26 -0700", "message": "[HUDI-2394] Implement Kafka Sink Protocol for Hudi for Ingesting Immutable Data (#3592)"}, {"oid": "2791fb9a964b39ef9aaec83eafd080013186b2eb", "committedDate": "2021-09-16 15:08:10 +0800", "message": "[HUDI-2423] Separate some config logic from HoodieMetricsConfig into HoodieMetricsGraphiteConfig HoodieMetricsJmxConfig (#3652)"}, {"oid": "61d009608899bc70c1372d5cb00a2f35e188c30c", "committedDate": "2021-09-17 19:39:55 +0800", "message": "[HUDI-2434] Make periodSeconds of GraphiteReporter configurable (#3667)"}, {"oid": "06c2cc2c8b1ad88bb4c9bbdb496053a079767e9b", "committedDate": "2021-09-24 13:33:34 +0800", "message": "[HUDI-2385] Make parquet dictionary encoding configurable (#3578)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "08c35a55b3133ddaead0581c9129e88a869421a1", "committedDate": "2021-11-05 13:03:41 -0400", "message": "[HUDI-2526] Make spark.sql.parquet.writeLegacyFormat configurable (#3917)"}, {"oid": "dfe3b84715e8fecfa96ef615c217f5eaf0da94e8", "committedDate": "2021-11-09 17:37:59 -0500", "message": "[HUDI-2579] Make deltastreamer checkpoint state merging more explicit (#3820)"}, {"oid": "4f217fe718b0b4e9656c2a45f7b89cb5df15a4f2", "committedDate": "2021-11-12 07:29:37 -0500", "message": "[HUDI-2151] Part1 Setting default parallelism to 200 for some of write configs (#3948)"}, {"oid": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "committedDate": "2021-11-13 09:12:33 +0800", "message": "[HUDI-2697] Minor changes about hbase index config. (#3927)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "ce7d2333078e4e1f16de1bce6d448c5eef1e4111", "committedDate": "2021-11-17 11:51:28 +0530", "message": "[HUDI-2151] Part3 Enabling marker based rollback as default rollback strategy (#3950)"}, {"oid": "2d3f2a3275ba615245fcabda96b8282cb86940ad", "committedDate": "2021-11-17 14:43:00 -0500", "message": "[HUDI-2734] Setting default metadata enable as false for Java (#4003)"}, {"oid": "3bdab01a498d605faede833af2d88cd8ec9237a0", "committedDate": "2021-11-22 19:19:59 -0500", "message": "[HUDI-2550] Expand File-Group candidates list for appending for MOR tables (#3986)"}, {"oid": "e22150fe15da2985b20077d2e0734fcd46b85a6f", "committedDate": "2021-11-23 07:29:03 +0530", "message": "[HUDI-1937] Rollback unfinished replace commit to allow updates (#3869)"}, {"oid": "ca9bfa2a4000575dbaa379c91898786f040a9917", "committedDate": "2021-11-23 14:23:28 +0530", "message": "[HUDI-2332] Add clustering and compaction in Kafka Connect Sink (#3857)"}, {"oid": "435ea1543c034194d7ca0b589b7b043fc49c07ac", "committedDate": "2021-11-24 18:26:40 -0500", "message": "[HUDI-2793] Fixing deltastreamer checkpoint fetch/copy over (#4034)"}, {"oid": "88067f57a23575aae3c371a7c7871e4207ca3bea", "committedDate": "2021-11-25 19:17:38 +0800", "message": "[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)"}, {"oid": "e0125a7911d77afd4a82a49caaccaa3c10df0377", "committedDate": "2021-11-25 13:33:16 -0800", "message": "[HUDI-2801] Add Amazon CloudWatch metrics reporter (#4081)"}, {"oid": "d1e83e4ba0b881f9410f0ae9f3799c967b6891cb", "committedDate": "2021-11-26 16:41:05 -0500", "message": "[HUDI-2767] Enabling timeline-server-based marker as default (#4112)"}, {"oid": "24380c20606d63c7c129cb45a73f786f223e7d39", "committedDate": "2021-11-30 17:47:16 -0800", "message": "Revert \"[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)\" (#4169)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "9797fdfbb27ca8f5f06875ad958b597becc27a8d", "committedDate": "2021-12-10 19:42:20 -0800", "message": "[HUDI-2974] Make the prefix for metrics name configurable (#4274)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "2444f40a4be5bbf0bf210dee5690267a9a1e35c8", "committedDate": "2021-12-31 11:07:52 +0530", "message": "[HUDI-3095] abstract partition filter logic to enable code reuse (#4454)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "827549949c4ac472fdc528a35ae421b20e2cc83a", "committedDate": "2022-01-08 10:22:44 -0500", "message": "[HUDI-2909] Handle logical type in TimestampBasedKeyGenerator (#4203)"}, {"oid": "251d4eb3b64704b9dd51bf6f6ecb5bf47089b745", "committedDate": "2022-01-10 08:40:24 +0530", "message": "[HUDI-3030] InProcessLockPovider as default when any async servcies enabled with no lock provider override (#4406)"}, {"oid": "9fe28e56b49c7bf68ae2d83bfe89755314aa793b", "committedDate": "2022-01-11 23:23:55 -0800", "message": "[HUDI-3045] New clustering regex match config to choose partitions when building clustering plan (#4346)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "14d08bb64c4bea20a692b3d3bced5cc9800cd541", "committedDate": "2022-01-20 15:34:56 +0400", "message": "[MINOR] Fix typo in the doc of BULK_INSERT_SORT_MODE (#4652)"}, {"oid": "bc7882cbe924ce8000f4a738b8673fe7a5cf69fb", "committedDate": "2022-01-24 16:53:54 -0500", "message": "[HUDI-2872][HUDI-2646] Refactoring layout optimization (clustering) flow to support linear ordering (#4606)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "538ec44fa8a23926b584c3bcdd24feb9894d4c51", "committedDate": "2022-02-15 09:49:53 -0500", "message": "[HUDI-2931] Add config to disable table services (#4777)"}, {"oid": "359fbfde798b50edc06ee1d0520efcd971a289bc", "committedDate": "2022-02-20 15:31:31 -0500", "message": "[HUDI-2648] Retry FileSystem action instead of failed directly. (#3887)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "92cdc5987a2b5a6faecec96224f545ab49ee6ef2", "committedDate": "2022-02-25 11:30:10 -0500", "message": "[HUDI-3515] Making rdd unpersist optional at the end of writes (#4898)"}, {"oid": "62f534d00228653059c4fed944d444632bc07091", "committedDate": "2022-03-04 09:33:16 +0800", "message": "[HUDI-3445] Support Clustering Command Based on Call Procedure Command for Spark SQL (#4901)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "f0bcee3c014cf59bdad3eaf8212d94a589073f0b", "committedDate": "2022-03-07 13:42:03 -0500", "message": "[HUDI-3561] Avoid including whole `MultipleSparkJobExecutionStrategy` object into the closure for Spark to serialize (#4954)"}, {"oid": "29040762fa511f89e678dc15ca5ae7f9f097fb8a", "committedDate": "2022-03-07 17:01:49 -0500", "message": "[HUDI-3576] Configuring timeline refreshes based on latest commit (#4973)"}, {"oid": "575bc6346825796e091a12be5a53b04980f82637", "committedDate": "2022-03-08 10:39:04 -0500", "message": "[HUDI-3356][HUDI-3203] HoodieData for metadata index records; BloomFilter construction from index based on the type param (#4848)"}, {"oid": "034addaef5834eff09cfd9ac5cc2656df95ca0e8", "committedDate": "2022-03-09 21:45:25 -0500", "message": "[HUDI-3396] Make sure `BaseFileOnlyViewRelation` only reads projected columns (#4818)"}, {"oid": "95e6e538109af9fe60aa99219e4aa1d7ce9511e2", "committedDate": "2022-03-17 01:25:04 -0400", "message": "[HUDI-3404] Automatically adjust write configs based on metadata table and write concurrency mode (#4975)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "5f570ea151d0212ab1bb2d1f5693035626b76d31", "committedDate": "2022-03-21 22:56:31 -0400", "message": "[HUDI-2883] Refactor hive sync tool / config to use reflection and standardize configs (#4175)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "fb45fc9cb9581abc40922ddcbee21dfc016d4edc", "committedDate": "2022-04-01 20:14:07 -0700", "message": "[HUDI-3773] Fix parallelism used for metadata table bloom filter index (#5209)"}, {"oid": "84064a9b081c246f306855ae125f0dae5eb8f6d0", "committedDate": "2022-04-02 23:44:10 -0700", "message": "[HUDI-3772] Fixing auto adjustment of lock configs for deltastreamer (#5207)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "3e97c88c4ff9cbeb312805daf6d52da5f1bcb0bd", "committedDate": "2022-04-09 15:30:11 -0400", "message": "[HUDI-3807] Add a new config to control the use of metadata index in HoodieBloomIndex (#5268)"}, {"oid": "bab691692e31ae3e432bb6cf4c90436fba408a2c", "committedDate": "2022-04-13 17:33:26 -0400", "message": "[HUDI-3686] Fix inline and async table service check in HoodieWriteConfig (#5307)"}, {"oid": "4e928a6fe1ddd7e126ab155bb1c03f8630bb873d", "committedDate": "2022-04-28 15:18:56 -0700", "message": "[HUDI-3943] Some description fixes for 0.10.1 docs (#5447)"}, {"oid": "f492c52ee4d2f3d6dfbbf574833f837322166fbd", "committedDate": "2022-04-29 16:21:52 -0700", "message": "[HUDI-3862] Fix default configurations of HoodieHBaseIndexConfig (#5308)"}, {"oid": "6e16e719cd614329018cd34a7c57d342fe2fa376", "committedDate": "2022-05-14 07:37:31 -0400", "message": "[HUDI-3980] Suport kerberos hbase index (#5464)"}, {"oid": "61030d8e7a5a05e215efed672267ac163b0cbcf6", "committedDate": "2022-05-16 11:07:01 +0800", "message": "[HUDI-3123] consistent hashing index: basic write path (upsert/insert) (#4480)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "cf837b49008fd351a3f89beb5e4e5c17c30b9a3c", "committedDate": "2022-05-25 19:38:56 +0530", "message": "[HUDI-3193] Decouple hudi-aws from hudi-client-common (#5666)"}, {"oid": "7f8630cc57fbb9d29e8dc7ca87b582264da073fd", "committedDate": "2022-06-02 09:48:48 +0800", "message": "[HUDI-4167] Remove the timeline refresh with initializing hoodie table (#5716)"}, {"oid": "4f6fc726d0d3d2dd427210228bbb36cf18893a92", "committedDate": "2022-06-06 10:21:00 -0700", "message": "[HUDI-4140] Fixing hive style partitioning and default partition with bulk insert row writer with SimpleKeyGen and virtual keys (#5664)"}, {"oid": "35afdb4316d496bbb37ebb9e1598d84bd8a4000d", "committedDate": "2022-06-07 16:30:46 -0700", "message": "[HUDI-4178] Addressing performance regressions in Spark DataSourceV2 Integration (#5737)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "da28e38fe3d25e3ab212dc02312f0ae395371072", "committedDate": "2022-07-23 14:37:04 -0500", "message": "[HUDI-4071] Make NONE sort mode as default for bulk insert (#6195)"}, {"oid": "a0ffd05b7773c4c83714a60dcaa79332ee3aada3", "committedDate": "2022-07-23 16:10:53 -0700", "message": "[HUDI-4448] Remove the latest commit refresh for timeline server (#6179)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "e5faf2cc8470f83323b34305a135461c7e43d14e", "committedDate": "2022-07-26 18:09:17 +0800", "message": "[HUDI-4210] Create custom hbase index to solve data skew issue on hbase regions (#5797)"}, {"oid": "cdaec5a8da060157eb7426b5b70419c5f8868e04", "committedDate": "2022-07-27 14:47:49 -0700", "message": "[HUDI-4186] Support Hudi with Spark 3.3.0 (#5943)"}, {"oid": "767c196631240aeda5a8ef4603c17f05a407d8f7", "committedDate": "2022-08-06 18:19:29 -0400", "message": "[HUDI-4303] Adding 4 to 5 upgrade handler to check for old deprecated \"default\" partition value (#6248)"}, {"oid": "6badae46f0f3e743f0502bd34e124cf6cfabcec0", "committedDate": "2022-09-12 12:05:40 +0530", "message": "[HUDI-3558] Consistent bucket index: bucket resizing (split&merge) & concurrent write during resizing (#4958)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5e624698f78f4707d62c7f26b044a69a250aae43", "committedDate": "2022-09-22 09:17:09 -0400", "message": "[HUDI-4363] Support Clustering row writer to improve performance (#6046)"}, {"oid": "efe553b327bc025d242afa37221a740dca9b1ea6", "committedDate": "2022-09-23 18:36:48 +0800", "message": "[HUDI-4897] Refactor the merge handle in CDC mode (#6740)"}, {"oid": "58fe71d2b316a83b30ce4b80a36dd7beed001e58", "committedDate": "2022-09-29 01:37:46 -0400", "message": "[HUDI-4722] Added locking metrics for Hudi (#6502)"}, {"oid": "f3d4ce919d4909f9533255ee2a9a0450c8e44c73", "committedDate": "2022-10-01 18:21:23 +0800", "message": "[HUDI-4916] Implement change log feed for Flink (#6840)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "a5434b6b4d9bef9eea29bf33f08e7f13753057a9", "committedDate": "2022-11-02 20:02:18 -0400", "message": "[HUDI-3963] Use Lock-Free Message Queue Disruptor Improving Hoodie Writing Efficiency  (#5416)"}, {"oid": "6b73c814f931f8dcdec500a4364354ac9488ce68", "committedDate": "2022-11-17 01:54:54 +0800", "message": "[HUDI-5209] Fixing `QueueBasedExecutor` in Spark bundles (missing Disruptor as dep) (#7188)"}, {"oid": "91e0db57b94c479a73e4f78d571b50c1e5ea541f", "committedDate": "2022-11-23 09:04:20 +0800", "message": "[MINOR] Use direct marker for spark engine when timeline server is disabled (#7272)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "1cdbf68d4ee641b9e7eb0129a26e0f969b37d8ca", "committedDate": "2022-11-28 22:48:06 -0500", "message": "[HUDI-5242] Do not fail Meta sync in Deltastreamer when inline table service fails (#7243)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "8d13a7e383c30ec1421b68dd052fbd33f438bc3d", "committedDate": "2022-12-15 09:18:54 -0800", "message": "[HUDI-5023] Consuming records from Iterator directly instead of using inner message queue (#7174)"}, {"oid": "16d33ba3cb953435660566ba2ec63a45204a7814", "committedDate": "2023-01-15 21:46:12 -0800", "message": "[HUDI-3654] Add new module `hudi-metaserver` (#5064)"}, {"oid": "c9bc03ed8681ab64eea2520f5511464915389c51", "committedDate": "2023-01-17 07:24:16 -0800", "message": "[HUDI-4148] Add client for Hudi table service manager (TSM) (#6732)"}, {"oid": "ec5022b4fdd94c106bf243038aadf781f31b5be9", "committedDate": "2023-01-18 09:30:52 -0800", "message": "[MINOR] Unify naming for record merger (#7660)"}, {"oid": "c18d6153e105ac34fb410c60ce8a153327931782", "committedDate": "2023-01-23 10:03:19 -0800", "message": "[HUDI-1575] Early Conflict Detection For Multi-writer (#6133)"}, {"oid": "2fc20c186b77017f7f1c6f6abe8559a9e8cfe578", "committedDate": "2023-01-24 20:04:55 +0530", "message": "[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)"}, {"oid": "c95abd3213f4806f535c6d7cb8b346616c3368fb", "committedDate": "2023-01-25 19:01:33 +0530", "message": "Revert \"[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)\" (#7747)"}, {"oid": "7e35874c7ba68dfa32b3b27ece35112cc434a7c6", "committedDate": "2023-01-25 14:34:18 -0800", "message": "[HUDI-5617] Rename configs for async conflict detector for clarity (#7750)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3a08bdc3f971b3534e8fb6f34772340cfdf055a9", "committedDate": "2023-01-25 19:29:42 -0800", "message": "[HUDI-5363] Removing default value for shuffle parallelism configs (#7723)"}, {"oid": "ff590c6d72c523b41c0790087053fd3933564ac8", "committedDate": "2023-01-27 18:56:32 -0800", "message": "[HUDI-5023] Switching default Write Executor type to `SIMPLE` (#7476)"}, {"oid": "c21eca564c6426413cbdc9e83bc40ad7c59c7e5d", "committedDate": "2023-01-28 13:23:30 -0500", "message": "[HUDI-5626] Rename CDC logging mode options (#7760)"}, {"oid": "2c56aa4ce994714a57402399c1bec99579926f66", "committedDate": "2023-01-28 14:03:02 -0600", "message": "[HUDI-5631] Improve defaults of early conflict detection configs (#7770)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "88d8e5e96d5f5ee553b8405e32c79388e3ed3c09", "committedDate": "2023-01-29 14:57:05 -0800", "message": "[MINOR] Cleaning up recently introduced configs (#7772)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "7064c380506814964dd85773e2ee7b7f187b88c3", "committedDate": "2023-02-01 11:19:45 -0800", "message": "[MINOR] Restoring existing behavior for `DeltaStreamer` Incremental Source (#7810)"}, {"oid": "ef3a17e3d97428a6f0d6e7ac888747d65a8792c5", "committedDate": "2023-02-04 15:14:52 +0800", "message": "[HUDI-5692] SpillableMapBasePath should be lazily loaded (#7837)"}, {"oid": "ff832f4d86091b71af42cc46c2aa209d80396899", "committedDate": "2023-02-04 17:39:58 +0800", "message": "[MINOR] Validate configs for OCC early conflict detection (#7848)"}, {"oid": "0c9465f2ab6cf6472df3046c681b72290e6034bd", "committedDate": "2023-02-05 00:28:20 -0800", "message": "[MINOR] Improve configuration configs (#7855)"}, {"oid": "53fca761be1db224dc384238a7d2853ff2a1d227", "committedDate": "2023-02-07 19:59:06 -0500", "message": "[MINOR] fixed docs for WRITE_EXECUTOR_TYPE (#7880)"}, {"oid": "be92be657a348954cc21062ca24e8a10caea17ee", "committedDate": "2023-02-20 10:05:09 +0800", "message": "[HUDI-5786] Add a new config to specific spark write rdd storage level (#7941)"}, {"oid": "0c84482aa915611db17f4505974c876240a8101c", "committedDate": "2023-02-20 20:00:31 +0530", "message": "[HUDI-5774] Fix prometheus configs for metadata table and support metric labels (#7933)"}, {"oid": "d705dcc4188223fbd824f36a5d211abeda7b1f23", "committedDate": "2023-02-24 10:23:25 +0800", "message": "[HUDI-5173] Skip if there is only one file in clusteringGroup  (#7159)"}, {"oid": "e131505a4f0bfef6e8cca04e102b3ea1c6d308e2", "committedDate": "2023-02-27 12:26:50 +0530", "message": "[HUDI-5838] Mask sensitive info while printing hudi properties in DeltaStreamer  (#8027)"}, {"oid": "c7cebf445f4dc6895cc343a396b0ea1871b362e7", "committedDate": "2023-02-27 23:57:08 -0500", "message": "[HUDI-5843] multiwriter deltastreamer checkpoints (#8043)"}, {"oid": "81e6e854883a94d41ae5b7187c608a8ddbc7bf35", "committedDate": "2023-03-03 21:06:43 +0530", "message": "[HUDI-5847] Add support for multiple metric reporters and metric labels (#8041)"}, {"oid": "a8312a9b8c39f4baabf753974fa092c4767abb72", "committedDate": "2023-03-08 14:46:01 +0800", "message": "[HUDI-5887] Distinguish the single writer enabling metadata table and multi-writer use cases for lock guard (#8111)"}, {"oid": "38e4078d23b00d0acdd02a28eec08560d175cdc9", "committedDate": "2023-03-13 21:30:19 -0700", "message": "[MINOR] Ignoring warn msg for timeline server for metadata table (#8168)"}, {"oid": "e51b4575cb7642eb61bcc02d95c99466dd3e8eda", "committedDate": "2023-03-17 15:17:24 -0700", "message": "[HUDI-5920] Improve documentation of parallelism configs (#8157)"}, {"oid": "cb1395a820febc4bb13c544369ca94a55fff0a29", "committedDate": "2023-03-30 11:07:29 -0700", "message": "[HUDI-5893] Mark advanced configs (#8295)"}, {"oid": "c53d9fbe019a43f31b3eb7556ff109d71287cf6c", "committedDate": "2023-03-31 13:30:34 -0700", "message": "[HUDI-5900] Clean up unused metadata configs (#8125)"}, {"oid": "6fd885fb3dc5c66caf6a1775fa87b4f7212056d8", "committedDate": "2023-03-31 21:21:28 -0700", "message": "[HUDI-5740] Refactor Deltastreamer and schema providers to use HoodieConfig/ConfigProperty (#8152)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "f9f110695fc69f2d7085648a6610888bb10ad8e4", "committedDate": "2023-04-19 04:09:48 -0400", "message": "[HUDI-6056] Validate archival configs alignment with cleaner configs with policy based on hours (#8422)"}, {"oid": "44a0c29560db2d85e3b0d963beed55225596baff", "committedDate": "2023-04-21 09:50:34 +0800", "message": "[HUDI-6100] Fixed overflow in setting log block size by making it long everywhere (#8495)"}, {"oid": "5a5c4863452197def89390beb0ab584eb08aabb6", "committedDate": "2023-04-21 23:06:46 -0700", "message": "[HUDI-5934] Remove archival configs for metadata table (#8319)"}, {"oid": "9a9dd3a82d3e69f1d5eebe46c79c8fd0dc2355db", "committedDate": "2023-04-23 16:37:48 +0800", "message": "[HUDI-6123] Auto adjust lock configs for single writer (#8542)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "e04dc0951cf21122f0d3dd4f673b87b663253109", "committedDate": "2023-05-04 04:05:13 -0700", "message": "[HUDI-5315] Use sample writes to estimate record size (#8390)"}, {"oid": "cabcb2bf2cddedeb3a34047af3935b27cfdfb858", "committedDate": "2023-05-05 06:28:14 -0700", "message": "[HUDI-5968] Fix global index duplicate and handle custom payload when update partition (#8490)"}, {"oid": "66e838c8f18b8ca50a3839f4768da5edfed0c416", "committedDate": "2023-05-06 15:09:03 +0800", "message": "[MINOR] Remove redundant advanced config marking (#8600)"}, {"oid": "6cdc1f583ef8d1281d2171c42fc982e2715f08f8", "committedDate": "2023-05-08 07:29:57 -0700", "message": "[HUDI-5895] Remove bootstrap key generator configs (#8557)"}, {"oid": "a5bd50c067f2a82be2470d4649f2be3007404c40", "committedDate": "2023-05-23 15:54:42 +0800", "message": "[MINOR] Disable schema validation in master (#8781)"}, {"oid": "9d58ee4b1f1fce213ee3f4ff478eb003da943924", "committedDate": "2023-05-24 20:09:54 +0800", "message": "[HUDI-5994] Bucket index supports bulk insert row writer (#8776)"}, {"oid": "59786113fae88382f03412c7c79f7a827bf9393f", "committedDate": "2023-05-26 12:06:32 +0530", "message": "[HUDI-5998] Speed up reads from bootstrapped tables in spark (#8303)"}, {"oid": "41e1e9a4fda2e399f4d50222e81bdfa713bbce23", "committedDate": "2023-05-30 14:58:33 -0700", "message": "[MINOR] Ensure metrics prefix does not contain any dot. (#8599)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353013", "body": "We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?", "bodyText": "We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?", "bodyHTML": "<p dir=\"auto\">We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?</p>", "author": "yanghua", "createdAt": "2021-01-18T07:07:50Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM2MDA2NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559360064", "bodyText": "Now we have 4 types of CompactType, and compactable may be true or false.\nwhich means  it will have 8 situations.  If we add log in to each case, I think a little bit tedious. wdyt?", "author": "Karl-WangSK", "createdAt": "2021-01-18T07:26:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM2NDg0Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559364846", "bodyText": "so I discuss with wangxianghu , just give a summative statement.", "author": "Karl-WangSK", "createdAt": "2021-01-18T07:38:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM3NTc4NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559375785", "bodyText": "reasonable, The key point that I want to raise up is: it would be better to describe the trigger condition.", "author": "yanghua", "createdAt": "2021-01-18T08:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTQ0Mzk3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559443976", "bodyText": "ok, added !", "author": "Karl-WangSK", "createdAt": "2021-01-18T09:58:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}], "type": "inlineReview", "revised_code": {"commit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..e0b951ab3e 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -109,20 +109,28 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n     Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n     switch (compactType) {\n       case COMMIT_NUM:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n         break;\n       case NUM_OR_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n         break;\n       case NUM_AND_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n         break;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n", "next_change": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -105,49 +105,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n         compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n-        break;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n+              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n+              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n     }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n-    }\n-    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -157,7 +157,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n         }\n         return compactable;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n     }\n   }\n \n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b7d4d58334..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,63 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzQ5OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353499", "body": "We may not tell why does not trigger compaction. It means nothing happened, right.", "bodyText": "We may not tell why does not trigger compaction. It means nothing happened, right.", "bodyHTML": "<p dir=\"auto\">We may not tell why does not trigger compaction. It means nothing happened, right.</p>", "author": "yanghua", "createdAt": "2021-01-18T07:09:10Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -105,41 +105,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n-        break;\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n+              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n+              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_OR_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_AND_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n     }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n-    }\n-    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -157,7 +157,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n         }\n         return compactable;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n     }\n   }\n \n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b7d4d58334..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 67db2dceda..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,63 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n-            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"oid": "fbd87b4915f52d3010edeebe66b966855e1abfdc", "url": "https://github.com/apache/hudi/commit/fbd87b4915f52d3010edeebe66b966855e1abfdc", "message": "update", "committedDate": "2021-01-18T07:34:35Z", "type": "commit"}, {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c", "url": "https://github.com/apache/hudi/commit/96e596a8c14dc2a45606a5363e39b87474747b5c", "message": "add log in each case", "committedDate": "2021-01-18T09:56:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTg3Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501872", "body": "Still exists?", "bodyText": "Still exists?", "bodyHTML": "<p dir=\"auto\">Still exists?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:32:35Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4259478b945ea0440e0c28c56dc6290ee2df4442", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ea52beced6..ccf53a56a4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ccf53a56a4..b1155e3bc9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+\n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "a318c91498d1899107f886b73c3404cd5f50258f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex b1155e3bc9..251ea2305f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +477,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ea52beced6..e3c1ef6819 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -477,7 +485,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-\n+  \n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex e3c1ef6819..4e493e4432 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -485,7 +515,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+\n   public long getClusteringSmallFileLimit() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n   }\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4e493e4432..1783535e82 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -372,251 +562,251 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n    * compaction properties.\n    */\n   public HoodieCleaningPolicy getCleanerPolicy() {\n-    return HoodieCleaningPolicy.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n+    return HoodieCleaningPolicy.valueOf(getString(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n   }\n \n   public int getCleanerFileVersionsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP);\n   }\n \n   public int getCleanerCommitsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP);\n   }\n \n   public int getMaxCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getMinCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getParquetSmallFileLimit() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES));\n+    return getInt(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES);\n   }\n \n   public double getRecordSizeEstimationThreshold() {\n-    return Double.parseDouble(props.getProperty(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP));\n+    return getDouble(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP);\n   }\n \n   public int getCopyOnWriteInsertSplitSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE);\n   }\n \n   public int getCopyOnWriteRecordSizeEstimate() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE);\n   }\n \n   public boolean shouldAutoTuneInsertSplits() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS));\n+    return getBoolean(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS);\n   }\n \n   public int getCleanerParallelism() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_PARALLELISM));\n+    return getInt(HoodieCompactionConfig.CLEANER_PARALLELISM);\n   }\n \n   public boolean isAutoClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.AUTO_CLEAN_PROP);\n   }\n \n   public boolean isAsyncClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.ASYNC_CLEAN_PROP);\n   }\n \n   public boolean incrementalCleanerModeEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE);\n   }\n \n-  public boolean isInlineCompaction() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_PROP));\n+  public boolean inlineCompactionEnabled() {\n+    return getBoolean(HoodieCompactionConfig.INLINE_COMPACT_PROP);\n   }\n \n   public CompactionTriggerStrategy getInlineCompactTriggerStrategy() {\n-    return CompactionTriggerStrategy.valueOf(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n+    return CompactionTriggerStrategy.valueOf(getString(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n   }\n \n   public int getInlineCompactDeltaCommitMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP);\n   }\n \n   public int getInlineCompactDeltaSecondsMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP);\n   }\n \n   public CompactionStrategy getCompactionStrategy() {\n-    return ReflectionUtils.loadClass(props.getProperty(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n+    return ReflectionUtils.loadClass(getString(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n   }\n \n   public Long getTargetIOPerCompactionInMB() {\n-    return Long.parseLong(props.getProperty(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP));\n+    return getLong(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP);\n   }\n \n   public Boolean getCompactionLazyBlockReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP);\n   }\n \n   public Boolean getCompactionReverseLogReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP);\n   }\n \n-  public boolean isInlineClustering() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n+  public boolean inlineClusteringEnabled() {\n+    return getBoolean(HoodieClusteringConfig.INLINE_CLUSTERING_PROP);\n   }\n \n   public boolean isAsyncClusteringEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    return getBoolean(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY);\n   }\n \n   public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClusteringEnabled();\n+    return inlineClusteringEnabled() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP));\n+    return getInt(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP);\n   }\n \n   public String getPayloadClass() {\n-    return props.getProperty(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n+    return getString(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n   }\n \n   public int getTargetPartitionsPerDayBasedCompaction() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP));\n+    return getInt(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP);\n   }\n \n   public int getCommitArchivalBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP));\n+    return getInt(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP);\n   }\n \n   public Boolean shouldCleanBootstrapBaseFile() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED);\n   }\n \n   public String getClusteringUpdatesStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n+    return getString(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n   }\n \n   public HoodieFailedWritesCleaningPolicy getFailedWritesCleanPolicy() {\n     return HoodieFailedWritesCleaningPolicy\n-        .valueOf(props.getProperty(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n+        .valueOf(getString(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n   }\n \n   /**\n    * Clustering properties.\n    */\n   public String getClusteringPlanStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n   }\n \n   public String getClusteringExecutionStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n   }\n \n   public long getClusteringMaxBytesInGroup() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP);\n   }\n \n   public long getClusteringSmallFileLimit() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT);\n   }\n \n   public int getClusteringMaxNumGroups() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS);\n   }\n \n   public long getClusteringTargetFileMaxBytes() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES);\n   }\n \n   public int getTargetPartitionsForClustering() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS);\n   }\n \n   public String getClusteringSortColumns() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n+    return getString(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n \n   /**\n    * index properties.\n    */\n   public HoodieIndex.IndexType getIndexType() {\n-    return HoodieIndex.IndexType.valueOf(props.getProperty(HoodieIndexConfig.INDEX_TYPE_PROP));\n+    return HoodieIndex.IndexType.valueOf(getString(HoodieIndexConfig.INDEX_TYPE_PROP));\n   }\n \n   public String getIndexClass() {\n-    return props.getProperty(HoodieIndexConfig.INDEX_CLASS_PROP);\n+    return getString(HoodieIndexConfig.INDEX_CLASS_PROP);\n   }\n \n   public int getBloomFilterNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES));\n+    return getInt(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES);\n   }\n \n   public double getBloomFilterFPP() {\n-    return Double.parseDouble(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_FPP));\n+    return getDouble(HoodieIndexConfig.BLOOM_FILTER_FPP);\n   }\n \n   public String getHbaseZkQuorum() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n   }\n \n   public int getHbaseZkPort() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP);\n   }\n \n   public String getHBaseZkZnodeParent() {\n-    return props.getProperty(HoodieIndexConfig.HBASE_ZK_ZNODEPARENT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_ZNODEPARENT);\n   }\n \n   public String getHbaseTableName() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n   }\n \n   public int getHbaseIndexGetBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHBaseIndexRollbackSync() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC);\n   }\n \n   public int getHbaseIndexPutBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHbaseIndexPutBatchSizeAutoCompute() {\n-    return Boolean.valueOf(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP);\n   }\n \n   public String getHBaseQPSResourceAllocatorClass() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n+    return getString(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n   }\n \n   public String getHBaseQPSZKnodePath() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n   }\n \n   public String getHBaseZkZnodeSessionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n   }\n \n   public String getHBaseZkZnodeConnectionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n   }\n \n   public boolean getHBaseIndexShouldComputeQPSDynamically() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY));\n+    return getBoolean(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY);\n   }\n \n   public int getHBaseIndexDesiredPutsTime() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS));\n+    return getInt(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS);\n   }\n \n   public String getBloomFilterType() {\n-    return props.getProperty(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n+    return getString(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n   }\n \n   public int getDynamicBloomFilterMaxNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES));\n+    return getInt(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES);\n   }\n \n   /**\n", "next_change": {"commit": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 1783535e82..4cbbbbc950 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -815,15 +947,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -947,15 +1294,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": null}, {"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -963,11 +1310,11 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * Hoodie jobs to an Hbase Region Server\n    */\n   public int getHbaseIndexMaxQPSPerRegionServer() {\n-    return getInt(HoodieHBaseIndexConfig.HBASE_MAX_QPS_PER_REGION_SERVER);\n+    return getInt(HoodieHBaseIndexConfig.MAX_QPS_PER_REGION_SERVER);\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_UPDATE_PARTITION_PATH);\n+    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 448ce9f7b4..eb3df38428 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1314,7 +1357,7 @@ public class HoodieWriteConfig extends HoodieConfig {\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n+    return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "e5faf2cc8470f83323b34305a135461c7e43d14e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex eb3df38428..1650a79ee9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1360,6 +1553,10 @@ public class HoodieWriteConfig extends HoodieConfig {\n     return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n+  public int getHBaseIndexRegionCount() {\n+    return getInt(HoodieHBaseIndexConfig.BUCKET_NUMBER);\n+  }\n+\n   public int getBloomIndexParallelism() {\n     return getInt(HoodieIndexConfig.BLOOM_INDEX_PARALLELISM);\n   }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "77ba561a6bacbf9e7dc9c1582eb068f7ad800cd9", "committedDate": "2021-02-23 20:56:58 -0500", "message": "[HUDI-1347] Fix Hbase index to make rollback synchronous (via config) (#2188)"}, {"oid": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "committedDate": "2021-03-02 21:58:41 -0800", "message": "[HUDI-1553] Configuration and metrics for the TimelineService. (#2495)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "7fed7352bd506e20e5316bb0b3ed9e5c1e9c76df", "committedDate": "2021-05-27 13:38:33 +0800", "message": "[HUDI-1865] Make embedded time line service singleton (#2899)"}, {"oid": "f760ec543ec9ea23b7d4c9f61c76a283bd737f27", "committedDate": "2021-06-07 23:24:32 -0700", "message": "[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)"}, {"oid": "7261f0850727aea611cd34e1bb07d684b44534f6", "committedDate": "2021-06-08 09:26:10 -0400", "message": "[HUDI-1929] Support configure KeyGenerator by type (#2993)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "b376cefc3e899aa28992796925708746561d6087", "committedDate": "2021-07-01 18:48:59 +0800", "message": "[MINOR] Add Documentation to KEYGENERATOR_TYPE_PROP (#3196)"}, {"oid": "62a1ad8b3a2a3c1dabba0a4622117636920b6c13", "committedDate": "2021-07-03 20:27:37 +0800", "message": "[HUDI-1930] Bootstrap support configure KeyGenerator by type (#3170)"}, {"oid": "a4dcbb5c5a2a94e4f69524194d8777d082af31ab", "committedDate": "2021-07-05 23:03:41 -0700", "message": "[HUDI-2028] Implement RockDbBasedMap as an alternate to DiskBasedMap in ExternalSpillableMap (#3194)"}, {"oid": "ea9e5d0e8b7557ef82631ac173d67f15bad13690", "committedDate": "2021-07-07 11:15:25 -0400", "message": "[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)"}, {"oid": "5804ad8e32ae05758ebc5e47f5d4fb4db371ab52", "committedDate": "2021-07-11 14:43:38 -0400", "message": "[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)"}, {"oid": "b0089b894ad12da11fbd6a0fb08508c7adee68e6", "committedDate": "2021-07-13 00:24:40 -0400", "message": "[MINOR] Fix EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION config (#3250)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "d024439764ceeca6366cb33689b729a1c69a6272", "committedDate": "2021-07-14 22:57:38 -0400", "message": "[HUDI-2029] Implement compression for DiskBasedMap in Spillable Map (#3128)"}, {"oid": "38cd74b56328d154c004f04f1335784529d8e93d", "committedDate": "2021-07-16 12:24:41 +0800", "message": "[MINOR] Allow users to choose ORC as base file format in Spark SQL (#3279)"}, {"oid": "d5026e9a24850bdcce9f6df3686bf2235d7d01c4", "committedDate": "2021-07-19 20:43:48 -0400", "message": "[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3", "committedDate": "2021-07-26 17:21:04 -0400", "message": "[HUDI-2176, 2178, 2179] Adding virtual key support to COW table (#3306)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "bbadac7de1bb57300ca7e796ebb401fdbb66a0f8", "committedDate": "2021-07-28 21:30:18 -0700", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write (#2296)"}, {"oid": "826a04d1425f47fdd80c293569a359021d1b6586", "committedDate": "2021-08-03 12:07:45 -0700", "message": "[HUDI-2072] Add pre-commit validator framework (#3153)"}, {"oid": "91bb0d13184c57ec08f02db3337e734bc20739c4", "committedDate": "2021-08-03 17:50:30 -0700", "message": "[HUDI-2255] Refactor Datasource options (#3373)"}, {"oid": "70b6bd485f8d1ef9a9b15999edda2472d0b4d65a", "committedDate": "2021-08-06 22:53:08 -0400", "message": "[HUDI-1468] Support custom clustering strategies and preserve commit metadata as part of clustering (#3419)"}, {"oid": "b4441abcf74951ec0ce28593b96baa84456a97d3", "committedDate": "2021-08-09 10:10:15 -0700", "message": "[HUDI-2194] Skip the latest N partitions when choosing partitions to create ClusteringPlan (#3300)"}, {"oid": "21db6d7a84d4a83ec98c110e92ff9c92d05dd530", "committedDate": "2021-08-10 20:23:23 +0800", "message": "[HUDI-1771] Propagate CDC format for hoodie (#3285)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "76bc686a77a485544c9e75cfefa59fa021470a0c", "committedDate": "2021-08-12 15:45:57 -0700", "message": "[HUDI-1292] Created a config to enable/disable syncing of metadata table. (#3427)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "642b1b671de8c6a35ae7858c9b03d3dff70889dd", "committedDate": "2021-08-13 19:29:22 -0400", "message": "[HUDI-2151]  Flipping defaults (#3452)"}, {"oid": "9056c68744a3f31ac2625e004ec6e155d2e86be9", "committedDate": "2021-08-14 08:18:49 -0400", "message": "[HUDI-2305] Add MARKERS.type and fix marker-based rollback (#3472)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "e39d0a2f2852ef51c524e5b16a1cecb099674eed", "committedDate": "2021-08-20 02:42:59 -0700", "message": "Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)"}, {"oid": "de94787a85b272f79181dff73907b0f20855ee78", "committedDate": "2021-08-24 21:45:17 +0800", "message": "[HUDI-2345] Hoodie columns sort partitioner for bulk insert (#3523)"}, {"oid": "21fd6edfe7721c674b40877fbbdbac71b36bf782", "committedDate": "2021-09-02 11:14:09 +0800", "message": "[HUDI-2384] Change log file size config to long (#3577)"}, {"oid": "e528dd798ab8ce6e4d444d2d771c107c503e8f25", "committedDate": "2021-09-10 18:20:26 -0700", "message": "[HUDI-2394] Implement Kafka Sink Protocol for Hudi for Ingesting Immutable Data (#3592)"}, {"oid": "2791fb9a964b39ef9aaec83eafd080013186b2eb", "committedDate": "2021-09-16 15:08:10 +0800", "message": "[HUDI-2423] Separate some config logic from HoodieMetricsConfig into HoodieMetricsGraphiteConfig HoodieMetricsJmxConfig (#3652)"}, {"oid": "61d009608899bc70c1372d5cb00a2f35e188c30c", "committedDate": "2021-09-17 19:39:55 +0800", "message": "[HUDI-2434] Make periodSeconds of GraphiteReporter configurable (#3667)"}, {"oid": "06c2cc2c8b1ad88bb4c9bbdb496053a079767e9b", "committedDate": "2021-09-24 13:33:34 +0800", "message": "[HUDI-2385] Make parquet dictionary encoding configurable (#3578)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "08c35a55b3133ddaead0581c9129e88a869421a1", "committedDate": "2021-11-05 13:03:41 -0400", "message": "[HUDI-2526] Make spark.sql.parquet.writeLegacyFormat configurable (#3917)"}, {"oid": "dfe3b84715e8fecfa96ef615c217f5eaf0da94e8", "committedDate": "2021-11-09 17:37:59 -0500", "message": "[HUDI-2579] Make deltastreamer checkpoint state merging more explicit (#3820)"}, {"oid": "4f217fe718b0b4e9656c2a45f7b89cb5df15a4f2", "committedDate": "2021-11-12 07:29:37 -0500", "message": "[HUDI-2151] Part1 Setting default parallelism to 200 for some of write configs (#3948)"}, {"oid": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "committedDate": "2021-11-13 09:12:33 +0800", "message": "[HUDI-2697] Minor changes about hbase index config. (#3927)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "ce7d2333078e4e1f16de1bce6d448c5eef1e4111", "committedDate": "2021-11-17 11:51:28 +0530", "message": "[HUDI-2151] Part3 Enabling marker based rollback as default rollback strategy (#3950)"}, {"oid": "2d3f2a3275ba615245fcabda96b8282cb86940ad", "committedDate": "2021-11-17 14:43:00 -0500", "message": "[HUDI-2734] Setting default metadata enable as false for Java (#4003)"}, {"oid": "3bdab01a498d605faede833af2d88cd8ec9237a0", "committedDate": "2021-11-22 19:19:59 -0500", "message": "[HUDI-2550] Expand File-Group candidates list for appending for MOR tables (#3986)"}, {"oid": "e22150fe15da2985b20077d2e0734fcd46b85a6f", "committedDate": "2021-11-23 07:29:03 +0530", "message": "[HUDI-1937] Rollback unfinished replace commit to allow updates (#3869)"}, {"oid": "ca9bfa2a4000575dbaa379c91898786f040a9917", "committedDate": "2021-11-23 14:23:28 +0530", "message": "[HUDI-2332] Add clustering and compaction in Kafka Connect Sink (#3857)"}, {"oid": "435ea1543c034194d7ca0b589b7b043fc49c07ac", "committedDate": "2021-11-24 18:26:40 -0500", "message": "[HUDI-2793] Fixing deltastreamer checkpoint fetch/copy over (#4034)"}, {"oid": "88067f57a23575aae3c371a7c7871e4207ca3bea", "committedDate": "2021-11-25 19:17:38 +0800", "message": "[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)"}, {"oid": "e0125a7911d77afd4a82a49caaccaa3c10df0377", "committedDate": "2021-11-25 13:33:16 -0800", "message": "[HUDI-2801] Add Amazon CloudWatch metrics reporter (#4081)"}, {"oid": "d1e83e4ba0b881f9410f0ae9f3799c967b6891cb", "committedDate": "2021-11-26 16:41:05 -0500", "message": "[HUDI-2767] Enabling timeline-server-based marker as default (#4112)"}, {"oid": "24380c20606d63c7c129cb45a73f786f223e7d39", "committedDate": "2021-11-30 17:47:16 -0800", "message": "Revert \"[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)\" (#4169)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "9797fdfbb27ca8f5f06875ad958b597becc27a8d", "committedDate": "2021-12-10 19:42:20 -0800", "message": "[HUDI-2974] Make the prefix for metrics name configurable (#4274)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "2444f40a4be5bbf0bf210dee5690267a9a1e35c8", "committedDate": "2021-12-31 11:07:52 +0530", "message": "[HUDI-3095] abstract partition filter logic to enable code reuse (#4454)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "827549949c4ac472fdc528a35ae421b20e2cc83a", "committedDate": "2022-01-08 10:22:44 -0500", "message": "[HUDI-2909] Handle logical type in TimestampBasedKeyGenerator (#4203)"}, {"oid": "251d4eb3b64704b9dd51bf6f6ecb5bf47089b745", "committedDate": "2022-01-10 08:40:24 +0530", "message": "[HUDI-3030] InProcessLockPovider as default when any async servcies enabled with no lock provider override (#4406)"}, {"oid": "9fe28e56b49c7bf68ae2d83bfe89755314aa793b", "committedDate": "2022-01-11 23:23:55 -0800", "message": "[HUDI-3045] New clustering regex match config to choose partitions when building clustering plan (#4346)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "14d08bb64c4bea20a692b3d3bced5cc9800cd541", "committedDate": "2022-01-20 15:34:56 +0400", "message": "[MINOR] Fix typo in the doc of BULK_INSERT_SORT_MODE (#4652)"}, {"oid": "bc7882cbe924ce8000f4a738b8673fe7a5cf69fb", "committedDate": "2022-01-24 16:53:54 -0500", "message": "[HUDI-2872][HUDI-2646] Refactoring layout optimization (clustering) flow to support linear ordering (#4606)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "538ec44fa8a23926b584c3bcdd24feb9894d4c51", "committedDate": "2022-02-15 09:49:53 -0500", "message": "[HUDI-2931] Add config to disable table services (#4777)"}, {"oid": "359fbfde798b50edc06ee1d0520efcd971a289bc", "committedDate": "2022-02-20 15:31:31 -0500", "message": "[HUDI-2648] Retry FileSystem action instead of failed directly. (#3887)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "92cdc5987a2b5a6faecec96224f545ab49ee6ef2", "committedDate": "2022-02-25 11:30:10 -0500", "message": "[HUDI-3515] Making rdd unpersist optional at the end of writes (#4898)"}, {"oid": "62f534d00228653059c4fed944d444632bc07091", "committedDate": "2022-03-04 09:33:16 +0800", "message": "[HUDI-3445] Support Clustering Command Based on Call Procedure Command for Spark SQL (#4901)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "f0bcee3c014cf59bdad3eaf8212d94a589073f0b", "committedDate": "2022-03-07 13:42:03 -0500", "message": "[HUDI-3561] Avoid including whole `MultipleSparkJobExecutionStrategy` object into the closure for Spark to serialize (#4954)"}, {"oid": "29040762fa511f89e678dc15ca5ae7f9f097fb8a", "committedDate": "2022-03-07 17:01:49 -0500", "message": "[HUDI-3576] Configuring timeline refreshes based on latest commit (#4973)"}, {"oid": "575bc6346825796e091a12be5a53b04980f82637", "committedDate": "2022-03-08 10:39:04 -0500", "message": "[HUDI-3356][HUDI-3203] HoodieData for metadata index records; BloomFilter construction from index based on the type param (#4848)"}, {"oid": "034addaef5834eff09cfd9ac5cc2656df95ca0e8", "committedDate": "2022-03-09 21:45:25 -0500", "message": "[HUDI-3396] Make sure `BaseFileOnlyViewRelation` only reads projected columns (#4818)"}, {"oid": "95e6e538109af9fe60aa99219e4aa1d7ce9511e2", "committedDate": "2022-03-17 01:25:04 -0400", "message": "[HUDI-3404] Automatically adjust write configs based on metadata table and write concurrency mode (#4975)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "5f570ea151d0212ab1bb2d1f5693035626b76d31", "committedDate": "2022-03-21 22:56:31 -0400", "message": "[HUDI-2883] Refactor hive sync tool / config to use reflection and standardize configs (#4175)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "fb45fc9cb9581abc40922ddcbee21dfc016d4edc", "committedDate": "2022-04-01 20:14:07 -0700", "message": "[HUDI-3773] Fix parallelism used for metadata table bloom filter index (#5209)"}, {"oid": "84064a9b081c246f306855ae125f0dae5eb8f6d0", "committedDate": "2022-04-02 23:44:10 -0700", "message": "[HUDI-3772] Fixing auto adjustment of lock configs for deltastreamer (#5207)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "3e97c88c4ff9cbeb312805daf6d52da5f1bcb0bd", "committedDate": "2022-04-09 15:30:11 -0400", "message": "[HUDI-3807] Add a new config to control the use of metadata index in HoodieBloomIndex (#5268)"}, {"oid": "bab691692e31ae3e432bb6cf4c90436fba408a2c", "committedDate": "2022-04-13 17:33:26 -0400", "message": "[HUDI-3686] Fix inline and async table service check in HoodieWriteConfig (#5307)"}, {"oid": "4e928a6fe1ddd7e126ab155bb1c03f8630bb873d", "committedDate": "2022-04-28 15:18:56 -0700", "message": "[HUDI-3943] Some description fixes for 0.10.1 docs (#5447)"}, {"oid": "f492c52ee4d2f3d6dfbbf574833f837322166fbd", "committedDate": "2022-04-29 16:21:52 -0700", "message": "[HUDI-3862] Fix default configurations of HoodieHBaseIndexConfig (#5308)"}, {"oid": "6e16e719cd614329018cd34a7c57d342fe2fa376", "committedDate": "2022-05-14 07:37:31 -0400", "message": "[HUDI-3980] Suport kerberos hbase index (#5464)"}, {"oid": "61030d8e7a5a05e215efed672267ac163b0cbcf6", "committedDate": "2022-05-16 11:07:01 +0800", "message": "[HUDI-3123] consistent hashing index: basic write path (upsert/insert) (#4480)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "cf837b49008fd351a3f89beb5e4e5c17c30b9a3c", "committedDate": "2022-05-25 19:38:56 +0530", "message": "[HUDI-3193] Decouple hudi-aws from hudi-client-common (#5666)"}, {"oid": "7f8630cc57fbb9d29e8dc7ca87b582264da073fd", "committedDate": "2022-06-02 09:48:48 +0800", "message": "[HUDI-4167] Remove the timeline refresh with initializing hoodie table (#5716)"}, {"oid": "4f6fc726d0d3d2dd427210228bbb36cf18893a92", "committedDate": "2022-06-06 10:21:00 -0700", "message": "[HUDI-4140] Fixing hive style partitioning and default partition with bulk insert row writer with SimpleKeyGen and virtual keys (#5664)"}, {"oid": "35afdb4316d496bbb37ebb9e1598d84bd8a4000d", "committedDate": "2022-06-07 16:30:46 -0700", "message": "[HUDI-4178] Addressing performance regressions in Spark DataSourceV2 Integration (#5737)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "da28e38fe3d25e3ab212dc02312f0ae395371072", "committedDate": "2022-07-23 14:37:04 -0500", "message": "[HUDI-4071] Make NONE sort mode as default for bulk insert (#6195)"}, {"oid": "a0ffd05b7773c4c83714a60dcaa79332ee3aada3", "committedDate": "2022-07-23 16:10:53 -0700", "message": "[HUDI-4448] Remove the latest commit refresh for timeline server (#6179)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "e5faf2cc8470f83323b34305a135461c7e43d14e", "committedDate": "2022-07-26 18:09:17 +0800", "message": "[HUDI-4210] Create custom hbase index to solve data skew issue on hbase regions (#5797)"}, {"oid": "cdaec5a8da060157eb7426b5b70419c5f8868e04", "committedDate": "2022-07-27 14:47:49 -0700", "message": "[HUDI-4186] Support Hudi with Spark 3.3.0 (#5943)"}, {"oid": "767c196631240aeda5a8ef4603c17f05a407d8f7", "committedDate": "2022-08-06 18:19:29 -0400", "message": "[HUDI-4303] Adding 4 to 5 upgrade handler to check for old deprecated \"default\" partition value (#6248)"}, {"oid": "6badae46f0f3e743f0502bd34e124cf6cfabcec0", "committedDate": "2022-09-12 12:05:40 +0530", "message": "[HUDI-3558] Consistent bucket index: bucket resizing (split&merge) & concurrent write during resizing (#4958)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5e624698f78f4707d62c7f26b044a69a250aae43", "committedDate": "2022-09-22 09:17:09 -0400", "message": "[HUDI-4363] Support Clustering row writer to improve performance (#6046)"}, {"oid": "efe553b327bc025d242afa37221a740dca9b1ea6", "committedDate": "2022-09-23 18:36:48 +0800", "message": "[HUDI-4897] Refactor the merge handle in CDC mode (#6740)"}, {"oid": "58fe71d2b316a83b30ce4b80a36dd7beed001e58", "committedDate": "2022-09-29 01:37:46 -0400", "message": "[HUDI-4722] Added locking metrics for Hudi (#6502)"}, {"oid": "f3d4ce919d4909f9533255ee2a9a0450c8e44c73", "committedDate": "2022-10-01 18:21:23 +0800", "message": "[HUDI-4916] Implement change log feed for Flink (#6840)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "a5434b6b4d9bef9eea29bf33f08e7f13753057a9", "committedDate": "2022-11-02 20:02:18 -0400", "message": "[HUDI-3963] Use Lock-Free Message Queue Disruptor Improving Hoodie Writing Efficiency  (#5416)"}, {"oid": "6b73c814f931f8dcdec500a4364354ac9488ce68", "committedDate": "2022-11-17 01:54:54 +0800", "message": "[HUDI-5209] Fixing `QueueBasedExecutor` in Spark bundles (missing Disruptor as dep) (#7188)"}, {"oid": "91e0db57b94c479a73e4f78d571b50c1e5ea541f", "committedDate": "2022-11-23 09:04:20 +0800", "message": "[MINOR] Use direct marker for spark engine when timeline server is disabled (#7272)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "1cdbf68d4ee641b9e7eb0129a26e0f969b37d8ca", "committedDate": "2022-11-28 22:48:06 -0500", "message": "[HUDI-5242] Do not fail Meta sync in Deltastreamer when inline table service fails (#7243)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "8d13a7e383c30ec1421b68dd052fbd33f438bc3d", "committedDate": "2022-12-15 09:18:54 -0800", "message": "[HUDI-5023] Consuming records from Iterator directly instead of using inner message queue (#7174)"}, {"oid": "16d33ba3cb953435660566ba2ec63a45204a7814", "committedDate": "2023-01-15 21:46:12 -0800", "message": "[HUDI-3654] Add new module `hudi-metaserver` (#5064)"}, {"oid": "c9bc03ed8681ab64eea2520f5511464915389c51", "committedDate": "2023-01-17 07:24:16 -0800", "message": "[HUDI-4148] Add client for Hudi table service manager (TSM) (#6732)"}, {"oid": "ec5022b4fdd94c106bf243038aadf781f31b5be9", "committedDate": "2023-01-18 09:30:52 -0800", "message": "[MINOR] Unify naming for record merger (#7660)"}, {"oid": "c18d6153e105ac34fb410c60ce8a153327931782", "committedDate": "2023-01-23 10:03:19 -0800", "message": "[HUDI-1575] Early Conflict Detection For Multi-writer (#6133)"}, {"oid": "2fc20c186b77017f7f1c6f6abe8559a9e8cfe578", "committedDate": "2023-01-24 20:04:55 +0530", "message": "[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)"}, {"oid": "c95abd3213f4806f535c6d7cb8b346616c3368fb", "committedDate": "2023-01-25 19:01:33 +0530", "message": "Revert \"[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)\" (#7747)"}, {"oid": "7e35874c7ba68dfa32b3b27ece35112cc434a7c6", "committedDate": "2023-01-25 14:34:18 -0800", "message": "[HUDI-5617] Rename configs for async conflict detector for clarity (#7750)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3a08bdc3f971b3534e8fb6f34772340cfdf055a9", "committedDate": "2023-01-25 19:29:42 -0800", "message": "[HUDI-5363] Removing default value for shuffle parallelism configs (#7723)"}, {"oid": "ff590c6d72c523b41c0790087053fd3933564ac8", "committedDate": "2023-01-27 18:56:32 -0800", "message": "[HUDI-5023] Switching default Write Executor type to `SIMPLE` (#7476)"}, {"oid": "c21eca564c6426413cbdc9e83bc40ad7c59c7e5d", "committedDate": "2023-01-28 13:23:30 -0500", "message": "[HUDI-5626] Rename CDC logging mode options (#7760)"}, {"oid": "2c56aa4ce994714a57402399c1bec99579926f66", "committedDate": "2023-01-28 14:03:02 -0600", "message": "[HUDI-5631] Improve defaults of early conflict detection configs (#7770)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "88d8e5e96d5f5ee553b8405e32c79388e3ed3c09", "committedDate": "2023-01-29 14:57:05 -0800", "message": "[MINOR] Cleaning up recently introduced configs (#7772)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "7064c380506814964dd85773e2ee7b7f187b88c3", "committedDate": "2023-02-01 11:19:45 -0800", "message": "[MINOR] Restoring existing behavior for `DeltaStreamer` Incremental Source (#7810)"}, {"oid": "ef3a17e3d97428a6f0d6e7ac888747d65a8792c5", "committedDate": "2023-02-04 15:14:52 +0800", "message": "[HUDI-5692] SpillableMapBasePath should be lazily loaded (#7837)"}, {"oid": "ff832f4d86091b71af42cc46c2aa209d80396899", "committedDate": "2023-02-04 17:39:58 +0800", "message": "[MINOR] Validate configs for OCC early conflict detection (#7848)"}, {"oid": "0c9465f2ab6cf6472df3046c681b72290e6034bd", "committedDate": "2023-02-05 00:28:20 -0800", "message": "[MINOR] Improve configuration configs (#7855)"}, {"oid": "53fca761be1db224dc384238a7d2853ff2a1d227", "committedDate": "2023-02-07 19:59:06 -0500", "message": "[MINOR] fixed docs for WRITE_EXECUTOR_TYPE (#7880)"}, {"oid": "be92be657a348954cc21062ca24e8a10caea17ee", "committedDate": "2023-02-20 10:05:09 +0800", "message": "[HUDI-5786] Add a new config to specific spark write rdd storage level (#7941)"}, {"oid": "0c84482aa915611db17f4505974c876240a8101c", "committedDate": "2023-02-20 20:00:31 +0530", "message": "[HUDI-5774] Fix prometheus configs for metadata table and support metric labels (#7933)"}, {"oid": "d705dcc4188223fbd824f36a5d211abeda7b1f23", "committedDate": "2023-02-24 10:23:25 +0800", "message": "[HUDI-5173] Skip if there is only one file in clusteringGroup  (#7159)"}, {"oid": "e131505a4f0bfef6e8cca04e102b3ea1c6d308e2", "committedDate": "2023-02-27 12:26:50 +0530", "message": "[HUDI-5838] Mask sensitive info while printing hudi properties in DeltaStreamer  (#8027)"}, {"oid": "c7cebf445f4dc6895cc343a396b0ea1871b362e7", "committedDate": "2023-02-27 23:57:08 -0500", "message": "[HUDI-5843] multiwriter deltastreamer checkpoints (#8043)"}, {"oid": "81e6e854883a94d41ae5b7187c608a8ddbc7bf35", "committedDate": "2023-03-03 21:06:43 +0530", "message": "[HUDI-5847] Add support for multiple metric reporters and metric labels (#8041)"}, {"oid": "a8312a9b8c39f4baabf753974fa092c4767abb72", "committedDate": "2023-03-08 14:46:01 +0800", "message": "[HUDI-5887] Distinguish the single writer enabling metadata table and multi-writer use cases for lock guard (#8111)"}, {"oid": "38e4078d23b00d0acdd02a28eec08560d175cdc9", "committedDate": "2023-03-13 21:30:19 -0700", "message": "[MINOR] Ignoring warn msg for timeline server for metadata table (#8168)"}, {"oid": "e51b4575cb7642eb61bcc02d95c99466dd3e8eda", "committedDate": "2023-03-17 15:17:24 -0700", "message": "[HUDI-5920] Improve documentation of parallelism configs (#8157)"}, {"oid": "cb1395a820febc4bb13c544369ca94a55fff0a29", "committedDate": "2023-03-30 11:07:29 -0700", "message": "[HUDI-5893] Mark advanced configs (#8295)"}, {"oid": "c53d9fbe019a43f31b3eb7556ff109d71287cf6c", "committedDate": "2023-03-31 13:30:34 -0700", "message": "[HUDI-5900] Clean up unused metadata configs (#8125)"}, {"oid": "6fd885fb3dc5c66caf6a1775fa87b4f7212056d8", "committedDate": "2023-03-31 21:21:28 -0700", "message": "[HUDI-5740] Refactor Deltastreamer and schema providers to use HoodieConfig/ConfigProperty (#8152)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "f9f110695fc69f2d7085648a6610888bb10ad8e4", "committedDate": "2023-04-19 04:09:48 -0400", "message": "[HUDI-6056] Validate archival configs alignment with cleaner configs with policy based on hours (#8422)"}, {"oid": "44a0c29560db2d85e3b0d963beed55225596baff", "committedDate": "2023-04-21 09:50:34 +0800", "message": "[HUDI-6100] Fixed overflow in setting log block size by making it long everywhere (#8495)"}, {"oid": "5a5c4863452197def89390beb0ab584eb08aabb6", "committedDate": "2023-04-21 23:06:46 -0700", "message": "[HUDI-5934] Remove archival configs for metadata table (#8319)"}, {"oid": "9a9dd3a82d3e69f1d5eebe46c79c8fd0dc2355db", "committedDate": "2023-04-23 16:37:48 +0800", "message": "[HUDI-6123] Auto adjust lock configs for single writer (#8542)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "e04dc0951cf21122f0d3dd4f673b87b663253109", "committedDate": "2023-05-04 04:05:13 -0700", "message": "[HUDI-5315] Use sample writes to estimate record size (#8390)"}, {"oid": "cabcb2bf2cddedeb3a34047af3935b27cfdfb858", "committedDate": "2023-05-05 06:28:14 -0700", "message": "[HUDI-5968] Fix global index duplicate and handle custom payload when update partition (#8490)"}, {"oid": "66e838c8f18b8ca50a3839f4768da5edfed0c416", "committedDate": "2023-05-06 15:09:03 +0800", "message": "[MINOR] Remove redundant advanced config marking (#8600)"}, {"oid": "6cdc1f583ef8d1281d2171c42fc982e2715f08f8", "committedDate": "2023-05-08 07:29:57 -0700", "message": "[HUDI-5895] Remove bootstrap key generator configs (#8557)"}, {"oid": "a5bd50c067f2a82be2470d4649f2be3007404c40", "committedDate": "2023-05-23 15:54:42 +0800", "message": "[MINOR] Disable schema validation in master (#8781)"}, {"oid": "9d58ee4b1f1fce213ee3f4ff478eb003da943924", "committedDate": "2023-05-24 20:09:54 +0800", "message": "[HUDI-5994] Bucket index supports bulk insert row writer (#8776)"}, {"oid": "59786113fae88382f03412c7c79f7a827bf9393f", "committedDate": "2023-05-26 12:06:32 +0530", "message": "[HUDI-5998] Speed up reads from bootstrapped tables in spark (#8303)"}, {"oid": "41e1e9a4fda2e399f4d50222e81bdfa713bbce23", "committedDate": "2023-05-30 14:58:33 -0700", "message": "[MINOR] Ensure metrics prefix does not contain any dot. (#8599)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTk3MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501971", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-18T11:32:49Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4259478b945ea0440e0c28c56dc6290ee2df4442", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ea52beced6..ccf53a56a4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ccf53a56a4..b1155e3bc9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+\n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "a318c91498d1899107f886b73c3404cd5f50258f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex b1155e3bc9..251ea2305f 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +493,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex ea52beced6..e3c1ef6819 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -493,7 +501,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-\n+  \n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex e3c1ef6819..4e493e4432 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -501,7 +531,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+\n   public String getClusteringSortColumns() {\n     return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4e493e4432..1783535e82 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -372,251 +562,251 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n    * compaction properties.\n    */\n   public HoodieCleaningPolicy getCleanerPolicy() {\n-    return HoodieCleaningPolicy.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n+    return HoodieCleaningPolicy.valueOf(getString(HoodieCompactionConfig.CLEANER_POLICY_PROP));\n   }\n \n   public int getCleanerFileVersionsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_FILE_VERSIONS_RETAINED_PROP);\n   }\n \n   public int getCleanerCommitsRetained() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP));\n+    return getInt(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP);\n   }\n \n   public int getMaxCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getMinCommitsToKeep() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP));\n+    return getInt(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP);\n   }\n \n   public int getParquetSmallFileLimit() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES));\n+    return getInt(HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT_BYTES);\n   }\n \n   public double getRecordSizeEstimationThreshold() {\n-    return Double.parseDouble(props.getProperty(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP));\n+    return getDouble(HoodieCompactionConfig.RECORD_SIZE_ESTIMATION_THRESHOLD_PROP);\n   }\n \n   public int getCopyOnWriteInsertSplitSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE);\n   }\n \n   public int getCopyOnWriteRecordSizeEstimate() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE));\n+    return getInt(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE);\n   }\n \n   public boolean shouldAutoTuneInsertSplits() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS));\n+    return getBoolean(HoodieCompactionConfig.COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS);\n   }\n \n   public int getCleanerParallelism() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_PARALLELISM));\n+    return getInt(HoodieCompactionConfig.CLEANER_PARALLELISM);\n   }\n \n   public boolean isAutoClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.AUTO_CLEAN_PROP);\n   }\n \n   public boolean isAsyncClean() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n+    return getBoolean(HoodieCompactionConfig.ASYNC_CLEAN_PROP);\n   }\n \n   public boolean incrementalCleanerModeEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_INCREMENTAL_MODE);\n   }\n \n-  public boolean isInlineCompaction() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_PROP));\n+  public boolean inlineCompactionEnabled() {\n+    return getBoolean(HoodieCompactionConfig.INLINE_COMPACT_PROP);\n   }\n \n   public CompactionTriggerStrategy getInlineCompactTriggerStrategy() {\n-    return CompactionTriggerStrategy.valueOf(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n+    return CompactionTriggerStrategy.valueOf(getString(HoodieCompactionConfig.INLINE_COMPACT_TRIGGER_STRATEGY_PROP));\n   }\n \n   public int getInlineCompactDeltaCommitMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP);\n   }\n \n   public int getInlineCompactDeltaSecondsMax() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP));\n+    return getInt(HoodieCompactionConfig.INLINE_COMPACT_TIME_DELTA_SECONDS_PROP);\n   }\n \n   public CompactionStrategy getCompactionStrategy() {\n-    return ReflectionUtils.loadClass(props.getProperty(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n+    return ReflectionUtils.loadClass(getString(HoodieCompactionConfig.COMPACTION_STRATEGY_PROP));\n   }\n \n   public Long getTargetIOPerCompactionInMB() {\n-    return Long.parseLong(props.getProperty(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP));\n+    return getLong(HoodieCompactionConfig.TARGET_IO_PER_COMPACTION_IN_MB_PROP);\n   }\n \n   public Boolean getCompactionLazyBlockReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP);\n   }\n \n   public Boolean getCompactionReverseLogReadEnabled() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP));\n+    return getBoolean(HoodieCompactionConfig.COMPACTION_REVERSE_LOG_READ_ENABLED_PROP);\n   }\n \n-  public boolean isInlineClustering() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n+  public boolean inlineClusteringEnabled() {\n+    return getBoolean(HoodieClusteringConfig.INLINE_CLUSTERING_PROP);\n   }\n \n   public boolean isAsyncClusteringEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    return getBoolean(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY);\n   }\n \n   public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClusteringEnabled();\n+    return inlineClusteringEnabled() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP));\n+    return getInt(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP);\n   }\n \n   public String getPayloadClass() {\n-    return props.getProperty(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n+    return getString(HoodieCompactionConfig.PAYLOAD_CLASS_PROP);\n   }\n \n   public int getTargetPartitionsPerDayBasedCompaction() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP));\n+    return getInt(HoodieCompactionConfig.TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP);\n   }\n \n   public int getCommitArchivalBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP));\n+    return getInt(HoodieCompactionConfig.COMMITS_ARCHIVAL_BATCH_SIZE_PROP);\n   }\n \n   public Boolean shouldCleanBootstrapBaseFile() {\n-    return Boolean.valueOf(props.getProperty(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED));\n+    return getBoolean(HoodieCompactionConfig.CLEANER_BOOTSTRAP_BASE_FILE_ENABLED);\n   }\n \n   public String getClusteringUpdatesStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n+    return getString(HoodieClusteringConfig.CLUSTERING_UPDATES_STRATEGY_PROP);\n   }\n \n   public HoodieFailedWritesCleaningPolicy getFailedWritesCleanPolicy() {\n     return HoodieFailedWritesCleaningPolicy\n-        .valueOf(props.getProperty(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n+        .valueOf(getString(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP));\n   }\n \n   /**\n    * Clustering properties.\n    */\n   public String getClusteringPlanStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_PLAN_STRATEGY_CLASS);\n   }\n \n   public String getClusteringExecutionStrategyClass() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n+    return getString(HoodieClusteringConfig.CLUSTERING_EXECUTION_STRATEGY_CLASS);\n   }\n \n   public long getClusteringMaxBytesInGroup() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP);\n   }\n \n   public long getClusteringSmallFileLimit() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_PLAN_SMALL_FILE_LIMIT);\n   }\n \n   public int getClusteringMaxNumGroups() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS);\n   }\n \n   public long getClusteringTargetFileMaxBytes() {\n-    return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES));\n+    return getLong(HoodieClusteringConfig.CLUSTERING_TARGET_FILE_MAX_BYTES);\n   }\n \n   public int getTargetPartitionsForClustering() {\n-    return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n+    return getInt(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS);\n   }\n \n   public String getClusteringSortColumns() {\n-    return props.getProperty(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n+    return getString(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY);\n   }\n \n   /**\n    * index properties.\n    */\n   public HoodieIndex.IndexType getIndexType() {\n-    return HoodieIndex.IndexType.valueOf(props.getProperty(HoodieIndexConfig.INDEX_TYPE_PROP));\n+    return HoodieIndex.IndexType.valueOf(getString(HoodieIndexConfig.INDEX_TYPE_PROP));\n   }\n \n   public String getIndexClass() {\n-    return props.getProperty(HoodieIndexConfig.INDEX_CLASS_PROP);\n+    return getString(HoodieIndexConfig.INDEX_CLASS_PROP);\n   }\n \n   public int getBloomFilterNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES));\n+    return getInt(HoodieIndexConfig.BLOOM_FILTER_NUM_ENTRIES);\n   }\n \n   public double getBloomFilterFPP() {\n-    return Double.parseDouble(props.getProperty(HoodieIndexConfig.BLOOM_FILTER_FPP));\n+    return getDouble(HoodieIndexConfig.BLOOM_FILTER_FPP);\n   }\n \n   public String getHbaseZkQuorum() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZKQUORUM_PROP);\n   }\n \n   public int getHbaseZkPort() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_ZKPORT_PROP);\n   }\n \n   public String getHBaseZkZnodeParent() {\n-    return props.getProperty(HoodieIndexConfig.HBASE_ZK_ZNODEPARENT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_ZNODEPARENT);\n   }\n \n   public String getHbaseTableName() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n+    return getString(HoodieHBaseIndexConfig.HBASE_TABLENAME_PROP);\n   }\n \n   public int getHbaseIndexGetBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_GET_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHBaseIndexRollbackSync() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_ROLLBACK_SYNC);\n   }\n \n   public int getHbaseIndexPutBatchSize() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP));\n+    return getInt(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_PROP);\n   }\n \n   public Boolean getHbaseIndexPutBatchSizeAutoCompute() {\n-    return Boolean.valueOf(props.getProperty(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP));\n+    return getBoolean(HoodieHBaseIndexConfig.HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP);\n   }\n \n   public String getHBaseQPSResourceAllocatorClass() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n+    return getString(HoodieHBaseIndexConfig.HBASE_INDEX_QPS_ALLOCATOR_CLASS);\n   }\n \n   public String getHBaseQPSZKnodePath() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n+    return getString(HoodieHBaseIndexConfig.HBASE_ZK_PATH_QPS_ROOT);\n   }\n \n   public String getHBaseZkZnodeSessionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_SESSION_TIMEOUT_MS);\n   }\n \n   public String getHBaseZkZnodeConnectionTimeout() {\n-    return props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n+    return getString(HoodieHBaseIndexConfig.HOODIE_INDEX_HBASE_ZK_CONNECTION_TIMEOUT_MS);\n   }\n \n   public boolean getHBaseIndexShouldComputeQPSDynamically() {\n-    return Boolean.parseBoolean(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY));\n+    return getBoolean(HoodieHBaseIndexConfig.HOODIE_INDEX_COMPUTE_QPS_DYNAMICALLY);\n   }\n \n   public int getHBaseIndexDesiredPutsTime() {\n-    return Integer.parseInt(props.getProperty(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS));\n+    return getInt(HoodieHBaseIndexConfig.HOODIE_INDEX_DESIRED_PUTS_TIME_IN_SECS);\n   }\n \n   public String getBloomFilterType() {\n-    return props.getProperty(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n+    return getString(HoodieIndexConfig.BLOOM_INDEX_FILTER_TYPE);\n   }\n \n   public int getDynamicBloomFilterMaxNumEntries() {\n-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES));\n+    return getInt(HoodieIndexConfig.HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES);\n   }\n \n   /**\n", "next_change": {"commit": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 1783535e82..4cbbbbc950 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -815,15 +947,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION_PROP);\n+    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -947,15 +1294,15 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * the jobs would be (0.17) 1/6, 0.33 (2/6) and 0.5 (3/6) respectively.\n    */\n   public float getHbaseIndexQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMinQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MIN_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MIN_QPS_FRACTION);\n   }\n \n   public float getHBaseIndexMaxQPSFraction() {\n-    return getFloat(HoodieHBaseIndexConfig.HBASE_MAX_QPS_FRACTION);\n+    return getFloat(HoodieHBaseIndexConfig.MAX_QPS_FRACTION);\n   }\n \n   /**\n", "next_change": null}, {"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 4cbbbbc950..448ce9f7b4 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -963,11 +1310,11 @@ public class HoodieWriteConfig extends HoodieConfig {\n    * Hoodie jobs to an Hbase Region Server\n    */\n   public int getHbaseIndexMaxQPSPerRegionServer() {\n-    return getInt(HoodieHBaseIndexConfig.HBASE_MAX_QPS_PER_REGION_SERVER);\n+    return getInt(HoodieHBaseIndexConfig.MAX_QPS_PER_REGION_SERVER);\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.HBASE_INDEX_UPDATE_PARTITION_PATH);\n+    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 448ce9f7b4..eb3df38428 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1314,7 +1357,7 @@ public class HoodieWriteConfig extends HoodieConfig {\n   }\n \n   public boolean getHbaseIndexUpdatePartitionPath() {\n-    return getBoolean(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n+    return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n   public int getBloomIndexParallelism() {\n", "next_change": {"commit": "e5faf2cc8470f83323b34305a135461c7e43d14e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex eb3df38428..1650a79ee9 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n", "chunk": "@@ -1360,6 +1553,10 @@ public class HoodieWriteConfig extends HoodieConfig {\n     return getBooleanOrDefault(HoodieHBaseIndexConfig.UPDATE_PARTITION_PATH_ENABLE);\n   }\n \n+  public int getHBaseIndexRegionCount() {\n+    return getInt(HoodieHBaseIndexConfig.BUCKET_NUMBER);\n+  }\n+\n   public int getBloomIndexParallelism() {\n     return getInt(HoodieIndexConfig.BLOOM_INDEX_PARALLELISM);\n   }\n", "next_change": null}]}}]}}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "77ba561a6bacbf9e7dc9c1582eb068f7ad800cd9", "committedDate": "2021-02-23 20:56:58 -0500", "message": "[HUDI-1347] Fix Hbase index to make rollback synchronous (via config) (#2188)"}, {"oid": "f11a6c7b2d4ef045419a4522e8e203f51292b816", "committedDate": "2021-03-02 21:58:41 -0800", "message": "[HUDI-1553] Configuration and metrics for the TimelineService. (#2495)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "7fed7352bd506e20e5316bb0b3ed9e5c1e9c76df", "committedDate": "2021-05-27 13:38:33 +0800", "message": "[HUDI-1865] Make embedded time line service singleton (#2899)"}, {"oid": "f760ec543ec9ea23b7d4c9f61c76a283bd737f27", "committedDate": "2021-06-07 23:24:32 -0700", "message": "[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)"}, {"oid": "7261f0850727aea611cd34e1bb07d684b44534f6", "committedDate": "2021-06-08 09:26:10 -0400", "message": "[HUDI-1929] Support configure KeyGenerator by type (#2993)"}, {"oid": "b8fe5b91d599418cd908d833fd63edc7f362c548", "committedDate": "2021-06-15 15:21:43 -0700", "message": "[HUDI-764] [HUDI-765] ORC reader writer Implementation (#2999)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "b376cefc3e899aa28992796925708746561d6087", "committedDate": "2021-07-01 18:48:59 +0800", "message": "[MINOR] Add Documentation to KEYGENERATOR_TYPE_PROP (#3196)"}, {"oid": "62a1ad8b3a2a3c1dabba0a4622117636920b6c13", "committedDate": "2021-07-03 20:27:37 +0800", "message": "[HUDI-1930] Bootstrap support configure KeyGenerator by type (#3170)"}, {"oid": "a4dcbb5c5a2a94e4f69524194d8777d082af31ab", "committedDate": "2021-07-05 23:03:41 -0700", "message": "[HUDI-2028] Implement RockDbBasedMap as an alternate to DiskBasedMap in ExternalSpillableMap (#3194)"}, {"oid": "ea9e5d0e8b7557ef82631ac173d67f15bad13690", "committedDate": "2021-07-07 11:15:25 -0400", "message": "[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)"}, {"oid": "5804ad8e32ae05758ebc5e47f5d4fb4db371ab52", "committedDate": "2021-07-11 14:43:38 -0400", "message": "[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)"}, {"oid": "b0089b894ad12da11fbd6a0fb08508c7adee68e6", "committedDate": "2021-07-13 00:24:40 -0400", "message": "[MINOR] Fix EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION config (#3250)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "d024439764ceeca6366cb33689b729a1c69a6272", "committedDate": "2021-07-14 22:57:38 -0400", "message": "[HUDI-2029] Implement compression for DiskBasedMap in Spillable Map (#3128)"}, {"oid": "38cd74b56328d154c004f04f1335784529d8e93d", "committedDate": "2021-07-16 12:24:41 +0800", "message": "[MINOR] Allow users to choose ORC as base file format in Spark SQL (#3279)"}, {"oid": "d5026e9a24850bdcce9f6df3686bf2235d7d01c4", "committedDate": "2021-07-19 20:43:48 -0400", "message": "[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3", "committedDate": "2021-07-26 17:21:04 -0400", "message": "[HUDI-2176, 2178, 2179] Adding virtual key support to COW table (#3306)"}, {"oid": "8fef50e237b2342ea3366be32950a2b87a9608c4", "committedDate": "2021-07-28 01:31:03 -0400", "message": "[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)"}, {"oid": "bbadac7de1bb57300ca7e796ebb401fdbb66a0f8", "committedDate": "2021-07-28 21:30:18 -0700", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write (#2296)"}, {"oid": "826a04d1425f47fdd80c293569a359021d1b6586", "committedDate": "2021-08-03 12:07:45 -0700", "message": "[HUDI-2072] Add pre-commit validator framework (#3153)"}, {"oid": "91bb0d13184c57ec08f02db3337e734bc20739c4", "committedDate": "2021-08-03 17:50:30 -0700", "message": "[HUDI-2255] Refactor Datasource options (#3373)"}, {"oid": "70b6bd485f8d1ef9a9b15999edda2472d0b4d65a", "committedDate": "2021-08-06 22:53:08 -0400", "message": "[HUDI-1468] Support custom clustering strategies and preserve commit metadata as part of clustering (#3419)"}, {"oid": "b4441abcf74951ec0ce28593b96baa84456a97d3", "committedDate": "2021-08-09 10:10:15 -0700", "message": "[HUDI-2194] Skip the latest N partitions when choosing partitions to create ClusteringPlan (#3300)"}, {"oid": "21db6d7a84d4a83ec98c110e92ff9c92d05dd530", "committedDate": "2021-08-10 20:23:23 +0800", "message": "[HUDI-1771] Propagate CDC format for hoodie (#3285)"}, {"oid": "4783176554e7d4ae7b7296cf633d750ae27e65d9", "committedDate": "2021-08-11 11:48:13 -0400", "message": "[HUDI-1138] Add timeline-server-based marker file strategy for improving marker-related latency (#3233)"}, {"oid": "76bc686a77a485544c9e75cfefa59fa021470a0c", "committedDate": "2021-08-12 15:45:57 -0700", "message": "[HUDI-1292] Created a config to enable/disable syncing of metadata table. (#3427)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "642b1b671de8c6a35ae7858c9b03d3dff70889dd", "committedDate": "2021-08-13 19:29:22 -0400", "message": "[HUDI-2151]  Flipping defaults (#3452)"}, {"oid": "9056c68744a3f31ac2625e004ec6e155d2e86be9", "committedDate": "2021-08-14 08:18:49 -0400", "message": "[HUDI-2305] Add MARKERS.type and fix marker-based rollback (#3472)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "e39d0a2f2852ef51c524e5b16a1cecb099674eed", "committedDate": "2021-08-20 02:42:59 -0700", "message": "Keep non-conflicting names for common configs between DataSourceOptions and HoodieWriteConfig (#3511)"}, {"oid": "de94787a85b272f79181dff73907b0f20855ee78", "committedDate": "2021-08-24 21:45:17 +0800", "message": "[HUDI-2345] Hoodie columns sort partitioner for bulk insert (#3523)"}, {"oid": "21fd6edfe7721c674b40877fbbdbac71b36bf782", "committedDate": "2021-09-02 11:14:09 +0800", "message": "[HUDI-2384] Change log file size config to long (#3577)"}, {"oid": "e528dd798ab8ce6e4d444d2d771c107c503e8f25", "committedDate": "2021-09-10 18:20:26 -0700", "message": "[HUDI-2394] Implement Kafka Sink Protocol for Hudi for Ingesting Immutable Data (#3592)"}, {"oid": "2791fb9a964b39ef9aaec83eafd080013186b2eb", "committedDate": "2021-09-16 15:08:10 +0800", "message": "[HUDI-2423] Separate some config logic from HoodieMetricsConfig into HoodieMetricsGraphiteConfig HoodieMetricsJmxConfig (#3652)"}, {"oid": "61d009608899bc70c1372d5cb00a2f35e188c30c", "committedDate": "2021-09-17 19:39:55 +0800", "message": "[HUDI-2434] Make periodSeconds of GraphiteReporter configurable (#3667)"}, {"oid": "06c2cc2c8b1ad88bb4c9bbdb496053a079767e9b", "committedDate": "2021-09-24 13:33:34 +0800", "message": "[HUDI-2385] Make parquet dictionary encoding configurable (#3578)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "d194643b49834a772657b61a90cd1e64aa754282", "committedDate": "2021-11-02 09:31:57 -0700", "message": "[HUDI-2101][RFC-28] support z-order for hudi (#3330)"}, {"oid": "08c35a55b3133ddaead0581c9129e88a869421a1", "committedDate": "2021-11-05 13:03:41 -0400", "message": "[HUDI-2526] Make spark.sql.parquet.writeLegacyFormat configurable (#3917)"}, {"oid": "dfe3b84715e8fecfa96ef615c217f5eaf0da94e8", "committedDate": "2021-11-09 17:37:59 -0500", "message": "[HUDI-2579] Make deltastreamer checkpoint state merging more explicit (#3820)"}, {"oid": "4f217fe718b0b4e9656c2a45f7b89cb5df15a4f2", "committedDate": "2021-11-12 07:29:37 -0500", "message": "[HUDI-2151] Part1 Setting default parallelism to 200 for some of write configs (#3948)"}, {"oid": "0e8461e9abc97537954a2c1dd716aed53e52dc62", "committedDate": "2021-11-13 09:12:33 +0800", "message": "[HUDI-2697] Minor changes about hbase index config. (#3927)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "ce7d2333078e4e1f16de1bce6d448c5eef1e4111", "committedDate": "2021-11-17 11:51:28 +0530", "message": "[HUDI-2151] Part3 Enabling marker based rollback as default rollback strategy (#3950)"}, {"oid": "2d3f2a3275ba615245fcabda96b8282cb86940ad", "committedDate": "2021-11-17 14:43:00 -0500", "message": "[HUDI-2734] Setting default metadata enable as false for Java (#4003)"}, {"oid": "3bdab01a498d605faede833af2d88cd8ec9237a0", "committedDate": "2021-11-22 19:19:59 -0500", "message": "[HUDI-2550] Expand File-Group candidates list for appending for MOR tables (#3986)"}, {"oid": "e22150fe15da2985b20077d2e0734fcd46b85a6f", "committedDate": "2021-11-23 07:29:03 +0530", "message": "[HUDI-1937] Rollback unfinished replace commit to allow updates (#3869)"}, {"oid": "ca9bfa2a4000575dbaa379c91898786f040a9917", "committedDate": "2021-11-23 14:23:28 +0530", "message": "[HUDI-2332] Add clustering and compaction in Kafka Connect Sink (#3857)"}, {"oid": "435ea1543c034194d7ca0b589b7b043fc49c07ac", "committedDate": "2021-11-24 18:26:40 -0500", "message": "[HUDI-2793] Fixing deltastreamer checkpoint fetch/copy over (#4034)"}, {"oid": "88067f57a23575aae3c371a7c7871e4207ca3bea", "committedDate": "2021-11-25 19:17:38 +0800", "message": "[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)"}, {"oid": "e0125a7911d77afd4a82a49caaccaa3c10df0377", "committedDate": "2021-11-25 13:33:16 -0800", "message": "[HUDI-2801] Add Amazon CloudWatch metrics reporter (#4081)"}, {"oid": "d1e83e4ba0b881f9410f0ae9f3799c967b6891cb", "committedDate": "2021-11-26 16:41:05 -0500", "message": "[HUDI-2767] Enabling timeline-server-based marker as default (#4112)"}, {"oid": "24380c20606d63c7c129cb45a73f786f223e7d39", "committedDate": "2021-11-30 17:47:16 -0800", "message": "Revert \"[HUDI-2855] Change the default value of 'PAYLOAD_CLASS_NAME' to 'DefaultHoodieRecordPayload' (#4115)\" (#4169)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "9797fdfbb27ca8f5f06875ad958b597becc27a8d", "committedDate": "2021-12-10 19:42:20 -0800", "message": "[HUDI-2974] Make the prefix for metrics name configurable (#4274)"}, {"oid": "a4e622ac61ecaf8520d137421f16bc206b864732", "committedDate": "2021-12-30 12:38:26 -0800", "message": "[HUDI-1951] Add bucket hash index, compatible with the hive bucket (#3173)"}, {"oid": "2444f40a4be5bbf0bf210dee5690267a9a1e35c8", "committedDate": "2021-12-31 11:07:52 +0530", "message": "[HUDI-3095] abstract partition filter logic to enable code reuse (#4454)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "827549949c4ac472fdc528a35ae421b20e2cc83a", "committedDate": "2022-01-08 10:22:44 -0500", "message": "[HUDI-2909] Handle logical type in TimestampBasedKeyGenerator (#4203)"}, {"oid": "251d4eb3b64704b9dd51bf6f6ecb5bf47089b745", "committedDate": "2022-01-10 08:40:24 +0530", "message": "[HUDI-3030] InProcessLockPovider as default when any async servcies enabled with no lock provider override (#4406)"}, {"oid": "9fe28e56b49c7bf68ae2d83bfe89755314aa793b", "committedDate": "2022-01-11 23:23:55 -0800", "message": "[HUDI-3045] New clustering regex match config to choose partitions when building clustering plan (#4346)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "14d08bb64c4bea20a692b3d3bced5cc9800cd541", "committedDate": "2022-01-20 15:34:56 +0400", "message": "[MINOR] Fix typo in the doc of BULK_INSERT_SORT_MODE (#4652)"}, {"oid": "bc7882cbe924ce8000f4a738b8673fe7a5cf69fb", "committedDate": "2022-01-24 16:53:54 -0500", "message": "[HUDI-2872][HUDI-2646] Refactoring layout optimization (clustering) flow to support linear ordering (#4606)"}, {"oid": "a68e1dc2dba475b9a63779f3afa0b5c558a7cd3b", "committedDate": "2022-02-02 14:35:05 -0500", "message": "[HUDI-431] Adding support for Parquet in MOR `LogBlock`s (#4333)"}, {"oid": "5927bdd1c0fab202474af47b9e035680b345c563", "committedDate": "2022-02-03 18:12:48 +0530", "message": "[HUDI-1295] Metadata Index - Bloom filter and Column stats index to speed up index lookups (#4352)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "538ec44fa8a23926b584c3bcdd24feb9894d4c51", "committedDate": "2022-02-15 09:49:53 -0500", "message": "[HUDI-2931] Add config to disable table services (#4777)"}, {"oid": "359fbfde798b50edc06ee1d0520efcd971a289bc", "committedDate": "2022-02-20 15:31:31 -0500", "message": "[HUDI-2648] Retry FileSystem action instead of failed directly. (#3887)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "92cdc5987a2b5a6faecec96224f545ab49ee6ef2", "committedDate": "2022-02-25 11:30:10 -0500", "message": "[HUDI-3515] Making rdd unpersist optional at the end of writes (#4898)"}, {"oid": "62f534d00228653059c4fed944d444632bc07091", "committedDate": "2022-03-04 09:33:16 +0800", "message": "[HUDI-3445] Support Clustering Command Based on Call Procedure Command for Spark SQL (#4901)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "f0bcee3c014cf59bdad3eaf8212d94a589073f0b", "committedDate": "2022-03-07 13:42:03 -0500", "message": "[HUDI-3561] Avoid including whole `MultipleSparkJobExecutionStrategy` object into the closure for Spark to serialize (#4954)"}, {"oid": "29040762fa511f89e678dc15ca5ae7f9f097fb8a", "committedDate": "2022-03-07 17:01:49 -0500", "message": "[HUDI-3576] Configuring timeline refreshes based on latest commit (#4973)"}, {"oid": "575bc6346825796e091a12be5a53b04980f82637", "committedDate": "2022-03-08 10:39:04 -0500", "message": "[HUDI-3356][HUDI-3203] HoodieData for metadata index records; BloomFilter construction from index based on the type param (#4848)"}, {"oid": "034addaef5834eff09cfd9ac5cc2656df95ca0e8", "committedDate": "2022-03-09 21:45:25 -0500", "message": "[HUDI-3396] Make sure `BaseFileOnlyViewRelation` only reads projected columns (#4818)"}, {"oid": "95e6e538109af9fe60aa99219e4aa1d7ce9511e2", "committedDate": "2022-03-17 01:25:04 -0400", "message": "[HUDI-3404] Automatically adjust write configs based on metadata table and write concurrency mode (#4975)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "5f570ea151d0212ab1bb2d1f5693035626b76d31", "committedDate": "2022-03-21 22:56:31 -0400", "message": "[HUDI-2883] Refactor hive sync tool / config to use reflection and standardize configs (#4175)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "fb45fc9cb9581abc40922ddcbee21dfc016d4edc", "committedDate": "2022-04-01 20:14:07 -0700", "message": "[HUDI-3773] Fix parallelism used for metadata table bloom filter index (#5209)"}, {"oid": "84064a9b081c246f306855ae125f0dae5eb8f6d0", "committedDate": "2022-04-02 23:44:10 -0700", "message": "[HUDI-3772] Fixing auto adjustment of lock configs for deltastreamer (#5207)"}, {"oid": "81b25c543a5eabd6d0dfe460ad7f9776d8cf5573", "committedDate": "2022-04-08 23:14:08 -0700", "message": "[HUDI-3825] Fixing Column Stats Index updating sequence (#5267)"}, {"oid": "3e97c88c4ff9cbeb312805daf6d52da5f1bcb0bd", "committedDate": "2022-04-09 15:30:11 -0400", "message": "[HUDI-3807] Add a new config to control the use of metadata index in HoodieBloomIndex (#5268)"}, {"oid": "bab691692e31ae3e432bb6cf4c90436fba408a2c", "committedDate": "2022-04-13 17:33:26 -0400", "message": "[HUDI-3686] Fix inline and async table service check in HoodieWriteConfig (#5307)"}, {"oid": "4e928a6fe1ddd7e126ab155bb1c03f8630bb873d", "committedDate": "2022-04-28 15:18:56 -0700", "message": "[HUDI-3943] Some description fixes for 0.10.1 docs (#5447)"}, {"oid": "f492c52ee4d2f3d6dfbbf574833f837322166fbd", "committedDate": "2022-04-29 16:21:52 -0700", "message": "[HUDI-3862] Fix default configurations of HoodieHBaseIndexConfig (#5308)"}, {"oid": "6e16e719cd614329018cd34a7c57d342fe2fa376", "committedDate": "2022-05-14 07:37:31 -0400", "message": "[HUDI-3980] Suport kerberos hbase index (#5464)"}, {"oid": "61030d8e7a5a05e215efed672267ac163b0cbcf6", "committedDate": "2022-05-16 11:07:01 +0800", "message": "[HUDI-3123] consistent hashing index: basic write path (upsert/insert) (#4480)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "cf837b49008fd351a3f89beb5e4e5c17c30b9a3c", "committedDate": "2022-05-25 19:38:56 +0530", "message": "[HUDI-3193] Decouple hudi-aws from hudi-client-common (#5666)"}, {"oid": "7f8630cc57fbb9d29e8dc7ca87b582264da073fd", "committedDate": "2022-06-02 09:48:48 +0800", "message": "[HUDI-4167] Remove the timeline refresh with initializing hoodie table (#5716)"}, {"oid": "4f6fc726d0d3d2dd427210228bbb36cf18893a92", "committedDate": "2022-06-06 10:21:00 -0700", "message": "[HUDI-4140] Fixing hive style partitioning and default partition with bulk insert row writer with SimpleKeyGen and virtual keys (#5664)"}, {"oid": "35afdb4316d496bbb37ebb9e1598d84bd8a4000d", "committedDate": "2022-06-07 16:30:46 -0700", "message": "[HUDI-4178] Addressing performance regressions in Spark DataSourceV2 Integration (#5737)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "da28e38fe3d25e3ab212dc02312f0ae395371072", "committedDate": "2022-07-23 14:37:04 -0500", "message": "[HUDI-4071] Make NONE sort mode as default for bulk insert (#6195)"}, {"oid": "a0ffd05b7773c4c83714a60dcaa79332ee3aada3", "committedDate": "2022-07-23 16:10:53 -0700", "message": "[HUDI-4448] Remove the latest commit refresh for timeline server (#6179)"}, {"oid": "6e7ac457352e007939ba3c44c9dc197de7b88ed3", "committedDate": "2022-07-25 13:42:29 -0500", "message": "[HUDI-3884] Support archival beyond savepoint commits (#5837)"}, {"oid": "e5faf2cc8470f83323b34305a135461c7e43d14e", "committedDate": "2022-07-26 18:09:17 +0800", "message": "[HUDI-4210] Create custom hbase index to solve data skew issue on hbase regions (#5797)"}, {"oid": "cdaec5a8da060157eb7426b5b70419c5f8868e04", "committedDate": "2022-07-27 14:47:49 -0700", "message": "[HUDI-4186] Support Hudi with Spark 3.3.0 (#5943)"}, {"oid": "767c196631240aeda5a8ef4603c17f05a407d8f7", "committedDate": "2022-08-06 18:19:29 -0400", "message": "[HUDI-4303] Adding 4 to 5 upgrade handler to check for old deprecated \"default\" partition value (#6248)"}, {"oid": "6badae46f0f3e743f0502bd34e124cf6cfabcec0", "committedDate": "2022-09-12 12:05:40 +0530", "message": "[HUDI-3558] Consistent bucket index: bucket resizing (split&merge) & concurrent write during resizing (#4958)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5e624698f78f4707d62c7f26b044a69a250aae43", "committedDate": "2022-09-22 09:17:09 -0400", "message": "[HUDI-4363] Support Clustering row writer to improve performance (#6046)"}, {"oid": "efe553b327bc025d242afa37221a740dca9b1ea6", "committedDate": "2022-09-23 18:36:48 +0800", "message": "[HUDI-4897] Refactor the merge handle in CDC mode (#6740)"}, {"oid": "58fe71d2b316a83b30ce4b80a36dd7beed001e58", "committedDate": "2022-09-29 01:37:46 -0400", "message": "[HUDI-4722] Added locking metrics for Hudi (#6502)"}, {"oid": "f3d4ce919d4909f9533255ee2a9a0450c8e44c73", "committedDate": "2022-10-01 18:21:23 +0800", "message": "[HUDI-4916] Implement change log feed for Flink (#6840)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "a5434b6b4d9bef9eea29bf33f08e7f13753057a9", "committedDate": "2022-11-02 20:02:18 -0400", "message": "[HUDI-3963] Use Lock-Free Message Queue Disruptor Improving Hoodie Writing Efficiency  (#5416)"}, {"oid": "6b73c814f931f8dcdec500a4364354ac9488ce68", "committedDate": "2022-11-17 01:54:54 +0800", "message": "[HUDI-5209] Fixing `QueueBasedExecutor` in Spark bundles (missing Disruptor as dep) (#7188)"}, {"oid": "91e0db57b94c479a73e4f78d571b50c1e5ea541f", "committedDate": "2022-11-23 09:04:20 +0800", "message": "[MINOR] Use direct marker for spark engine when timeline server is disabled (#7272)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "1cdbf68d4ee641b9e7eb0129a26e0f969b37d8ca", "committedDate": "2022-11-28 22:48:06 -0500", "message": "[HUDI-5242] Do not fail Meta sync in Deltastreamer when inline table service fails (#7243)"}, {"oid": "ca3333d739ffce1723b5615d8751414b86211c8c", "committedDate": "2022-12-09 19:04:44 -0800", "message": "[HUDI-5342] Add new bulk insert sort modes repartitioning data by partition path (#7402)"}, {"oid": "a5bda3ab0c0fb0a0d2e0ce5b792400c5ed09c560", "committedDate": "2022-12-14 06:29:40 -0800", "message": "[HUDI-3378][RFC-46] Optimize Record Payload handling (#7345)"}, {"oid": "8d13a7e383c30ec1421b68dd052fbd33f438bc3d", "committedDate": "2022-12-15 09:18:54 -0800", "message": "[HUDI-5023] Consuming records from Iterator directly instead of using inner message queue (#7174)"}, {"oid": "16d33ba3cb953435660566ba2ec63a45204a7814", "committedDate": "2023-01-15 21:46:12 -0800", "message": "[HUDI-3654] Add new module `hudi-metaserver` (#5064)"}, {"oid": "c9bc03ed8681ab64eea2520f5511464915389c51", "committedDate": "2023-01-17 07:24:16 -0800", "message": "[HUDI-4148] Add client for Hudi table service manager (TSM) (#6732)"}, {"oid": "ec5022b4fdd94c106bf243038aadf781f31b5be9", "committedDate": "2023-01-18 09:30:52 -0800", "message": "[MINOR] Unify naming for record merger (#7660)"}, {"oid": "c18d6153e105ac34fb410c60ce8a153327931782", "committedDate": "2023-01-23 10:03:19 -0800", "message": "[HUDI-1575] Early Conflict Detection For Multi-writer (#6133)"}, {"oid": "2fc20c186b77017f7f1c6f6abe8559a9e8cfe578", "committedDate": "2023-01-24 20:04:55 +0530", "message": "[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)"}, {"oid": "c95abd3213f4806f535c6d7cb8b346616c3368fb", "committedDate": "2023-01-25 19:01:33 +0530", "message": "Revert \"[HUDI-5575] Adding/Fixing auto generation of record keys w/ hudi (#7726)\" (#7747)"}, {"oid": "7e35874c7ba68dfa32b3b27ece35112cc434a7c6", "committedDate": "2023-01-25 14:34:18 -0800", "message": "[HUDI-5617] Rename configs for async conflict detector for clarity (#7750)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3a08bdc3f971b3534e8fb6f34772340cfdf055a9", "committedDate": "2023-01-25 19:29:42 -0800", "message": "[HUDI-5363] Removing default value for shuffle parallelism configs (#7723)"}, {"oid": "ff590c6d72c523b41c0790087053fd3933564ac8", "committedDate": "2023-01-27 18:56:32 -0800", "message": "[HUDI-5023] Switching default Write Executor type to `SIMPLE` (#7476)"}, {"oid": "c21eca564c6426413cbdc9e83bc40ad7c59c7e5d", "committedDate": "2023-01-28 13:23:30 -0500", "message": "[HUDI-5626] Rename CDC logging mode options (#7760)"}, {"oid": "2c56aa4ce994714a57402399c1bec99579926f66", "committedDate": "2023-01-28 14:03:02 -0600", "message": "[HUDI-5631] Improve defaults of early conflict detection configs (#7770)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "88d8e5e96d5f5ee553b8405e32c79388e3ed3c09", "committedDate": "2023-01-29 14:57:05 -0800", "message": "[MINOR] Cleaning up recently introduced configs (#7772)"}, {"oid": "5e616ab115ce0198d01cbb8761dd135ff55d48a2", "committedDate": "2023-02-01 18:13:25 +0530", "message": "[HUDI-5646] Guard dropping columns by a config, do not allow by default (#7787)"}, {"oid": "7064c380506814964dd85773e2ee7b7f187b88c3", "committedDate": "2023-02-01 11:19:45 -0800", "message": "[MINOR] Restoring existing behavior for `DeltaStreamer` Incremental Source (#7810)"}, {"oid": "ef3a17e3d97428a6f0d6e7ac888747d65a8792c5", "committedDate": "2023-02-04 15:14:52 +0800", "message": "[HUDI-5692] SpillableMapBasePath should be lazily loaded (#7837)"}, {"oid": "ff832f4d86091b71af42cc46c2aa209d80396899", "committedDate": "2023-02-04 17:39:58 +0800", "message": "[MINOR] Validate configs for OCC early conflict detection (#7848)"}, {"oid": "0c9465f2ab6cf6472df3046c681b72290e6034bd", "committedDate": "2023-02-05 00:28:20 -0800", "message": "[MINOR] Improve configuration configs (#7855)"}, {"oid": "53fca761be1db224dc384238a7d2853ff2a1d227", "committedDate": "2023-02-07 19:59:06 -0500", "message": "[MINOR] fixed docs for WRITE_EXECUTOR_TYPE (#7880)"}, {"oid": "be92be657a348954cc21062ca24e8a10caea17ee", "committedDate": "2023-02-20 10:05:09 +0800", "message": "[HUDI-5786] Add a new config to specific spark write rdd storage level (#7941)"}, {"oid": "0c84482aa915611db17f4505974c876240a8101c", "committedDate": "2023-02-20 20:00:31 +0530", "message": "[HUDI-5774] Fix prometheus configs for metadata table and support metric labels (#7933)"}, {"oid": "d705dcc4188223fbd824f36a5d211abeda7b1f23", "committedDate": "2023-02-24 10:23:25 +0800", "message": "[HUDI-5173] Skip if there is only one file in clusteringGroup  (#7159)"}, {"oid": "e131505a4f0bfef6e8cca04e102b3ea1c6d308e2", "committedDate": "2023-02-27 12:26:50 +0530", "message": "[HUDI-5838] Mask sensitive info while printing hudi properties in DeltaStreamer  (#8027)"}, {"oid": "c7cebf445f4dc6895cc343a396b0ea1871b362e7", "committedDate": "2023-02-27 23:57:08 -0500", "message": "[HUDI-5843] multiwriter deltastreamer checkpoints (#8043)"}, {"oid": "81e6e854883a94d41ae5b7187c608a8ddbc7bf35", "committedDate": "2023-03-03 21:06:43 +0530", "message": "[HUDI-5847] Add support for multiple metric reporters and metric labels (#8041)"}, {"oid": "a8312a9b8c39f4baabf753974fa092c4767abb72", "committedDate": "2023-03-08 14:46:01 +0800", "message": "[HUDI-5887] Distinguish the single writer enabling metadata table and multi-writer use cases for lock guard (#8111)"}, {"oid": "38e4078d23b00d0acdd02a28eec08560d175cdc9", "committedDate": "2023-03-13 21:30:19 -0700", "message": "[MINOR] Ignoring warn msg for timeline server for metadata table (#8168)"}, {"oid": "e51b4575cb7642eb61bcc02d95c99466dd3e8eda", "committedDate": "2023-03-17 15:17:24 -0700", "message": "[HUDI-5920] Improve documentation of parallelism configs (#8157)"}, {"oid": "cb1395a820febc4bb13c544369ca94a55fff0a29", "committedDate": "2023-03-30 11:07:29 -0700", "message": "[HUDI-5893] Mark advanced configs (#8295)"}, {"oid": "c53d9fbe019a43f31b3eb7556ff109d71287cf6c", "committedDate": "2023-03-31 13:30:34 -0700", "message": "[HUDI-5900] Clean up unused metadata configs (#8125)"}, {"oid": "6fd885fb3dc5c66caf6a1775fa87b4f7212056d8", "committedDate": "2023-03-31 21:21:28 -0700", "message": "[HUDI-5740] Refactor Deltastreamer and schema providers to use HoodieConfig/ConfigProperty (#8152)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "f9f110695fc69f2d7085648a6610888bb10ad8e4", "committedDate": "2023-04-19 04:09:48 -0400", "message": "[HUDI-6056] Validate archival configs alignment with cleaner configs with policy based on hours (#8422)"}, {"oid": "44a0c29560db2d85e3b0d963beed55225596baff", "committedDate": "2023-04-21 09:50:34 +0800", "message": "[HUDI-6100] Fixed overflow in setting log block size by making it long everywhere (#8495)"}, {"oid": "5a5c4863452197def89390beb0ab584eb08aabb6", "committedDate": "2023-04-21 23:06:46 -0700", "message": "[HUDI-5934] Remove archival configs for metadata table (#8319)"}, {"oid": "9a9dd3a82d3e69f1d5eebe46c79c8fd0dc2355db", "committedDate": "2023-04-23 16:37:48 +0800", "message": "[HUDI-6123] Auto adjust lock configs for single writer (#8542)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "e04dc0951cf21122f0d3dd4f673b87b663253109", "committedDate": "2023-05-04 04:05:13 -0700", "message": "[HUDI-5315] Use sample writes to estimate record size (#8390)"}, {"oid": "cabcb2bf2cddedeb3a34047af3935b27cfdfb858", "committedDate": "2023-05-05 06:28:14 -0700", "message": "[HUDI-5968] Fix global index duplicate and handle custom payload when update partition (#8490)"}, {"oid": "66e838c8f18b8ca50a3839f4768da5edfed0c416", "committedDate": "2023-05-06 15:09:03 +0800", "message": "[MINOR] Remove redundant advanced config marking (#8600)"}, {"oid": "6cdc1f583ef8d1281d2171c42fc982e2715f08f8", "committedDate": "2023-05-08 07:29:57 -0700", "message": "[HUDI-5895] Remove bootstrap key generator configs (#8557)"}, {"oid": "a5bd50c067f2a82be2470d4649f2be3007404c40", "committedDate": "2023-05-23 15:54:42 +0800", "message": "[MINOR] Disable schema validation in master (#8781)"}, {"oid": "9d58ee4b1f1fce213ee3f4ff478eb003da943924", "committedDate": "2023-05-24 20:09:54 +0800", "message": "[HUDI-5994] Bucket index supports bulk insert row writer (#8776)"}, {"oid": "59786113fae88382f03412c7c79f7a827bf9393f", "committedDate": "2023-05-26 12:06:32 +0530", "message": "[HUDI-5998] Speed up reads from bootstrapped tables in spark (#8303)"}, {"oid": "41e1e9a4fda2e399f4d50222e81bdfa713bbce23", "committedDate": "2023-05-30 14:58:33 -0700", "message": "[MINOR] Ensure metrics prefix does not contain any dot. (#8599)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNjEzOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559506139", "body": "`COMMIT_NUM` and `NUM ` do not keep consistent. What about `NUM` or `COMMITS`?\r\n\r\nIMO, `CompactType` may make users confused. what about `CompactionTriggerStrategy` Or `CompactionScheduleStrategy`", "bodyText": "COMMIT_NUM and NUM  do not keep consistent. What about NUM or COMMITS?\nIMO, CompactType may make users confused. what about CompactionTriggerStrategy Or CompactionScheduleStrategy", "bodyHTML": "<p dir=\"auto\"><code>COMMIT_NUM</code> and <code>NUM </code> do not keep consistent. What about <code>NUM</code> or <code>COMMITS</code>?</p>\n<p dir=\"auto\">IMO, <code>CompactType</code> may make users confused. what about <code>CompactionTriggerStrategy</code> Or <code>CompactionScheduleStrategy</code></p>", "author": "yanghua", "createdAt": "2021-01-18T11:40:13Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum  CompactType {\n+    COMMIT_NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nsimilarity index 90%\nrename from hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nindex 0adf269386..faba023b01 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n", "chunk": "@@ -18,6 +18,6 @@\n \n package org.apache.hudi.table.action.compact;\n \n-public enum  CompactType {\n-    COMMIT_NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME\n+public enum CompactionTriggerStrategy {\n+    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME\n }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java b/hudi-common/src/main/java/org/apache/hudi/common/function/SerializableSupplier.java\nsimilarity index 82%\nrename from hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java\nrename to hudi-common/src/main/java/org/apache/hudi/common/function/SerializableSupplier.java\nindex 0adf269386..0500955f94 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/function/SerializableSupplier.java\n", "chunk": "@@ -16,8 +16,11 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.table.action.compact;\n+package org.apache.hudi.common.function;\n \n-public enum  CompactType {\n-    COMMIT_NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME\n+import java.io.Serializable;\n+\n+@FunctionalInterface\n+public interface SerializableSupplier<T> extends Serializable {\n+  T get();\n }\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNzI3Nw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559507277", "body": "Revert this change, please.", "bodyText": "Revert this change, please.", "bodyHTML": "<p dir=\"auto\">Revert this change, please.</p>", "author": "yanghua", "createdAt": "2021-01-18T11:42:18Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,9 +83,9 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n     String lastCompactionTs;\n", "next_change": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +83,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,71 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwODczOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559508739", "body": "IMO, `getLastDeltaCommitInfo` sounds better? Correct me, if it's not good for you.", "bodyText": "IMO, getLastDeltaCommitInfo sounds better? Correct me, if it's not good for you.", "bodyHTML": "<p dir=\"auto\">IMO, <code>getLastDeltaCommitInfo</code> sounds better? Correct me, if it's not good for you.</p>", "author": "yanghua", "createdAt": "2021-01-18T11:44:58Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5MDcwOA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559690708", "bodyText": "ok, better than mine", "author": "Karl-WangSK", "createdAt": "2021-01-18T16:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwODczOQ=="}], "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,9 +83,9 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n     String lastCompactionTs;\n", "next_change": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +83,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,71 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMDY3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559510676", "body": "`The commit number is larger than xxx, trigger compaction scheduler.` sounds better?", "bodyText": "The commit number is larger than xxx, trigger compaction scheduler. sounds better?", "bodyHTML": "<p dir=\"auto\"><code>The commit number is larger than xxx, trigger compaction scheduler.</code> sounds better?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:48:17Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -105,49 +105,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n         compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n-        break;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n+              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n+              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n     }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n-    }\n-    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -157,7 +157,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n         }\n         return compactable;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n     }\n   }\n \n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b7d4d58334..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,71 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMTk3NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559511975", "body": "`Timestamp` is not clear, it means `second` or `mills` or something else?", "bodyText": "Timestamp is not clear, it means second or mills or something else?", "bodyHTML": "<p dir=\"auto\"><code>Timestamp</code> is not clear, it means <code>second</code> or <code>mills</code> or something else?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:50:35Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n+                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n+              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n     }\n+    return compactable;\n   }\n \n+  public Long parseToTimestamp(String time) {", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5NTcxNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559695716", "bodyText": "parsedToSeconds ? sry . Do u have any good name?", "author": "Karl-WangSK", "createdAt": "2021-01-18T16:51:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMTk3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..46bb000ff1 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -105,49 +105,63 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n         compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n-        break;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n+              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n+              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n-        break;\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n+                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n       default:\n         throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n     }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n-    }\n-    return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 46bb000ff1..b7d4d58334 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -157,7 +157,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n         }\n         return compactable;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n     }\n   }\n \n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex b7d4d58334..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\" +\n-              \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\" +\n-              \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\" +\n-                  \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex e0b951ab3e..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,71 +82,65 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-            .filterCompletedInstants().lastInstant();\n+        .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactType != CompactType.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n-  public boolean needCompact(CompactType compactType) {\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    switch (compactType) {\n-      case COMMIT_NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n+    switch (compactionTriggerStrategy) {\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        }\n         break;\n       case TIME_ELAPSED:\n-        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1\n-            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n-        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n-                inlineCompactDeltaElapsedTimeMax));\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaSecondsMax));\n+        }\n         break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n-    }\n-\n-    if (compactable) {\n-      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n-              compactType.name(), threshold._1, threshold._2));\n-    } else {\n-      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n-                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n-              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n     return compactable;\n   }\n \n-  public Long parseToTimestamp(String time) {\n+  public Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"oid": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "url": "https://github.com/apache/hudi/commit/a74ea8120c56e5b40de3491d0a4d61a93981011d", "message": "update", "committedDate": "2021-01-18T16:32:35Z", "type": "commit"}, {"oid": "4259478b945ea0440e0c28c56dc6290ee2df4442", "url": "https://github.com/apache/hudi/commit/4259478b945ea0440e0c28c56dc6290ee2df4442", "message": "Update HoodieWriteConfig.java", "committedDate": "2021-01-18T16:44:59Z", "type": "commit"}, {"oid": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "url": "https://github.com/apache/hudi/commit/3c3e12ef34026b9d5b89d3d537a37040046abae3", "message": "update", "committedDate": "2021-01-18T17:02:14Z", "type": "commit"}, {"oid": "a318c91498d1899107f886b73c3404cd5f50258f", "url": "https://github.com/apache/hudi/commit/a318c91498d1899107f886b73c3404cd5f50258f", "message": "Merge branch 'HUDI-1381' of github.com:Karl-WangSK/hudi into HUDI-1381", "committedDate": "2021-01-18T17:02:33Z", "type": "commit"}, {"oid": "b9a1a63910825d68b9a7e99cd9dd9995a4312448", "url": "https://github.com/apache/hudi/commit/b9a1a63910825d68b9a7e99cd9dd9995a4312448", "message": "update", "committedDate": "2021-01-19T01:49:36Z", "type": "commit"}, {"oid": "d720b4e939dab49e8493dbce073c7ee859b03594", "url": "https://github.com/apache/hudi/commit/d720b4e939dab49e8493dbce073c7ee859b03594", "message": "update", "committedDate": "2021-01-19T01:53:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560098640", "body": "how about renaming it to `hoodie.compact.inline.max.delta.seconds`, it seems more readable cc @yanghua ", "bodyText": "how about renaming it to hoodie.compact.inline.max.delta.seconds, it seems more readable cc @yanghua", "bodyHTML": "<p dir=\"auto\">how about renaming it to <code>hoodie.compact.inline.max.delta.seconds</code>, it seems more readable cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/yanghua/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanghua\">@yanghua</a></p>", "author": "wangxianghu", "createdAt": "2021-01-19T11:10:51Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.time\";", "originalCommit": "d720b4e939dab49e8493dbce073c7ee859b03594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIwMDQ3NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560200475", "bodyText": "sounds good~", "author": "yanghua", "createdAt": "2021-01-19T14:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIyNjI5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560226296", "bodyText": "edited.\nanything else  need to improve?", "author": "Karl-WangSK", "createdAt": "2021-01-19T14:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI1ODg1MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560258851", "bodyText": "thanks for your patient, will do a final check tomorrow.", "author": "yanghua", "createdAt": "2021-01-19T15:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}], "type": "inlineReview", "revised_code": {"commit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 2710b836a5..9d43733d29 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,7 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.time\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n   public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n", "next_change": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 9d43733d29..00b8d5eec7 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,7 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n   public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n", "next_change": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 00b8d5eec7..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,6 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n   public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n", "next_change": null}]}}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 2710b836a5..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,7 +47,8 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.time\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n   public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 934d91a274..aa9e75ca55 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -37,97 +39,176 @@ import java.util.Properties;\n  * Compaction related config.\n  */\n @Immutable\n-public class HoodieCompactionConfig extends DefaultHoodieConfig {\n-\n-  public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n-  public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n-  public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n-  // Turn on inline compaction - after fw delta commits a inline compaction will be run\n-  public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n-  // Run a compaction every N delta commits\n-  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  // Run a compaction when time elapsed > N seconds since last compaction\n-  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n-  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n-  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n-  public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n-  public static final String CLEANER_INCREMENTAL_MODE = \"hoodie.cleaner.incremental.mode\";\n-  public static final String MAX_COMMITS_TO_KEEP_PROP = \"hoodie.keep.max.commits\";\n-  public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n-  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n-  // Set true to clean bootstrap source files when necessary\n-  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.delete.bootstrap.base.file\";\n-  // Upsert uses this file size to compact new data onto existing files..\n-  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n-  // By default, treat any file <= 100MB as a small file.\n-  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);\n-  // Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten.\n-  // If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order,\n-  // until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\n-  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = \"hoodie.record.size.estimation.threshold\";\n-  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = \"1.0\";\n+public class HoodieCompactionConfig extends HoodieConfig {\n+\n+  public static final ConfigProperty<String> CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy\")\n+      .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())\n+      .withDocumentation(\"Cleaning policy to be used. Hudi will delete older versions of parquet files to re-claim space.\"\n+          + \" Any Query/Computation referring to this version of the file will fail. \"\n+          + \"It is good to make sure that the data is retained for more than the maximum query execution time.\");\n+\n+  public static final ConfigProperty<String> AUTO_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.automatic\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Should cleanup if there is anything to cleanup immediately after the commit\");\n+\n+  public static final ConfigProperty<String> ASYNC_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Only applies when #withAutoClean is turned on. When turned on runs cleaner async with writing.\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When set to true, compaction is triggered by the ingestion itself, \"\n+          + \"right after a commit/deltacommit action as part of insert/upsert/bulk_insert\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.commits\")\n+      .defaultValue(\"5\")\n+      .withDocumentation(\"Number of max delta commits to keep before triggering an inline compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.seconds\")\n+      .defaultValue(String.valueOf(60 * 60))\n+      .withDocumentation(\"Run a compaction when time elapsed > N seconds since last compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TRIGGER_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.trigger.strategy\")\n+      .defaultValue(CompactionTriggerStrategy.NUM_COMMITS.name())\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_FILE_VERSIONS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.fileversions.retained\")\n+      .defaultValue(\"3\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_COMMITS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.commits.retained\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Number of commits to retain. So data will be retained for num_of_commits * time_between_commits \"\n+          + \"(scheduled). This also directly translates into how much you can incrementally pull on this table\");\n+\n+  public static final ConfigProperty<String> CLEANER_INCREMENTAL_MODE = ConfigProperty\n+      .key(\"hoodie.cleaner.incremental.mode\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> MAX_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.max.commits\")\n+      .defaultValue(\"30\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> MIN_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.min.commits\")\n+      .defaultValue(\"20\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> COMMITS_ARCHIVAL_BATCH_SIZE_PROP = ConfigProperty\n+      .key(\"hoodie.commits.archival.batch\")\n+      .defaultValue(String.valueOf(10))\n+      .withDocumentation(\"This controls the number of commit instants read in memory as a batch and archived together.\");\n+\n+  public static final ConfigProperty<String> CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = ConfigProperty\n+      .key(\"hoodie.cleaner.delete.bootstrap.base.file\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Set true to clean bootstrap source files when necessary\");\n+\n+  public static final ConfigProperty<String> PARQUET_SMALL_FILE_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.parquet.small.file.limit\")\n+      .defaultValue(String.valueOf(104857600))\n+      .withDocumentation(\"Upsert uses this file size to compact new data onto existing files. \"\n+          + \"By default, treat any file <= 100MB as a small file.\");\n+\n+  public static final ConfigProperty<String> RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = ConfigProperty\n+      .key(\"hoodie.record.size.estimation.threshold\")\n+      .defaultValue(\"1.0\")\n+      .withDocumentation(\"Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten. \"\n+          + \"If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order, \"\n+          + \"until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\");\n+\n+  public static final ConfigProperty<String> CLEANER_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.cleaner.parallelism\")\n+      .defaultValue(\"200\")\n+      .withDocumentation(\"Increase this if cleaning becomes slow.\");\n+\n+  // 500GB of target IO per compaction (both read and write\n+  public static final ConfigProperty<String> TARGET_IO_PER_COMPACTION_IN_MB_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.target.io\")\n+      .defaultValue(String.valueOf(500 * 1024))\n+      .withDocumentation(\"Amount of MBs to spend during compaction run for the LogFileSizeBasedCompactionStrategy. \"\n+          + \"This value helps bound ingestion latency while compaction is run inline mode.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.strategy\")\n+      .defaultValue(LogFileSizeBasedCompactionStrategy.class.getName())\n+      .withDocumentation(\"Compaction strategy decides which file groups are picked up for \"\n+          + \"compaction during each compaction run. By default. Hudi picks the log file \"\n+          + \"with most accumulated unmerged data\");\n+\n+  public static final ConfigProperty<String> PAYLOAD_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"This needs to be same as class used during insert/upserts. Just like writing, compaction also uses \"\n+          + \"the record payload class to merge records in the log against each other, merge again with the base file and \"\n+          + \"produce the final record to be written after compaction.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.lazy.block.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When a CompactedLogScanner merges all log files, this config helps to choose whether the logblocks \"\n+          + \"should be read lazily or not. Choose true to use I/O intensive lazy block reading (low memory usage) or false \"\n+          + \"for Memory intensive immediate block read (high memory usage)\");\n+\n+  public static final ConfigProperty<String> COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.reverse.log.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"HoodieLogFormatReader reads a logfile in the forward direction starting from pos=0 to pos=file_length. \"\n+          + \"If this config is set to true, the Reader reads the logfile in reverse direction, from pos=file_length to pos=0\");\n+\n+  public static final ConfigProperty<String> FAILED_WRITES_CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy.failed.writes\")\n+      .defaultValue(HoodieFailedWritesCleaningPolicy.EAGER.name())\n+      .withDocumentation(\"Cleaning policy for failed writes to be used. Hudi will delete any files written by \"\n+          + \"failed writes to re-claim space. Choose to perform this rollback of failed writes eagerly before \"\n+          + \"every writer starts (only supported for single writer) or lazily by the cleaner (required for multi-writers)\");\n+\n+  public static final ConfigProperty<String> TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.daybased.target.partitions\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Used by org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy to denote the number of \"\n+          + \"latest partitions to compact during a compaction run.\");\n \n   /**\n    * Configs related to specific table types.\n    */\n-  // Number of inserts, that will be put each partition/bucket for writing\n-  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = \"hoodie.copyonwrite.insert.split.size\";\n-  // The rationale to pick the insert parallelism is the following. Writing out 100MB files,\n-  // with atleast 1kb records, means 100K records per file. we just overprovision to 500K\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = String.valueOf(500000);\n-  // Config to control whether we control insert split sizes automatically based on average\n-  // record sizes\n-  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = \"hoodie.copyonwrite.insert.auto.split\";\n-  // its off by default\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);\n-  // This value is used as a guesstimate for the record size, if we can't determine this from\n-  // previous commits\n-  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = \"hoodie.copyonwrite.record.size.estimate\";\n-  // Used to determine how much more can be packed into a small file, before it exceeds the size\n-  // limit.\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = String.valueOf(1024);\n-  public static final String CLEANER_PARALLELISM = \"hoodie.cleaner.parallelism\";\n-  public static final String DEFAULT_CLEANER_PARALLELISM = String.valueOf(200);\n-  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = \"hoodie.compaction.target.io\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = String.valueOf(500 * 1024);\n-  public static final String COMPACTION_STRATEGY_PROP = \"hoodie.compaction.strategy\";\n-  // 200GB of target IO per compaction\n-  public static final String DEFAULT_COMPACTION_STRATEGY = LogFileSizeBasedCompactionStrategy.class.getName();\n-  // used to merge records written to log file\n-  public static final String DEFAULT_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-  public static final String PAYLOAD_CLASS_PROP = \"hoodie.compaction.payload.class\";\n-\n-  // used to choose a trade off between IO vs Memory when performing compaction process\n-  // Depending on outputfile_size and memory provided, choose true to avoid OOM for large file\n-  // size + small memory\n-  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"hoodie.compaction.lazy.block.read\";\n-  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"false\";\n-  // used to choose whether to enable reverse log reading (reverse log traversal)\n-  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = \"hoodie.compaction.reverse.log.read\";\n-  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = \"false\";\n-  private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();\n-  private static final String DEFAULT_AUTO_CLEAN = \"true\";\n-  private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n-  private static final String DEFAULT_INLINE_COMPACT = \"false\";\n-  private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n-  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n-  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n-  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n-  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n-  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = \"20\";\n-  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = String.valueOf(10);\n-  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"false\";\n-  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP =\n-      \"hoodie.compaction.daybased.target.partitions\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = String.valueOf(10);\n-\n-  private HoodieCompactionConfig(Properties props) {\n-    super(props);\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.split.size\")\n+      .defaultValue(String.valueOf(500000))\n+      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n+          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n+          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.auto.split\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n+          + \" record sizes.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.record.size.estimate\")\n+      .defaultValue(String.valueOf(1024))\n+      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n+          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n+          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+\n+  private HoodieCompactionConfig() {\n+    super();\n   }\n \n   public static HoodieCompactionConfig.Builder newBuilder() {\n", "next_change": {"commit": "75040ee9e5caa0783009b6ef529d6605e82d4135", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex aa9e75ca55..e8d55934be 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -190,22 +205,24 @@ public class HoodieCompactionConfig extends HoodieConfig {\n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.split.size\")\n       .defaultValue(String.valueOf(500000))\n-      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n-          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n-          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+      .withDocumentation(\"Number of inserts assigned for each partition/bucket for writing. \"\n+          + \"We based the default on writing out 100MB files, with at least 1kb records (100K records per file), and \"\n+          + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n+          + \"  write, where there is no history to learn record sizes from.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n-          + \" record sizes.\");\n+          + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n+          + \" cumbersome.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n-      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n-          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n-          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+      .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n+          + \"record size estimate compute dynamically based on commit metadata. \"\n+          + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n   private HoodieCompactionConfig() {\n     super();\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex e8d55934be..ce74aad6b0 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -210,20 +217,237 @@ public class HoodieCompactionConfig extends HoodieConfig {\n           + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n           + \"  write, where there is no history to learn record sizes from.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n           + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n           + \" cumbersome.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n       .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n           + \"record size estimate compute dynamically based on commit metadata. \"\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String AUTO_CLEAN_PROP = AUTO_CLEAN.key();\n+  /** @deprecated Use {@link #ASYNC_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String ASYNC_CLEAN_PROP = ASYNC_CLEAN.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_PROP = INLINE_COMPACT.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = INLINE_COMPACT_NUM_DELTA_COMMITS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = INLINE_COMPACT_TIME_DELTA_SECONDS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = INLINE_COMPACT_TRIGGER_STRATEGY.key();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = CLEANER_FILE_VERSIONS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_COMMITS_RETAINED_PROP = CLEANER_COMMITS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_INCREMENTAL_MODE = CLEANER_INCREMENTAL_MODE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MAX_COMMITS_TO_KEEP_PROP = MAX_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MIN_COMMITS_TO_KEEP_PROP = MIN_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = COMMITS_ARCHIVAL_BATCH_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = RECORD_SIZE_ESTIMATION_THRESHOLD.key();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = RECORD_SIZE_ESTIMATION_THRESHOLD.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = TARGET_IO_PER_COMPACTION_IN_MB.key();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = TARGET_IO_PER_COMPACTION_IN_MB.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMPACTION_STRATEGY_PROP = COMPACTION_STRATEGY.key();\n+  /** @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_STRATEGY = COMPACTION_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_PAYLOAD_CLASS = PAYLOAD_CLASS_NAME.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String PAYLOAD_CLASS_PROP = PAYLOAD_CLASS_NAME.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = COMPACTION_LAZY_BLOCK_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = COMPACTION_REVERSE_LOG_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_POLICY = CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = FAILED_WRITES_CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private  static final String DEFAULT_FAILED_WRITES_CLEANER_POLICY = FAILED_WRITES_CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_AUTO_CLEAN = AUTO_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #ASYNC_CLEAN} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_ASYNC_CLEAN = ASYNC_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #INLINE_COMPACT} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT = INLINE_COMPACT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INCREMENTAL_CLEANER = CLEANER_INCREMENTAL_MODE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = INLINE_COMPACT_NUM_DELTA_COMMITS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = INLINE_COMPACT_TIME_DELTA_SECONDS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = INLINE_COMPACT_TRIGGER_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = CLEANER_FILE_VERSIONS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = CLEANER_COMMITS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = MAX_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = MIN_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = COMMITS_ARCHIVAL_BATCH_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.key();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.defaultValue();\n+\n   private HoodieCompactionConfig() {\n     super();\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "4b388c104e024f32ae0705f6627e48b72b3408b4", "committedDate": "2022-01-31 22:36:17 -0500", "message": "[HUDI-3292] Enabling lazy read by default for log blocks during compaction (#4661)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "5009138d044b4d859237f0f581aeeb71065dc526", "committedDate": "2022-02-18 08:57:04 -0500", "message": "[HUDI-3438] Avoid getSmallFiles if hoodie.parquet.small.file.limit is 0 (#4823)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5a28f7f15358839388c9db9d4fce2aa81862b46a", "committedDate": "2022-09-19 01:03:16 +0800", "message": "[HUDI-4870] Improve compaction config description (#6706)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "url": "https://github.com/apache/hudi/commit/48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "message": "Update HoodieCompactionConfig.java", "committedDate": "2021-01-19T14:38:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTE0Mw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755143", "body": "Can we unify the constant name to `INLINE_COMPACT_TIME_DELTA_SECONDS_PROP ` so that we can align with `INLINE_COMPACT_NUM_DELTA_COMMITS_PROP ` and `withMaxDeltaTimeBeforeCompaction `", "bodyText": "Can we unify the constant name to INLINE_COMPACT_TIME_DELTA_SECONDS_PROP  so that we can align with INLINE_COMPACT_NUM_DELTA_COMMITS_PROP  and withMaxDeltaTimeBeforeCompaction", "bodyHTML": "<p dir=\"auto\">Can we unify the constant name to <code>INLINE_COMPACT_TIME_DELTA_SECONDS_PROP </code> so that we can align with <code>INLINE_COMPACT_NUM_DELTA_COMMITS_PROP </code> and <code>withMaxDeltaTimeBeforeCompaction </code></p>", "author": "yanghua", "createdAt": "2021-01-20T08:17:22Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 9d43733d29..00b8d5eec7 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,7 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n   public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n", "next_change": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 00b8d5eec7..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,6 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n   public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 9d43733d29..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,7 +47,8 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n   public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 934d91a274..aa9e75ca55 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -37,97 +39,176 @@ import java.util.Properties;\n  * Compaction related config.\n  */\n @Immutable\n-public class HoodieCompactionConfig extends DefaultHoodieConfig {\n-\n-  public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n-  public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n-  public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n-  // Turn on inline compaction - after fw delta commits a inline compaction will be run\n-  public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n-  // Run a compaction every N delta commits\n-  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  // Run a compaction when time elapsed > N seconds since last compaction\n-  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n-  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n-  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n-  public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n-  public static final String CLEANER_INCREMENTAL_MODE = \"hoodie.cleaner.incremental.mode\";\n-  public static final String MAX_COMMITS_TO_KEEP_PROP = \"hoodie.keep.max.commits\";\n-  public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n-  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n-  // Set true to clean bootstrap source files when necessary\n-  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.delete.bootstrap.base.file\";\n-  // Upsert uses this file size to compact new data onto existing files..\n-  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n-  // By default, treat any file <= 100MB as a small file.\n-  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);\n-  // Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten.\n-  // If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order,\n-  // until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\n-  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = \"hoodie.record.size.estimation.threshold\";\n-  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = \"1.0\";\n+public class HoodieCompactionConfig extends HoodieConfig {\n+\n+  public static final ConfigProperty<String> CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy\")\n+      .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())\n+      .withDocumentation(\"Cleaning policy to be used. Hudi will delete older versions of parquet files to re-claim space.\"\n+          + \" Any Query/Computation referring to this version of the file will fail. \"\n+          + \"It is good to make sure that the data is retained for more than the maximum query execution time.\");\n+\n+  public static final ConfigProperty<String> AUTO_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.automatic\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Should cleanup if there is anything to cleanup immediately after the commit\");\n+\n+  public static final ConfigProperty<String> ASYNC_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Only applies when #withAutoClean is turned on. When turned on runs cleaner async with writing.\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When set to true, compaction is triggered by the ingestion itself, \"\n+          + \"right after a commit/deltacommit action as part of insert/upsert/bulk_insert\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.commits\")\n+      .defaultValue(\"5\")\n+      .withDocumentation(\"Number of max delta commits to keep before triggering an inline compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.seconds\")\n+      .defaultValue(String.valueOf(60 * 60))\n+      .withDocumentation(\"Run a compaction when time elapsed > N seconds since last compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TRIGGER_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.trigger.strategy\")\n+      .defaultValue(CompactionTriggerStrategy.NUM_COMMITS.name())\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_FILE_VERSIONS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.fileversions.retained\")\n+      .defaultValue(\"3\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_COMMITS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.commits.retained\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Number of commits to retain. So data will be retained for num_of_commits * time_between_commits \"\n+          + \"(scheduled). This also directly translates into how much you can incrementally pull on this table\");\n+\n+  public static final ConfigProperty<String> CLEANER_INCREMENTAL_MODE = ConfigProperty\n+      .key(\"hoodie.cleaner.incremental.mode\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> MAX_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.max.commits\")\n+      .defaultValue(\"30\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> MIN_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.min.commits\")\n+      .defaultValue(\"20\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> COMMITS_ARCHIVAL_BATCH_SIZE_PROP = ConfigProperty\n+      .key(\"hoodie.commits.archival.batch\")\n+      .defaultValue(String.valueOf(10))\n+      .withDocumentation(\"This controls the number of commit instants read in memory as a batch and archived together.\");\n+\n+  public static final ConfigProperty<String> CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = ConfigProperty\n+      .key(\"hoodie.cleaner.delete.bootstrap.base.file\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Set true to clean bootstrap source files when necessary\");\n+\n+  public static final ConfigProperty<String> PARQUET_SMALL_FILE_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.parquet.small.file.limit\")\n+      .defaultValue(String.valueOf(104857600))\n+      .withDocumentation(\"Upsert uses this file size to compact new data onto existing files. \"\n+          + \"By default, treat any file <= 100MB as a small file.\");\n+\n+  public static final ConfigProperty<String> RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = ConfigProperty\n+      .key(\"hoodie.record.size.estimation.threshold\")\n+      .defaultValue(\"1.0\")\n+      .withDocumentation(\"Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten. \"\n+          + \"If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order, \"\n+          + \"until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\");\n+\n+  public static final ConfigProperty<String> CLEANER_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.cleaner.parallelism\")\n+      .defaultValue(\"200\")\n+      .withDocumentation(\"Increase this if cleaning becomes slow.\");\n+\n+  // 500GB of target IO per compaction (both read and write\n+  public static final ConfigProperty<String> TARGET_IO_PER_COMPACTION_IN_MB_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.target.io\")\n+      .defaultValue(String.valueOf(500 * 1024))\n+      .withDocumentation(\"Amount of MBs to spend during compaction run for the LogFileSizeBasedCompactionStrategy. \"\n+          + \"This value helps bound ingestion latency while compaction is run inline mode.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.strategy\")\n+      .defaultValue(LogFileSizeBasedCompactionStrategy.class.getName())\n+      .withDocumentation(\"Compaction strategy decides which file groups are picked up for \"\n+          + \"compaction during each compaction run. By default. Hudi picks the log file \"\n+          + \"with most accumulated unmerged data\");\n+\n+  public static final ConfigProperty<String> PAYLOAD_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"This needs to be same as class used during insert/upserts. Just like writing, compaction also uses \"\n+          + \"the record payload class to merge records in the log against each other, merge again with the base file and \"\n+          + \"produce the final record to be written after compaction.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.lazy.block.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When a CompactedLogScanner merges all log files, this config helps to choose whether the logblocks \"\n+          + \"should be read lazily or not. Choose true to use I/O intensive lazy block reading (low memory usage) or false \"\n+          + \"for Memory intensive immediate block read (high memory usage)\");\n+\n+  public static final ConfigProperty<String> COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.reverse.log.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"HoodieLogFormatReader reads a logfile in the forward direction starting from pos=0 to pos=file_length. \"\n+          + \"If this config is set to true, the Reader reads the logfile in reverse direction, from pos=file_length to pos=0\");\n+\n+  public static final ConfigProperty<String> FAILED_WRITES_CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy.failed.writes\")\n+      .defaultValue(HoodieFailedWritesCleaningPolicy.EAGER.name())\n+      .withDocumentation(\"Cleaning policy for failed writes to be used. Hudi will delete any files written by \"\n+          + \"failed writes to re-claim space. Choose to perform this rollback of failed writes eagerly before \"\n+          + \"every writer starts (only supported for single writer) or lazily by the cleaner (required for multi-writers)\");\n+\n+  public static final ConfigProperty<String> TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.daybased.target.partitions\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Used by org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy to denote the number of \"\n+          + \"latest partitions to compact during a compaction run.\");\n \n   /**\n    * Configs related to specific table types.\n    */\n-  // Number of inserts, that will be put each partition/bucket for writing\n-  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = \"hoodie.copyonwrite.insert.split.size\";\n-  // The rationale to pick the insert parallelism is the following. Writing out 100MB files,\n-  // with atleast 1kb records, means 100K records per file. we just overprovision to 500K\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = String.valueOf(500000);\n-  // Config to control whether we control insert split sizes automatically based on average\n-  // record sizes\n-  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = \"hoodie.copyonwrite.insert.auto.split\";\n-  // its off by default\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);\n-  // This value is used as a guesstimate for the record size, if we can't determine this from\n-  // previous commits\n-  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = \"hoodie.copyonwrite.record.size.estimate\";\n-  // Used to determine how much more can be packed into a small file, before it exceeds the size\n-  // limit.\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = String.valueOf(1024);\n-  public static final String CLEANER_PARALLELISM = \"hoodie.cleaner.parallelism\";\n-  public static final String DEFAULT_CLEANER_PARALLELISM = String.valueOf(200);\n-  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = \"hoodie.compaction.target.io\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = String.valueOf(500 * 1024);\n-  public static final String COMPACTION_STRATEGY_PROP = \"hoodie.compaction.strategy\";\n-  // 200GB of target IO per compaction\n-  public static final String DEFAULT_COMPACTION_STRATEGY = LogFileSizeBasedCompactionStrategy.class.getName();\n-  // used to merge records written to log file\n-  public static final String DEFAULT_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-  public static final String PAYLOAD_CLASS_PROP = \"hoodie.compaction.payload.class\";\n-\n-  // used to choose a trade off between IO vs Memory when performing compaction process\n-  // Depending on outputfile_size and memory provided, choose true to avoid OOM for large file\n-  // size + small memory\n-  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"hoodie.compaction.lazy.block.read\";\n-  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"false\";\n-  // used to choose whether to enable reverse log reading (reverse log traversal)\n-  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = \"hoodie.compaction.reverse.log.read\";\n-  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = \"false\";\n-  private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();\n-  private static final String DEFAULT_AUTO_CLEAN = \"true\";\n-  private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n-  private static final String DEFAULT_INLINE_COMPACT = \"false\";\n-  private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n-  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n-  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n-  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n-  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n-  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = \"20\";\n-  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = String.valueOf(10);\n-  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"false\";\n-  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP =\n-      \"hoodie.compaction.daybased.target.partitions\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = String.valueOf(10);\n-\n-  private HoodieCompactionConfig(Properties props) {\n-    super(props);\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.split.size\")\n+      .defaultValue(String.valueOf(500000))\n+      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n+          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n+          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.auto.split\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n+          + \" record sizes.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.record.size.estimate\")\n+      .defaultValue(String.valueOf(1024))\n+      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n+          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n+          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+\n+  private HoodieCompactionConfig() {\n+    super();\n   }\n \n   public static HoodieCompactionConfig.Builder newBuilder() {\n", "next_change": {"commit": "75040ee9e5caa0783009b6ef529d6605e82d4135", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex aa9e75ca55..e8d55934be 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -190,22 +205,24 @@ public class HoodieCompactionConfig extends HoodieConfig {\n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.split.size\")\n       .defaultValue(String.valueOf(500000))\n-      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n-          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n-          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+      .withDocumentation(\"Number of inserts assigned for each partition/bucket for writing. \"\n+          + \"We based the default on writing out 100MB files, with at least 1kb records (100K records per file), and \"\n+          + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n+          + \"  write, where there is no history to learn record sizes from.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n-          + \" record sizes.\");\n+          + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n+          + \" cumbersome.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n-      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n-          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n-          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+      .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n+          + \"record size estimate compute dynamically based on commit metadata. \"\n+          + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n   private HoodieCompactionConfig() {\n     super();\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex e8d55934be..ce74aad6b0 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -210,20 +217,237 @@ public class HoodieCompactionConfig extends HoodieConfig {\n           + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n           + \"  write, where there is no history to learn record sizes from.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n           + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n           + \" cumbersome.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n       .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n           + \"record size estimate compute dynamically based on commit metadata. \"\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String AUTO_CLEAN_PROP = AUTO_CLEAN.key();\n+  /** @deprecated Use {@link #ASYNC_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String ASYNC_CLEAN_PROP = ASYNC_CLEAN.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_PROP = INLINE_COMPACT.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = INLINE_COMPACT_NUM_DELTA_COMMITS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = INLINE_COMPACT_TIME_DELTA_SECONDS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = INLINE_COMPACT_TRIGGER_STRATEGY.key();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = CLEANER_FILE_VERSIONS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_COMMITS_RETAINED_PROP = CLEANER_COMMITS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_INCREMENTAL_MODE = CLEANER_INCREMENTAL_MODE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MAX_COMMITS_TO_KEEP_PROP = MAX_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MIN_COMMITS_TO_KEEP_PROP = MIN_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = COMMITS_ARCHIVAL_BATCH_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = RECORD_SIZE_ESTIMATION_THRESHOLD.key();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = RECORD_SIZE_ESTIMATION_THRESHOLD.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = TARGET_IO_PER_COMPACTION_IN_MB.key();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = TARGET_IO_PER_COMPACTION_IN_MB.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMPACTION_STRATEGY_PROP = COMPACTION_STRATEGY.key();\n+  /** @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_STRATEGY = COMPACTION_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_PAYLOAD_CLASS = PAYLOAD_CLASS_NAME.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String PAYLOAD_CLASS_PROP = PAYLOAD_CLASS_NAME.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = COMPACTION_LAZY_BLOCK_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = COMPACTION_REVERSE_LOG_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_POLICY = CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = FAILED_WRITES_CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private  static final String DEFAULT_FAILED_WRITES_CLEANER_POLICY = FAILED_WRITES_CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_AUTO_CLEAN = AUTO_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #ASYNC_CLEAN} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_ASYNC_CLEAN = ASYNC_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #INLINE_COMPACT} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT = INLINE_COMPACT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INCREMENTAL_CLEANER = CLEANER_INCREMENTAL_MODE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = INLINE_COMPACT_NUM_DELTA_COMMITS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = INLINE_COMPACT_TIME_DELTA_SECONDS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = INLINE_COMPACT_TRIGGER_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = CLEANER_FILE_VERSIONS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = CLEANER_COMMITS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = MAX_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = MIN_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = COMMITS_ARCHIVAL_BATCH_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.key();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.defaultValue();\n+\n   private HoodieCompactionConfig() {\n     super();\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "4b388c104e024f32ae0705f6627e48b72b3408b4", "committedDate": "2022-01-31 22:36:17 -0500", "message": "[HUDI-3292] Enabling lazy read by default for log blocks during compaction (#4661)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "5009138d044b4d859237f0f581aeeb71065dc526", "committedDate": "2022-02-18 08:57:04 -0500", "message": "[HUDI-3438] Avoid getSmallFiles if hoodie.parquet.small.file.limit is 0 (#4823)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5a28f7f15358839388c9db9d4fce2aa81862b46a", "committedDate": "2022-09-19 01:03:16 +0800", "message": "[HUDI-4870] Improve compaction config description (#6706)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTIwNQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755205", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:17:30Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -109,6 +112,8 @@\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n+  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 9d43733d29..00b8d5eec7 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -112,7 +112,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60);\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n   private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM.name();\n   private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n   private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n", "next_change": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 00b8d5eec7..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -113,7 +114,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n   private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM.name();\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n   private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n   private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n   private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n", "next_change": null}]}}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 9d43733d29..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -112,8 +113,8 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM.name();\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n   private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n   private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n   private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 934d91a274..aa9e75ca55 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -37,97 +39,176 @@ import java.util.Properties;\n  * Compaction related config.\n  */\n @Immutable\n-public class HoodieCompactionConfig extends DefaultHoodieConfig {\n-\n-  public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n-  public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n-  public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n-  // Turn on inline compaction - after fw delta commits a inline compaction will be run\n-  public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n-  // Run a compaction every N delta commits\n-  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  // Run a compaction when time elapsed > N seconds since last compaction\n-  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n-  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n-  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n-  public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n-  public static final String CLEANER_INCREMENTAL_MODE = \"hoodie.cleaner.incremental.mode\";\n-  public static final String MAX_COMMITS_TO_KEEP_PROP = \"hoodie.keep.max.commits\";\n-  public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n-  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n-  // Set true to clean bootstrap source files when necessary\n-  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.delete.bootstrap.base.file\";\n-  // Upsert uses this file size to compact new data onto existing files..\n-  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n-  // By default, treat any file <= 100MB as a small file.\n-  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);\n-  // Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten.\n-  // If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order,\n-  // until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\n-  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = \"hoodie.record.size.estimation.threshold\";\n-  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = \"1.0\";\n+public class HoodieCompactionConfig extends HoodieConfig {\n+\n+  public static final ConfigProperty<String> CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy\")\n+      .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())\n+      .withDocumentation(\"Cleaning policy to be used. Hudi will delete older versions of parquet files to re-claim space.\"\n+          + \" Any Query/Computation referring to this version of the file will fail. \"\n+          + \"It is good to make sure that the data is retained for more than the maximum query execution time.\");\n+\n+  public static final ConfigProperty<String> AUTO_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.automatic\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Should cleanup if there is anything to cleanup immediately after the commit\");\n+\n+  public static final ConfigProperty<String> ASYNC_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Only applies when #withAutoClean is turned on. When turned on runs cleaner async with writing.\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When set to true, compaction is triggered by the ingestion itself, \"\n+          + \"right after a commit/deltacommit action as part of insert/upsert/bulk_insert\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.commits\")\n+      .defaultValue(\"5\")\n+      .withDocumentation(\"Number of max delta commits to keep before triggering an inline compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.seconds\")\n+      .defaultValue(String.valueOf(60 * 60))\n+      .withDocumentation(\"Run a compaction when time elapsed > N seconds since last compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TRIGGER_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.trigger.strategy\")\n+      .defaultValue(CompactionTriggerStrategy.NUM_COMMITS.name())\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_FILE_VERSIONS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.fileversions.retained\")\n+      .defaultValue(\"3\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_COMMITS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.commits.retained\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Number of commits to retain. So data will be retained for num_of_commits * time_between_commits \"\n+          + \"(scheduled). This also directly translates into how much you can incrementally pull on this table\");\n+\n+  public static final ConfigProperty<String> CLEANER_INCREMENTAL_MODE = ConfigProperty\n+      .key(\"hoodie.cleaner.incremental.mode\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> MAX_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.max.commits\")\n+      .defaultValue(\"30\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> MIN_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.min.commits\")\n+      .defaultValue(\"20\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> COMMITS_ARCHIVAL_BATCH_SIZE_PROP = ConfigProperty\n+      .key(\"hoodie.commits.archival.batch\")\n+      .defaultValue(String.valueOf(10))\n+      .withDocumentation(\"This controls the number of commit instants read in memory as a batch and archived together.\");\n+\n+  public static final ConfigProperty<String> CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = ConfigProperty\n+      .key(\"hoodie.cleaner.delete.bootstrap.base.file\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Set true to clean bootstrap source files when necessary\");\n+\n+  public static final ConfigProperty<String> PARQUET_SMALL_FILE_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.parquet.small.file.limit\")\n+      .defaultValue(String.valueOf(104857600))\n+      .withDocumentation(\"Upsert uses this file size to compact new data onto existing files. \"\n+          + \"By default, treat any file <= 100MB as a small file.\");\n+\n+  public static final ConfigProperty<String> RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = ConfigProperty\n+      .key(\"hoodie.record.size.estimation.threshold\")\n+      .defaultValue(\"1.0\")\n+      .withDocumentation(\"Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten. \"\n+          + \"If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order, \"\n+          + \"until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\");\n+\n+  public static final ConfigProperty<String> CLEANER_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.cleaner.parallelism\")\n+      .defaultValue(\"200\")\n+      .withDocumentation(\"Increase this if cleaning becomes slow.\");\n+\n+  // 500GB of target IO per compaction (both read and write\n+  public static final ConfigProperty<String> TARGET_IO_PER_COMPACTION_IN_MB_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.target.io\")\n+      .defaultValue(String.valueOf(500 * 1024))\n+      .withDocumentation(\"Amount of MBs to spend during compaction run for the LogFileSizeBasedCompactionStrategy. \"\n+          + \"This value helps bound ingestion latency while compaction is run inline mode.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.strategy\")\n+      .defaultValue(LogFileSizeBasedCompactionStrategy.class.getName())\n+      .withDocumentation(\"Compaction strategy decides which file groups are picked up for \"\n+          + \"compaction during each compaction run. By default. Hudi picks the log file \"\n+          + \"with most accumulated unmerged data\");\n+\n+  public static final ConfigProperty<String> PAYLOAD_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"This needs to be same as class used during insert/upserts. Just like writing, compaction also uses \"\n+          + \"the record payload class to merge records in the log against each other, merge again with the base file and \"\n+          + \"produce the final record to be written after compaction.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.lazy.block.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When a CompactedLogScanner merges all log files, this config helps to choose whether the logblocks \"\n+          + \"should be read lazily or not. Choose true to use I/O intensive lazy block reading (low memory usage) or false \"\n+          + \"for Memory intensive immediate block read (high memory usage)\");\n+\n+  public static final ConfigProperty<String> COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.reverse.log.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"HoodieLogFormatReader reads a logfile in the forward direction starting from pos=0 to pos=file_length. \"\n+          + \"If this config is set to true, the Reader reads the logfile in reverse direction, from pos=file_length to pos=0\");\n+\n+  public static final ConfigProperty<String> FAILED_WRITES_CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy.failed.writes\")\n+      .defaultValue(HoodieFailedWritesCleaningPolicy.EAGER.name())\n+      .withDocumentation(\"Cleaning policy for failed writes to be used. Hudi will delete any files written by \"\n+          + \"failed writes to re-claim space. Choose to perform this rollback of failed writes eagerly before \"\n+          + \"every writer starts (only supported for single writer) or lazily by the cleaner (required for multi-writers)\");\n+\n+  public static final ConfigProperty<String> TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.daybased.target.partitions\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Used by org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy to denote the number of \"\n+          + \"latest partitions to compact during a compaction run.\");\n \n   /**\n    * Configs related to specific table types.\n    */\n-  // Number of inserts, that will be put each partition/bucket for writing\n-  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = \"hoodie.copyonwrite.insert.split.size\";\n-  // The rationale to pick the insert parallelism is the following. Writing out 100MB files,\n-  // with atleast 1kb records, means 100K records per file. we just overprovision to 500K\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = String.valueOf(500000);\n-  // Config to control whether we control insert split sizes automatically based on average\n-  // record sizes\n-  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = \"hoodie.copyonwrite.insert.auto.split\";\n-  // its off by default\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);\n-  // This value is used as a guesstimate for the record size, if we can't determine this from\n-  // previous commits\n-  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = \"hoodie.copyonwrite.record.size.estimate\";\n-  // Used to determine how much more can be packed into a small file, before it exceeds the size\n-  // limit.\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = String.valueOf(1024);\n-  public static final String CLEANER_PARALLELISM = \"hoodie.cleaner.parallelism\";\n-  public static final String DEFAULT_CLEANER_PARALLELISM = String.valueOf(200);\n-  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = \"hoodie.compaction.target.io\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = String.valueOf(500 * 1024);\n-  public static final String COMPACTION_STRATEGY_PROP = \"hoodie.compaction.strategy\";\n-  // 200GB of target IO per compaction\n-  public static final String DEFAULT_COMPACTION_STRATEGY = LogFileSizeBasedCompactionStrategy.class.getName();\n-  // used to merge records written to log file\n-  public static final String DEFAULT_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-  public static final String PAYLOAD_CLASS_PROP = \"hoodie.compaction.payload.class\";\n-\n-  // used to choose a trade off between IO vs Memory when performing compaction process\n-  // Depending on outputfile_size and memory provided, choose true to avoid OOM for large file\n-  // size + small memory\n-  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"hoodie.compaction.lazy.block.read\";\n-  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"false\";\n-  // used to choose whether to enable reverse log reading (reverse log traversal)\n-  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = \"hoodie.compaction.reverse.log.read\";\n-  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = \"false\";\n-  private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();\n-  private static final String DEFAULT_AUTO_CLEAN = \"true\";\n-  private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n-  private static final String DEFAULT_INLINE_COMPACT = \"false\";\n-  private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n-  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n-  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n-  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n-  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n-  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = \"20\";\n-  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = String.valueOf(10);\n-  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"false\";\n-  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP =\n-      \"hoodie.compaction.daybased.target.partitions\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = String.valueOf(10);\n-\n-  private HoodieCompactionConfig(Properties props) {\n-    super(props);\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.split.size\")\n+      .defaultValue(String.valueOf(500000))\n+      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n+          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n+          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.auto.split\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n+          + \" record sizes.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.record.size.estimate\")\n+      .defaultValue(String.valueOf(1024))\n+      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n+          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n+          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+\n+  private HoodieCompactionConfig() {\n+    super();\n   }\n \n   public static HoodieCompactionConfig.Builder newBuilder() {\n", "next_change": {"commit": "75040ee9e5caa0783009b6ef529d6605e82d4135", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex aa9e75ca55..e8d55934be 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -190,22 +205,24 @@ public class HoodieCompactionConfig extends HoodieConfig {\n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.split.size\")\n       .defaultValue(String.valueOf(500000))\n-      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n-          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n-          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+      .withDocumentation(\"Number of inserts assigned for each partition/bucket for writing. \"\n+          + \"We based the default on writing out 100MB files, with at least 1kb records (100K records per file), and \"\n+          + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n+          + \"  write, where there is no history to learn record sizes from.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n-          + \" record sizes.\");\n+          + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n+          + \" cumbersome.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n-      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n-          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n-          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+      .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n+          + \"record size estimate compute dynamically based on commit metadata. \"\n+          + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n   private HoodieCompactionConfig() {\n     super();\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex e8d55934be..ce74aad6b0 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -210,20 +217,237 @@ public class HoodieCompactionConfig extends HoodieConfig {\n           + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n           + \"  write, where there is no history to learn record sizes from.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n           + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n           + \" cumbersome.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n       .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n           + \"record size estimate compute dynamically based on commit metadata. \"\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String AUTO_CLEAN_PROP = AUTO_CLEAN.key();\n+  /** @deprecated Use {@link #ASYNC_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String ASYNC_CLEAN_PROP = ASYNC_CLEAN.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_PROP = INLINE_COMPACT.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = INLINE_COMPACT_NUM_DELTA_COMMITS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = INLINE_COMPACT_TIME_DELTA_SECONDS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = INLINE_COMPACT_TRIGGER_STRATEGY.key();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = CLEANER_FILE_VERSIONS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_COMMITS_RETAINED_PROP = CLEANER_COMMITS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_INCREMENTAL_MODE = CLEANER_INCREMENTAL_MODE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MAX_COMMITS_TO_KEEP_PROP = MAX_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MIN_COMMITS_TO_KEEP_PROP = MIN_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = COMMITS_ARCHIVAL_BATCH_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = RECORD_SIZE_ESTIMATION_THRESHOLD.key();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = RECORD_SIZE_ESTIMATION_THRESHOLD.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = TARGET_IO_PER_COMPACTION_IN_MB.key();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = TARGET_IO_PER_COMPACTION_IN_MB.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMPACTION_STRATEGY_PROP = COMPACTION_STRATEGY.key();\n+  /** @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_STRATEGY = COMPACTION_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_PAYLOAD_CLASS = PAYLOAD_CLASS_NAME.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String PAYLOAD_CLASS_PROP = PAYLOAD_CLASS_NAME.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = COMPACTION_LAZY_BLOCK_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = COMPACTION_REVERSE_LOG_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_POLICY = CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = FAILED_WRITES_CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private  static final String DEFAULT_FAILED_WRITES_CLEANER_POLICY = FAILED_WRITES_CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_AUTO_CLEAN = AUTO_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #ASYNC_CLEAN} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_ASYNC_CLEAN = ASYNC_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #INLINE_COMPACT} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT = INLINE_COMPACT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INCREMENTAL_CLEANER = CLEANER_INCREMENTAL_MODE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = INLINE_COMPACT_NUM_DELTA_COMMITS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = INLINE_COMPACT_TIME_DELTA_SECONDS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = INLINE_COMPACT_TRIGGER_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = CLEANER_FILE_VERSIONS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = CLEANER_COMMITS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = MAX_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = MIN_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = COMMITS_ARCHIVAL_BATCH_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.key();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.defaultValue();\n+\n   private HoodieCompactionConfig() {\n     super();\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "4b388c104e024f32ae0705f6627e48b72b3408b4", "committedDate": "2022-01-31 22:36:17 -0500", "message": "[HUDI-3292] Enabling lazy read by default for log blocks during compaction (#4661)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "5009138d044b4d859237f0f581aeeb71065dc526", "committedDate": "2022-02-18 08:57:04 -0500", "message": "[HUDI-3438] Avoid getSmallFiles if hoodie.parquet.small.file.limit is 0 (#4823)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5a28f7f15358839388c9db9d4fce2aa81862b46a", "committedDate": "2022-09-19 01:03:16 +0800", "message": "[HUDI-4870] Improve compaction config description (#6706)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDMwMA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760300", "body": "Do we also need to judge `NUM_OR_TIME `?", "bodyText": "Do we also need to judge NUM_OR_TIME ?", "bodyHTML": "<p dir=\"auto\">Do we also need to judge <code>NUM_OR_TIME </code>?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:26:05Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5ODkyNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560998926", "bodyText": "no need. only TIME_ELAPSED don't need deltaCommitsSinceLastCompaction", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDMwMA=="}], "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +83,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDc5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760796", "body": "`return` -> `get`", "bodyText": "return -> get", "bodyHTML": "<p dir=\"auto\"><code>return</code> -&gt; <code>get</code></p>", "author": "yanghua", "createdAt": "2021-01-20T08:26:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MzAyNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763026", "body": "Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  `threshold` is immutable.", "bodyText": "Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  threshold is immutable.", "bodyHTML": "<p dir=\"auto\">Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  <code>threshold</code> is immutable.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:30:27Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2Mzk5MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763990", "body": "IMO, we do not need all the `else` statement, right? It's too normal if we do not match the compaction strategy.", "bodyText": "IMO, we do not need all the else statement, right? It's too normal if we do not match the compaction strategy.", "bodyHTML": "<p dir=\"auto\">IMO, we do not need all the <code>else</code> statement, right? It's too normal if we do not match the compaction strategy.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:32:06Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NTQ4OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560765489", "body": "Since you have defined a `compactable `, let's use `break` here and return it in the end.", "bodyText": "Since you have defined a compactable , let's use break here and return it in the end.", "bodyHTML": "<p dir=\"auto\">Since you have defined a <code>compactable </code>, let's use <code>break</code> here and return it in the end.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:34:35Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NjI3MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560766271", "body": "`compact type` -> `compaction trigger strategy`.", "bodyText": "compact type -> compaction trigger strategy.", "bodyHTML": "<p dir=\"auto\"><code>compact type</code> -&gt; <code>compaction trigger strategy</code>.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:35:47Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n+      case TIME_ELAPSED:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n+              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_OR_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_AND_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..1ec867cb39 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -107,58 +107,43 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n       case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 8492b229a3..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,82 +82,62 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n-    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    // get deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n-    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n-    long elapsedTime;\n+    int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n-              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n         }\n-        return compactable;\n+        break;\n       case TIME_ELAPSED:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n-          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n-              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_OR_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       case NUM_AND_TIME:\n-        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n-        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n-              inlineCompactDeltaElapsedTimeMax));\n-        } else {\n-          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n-                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n-              threshold._1, elapsedTime));\n+              inlineCompactDeltaSecondsMax));\n         }\n-        return compactable;\n+        break;\n       default:\n-        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());\n+        throw new HoodieCompactionException(\"Unsupported compaction trigger strategy: \" + config.getInlineCompactTriggerStrategy());\n     }\n+    return compactable;\n   }\n \n   public Long parsedToSeconds(String time) {\n", "next_change": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 63%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 9c44499a8f..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -140,7 +181,7 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return compactable;\n   }\n \n-  public Long parsedToSeconds(String time) {\n+  private Long parsedToSeconds(String time) {\n     long timestamp;\n     try {\n       timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2ODgyNw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560768827", "body": "We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?", "bodyText": "We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?", "bodyHTML": "<p dir=\"auto\">We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:39:55Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5OTc4OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560999788", "bodyText": "good catch", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:26:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2ODgyNw=="}], "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..9e99e26c18 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -99,8 +99,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n \n       // after 10s, that will trigger compaction\n-      Thread.sleep(10000);\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(10000);\n       HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n \n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -99,8 +99,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n \n       // after 10s, that will trigger compaction\n-      Thread.sleep(10000);\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(10000);\n       HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n \n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -100,11 +100,11 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // after 10s, that will trigger compaction\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(10000);\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n \n       // Then: ensure the file slices are compacted as per policy\n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n       assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n     }\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -105,7 +105,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // Then: ensure the file slices are compacted as per policy\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n       assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n     }\n   }\n", "next_change": {"commit": "cc3737be506475c11c12471be6d0296ea14c7f39", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..7f1046ba90 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -113,7 +119,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n   @Test\n   public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n     // Given: make three commits\n-    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_OR_TIME);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n", "next_change": {"commit": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 7f1046ba90..32d2dcda95 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -122,7 +196,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n     HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_OR_TIME);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n-      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      SparkRDDReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       // Then: trigger the compaction because reach 3 commits.\n", "next_change": null}]}}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTQzNA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769434", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:40:47Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..9e99e26c18 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -127,10 +126,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n       assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction because reach the time elapsed\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -127,10 +126,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n       assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction because reach the time elapsed\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -121,17 +121,17 @@ public class TestInlineCompaction extends CompactionTestBase {\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       // Then: trigger the compaction because reach 3 commits.\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n       // 4th commit, that will trigger compaction because reach the time elapsed\n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n     }\n   }\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -125,14 +125,14 @@ public class TestInlineCompaction extends CompactionTestBase {\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n       // 4th commit, that will trigger compaction because reach the time elapsed\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(6, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": {"commit": "cc3737be506475c11c12471be6d0296ea14c7f39", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..7f1046ba90 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -128,7 +134,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n       // 4th commit, that will trigger compaction because reach the time elapsed\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime(60000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTYyMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769623", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:06Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..9e99e26c18 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -151,10 +149,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -151,10 +149,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -145,16 +145,16 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n       // 4th commit, that will trigger compaction\n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n     }\n   }\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -148,14 +148,14 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n-      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n       // 4th commit, that will trigger compaction\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(5, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": {"commit": "b6124ff85a107ab170430947a24bc71df8612f1c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..47ebc8b259 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -142,20 +222,20 @@ public class TestInlineCompaction extends CompactionTestBase {\n     HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n-      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n-      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      SparkRDDReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n-      // Then: ensure no compaction is executedm since there are only 3 delta commits\n-      assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n-      // 4th commit, that will trigger compaction\n+      // Then: ensure no compaction is executed since there are only 3 delta commits\n+      assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n+      // 3d commit, that will trigger compaction\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(5, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n+      assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTg0NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769845", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:27Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..9e99e26c18 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -151,10 +149,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -151,10 +149,9 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n-      Thread.sleep(20000);\n       // 4th commit, that will trigger compaction\n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n-      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -145,16 +145,16 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n       assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n       // 4th commit, that will trigger compaction\n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n-      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n     }\n   }\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -148,14 +148,14 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n       // Then: ensure no compaction is executedm since there are only 3 delta commits\n-      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n       // 4th commit, that will trigger compaction\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(5, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": {"commit": "b6124ff85a107ab170430947a24bc71df8612f1c", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..47ebc8b259 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -142,20 +222,20 @@ public class TestInlineCompaction extends CompactionTestBase {\n     HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n-      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n-      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      SparkRDDReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n \n-      // Then: ensure no compaction is executedm since there are only 3 delta commits\n-      assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n-      // 4th commit, that will trigger compaction\n+      // Then: ensure no compaction is executed since there are only 3 delta commits\n+      assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n+      // 3d commit, that will trigger compaction\n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       String finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n \n       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-      assertEquals(5, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n+      assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     }\n   }\n \n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTkzMQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769931", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:37Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnNumCommits() throws Exception {\n+    // Given: two commits, schedule compaction and its failed/in-flight\n+    HoodieWriteConfig cfg = getConfigBuilder(false)\n+        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n+            .withInlineCompaction(false)\n+            .withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+        .build();\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+    String instantTime2;\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Schedule compaction instant2, make it in-flight (simulates inline compaction failing)\n+      instantTime2 = HoodieActiveTimeline.createNewInstantTime();\n+      scheduleCompaction(instantTime2, writeClient, cfg);\n+      moveCompactionFromRequestedToInflight(instantTime2, cfg);\n+    }\n+\n+    // When: a third commit happens\n+    HoodieWriteConfig inlineCfg = getConfigForInlineCompaction(2, 60, CompactionTriggerStrategy.NUM);\n+    String instantTime3 = HoodieActiveTimeline.createNewInstantTime();\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(inlineCfg)) {\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(instantTime3, dataGen.generateUpdates(instantTime3, 100), writeClient, metaClient, inlineCfg, false);\n+    }\n+\n+    // Then: 1 delta commit is done, the failed compaction is retried\n+    metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+    assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    assertEquals(instantTime2, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnTime() throws Exception {\n     // Given: two commits, schedule compaction and its failed/in-flight\n     HoodieWriteConfig cfg = getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+            .withInlineCompaction(false)\n+            .withMaxDeltaTimeBeforeCompaction(5)\n+            .withInlineCompactionTriggerStrategy(CompactionTriggerStrategy.TIME_ELAPSED).build())\n         .build();\n-    List<String> instants = CollectionUtils.createImmutableList(\"000\", \"001\");\n+    String instantTime;\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      // Schedule compaction 002, make it in-flight (simulates inline compaction failing)\n-      scheduleCompaction(\"002\", writeClient, cfg);\n-      moveCompactionFromRequestedToInflight(\"002\", cfg);\n+      // Schedule compaction instantTime, make it in-flight (simulates inline compaction failing)\n+      Thread.sleep(10000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..9e99e26c18 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -212,8 +209,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       // Schedule compaction instantTime, make it in-flight (simulates inline compaction failing)\n-      Thread.sleep(10000);\n-      instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      instantTime = HoodieActiveTimeline.createNewInstantTime(10000);\n       scheduleCompaction(instantTime, writeClient, cfg);\n       moveCompactionFromRequestedToInflight(instantTime, cfg);\n     }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 1e7b37272a..80542edfa7 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -212,8 +209,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n       // Schedule compaction instantTime, make it in-flight (simulates inline compaction failing)\n-      Thread.sleep(10000);\n-      instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      instantTime = HoodieActiveTimeline.createNewInstantTime(10000);\n       scheduleCompaction(instantTime, writeClient, cfg);\n       moveCompactionFromRequestedToInflight(instantTime, cfg);\n     }\n", "next_change": {"commit": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 80542edfa7..97d287592b 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -218,13 +218,13 @@ public class TestInlineCompaction extends CompactionTestBase {\n     HoodieWriteConfig inlineCfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n     String instantTime2;\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(inlineCfg)) {\n-      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n       instantTime2 = HoodieActiveTimeline.createNewInstantTime();\n       createNextDeltaCommit(instantTime2, dataGen.generateUpdates(instantTime2, 10), writeClient, metaClient, inlineCfg, false);\n     }\n \n     // Then: 1 delta commit is done, the failed compaction is retried\n-    metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+    metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n     assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n     assertEquals(instantTime, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n   }\n", "next_change": {"commit": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 97d287592b..823d651aa1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -225,7 +225,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n     // Then: 1 delta commit is done, the failed compaction is retried\n     metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n-    assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     assertEquals(instantTime, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n   }\n \n", "next_change": {"commit": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\nindex 823d651aa1..24d387ec3f 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java\n", "chunk": "@@ -225,6 +306,7 @@ public class TestInlineCompaction extends CompactionTestBase {\n \n     // Then: 1 delta commit is done, the failed compaction is retried\n     metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n+    // 2 delta commits at the beginning. 1 compaction, 1 delta commit following it.\n     assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n     assertEquals(instantTime, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "c9fcf964b2bae56a54cb72951c8d8999eb323ed6", "committedDate": "2021-02-20 09:54:26 +0800", "message": "[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)"}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "b4b23e401bb66793c924dfe5b78d641c65e207a6", "committedDate": "2021-09-23 15:40:58 -0400", "message": "[HUDI-2383] Clean the marker files after compaction (#3576)"}, {"oid": "6aba00e84fade0b800e2d73c2f16be948af48d54", "committedDate": "2022-02-13 06:41:58 -0800", "message": "[MINOR] Fix typos in Spark client related classes (#4781)"}, {"oid": "cc3737be506475c11c12471be6d0296ea14c7f39", "committedDate": "2022-04-02 17:15:52 -0700", "message": "[HUDI-3664] Fixing Column Stats Index composition  (#5181)"}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "d7a52400a81b344f120c8b257fafb6b2886a46b6", "committedDate": "2022-08-30 10:26:43 -0400", "message": "[HUDI-4695] Fixing flaky TestInlineCompaction#testCompactionRetryOnFailureBasedOnTime (#6534)"}, {"oid": "05adfa2930166e8c3ac0ee905ee5cc4bb0530cce", "committedDate": "2022-09-17 15:16:52 -0700", "message": "[HUDI-3959] Rename class name for spark rdd reader (#5409)"}, {"oid": "b6124ff85a107ab170430947a24bc71df8612f1c", "committedDate": "2022-11-24 01:33:24 -0800", "message": "[HUDI-4588][HUDI-4472] Addressing schema handling issues in the write path (#6358)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "e849ad828e5857d71147d69e7856213bda6a566b", "committedDate": "2023-03-22 12:52:24 +0530", "message": "[MINOR] Fix typos in hudi-client and hudi-spark-datasource (#8230)"}, {"oid": "545a26222da67fa271266f883912f710c63d3178", "committedDate": "2023-05-08 13:08:10 -0400", "message": "[HUDI-6147] Deltastreamer finish failed compaction before ingestion (#8589)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc3MDQxNQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560770415", "body": "Useless exception class?", "bodyText": "Useless exception class?", "bodyHTML": "<p dir=\"auto\">Useless exception class?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:42:28Z", "path": "hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.exception;\n+\n+public class HoodieCompactException extends HoodieException {", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTAwMDM4NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r561000385", "bodyText": "used in SparkScheduleCompactionActionExecutor", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:27:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc3MDQxNQ=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": {"commit": "3f37d4fb08169c95930f9cc32389abf4e5cd5551", "changed_code": [{"header": "diff --git a/hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java b/hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java\nindex 0d51706bbe..5c1e30775f 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java\n", "chunk": "@@ -18,6 +18,9 @@\n \n package org.apache.hudi.exception;\n \n+/**\n+ * Exception for Hudi compaction.\n+ */\n public class HoodieCompactException extends HoodieException {\n \n   public HoodieCompactException(String msg) {\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "3f37d4fb08169c95930f9cc32389abf4e5cd5551", "committedDate": "2022-11-23 10:27:33 -0800", "message": "[HUDI-5258] Fix checkstyle issues in hudi-common (#7270)"}]}, {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "url": "https://github.com/apache/hudi/commit/c2a695a7fc90389ed68bedbd0677bea8820e47a0", "message": "update", "committedDate": "2021-01-20T14:44:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTczMzE0MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569733140", "body": "please add comments/java docs explaining what this controls ", "bodyText": "please add comments/java docs explaining what this controls", "bodyHTML": "<p dir=\"auto\">please add comments/java docs explaining what this controls</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:40:50Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 00b8d5eec7..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,6 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n   public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 00b8d5eec7..934d91a274 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -47,6 +47,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  // Run a compaction when time elapsed > N seconds since last compaction\n   public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n   public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n   public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n", "next_change": {"commit": "d412fb2fe642417460532044cac162bb68f4bec4", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 934d91a274..aa9e75ca55 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -37,97 +39,176 @@ import java.util.Properties;\n  * Compaction related config.\n  */\n @Immutable\n-public class HoodieCompactionConfig extends DefaultHoodieConfig {\n-\n-  public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n-  public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n-  public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n-  // Turn on inline compaction - after fw delta commits a inline compaction will be run\n-  public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n-  // Run a compaction every N delta commits\n-  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n-  // Run a compaction when time elapsed > N seconds since last compaction\n-  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";\n-  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = \"hoodie.compact.inline.trigger.strategy\";\n-  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = \"hoodie.cleaner.fileversions.retained\";\n-  public static final String CLEANER_COMMITS_RETAINED_PROP = \"hoodie.cleaner.commits.retained\";\n-  public static final String CLEANER_INCREMENTAL_MODE = \"hoodie.cleaner.incremental.mode\";\n-  public static final String MAX_COMMITS_TO_KEEP_PROP = \"hoodie.keep.max.commits\";\n-  public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n-  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n-  // Set true to clean bootstrap source files when necessary\n-  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.delete.bootstrap.base.file\";\n-  // Upsert uses this file size to compact new data onto existing files..\n-  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n-  // By default, treat any file <= 100MB as a small file.\n-  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);\n-  // Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten.\n-  // If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order,\n-  // until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\n-  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = \"hoodie.record.size.estimation.threshold\";\n-  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = \"1.0\";\n+public class HoodieCompactionConfig extends HoodieConfig {\n+\n+  public static final ConfigProperty<String> CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy\")\n+      .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())\n+      .withDocumentation(\"Cleaning policy to be used. Hudi will delete older versions of parquet files to re-claim space.\"\n+          + \" Any Query/Computation referring to this version of the file will fail. \"\n+          + \"It is good to make sure that the data is retained for more than the maximum query execution time.\");\n+\n+  public static final ConfigProperty<String> AUTO_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.automatic\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Should cleanup if there is anything to cleanup immediately after the commit\");\n+\n+  public static final ConfigProperty<String> ASYNC_CLEAN_PROP = ConfigProperty\n+      .key(\"hoodie.clean.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Only applies when #withAutoClean is turned on. When turned on runs cleaner async with writing.\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When set to true, compaction is triggered by the ingestion itself, \"\n+          + \"right after a commit/deltacommit action as part of insert/upsert/bulk_insert\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.commits\")\n+      .defaultValue(\"5\")\n+      .withDocumentation(\"Number of max delta commits to keep before triggering an inline compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.max.delta.seconds\")\n+      .defaultValue(String.valueOf(60 * 60))\n+      .withDocumentation(\"Run a compaction when time elapsed > N seconds since last compaction\");\n+\n+  public static final ConfigProperty<String> INLINE_COMPACT_TRIGGER_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compact.inline.trigger.strategy\")\n+      .defaultValue(CompactionTriggerStrategy.NUM_COMMITS.name())\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_FILE_VERSIONS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.fileversions.retained\")\n+      .defaultValue(\"3\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> CLEANER_COMMITS_RETAINED_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.commits.retained\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Number of commits to retain. So data will be retained for num_of_commits * time_between_commits \"\n+          + \"(scheduled). This also directly translates into how much you can incrementally pull on this table\");\n+\n+  public static final ConfigProperty<String> CLEANER_INCREMENTAL_MODE = ConfigProperty\n+      .key(\"hoodie.cleaner.incremental.mode\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> MAX_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.max.commits\")\n+      .defaultValue(\"30\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> MIN_COMMITS_TO_KEEP_PROP = ConfigProperty\n+      .key(\"hoodie.keep.min.commits\")\n+      .defaultValue(\"20\")\n+      .withDocumentation(\"Each commit is a small file in the .hoodie directory. Since DFS typically does not favor lots of \"\n+          + \"small files, Hudi archives older commits into a sequential log. A commit is published atomically \"\n+          + \"by a rename of the commit file.\");\n+\n+  public static final ConfigProperty<String> COMMITS_ARCHIVAL_BATCH_SIZE_PROP = ConfigProperty\n+      .key(\"hoodie.commits.archival.batch\")\n+      .defaultValue(String.valueOf(10))\n+      .withDocumentation(\"This controls the number of commit instants read in memory as a batch and archived together.\");\n+\n+  public static final ConfigProperty<String> CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = ConfigProperty\n+      .key(\"hoodie.cleaner.delete.bootstrap.base.file\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Set true to clean bootstrap source files when necessary\");\n+\n+  public static final ConfigProperty<String> PARQUET_SMALL_FILE_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.parquet.small.file.limit\")\n+      .defaultValue(String.valueOf(104857600))\n+      .withDocumentation(\"Upsert uses this file size to compact new data onto existing files. \"\n+          + \"By default, treat any file <= 100MB as a small file.\");\n+\n+  public static final ConfigProperty<String> RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = ConfigProperty\n+      .key(\"hoodie.record.size.estimation.threshold\")\n+      .defaultValue(\"1.0\")\n+      .withDocumentation(\"Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten. \"\n+          + \"If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order, \"\n+          + \"until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\");\n+\n+  public static final ConfigProperty<String> CLEANER_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.cleaner.parallelism\")\n+      .defaultValue(\"200\")\n+      .withDocumentation(\"Increase this if cleaning becomes slow.\");\n+\n+  // 500GB of target IO per compaction (both read and write\n+  public static final ConfigProperty<String> TARGET_IO_PER_COMPACTION_IN_MB_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.target.io\")\n+      .defaultValue(String.valueOf(500 * 1024))\n+      .withDocumentation(\"Amount of MBs to spend during compaction run for the LogFileSizeBasedCompactionStrategy. \"\n+          + \"This value helps bound ingestion latency while compaction is run inline mode.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_STRATEGY_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.strategy\")\n+      .defaultValue(LogFileSizeBasedCompactionStrategy.class.getName())\n+      .withDocumentation(\"Compaction strategy decides which file groups are picked up for \"\n+          + \"compaction during each compaction run. By default. Hudi picks the log file \"\n+          + \"with most accumulated unmerged data\");\n+\n+  public static final ConfigProperty<String> PAYLOAD_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"This needs to be same as class used during insert/upserts. Just like writing, compaction also uses \"\n+          + \"the record payload class to merge records in the log against each other, merge again with the base file and \"\n+          + \"produce the final record to be written after compaction.\");\n+\n+  public static final ConfigProperty<String> COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.lazy.block.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"When a CompactedLogScanner merges all log files, this config helps to choose whether the logblocks \"\n+          + \"should be read lazily or not. Choose true to use I/O intensive lazy block reading (low memory usage) or false \"\n+          + \"for Memory intensive immediate block read (high memory usage)\");\n+\n+  public static final ConfigProperty<String> COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.reverse.log.read\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"HoodieLogFormatReader reads a logfile in the forward direction starting from pos=0 to pos=file_length. \"\n+          + \"If this config is set to true, the Reader reads the logfile in reverse direction, from pos=file_length to pos=0\");\n+\n+  public static final ConfigProperty<String> FAILED_WRITES_CLEANER_POLICY_PROP = ConfigProperty\n+      .key(\"hoodie.cleaner.policy.failed.writes\")\n+      .defaultValue(HoodieFailedWritesCleaningPolicy.EAGER.name())\n+      .withDocumentation(\"Cleaning policy for failed writes to be used. Hudi will delete any files written by \"\n+          + \"failed writes to re-claim space. Choose to perform this rollback of failed writes eagerly before \"\n+          + \"every writer starts (only supported for single writer) or lazily by the cleaner (required for multi-writers)\");\n+\n+  public static final ConfigProperty<String> TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = ConfigProperty\n+      .key(\"hoodie.compaction.daybased.target.partitions\")\n+      .defaultValue(\"10\")\n+      .withDocumentation(\"Used by org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy to denote the number of \"\n+          + \"latest partitions to compact during a compaction run.\");\n \n   /**\n    * Configs related to specific table types.\n    */\n-  // Number of inserts, that will be put each partition/bucket for writing\n-  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = \"hoodie.copyonwrite.insert.split.size\";\n-  // The rationale to pick the insert parallelism is the following. Writing out 100MB files,\n-  // with atleast 1kb records, means 100K records per file. we just overprovision to 500K\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = String.valueOf(500000);\n-  // Config to control whether we control insert split sizes automatically based on average\n-  // record sizes\n-  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = \"hoodie.copyonwrite.insert.auto.split\";\n-  // its off by default\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);\n-  // This value is used as a guesstimate for the record size, if we can't determine this from\n-  // previous commits\n-  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = \"hoodie.copyonwrite.record.size.estimate\";\n-  // Used to determine how much more can be packed into a small file, before it exceeds the size\n-  // limit.\n-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = String.valueOf(1024);\n-  public static final String CLEANER_PARALLELISM = \"hoodie.cleaner.parallelism\";\n-  public static final String DEFAULT_CLEANER_PARALLELISM = String.valueOf(200);\n-  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = \"hoodie.compaction.target.io\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = String.valueOf(500 * 1024);\n-  public static final String COMPACTION_STRATEGY_PROP = \"hoodie.compaction.strategy\";\n-  // 200GB of target IO per compaction\n-  public static final String DEFAULT_COMPACTION_STRATEGY = LogFileSizeBasedCompactionStrategy.class.getName();\n-  // used to merge records written to log file\n-  public static final String DEFAULT_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-  public static final String PAYLOAD_CLASS_PROP = \"hoodie.compaction.payload.class\";\n-\n-  // used to choose a trade off between IO vs Memory when performing compaction process\n-  // Depending on outputfile_size and memory provided, choose true to avoid OOM for large file\n-  // size + small memory\n-  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"hoodie.compaction.lazy.block.read\";\n-  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"false\";\n-  // used to choose whether to enable reverse log reading (reverse log traversal)\n-  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = \"hoodie.compaction.reverse.log.read\";\n-  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = \"false\";\n-  private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();\n-  private static final String DEFAULT_AUTO_CLEAN = \"true\";\n-  private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n-  private static final String DEFAULT_INLINE_COMPACT = \"false\";\n-  private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n-  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n-  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = String.valueOf(60 * 60);\n-  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = CompactionTriggerStrategy.NUM_COMMITS.name();\n-  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = \"3\";\n-  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = \"10\";\n-  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = \"30\";\n-  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = \"20\";\n-  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = String.valueOf(10);\n-  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"false\";\n-  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP =\n-      \"hoodie.compaction.daybased.target.partitions\";\n-  // 500GB of target IO per compaction (both read and write)\n-  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = String.valueOf(10);\n-\n-  private HoodieCompactionConfig(Properties props) {\n-    super(props);\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.split.size\")\n+      .defaultValue(String.valueOf(500000))\n+      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n+          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n+          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+      .key(\"hoodie.copyonwrite.insert.auto.split\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n+          + \" record sizes.\");\n+\n+  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+      .key(\"hoodie.copyonwrite.record.size.estimate\")\n+      .defaultValue(String.valueOf(1024))\n+      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n+          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n+          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+\n+  private HoodieCompactionConfig() {\n+    super();\n   }\n \n   public static HoodieCompactionConfig.Builder newBuilder() {\n", "next_change": {"commit": "75040ee9e5caa0783009b6ef529d6605e82d4135", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex aa9e75ca55..e8d55934be 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -190,22 +205,24 @@ public class HoodieCompactionConfig extends HoodieConfig {\n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.split.size\")\n       .defaultValue(String.valueOf(500000))\n-      .withDocumentation(\"Number of inserts, that will be put each partition/bucket for writing. \"\n-          + \"The rationale to pick the insert parallelism is the following. Writing out 100MB files, \"\n-          + \"with at least 1kb records, means 100K records per file. we just over provision to 500K.\");\n+      .withDocumentation(\"Number of inserts assigned for each partition/bucket for writing. \"\n+          + \"We based the default on writing out 100MB files, with at least 1kb records (100K records per file), and \"\n+          + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n+          + \"  write, where there is no history to learn record sizes from.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n-          + \" record sizes.\");\n+          + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n+          + \" cumbersome.\");\n \n   public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n-      .withDocumentation(\"The average record size. If specified, hudi will use this and not compute dynamically \"\n-          + \"based on the last 24 commit\u2019s metadata. No value set as default. This is critical in computing \"\n-          + \"the insert parallelism and bin-packing inserts into small files. See above.\");\n+      .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n+          + \"record size estimate compute dynamically based on commit metadata. \"\n+          + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n   private HoodieCompactionConfig() {\n     super();\n", "next_change": {"commit": "c350d05dd3301f14fa9d688746c9de2416db3f11", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex e8d55934be..ce74aad6b0 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n", "chunk": "@@ -210,20 +217,237 @@ public class HoodieCompactionConfig extends HoodieConfig {\n           + \"  over provision to 500K. As long as auto-tuning of splits is turned on, this only affects the first \"\n           + \"  write, where there is no history to learn record sizes from.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_AUTO_SPLIT_INSERTS = ConfigProperty\n       .key(\"hoodie.copyonwrite.insert.auto.split\")\n       .defaultValue(\"true\")\n       .withDocumentation(\"Config to control whether we control insert split sizes automatically based on average\"\n           + \" record sizes. It's recommended to keep this turned on, since hand tuning is otherwise extremely\"\n           + \" cumbersome.\");\n \n-  public static final ConfigProperty<String> COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = ConfigProperty\n+  public static final ConfigProperty<String> COPY_ON_WRITE_RECORD_SIZE_ESTIMATE = ConfigProperty\n       .key(\"hoodie.copyonwrite.record.size.estimate\")\n       .defaultValue(String.valueOf(1024))\n       .withDocumentation(\"The average record size. If not explicitly specified, hudi will compute the \"\n           + \"record size estimate compute dynamically based on commit metadata. \"\n           + \" This is critical in computing the insert parallelism and bin-packing inserts into small files.\");\n \n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_POLICY_PROP = CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String AUTO_CLEAN_PROP = AUTO_CLEAN.key();\n+  /** @deprecated Use {@link #ASYNC_CLEAN} and its methods instead */\n+  @Deprecated\n+  public static final String ASYNC_CLEAN_PROP = ASYNC_CLEAN.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_PROP = INLINE_COMPACT.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = INLINE_COMPACT_NUM_DELTA_COMMITS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = INLINE_COMPACT_TIME_DELTA_SECONDS.key();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String INLINE_COMPACT_TRIGGER_STRATEGY_PROP = INLINE_COMPACT_TRIGGER_STRATEGY.key();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP = CLEANER_FILE_VERSIONS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_COMMITS_RETAINED_PROP = CLEANER_COMMITS_RETAINED.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_INCREMENTAL_MODE = CLEANER_INCREMENTAL_MODE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MAX_COMMITS_TO_KEEP_PROP = MAX_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String MIN_COMMITS_TO_KEEP_PROP = MIN_COMMITS_TO_KEEP.key();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = COMMITS_ARCHIVAL_BATCH_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.key();\n+  /**\n+   * @deprecated Use {@link #PARQUET_SMALL_FILE_LIMIT} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = PARQUET_SMALL_FILE_LIMIT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD_PROP = RECORD_SIZE_ESTIMATION_THRESHOLD.key();\n+  /**\n+   * @deprecated Use {@link #RECORD_SIZE_ESTIMATION_THRESHOLD} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_RECORD_SIZE_ESTIMATION_THRESHOLD = RECORD_SIZE_ESTIMATION_THRESHOLD.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_INSERT_SPLIT_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_INSERT_SPLIT_SIZE = COPY_ON_WRITE_INSERT_SPLIT_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_AUTO_SPLIT_INSERTS} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = COPY_ON_WRITE_AUTO_SPLIT_INSERTS.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.key();\n+  /**\n+   * @deprecated Use {@link #COPY_ON_WRITE_RECORD_SIZE_ESTIMATE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = COPY_ON_WRITE_RECORD_SIZE_ESTIMATE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.key();\n+  /**\n+   * @deprecated Use {@link #CLEANER_PARALLELISM_VALUE} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_CLEANER_PARALLELISM = CLEANER_PARALLELISM_VALUE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String TARGET_IO_PER_COMPACTION_IN_MB_PROP = TARGET_IO_PER_COMPACTION_IN_MB.key();\n+  /**\n+   * @deprecated Use {@link #TARGET_IO_PER_COMPACTION_IN_MB} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = TARGET_IO_PER_COMPACTION_IN_MB.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead\n+   */\n+  @Deprecated\n+  public static final String COMPACTION_STRATEGY_PROP = COMPACTION_STRATEGY.key();\n+  /** @deprecated Use {@link #COMPACTION_STRATEGY} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_STRATEGY = COMPACTION_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_PAYLOAD_CLASS = PAYLOAD_CLASS_NAME.defaultValue();\n+  /** @deprecated Use {@link #PAYLOAD_CLASS_NAME} and its methods instead */\n+  @Deprecated\n+  public static final String PAYLOAD_CLASS_PROP = PAYLOAD_CLASS_NAME.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = COMPACTION_LAZY_BLOCK_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_LAZY_BLOCK_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = COMPACTION_REVERSE_LOG_READ_ENABLE.key();\n+  /** @deprecated Use {@link #COMPACTION_REVERSE_LOG_READ_ENABLE} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_POLICY = CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = FAILED_WRITES_CLEANER_POLICY.key();\n+  /** @deprecated Use {@link #FAILED_WRITES_CLEANER_POLICY} and its methods instead */\n+  @Deprecated\n+  private  static final String DEFAULT_FAILED_WRITES_CLEANER_POLICY = FAILED_WRITES_CLEANER_POLICY.defaultValue();\n+  /** @deprecated Use {@link #AUTO_CLEAN} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_AUTO_CLEAN = AUTO_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #ASYNC_CLEAN} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_ASYNC_CLEAN = ASYNC_CLEAN.defaultValue();\n+  /**\n+   * @deprecated Use {@link #INLINE_COMPACT} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT = INLINE_COMPACT.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_INCREMENTAL_MODE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_INCREMENTAL_CLEANER = CLEANER_INCREMENTAL_MODE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_NUM_DELTA_COMMITS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = INLINE_COMPACT_NUM_DELTA_COMMITS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TIME_DELTA_SECONDS} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TIME_DELTA_SECONDS = INLINE_COMPACT_TIME_DELTA_SECONDS.defaultValue();\n+  /** @deprecated Use {@link #INLINE_COMPACT_TRIGGER_STRATEGY} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_INLINE_COMPACT_TRIGGER_STRATEGY = INLINE_COMPACT_TRIGGER_STRATEGY.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_FILE_VERSIONS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = CLEANER_FILE_VERSIONS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #CLEANER_COMMITS_RETAINED} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_COMMITS_RETAINED = CLEANER_COMMITS_RETAINED.defaultValue();\n+  /** @deprecated Use {@link #MAX_COMMITS_TO_KEEP} and its methods instead */\n+  @Deprecated\n+  private static final String DEFAULT_MAX_COMMITS_TO_KEEP = MAX_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #MIN_COMMITS_TO_KEEP} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_MIN_COMMITS_TO_KEEP = MIN_COMMITS_TO_KEEP.defaultValue();\n+  /**\n+   * @deprecated Use {@link #COMMITS_ARCHIVAL_BATCH_SIZE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_COMMITS_ARCHIVAL_BATCH_SIZE = COMMITS_ARCHIVAL_BATCH_SIZE.defaultValue();\n+  /**\n+   * @deprecated Use {@link #CLEANER_BOOTSTRAP_BASE_FILE_ENABLE} and its methods instead\n+   */\n+  @Deprecated\n+  private static final String DEFAULT_CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = CLEANER_BOOTSTRAP_BASE_FILE_ENABLE.defaultValue();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String TARGET_PARTITIONS_PER_DAYBASED_COMPACTION_PROP = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.key();\n+  /** @deprecated Use {@link #TARGET_PARTITIONS_PER_DAYBASED_COMPACTION} and its methods instead */\n+  @Deprecated\n+  public static final String DEFAULT_TARGET_PARTITIONS_PER_DAYBASED_COMPACTION = TARGET_PARTITIONS_PER_DAYBASED_COMPACTION.defaultValue();\n+\n   private HoodieCompactionConfig() {\n     super();\n   }\n", "next_change": null}]}}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "74241947c123c860a1b0344f25cef316440a70d6", "committedDate": "2021-03-16 16:43:53 -0700", "message": "[HUDI-845] Added locking capability to allow multiple writers (#2374)"}, {"oid": "bec70413c0943f38ee5cdf62fa3a79af44d8cded", "committedDate": "2021-03-27 10:07:10 -0700", "message": "[HUDI-1728] Fix MethodNotFound for HiveMetastore Locks (#2731)"}, {"oid": "d412fb2fe642417460532044cac162bb68f4bec4", "committedDate": "2021-06-30 14:26:30 -0700", "message": "[HUDI-89] Add configOption & refactor all configs based on that (#2833)"}, {"oid": "75040ee9e5caa0783009b6ef529d6605e82d4135", "committedDate": "2021-07-14 10:56:08 -0700", "message": "[HUDI-2149] Ensure and Audit docs for every configuration class in the codebase (#3272)"}, {"oid": "a14b19fdd5d68717d3b850a69d4ce27ca3b3d595", "committedDate": "2021-07-23 21:33:34 -0700", "message": "[HUDI-1241] Automate the generation of configs webpage as configs are added to Hudi repo (#3302)"}, {"oid": "0544d70d8f4204f4e5edfe9144c17f1ed221eb7c", "committedDate": "2021-08-12 20:31:04 -0700", "message": "[MINOR] Deprecate older configs (#3464)"}, {"oid": "c350d05dd3301f14fa9d688746c9de2416db3f11", "committedDate": "2021-08-19 13:36:40 -0700", "message": "Restore 0.8.0 config keys with deprecated annotation (#3506)"}, {"oid": "38b6934352abd27b98332cce005f18102b388679", "committedDate": "2021-11-15 22:36:54 +0800", "message": "[HUDI-2683] Parallelize deleting archived hoodie commits (#3920)"}, {"oid": "5284730175df4637eee43b179c774606b07a10a9", "committedDate": "2021-12-02 09:41:04 +0800", "message": "[HUDI-2881] Compact the file group with larger log files to reduce write amplification (#4152)"}, {"oid": "91d2e61433e74abb44cb4d0ae236ee8f4a94e1f8", "committedDate": "2021-12-02 13:32:26 -0500", "message": "[HUDI-2904] Fix metadata table archival overstepping between regular writers and table services (#4186)"}, {"oid": "b6891d253fef16f7dbbbec2def69a474c593c97e", "committedDate": "2022-01-06 20:27:37 +0530", "message": "[HUDI-44] Adding support to preserve commit metadata for compaction (#4428)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "4b388c104e024f32ae0705f6627e48b72b3408b4", "committedDate": "2022-01-31 22:36:17 -0500", "message": "[HUDI-3292] Enabling lazy read by default for log blocks during compaction (#4661)"}, {"oid": "0ababcfaa7c8cb34c399c0da57202fd48676f5d2", "committedDate": "2022-02-10 08:04:55 -0500", "message": "[HUDI-1847] Adding inline scheduling support for spark datasource path for compaction and clustering (#4420)"}, {"oid": "27bd7b538e46524d6863e36e334b4a6da665ed32", "committedDate": "2022-02-14 21:15:06 -0500", "message": "[HUDI-1576] Make archiving an async service (#4795)"}, {"oid": "5009138d044b4d859237f0f581aeeb71065dc526", "committedDate": "2022-02-18 08:57:04 -0500", "message": "[HUDI-3438] Avoid getSmallFiles if hoodie.parquet.small.file.limit is 0 (#4823)"}, {"oid": "bf16bc122a2135ad3bc3f84d55a91f25d2543d55", "committedDate": "2022-02-21 09:04:42 -0500", "message": "[HUDI-349]: Added new cleaning policy based on number of hours  (#3646)"}, {"oid": "0dee8edc9741ee99e1e2bf98efd9673003fcb1e7", "committedDate": "2022-02-21 21:53:03 -0500", "message": "[HUDI-2925] Fix duplicate cleaning of same files when unfinished clean operations are present using a config. (#4212)"}, {"oid": "3539578ccbcca4738a3e22a63635f96b313234c0", "committedDate": "2022-03-07 18:02:05 +0530", "message": "[HUDI-3213] Making commit preserve metadata to true for compaction (#4811)"}, {"oid": "ca0931d332234d0b743b4a035901a3bc9325d47c", "committedDate": "2022-03-21 20:06:30 -0400", "message": "[HUDI-1436]: Provide an option to trigger clean every nth commit (#4385)"}, {"oid": "126b88b48ddf3af4ad6b48551cab09eea4c800c9", "committedDate": "2022-07-09 20:00:48 +0530", "message": "[HUDI-2150] Rename/Restructure configs for better modularity (#6061)"}, {"oid": "cd2ea2a10b5b1f4e44a5fc844198c25d768fb2ca", "committedDate": "2022-09-17 10:08:19 -0700", "message": "[HUDI-4842] Support compaction strategy based on delta log file num (#6670)"}, {"oid": "5a28f7f15358839388c9db9d4fce2aa81862b46a", "committedDate": "2022-09-19 01:03:16 +0800", "message": "[HUDI-4870] Improve compaction config description (#6706)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "d4dcb3d1190261687ee4f46ba7a2e89d8424aafb", "committedDate": "2023-01-25 17:28:42 -0800", "message": "[HUDI-5618] Add `since version` to new configs for 0.13.0 release (#7751)"}, {"oid": "3979848a499131db594bbb49eb9ab160531a729d", "committedDate": "2023-01-28 19:37:22 -0500", "message": "[HUDI-5628] Fixing log record reader scan V2 config name (#7764)"}, {"oid": "8906b0dfeea3decfbfd6c0645c67fac729c24cbb", "committedDate": "2023-04-05 16:14:36 -0700", "message": "[HUDI-5782] Tweak defaults and remove unnecessary configs after config review (#8128)"}, {"oid": "b937b081c718b64a2646e8e28dc347c2a63e667e", "committedDate": "2023-04-14 11:30:12 -0700", "message": "[HUDI-5893] Mark additional advanced configs (#8329)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}, {"oid": "195ae3a9a23eb7c241b89d2a51ef902715d4b20b", "committedDate": "2023-06-09 19:53:27 +0530", "message": "[HUDI-6334] Integrate logcompaction table service to metadata table and provides various bugfixes to metadata table (#8900)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0MjM1MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569742350", "body": "rename to `NUM_COMMITS` and add a line of description for each?", "bodyText": "rename to NUM_COMMITS and add a line of description for each?", "bodyHTML": "<p dir=\"auto\">rename to <code>NUM_COMMITS</code> and add a line of description for each?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:56:47Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum CompactionTriggerStrategy {\n+    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nindex faba023b01..6a4e634bc7 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n", "chunk": "@@ -19,5 +19,12 @@\n package org.apache.hudi.table.action.compact;\n \n public enum CompactionTriggerStrategy {\n-    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME\n+    // trigger compaction when reach N delta commits\n+    NUM_COMMITS,\n+    // trigger compaction when time elapsed > N seconds since last compaction\n+    TIME_ELAPSED,\n+    // trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied\n+    NUM_AND_TIME,\n+    // trigger compaction when NUM_COMMITS or TIME_ELAPSED is satisfied\n+    NUM_OR_TIME\n }\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nindex faba023b01..6a4e634bc7 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n", "chunk": "@@ -19,5 +19,12 @@\n package org.apache.hudi.table.action.compact;\n \n public enum CompactionTriggerStrategy {\n-    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME\n+    // trigger compaction when reach N delta commits\n+    NUM_COMMITS,\n+    // trigger compaction when time elapsed > N seconds since last compaction\n+    TIME_ELAPSED,\n+    // trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied\n+    NUM_AND_TIME,\n+    // trigger compaction when NUM_COMMITS or TIME_ELAPSED is satisfied\n+    NUM_OR_TIME\n }\n", "next_change": {"commit": "59f652a19c51b9cc208728843c87eec32a7cac12", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nindex 6a4e634bc7..ec85978552 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n", "chunk": "@@ -21,6 +21,8 @@ package org.apache.hudi.table.action.compact;\n public enum CompactionTriggerStrategy {\n     // trigger compaction when reach N delta commits\n     NUM_COMMITS,\n+    // trigger compaction when reach N delta commits since last compaction request\n+    NUM_COMMITS_AFTER_LAST_REQUEST,\n     // trigger compaction when time elapsed > N seconds since last compaction\n     TIME_ELAPSED,\n     // trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied\n", "next_change": {"commit": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "changed_code": [{"header": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\nindex ec85978552..6c23003161 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java\n", "chunk": "@@ -18,15 +18,33 @@\n \n package org.apache.hudi.table.action.compact;\n \n+import org.apache.hudi.common.config.EnumDescription;\n+import org.apache.hudi.common.config.EnumFieldDescription;\n+\n+@EnumDescription(\"Controls when compaction is scheduled.\")\n public enum CompactionTriggerStrategy {\n+\n     // trigger compaction when reach N delta commits\n+    @EnumFieldDescription(\"triggers compaction when there are at least N delta commits after \"\n+        + \"last completed compaction.\")\n     NUM_COMMITS,\n+\n     // trigger compaction when reach N delta commits since last compaction request\n+    @EnumFieldDescription(\"triggers compaction when there are at least N delta commits after \"\n+        + \"last completed or requested compaction.\")\n     NUM_COMMITS_AFTER_LAST_REQUEST,\n+\n     // trigger compaction when time elapsed > N seconds since last compaction\n+    @EnumFieldDescription(\"triggers compaction after N seconds since last compaction.\")\n     TIME_ELAPSED,\n+\n     // trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied\n+    @EnumFieldDescription(\"triggers compaction when both there are at least N delta commits and \"\n+        + \"N seconds elapsed (both must be satisfied) after last completed compaction.\")\n     NUM_AND_TIME,\n+\n     // trigger compaction when NUM_COMMITS or TIME_ELAPSED is satisfied\n+    @EnumFieldDescription(\"triggers compaction when both there are at least N delta commits or \"\n+        + \"N seconds elapsed (either condition is satisfied) after last completed compaction.\")\n     NUM_OR_TIME\n }\n", "next_change": null}]}}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "59f652a19c51b9cc208728843c87eec32a7cac12", "committedDate": "2022-08-08 14:14:04 +0800", "message": "[HUDI-4424] Add new compactoin trigger stratgy: NUM_COMMITS_AFTER_REQ\u2026 (#6144)"}, {"oid": "fc338305e5b8f70a7849fbe64b8016a793f1f077", "committedDate": "2023-04-23 12:50:54 -0700", "message": "[HUDI-5723] Automate and standardize enum configs (#7881)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NDI5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569744296", "body": "can this call the method above or otherwise and reduce the code duplication?", "bodyText": "can this call the method above or otherwise and reduce the code duplication?", "bodyHTML": "<p dir=\"auto\">can this call the method above or otherwise and reduce the code duplication?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:59:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java", "diffHunk": "@@ -90,6 +90,20 @@ public static String createNewInstantTime() {\n     });\n   }\n \n+  /**\n+   * Returns next instant time that adds milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n+   */\n+  public static String createNewInstantTime(long milliseconds) {", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc3MTQzNA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570771434", "bodyText": "ok", "author": "Karl-WangSK", "createdAt": "2021-02-05T07:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NDI5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\nindex c8a49fdc44..846eabe566 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n", "chunk": "@@ -81,17 +81,11 @@ public class HoodieActiveTimeline extends HoodieDefaultTimeline {\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n    */\n   public static String createNewInstantTime() {\n-    return lastInstantTime.updateAndGet((oldVal) -> {\n-      String newCommitTime;\n-      do {\n-        newCommitTime = HoodieActiveTimeline.COMMIT_FORMATTER.format(new Date());\n-      } while (HoodieTimeline.compareTimestamps(newCommitTime, LESSER_THAN_OR_EQUALS, oldVal));\n-      return newCommitTime;\n-    });\n+    return createNewInstantTime(0);\n   }\n \n   /**\n-   * Returns next instant time that adds milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Returns next instant time that adds N milliseconds in the {@link #COMMIT_FORMATTER} format.\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n    */\n   public static String createNewInstantTime(long milliseconds) {\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\nindex c8a49fdc44..4a03cd4141 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n", "chunk": "@@ -81,17 +81,11 @@ public class HoodieActiveTimeline extends HoodieDefaultTimeline {\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n    */\n   public static String createNewInstantTime() {\n-    return lastInstantTime.updateAndGet((oldVal) -> {\n-      String newCommitTime;\n-      do {\n-        newCommitTime = HoodieActiveTimeline.COMMIT_FORMATTER.format(new Date());\n-      } while (HoodieTimeline.compareTimestamps(newCommitTime, LESSER_THAN_OR_EQUALS, oldVal));\n-      return newCommitTime;\n-    });\n+    return createNewInstantTime(0);\n   }\n \n   /**\n-   * Returns next instant time that adds milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Returns next instant time that adds N milliseconds in the {@link #COMMIT_FORMATTER} format.\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n    */\n   public static String createNewInstantTime(long milliseconds) {\n", "next_change": {"commit": "b7ee341e14507f2562f8c3a3212daf8686631e05", "changed_code": [{"header": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\nindex 4a03cd4141..37631b0253 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java\n", "chunk": "@@ -68,34 +66,48 @@ public class HoodieActiveTimeline extends HoodieDefaultTimeline {\n       CLEAN_EXTENSION, REQUESTED_CLEAN_EXTENSION, INFLIGHT_CLEAN_EXTENSION,\n       INFLIGHT_COMPACTION_EXTENSION, REQUESTED_COMPACTION_EXTENSION,\n       INFLIGHT_RESTORE_EXTENSION, RESTORE_EXTENSION,\n-      ROLLBACK_EXTENSION, INFLIGHT_ROLLBACK_EXTENSION,\n-      REQUESTED_REPLACE_COMMIT_EXTENSION, INFLIGHT_REPLACE_COMMIT_EXTENSION, REPLACE_COMMIT_EXTENSION\n-  ));\n-  \n+      ROLLBACK_EXTENSION, REQUESTED_ROLLBACK_EXTENSION, INFLIGHT_ROLLBACK_EXTENSION,\n+      REQUESTED_REPLACE_COMMIT_EXTENSION, INFLIGHT_REPLACE_COMMIT_EXTENSION, REPLACE_COMMIT_EXTENSION));\n   private static final Logger LOG = LogManager.getLogger(HoodieActiveTimeline.class);\n   protected HoodieTableMetaClient metaClient;\n-  private static AtomicReference<String> lastInstantTime = new AtomicReference<>(String.valueOf(Integer.MIN_VALUE));\n \n   /**\n-   * Returns next instant time in the {@link #COMMIT_FORMATTER} format.\n+   * Parse the timestamp of an Instant and return a {@code SimpleDateFormat}.\n+   */\n+  public static Date parseInstantTime(String timestamp) throws ParseException {\n+    return HoodieInstantTimeGenerator.parseInstantTime(timestamp);\n+  }\n+\n+  /**\n+   * Format the java.time.Instant to a String representing the timestamp of a Hoodie Instant.\n+   */\n+  public static String formatInstantTime(Instant timestamp) {\n+    return HoodieInstantTimeGenerator.formatInstantTime(timestamp);\n+  }\n+\n+  /**\n+   * Format the Date to a String representing the timestamp of a Hoodie Instant.\n+   */\n+  public static String formatInstantTime(Date timestamp) {\n+    return HoodieInstantTimeGenerator.formatInstantTime(timestamp);\n+  }\n+\n+  /**\n+   * Returns next instant time in the correct format.\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n    */\n   public static String createNewInstantTime() {\n-    return createNewInstantTime(0);\n+    return HoodieInstantTimeGenerator.createNewInstantTime(0);\n   }\n \n   /**\n-   * Returns next instant time that adds N milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Returns next instant time that adds N milliseconds to current time.\n    * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n+   *\n+   * @param milliseconds Milliseconds to add to current time while generating the new instant time\n    */\n   public static String createNewInstantTime(long milliseconds) {\n-    return lastInstantTime.updateAndGet((oldVal) -> {\n-      String newCommitTime;\n-      do {\n-        newCommitTime = HoodieActiveTimeline.COMMIT_FORMATTER.format(new Date(System.currentTimeMillis() + milliseconds));\n-      } while (HoodieTimeline.compareTimestamps(newCommitTime, LESSER_THAN_OR_EQUALS, oldVal));\n-      return newCommitTime;\n-    });\n+    return HoodieInstantTimeGenerator.createNewInstantTime(milliseconds);\n   }\n \n   protected HoodieActiveTimeline(HoodieTableMetaClient metaClient, Set<String> includedExtensions) {\n", "next_change": null}]}}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "ffcfb58bacab377bc72d20041baa54a3fd8fc812", "committedDate": "2021-02-19 20:12:22 -0800", "message": "[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)"}, {"oid": "9804662bc8e17d6936c20326f17ec7c0360dcaf6", "committedDate": "2021-04-01 15:25:31 +0800", "message": "[HUDI-1738] Emit deletes for flink MOR table streaming read (#2742)"}, {"oid": "8a48d16e41b544787459454ef4b81e6852cd500b", "committedDate": "2021-05-10 07:16:02 -0700", "message": "[HUDI-1707] Reduces log level for too verbose messages from info to debug level. (#2714)"}, {"oid": "b8dad628e584e0acfa8ef6ba0056f7cb6efafad0", "committedDate": "2021-09-16 11:16:06 -0400", "message": "[HUDI-2422] Adding rollback plan and rollback requested instant (#3651)"}, {"oid": "5f32162a2fad0cd6db87972d29336dc09599bf8a", "committedDate": "2021-10-06 00:17:52 -0400", "message": "[HUDI-2285][HUDI-2476] Metadata table synchronous design. Rebased and Squashed from pull/3426 (#3590)"}, {"oid": "2eda3de7f91335c17458f106f4edde8d8ffbb978", "committedDate": "2021-10-18 10:45:39 +0800", "message": "[HUDI-2562] Embedded timeline server on JobManager (#3812)"}, {"oid": "b7ee341e14507f2562f8c3a3212daf8686631e05", "committedDate": "2021-11-05 09:31:42 -0400", "message": "[HUDI-1794] Moved static COMMIT_FORMATTER to thread local variable as SimpleDateFormat is not thread safe. (#2819)"}, {"oid": "fc9ca6a07aab51db4bc36ade71c6c661f9e4ed42", "committedDate": "2021-11-22 11:44:38 -0500", "message": "[HUDI-2559] Converting commit timestamp format to millisecs (#4024)"}, {"oid": "f166ddad12ebb0b1e2a3cd4f03c2bd6ca9318379", "committedDate": "2021-12-20 22:19:37 +0800", "message": "[MINOR] Remove unused method in HoodieActiveTimeline (#4401)"}, {"oid": "4721073b4397d76bbd813a988cb79cc9a9558358", "committedDate": "2021-12-24 22:29:34 +0800", "message": "[MINOR] Remove unused method in HoodieActiveTimeline (#4435)"}, {"oid": "c81df99e50f2df84d85f08ff3a839595dad974d7", "committedDate": "2021-12-25 18:10:43 +0800", "message": "[HUDI-3102] Do not store rollback plan in inflight instant (#4445)"}, {"oid": "436becf3ea07469906804888fe9c5409a5115e7a", "committedDate": "2021-12-29 22:53:17 -0500", "message": "[HUDI-2675] Fix the exception 'Not an Avro data file' when archive and clean (#4016)"}, {"oid": "20e798386684911046a7edef2129fb3496359dda", "committedDate": "2022-01-17 17:28:18 -0500", "message": "[HUDI-3252] Avoid creating empty requestedReplaceCommit in the startCommit method (#4515)"}, {"oid": "a09c2319119d3e1a4cd9200f95e71663da4d7458", "committedDate": "2022-01-18 10:50:30 -0500", "message": "[HUDI-2903] get table schema from the last commit with data written (#4180)"}, {"oid": "7647562dad9e0615273bd76f75e7280f5ae7b7ce", "committedDate": "2022-01-18 22:42:35 -0800", "message": "[HUDI-2833][Design] Merge small archive files instead of expanding indefinitely. (#4078)"}, {"oid": "e7ec3a82dc274b8d683d74e59ee7bf35d7827ce0", "committedDate": "2022-02-10 08:06:23 -0500", "message": "[HUDI-2432] Adding restore.requested instant and restore plan for restore action (#4605)"}, {"oid": "3b401d839c1a3a5a8060f671b13df71b76bf88ac", "committedDate": "2022-02-14 17:38:01 -0800", "message": "[HUDI-3200] deprecate hoodie.file.index.enable and unify to use BaseFileOnlyViewRelation to handle (#4798)"}, {"oid": "73a21092f8f708d6c90ff2ab95b677b58237be9a", "committedDate": "2022-03-31 17:25:24 +0530", "message": "[HUDI-3732] Fixing rollback validation (#5157)"}, {"oid": "28dafa774ee058a4d00fc15b1d7fffc0c020ec3e", "committedDate": "2022-04-01 01:33:12 +0530", "message": "[HUDI-2488][HUDI-3175] Implement async metadata indexing (#4693)"}, {"oid": "444ff496a444ff82385421844aef5b4db01d8892", "committedDate": "2022-04-01 13:20:24 -0700", "message": "[RFC-33] [HUDI-2429][Stacked on HUDI-2560] Support full Schema evolution for Spark (#4910)"}, {"oid": "ad773b3d9622ebed9a8419eb5095aa6dbb8d08f0", "committedDate": "2022-05-17 09:47:10 +0800", "message": "[HUDI-3654] Preparations for hudi metastore. (#5572)"}, {"oid": "4f7ea8c79a9d13accf72d094296993c588d87beb", "committedDate": "2022-06-06 13:14:26 -0400", "message": "[HUDI-4176] Fixing `TableSchemaResolver` to avoid repeated `HoodieCommitMetadata` parsing (#5733)"}, {"oid": "d4f0326b4bbbabefc5c75617b2b5d6b8bf55fe11", "committedDate": "2022-06-20 14:29:21 +0800", "message": "[HUDI-4275] Refactor rollback inflight instant for clustering/compaction to reuse some code (#5894)"}, {"oid": "62a0c962aceae5d1c803d48c943c2155ec3ef5f1", "committedDate": "2022-06-30 11:07:40 -0700", "message": "[HUDI-3634] Could read empty or partial HoodieCommitMetaData in downstream if using HDFS (#5048)"}, {"oid": "71b81740589c9c3ec1ee1bd4910342fde0656790", "committedDate": "2022-08-29 22:28:23 -0400", "message": "[HUDI-4340] fix not parsable text DateTimeParseException by addng a method parseDateFromInstantTimeSafely for parsing timestamp when output metrics (#6000)"}, {"oid": "86a1efbff1300603a8180111eae117c7f9dbd8a5", "committedDate": "2022-10-09 19:41:35 -0400", "message": "[HUDI-3900] [UBER] Support log compaction action for MOR tables (#5958)"}, {"oid": "78a0047ed993c9caa888b16fbb47481850f4e5e7", "committedDate": "2022-11-28 17:41:20 +0800", "message": "[HUDI-5241] Optimize HoodieDefaultTimeline API (#7241)"}, {"oid": "fdce2b80c8aec49be0e3506b7120283861eafa6e", "committedDate": "2022-11-30 18:11:23 +0800", "message": "[HUDI-5279] move logic for deleting active instant to HoodieActiveTimeline (#7196)"}, {"oid": "31b02d1798ed88c9b63e89cc9283cc1c4ac3546f", "committedDate": "2023-01-25 11:59:54 -0600", "message": "[HUDI-5594] Add metaserver bundle validation (#7722)"}, {"oid": "9a79a6d463106dc1c579ae5bc194a2f1605980ad", "committedDate": "2023-04-01 20:17:48 +0800", "message": "[HUDI-5649] Unify all the loggers to slf4j (#7955) (#7955)"}, {"oid": "8db07a8b75f99bcfc4e38aa5fee368a9fd739139", "committedDate": "2023-05-09 20:29:56 +0800", "message": "[HUDI-6040] Stop writing and reading compaction plans from .aux folder (#8385)"}, {"oid": "5b22070356799e7470e0999781f9168c4e5ebcc6", "committedDate": "2023-05-31 10:12:39 -0400", "message": "[HUDI-6060] Added a config to backup instants before deletion during rollbacks and restores. (#8430)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NTQ3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569745476", "body": "IIUC this block is just moved, no changes to code here within the if block?", "bodyText": "IIUC this block is just moved, no changes to code here within the if block?", "bodyHTML": "<p dir=\"auto\">IIUC this block is just moved, no changes to code here within the if block?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:01:48Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0ODQyMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570748423", "bodyText": "yes.just exchange the order.", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NTQ3Ng=="}], "type": "inlineReview", "revised_code": null, "revised_code_in_main": {"commit": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nsimilarity index 52%\nrename from hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nrename to hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\nindex 1ec867cb39..31ced7b72d 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java\n", "chunk": "@@ -19,54 +19,94 @@\n package org.apache.hudi.table.action.compact;\n \n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n-import org.apache.hudi.client.WriteStatus;\n-import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieFileGroupId;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n import org.apache.hudi.common.table.view.SyncableFileSystemView;\n import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieCompactionException;\n+import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.api.java.JavaRDD;\n-import scala.Tuple2;\n \n import java.io.IOException;\n import java.text.ParseException;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.stream.Collectors;\n \n-@SuppressWarnings(\"checkstyle:LineLength\")\n-public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload> extends\n-    BaseScheduleCompactionActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+public class ScheduleCompactionActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCompactionPlan>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(ScheduleCompactionActionExecutor.class);\n \n-  private static final Logger LOG = LogManager.getLogger(SparkScheduleCompactionActionExecutor.class);\n+  private final Option<Map<String, String>> extraMetadata;\n+  private final HoodieCompactor compactor;\n \n-  public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n-                                               HoodieWriteConfig config,\n-                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                               String instantTime,\n-                                               Option<Map<String, String>> extraMetadata) {\n-    super(context, config, table, instantTime, extraMetadata);\n+  public ScheduleCompactionActionExecutor(HoodieEngineContext context,\n+                                          HoodieWriteConfig config,\n+                                          HoodieTable<T, I, K, O> table,\n+                                          String instantTime,\n+                                          Option<Map<String, String>> extraMetadata,\n+                                          HoodieCompactor compactor) {\n+    super(context, config, table, instantTime);\n+    this.extraMetadata = extraMetadata;\n+    this.compactor = compactor;\n   }\n \n   @Override\n-  protected HoodieCompactionPlan scheduleCompaction() {\n+  public Option<HoodieCompactionPlan> execute() {\n+    if (!config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()\n+        && !config.getFailedWritesCleanPolicy().isLazy()) {\n+      // if there are inflight writes, their instantTime must not be less than that of compaction instant time\n+      table.getActiveTimeline().getCommitsTimeline().filterPendingExcludingCompaction().firstInstant()\n+          .ifPresent(earliestInflight -> ValidationUtils.checkArgument(\n+              HoodieTimeline.compareTimestamps(earliestInflight.getTimestamp(), HoodieTimeline.GREATER_THAN, instantTime),\n+              \"Earliest write inflight instant time must be later than compaction time. Earliest :\" + earliestInflight\n+                  + \", Compaction scheduled at \" + instantTime));\n+      // Committed and pending compaction instants should have strictly lower timestamps\n+      List<HoodieInstant> conflictingInstants = table.getActiveTimeline()\n+          .getWriteTimeline().filterCompletedAndCompactionInstants().getInstants()\n+          .filter(instant -> HoodieTimeline.compareTimestamps(\n+              instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))\n+          .collect(Collectors.toList());\n+      ValidationUtils.checkArgument(conflictingInstants.isEmpty(),\n+          \"Following instants have timestamps >= compactionInstant (\" + instantTime + \") Instants :\"\n+              + conflictingInstants);\n+    }\n+\n+    HoodieCompactionPlan plan = scheduleCompaction();\n+    if (plan != null && (plan.getOperations() != null) && (!plan.getOperations().isEmpty())) {\n+      extraMetadata.ifPresent(plan::setExtraMetadata);\n+      HoodieInstant compactionInstant =\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instantTime);\n+      try {\n+        table.getActiveTimeline().saveToCompactionRequested(compactionInstant,\n+            TimelineMetadataUtils.serializeCompactionPlan(plan));\n+      } catch (IOException ioe) {\n+        throw new HoodieIOException(\"Exception scheduling compaction\", ioe);\n+      }\n+      return Option.of(plan);\n+    }\n+    return Option.empty();\n+  }\n+\n+  private HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     // judge if we need to compact according to num delta commits and time elapsed\n     boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n     if (compactable) {\n       LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n       try {\n         SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n         Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NjA1NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569746054", "body": "can we use `Pair` instead of Tuple?", "bodyText": "can we use Pair instead of Tuple?", "bodyHTML": "<p dir=\"auto\">can we use <code>Pair</code> instead of Tuple?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:02:54Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0ODQ1OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570748458", "bodyText": "sure", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NjA1NA=="}], "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +83,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +82,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1MTk2Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569751962", "body": "I understand this is how it was. but overloading `lastCompactionTs` with the first delta commit and reusing this again is hard to grok. Can we atleast rename `lastCompactionTs` -> `latestInstantTs` or something more generic", "bodyText": "I understand this is how it was. but overloading lastCompactionTs with the first delta commit and reusing this again is hard to grok. Can we atleast rename lastCompactionTs -> latestInstantTs or something more generic", "bodyHTML": "<p dir=\"auto\">I understand this is how it was. but overloading <code>lastCompactionTs</code> with the first delta commit and reusing this again is hard to grok. Can we atleast rename <code>lastCompactionTs</code> -&gt; <code>latestInstantTs</code> or something more generic</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:10:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +83,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +82,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1Mjk1NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569752955", "body": "can we always compute `deltaCommitsSinceLastCompaction` regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together", "bodyText": "can we always compute deltaCommitsSinceLastCompaction regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together", "bodyHTML": "<p dir=\"auto\">can we always compute <code>deltaCommitsSinceLastCompaction</code> regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:12:23Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0NjU1Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570746552", "bodyText": "yes. I think so. wdyt? @wangxianghu", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1Mjk1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..29e408c244 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +83,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "revised_code_in_main": {"commit": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "changed_code": [{"header": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\nindex 1ec867cb39..9c44499a8f 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java\n", "chunk": "@@ -83,58 +82,53 @@ public class SparkScheduleCompactionActionExecutor<T extends HoodieRecordPayload\n     return new HoodieCompactionPlan();\n   }\n \n-  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n+  public Pair<Integer, String> getLatestDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n     HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n \n-    String lastCompactionTs;\n+    String latestInstantTs;\n     int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n-      lastCompactionTs = lastCompaction.get().getTimestamp();\n+      latestInstantTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     } else {\n-      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      latestInstantTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(latestInstantTs, Integer.MAX_VALUE).countInstants();\n     }\n-    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n-      if (lastCompaction.isPresent()) {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      } else {\n-        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-      }\n-    }\n-    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+    return Pair.of(deltaCommitsSinceLastCompaction, latestInstantTs);\n   }\n \n   public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n     boolean compactable;\n     // get deltaCommitsSinceLastCompaction and lastCompactionTs\n-    Tuple2<Integer, String> lastDeltaCommitInfo = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    Pair<Integer, String> latestDeltaCommitInfo = getLatestDeltaCommitInfo(compactionTriggerStrategy);\n     int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n     int inlineCompactDeltaSecondsMax = config.getInlineCompactDeltaSecondsMax();\n     switch (compactionTriggerStrategy) {\n-      case NUM:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1;\n+      case NUM_COMMITS:\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft();\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n         }\n         break;\n       case TIME_ELAPSED:\n-        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_OR_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            || inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n         }\n         break;\n       case NUM_AND_TIME:\n-        compactable = inlineCompactDeltaCommitMax <= lastDeltaCommitInfo._1\n-            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(lastDeltaCommitInfo._2);\n+        compactable = inlineCompactDeltaCommitMax <= latestDeltaCommitInfo.getLeft()\n+            && inlineCompactDeltaSecondsMax <= parsedToSeconds(instantTime) - parsedToSeconds(latestDeltaCommitInfo.getRight());\n         if (compactable) {\n           LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n               inlineCompactDeltaSecondsMax));\n", "next_change": null}]}, "commits_in_main": [{"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e", "message": "Merge commit", "committedDate": null}, {"oid": "5ed35bff836f898d420e9a7ef8c47dc2ded7dca3", "committedDate": "2021-10-22 15:58:51 -0400", "message": "[HUDI-2501] Add HoodieData abstraction and refactor compaction actions in hudi-client module (#3741)"}]}, {"oid": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "url": "https://github.com/apache/hudi/commit/1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "message": "update", "committedDate": "2021-02-05T07:40:40Z", "type": "commit"}, {"oid": "b34c416c83f5d651cbf9d10fe35c0047d69fc964", "url": "https://github.com/apache/hudi/commit/b34c416c83f5d651cbf9d10fe35c0047d69fc964", "message": "update", "committedDate": "2021-02-05T08:28:56Z", "type": "commit"}]}