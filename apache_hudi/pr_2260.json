{"pr_number": 2260, "pr_title": "[HUDI-1381] Schedule compaction based on time elapsed", "pr_author": "Karl-WangSK", "pr_createdAt": "2020-11-18T08:05:50Z", "pr_url": "https://github.com/apache/hudi/pull/2260", "timeline": [{"oid": "679310a69fdb607d084bae429e03c528b5d60159", "url": "https://github.com/apache/hudi/commit/679310a69fdb607d084bae429e03c528b5d60159", "message": "update", "committedDate": "2020-11-18T08:01:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjI3NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042274", "body": "Missing doc about `initialTime `.", "bodyText": "Missing doc about initialTime .", "bodyHTML": "<p dir=\"auto\">Missing doc about <code>initialTime </code>.</p>", "author": "yanghua", "createdAt": "2020-11-28T13:49:35Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -315,7 +315,8 @@ public HoodieActiveTimeline getActiveTimeline() {\n    */\n   public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,\n                                                                   String instantTime,\n-                                                                  Option<Map<String, String>> extraMetadata);\n+                                                                  Option<Map<String, String>> extraMetadata,\n+                                                                  String initialTime);", "originalCommit": "6dac8043125b788234a2184f423ecc9d21c5757a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjU1MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042551", "body": "break the second `with` to a new line?", "bodyText": "break the second with to a new line?", "bodyHTML": "<p dir=\"auto\">break the second <code>with</code> to a new line?</p>", "author": "yanghua", "createdAt": "2020-11-28T13:52:31Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -39,17 +39,18 @@\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)", "originalCommit": "6dac8043125b788234a2184f423ecc9d21c5757a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a", "url": "https://github.com/apache/hudi/commit/6dac8043125b788234a2184f423ecc9d21c5757a", "message": "support flink", "committedDate": "2020-11-18T15:28:28Z", "type": "forcePushed"}, {"oid": "b593f1062931a4d017ae8bd7dd42e47a8873a39f", "url": "https://github.com/apache/hudi/commit/b593f1062931a4d017ae8bd7dd42e47a8873a39f", "message": "[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)", "committedDate": "2021-01-06T15:07:24Z", "type": "forcePushed"}, {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "url": "https://github.com/apache/hudi/commit/ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "message": "update", "committedDate": "2021-01-07T06:27:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553699091", "body": "Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).\r\n\r\nbesides, we got 4 conditions here:\r\n1. compact with commit num only;\r\n2. compact with time elapsed only;\r\n3. compact when both commit num and time elapsed meet requirements\uff1b\r\n4. compact when one of them is met\r\n\r\nWDYT @Karl-WangSK  cc @yanghua ", "bodyText": "Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).\nbesides, we got 4 conditions here:\n\ncompact with commit num only;\ncompact with time elapsed only;\ncompact when both commit num and time elapsed meet requirements\uff1b\ncompact when one of them is met\n\nWDYT @Karl-WangSK  cc @yanghua", "bodyHTML": "<p dir=\"auto\">Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).</p>\n<p dir=\"auto\">besides, we got 4 conditions here:</p>\n<ol dir=\"auto\">\n<li>compact with commit num only;</li>\n<li>compact with time elapsed only;</li>\n<li>compact when both commit num and time elapsed meet requirements\uff1b</li>\n<li>compact when one of them is met</li>\n</ol>\n<p dir=\"auto\">WDYT <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/Karl-WangSK/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Karl-WangSK\">@Karl-WangSK</a>  cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/yanghua/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanghua\">@yanghua</a></p>", "author": "wangxianghu", "createdAt": "2021-01-08T02:04:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,32 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n+    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n+    boolean compactable;\n+    if (numCommitEnabled && !timeEnabled) {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n+    } else if (!numCommitEnabled && timeEnabled) {\n+      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    } else {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    }", "originalCommit": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MTc1NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559351754", "bodyText": "if we call this function. it means we turn ASYNC_COMPACT_ENABLE_OPT_KEY on. Do we still need to check first?\n\nThe switch of ASYNC_COMPACT_ENABLE_OPT_KEY  must be the pre-condition.", "author": "yanghua", "createdAt": "2021-01-18T07:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ=="}], "type": "inlineReview"}, {"oid": "d087f09d4da2b2339413799f4eb13a091faacb9a", "url": "https://github.com/apache/hudi/commit/d087f09d4da2b2339413799f4eb13a091faacb9a", "message": "update", "committedDate": "2021-01-10T06:53:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM5MTA4MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r556391080", "body": "how about extract all these logic to one method `needCompact(Table table, CompactType compactType )`, and init proper vars when need.", "bodyText": "how about extract all these logic to one method needCompact(Table table, CompactType compactType ), and init proper vars when need.", "bodyHTML": "<p dir=\"auto\">how about extract all these logic to one method <code>needCompact(Table table, CompactType compactType )</code>, and init proper vars when need.</p>", "author": "wangxianghu", "createdAt": "2021-01-13T09:48:29Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,34 +63,64 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);", "originalCommit": "d087f09d4da2b2339413799f4eb13a091faacb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f", "url": "https://github.com/apache/hudi/commit/7d0453e72c7f9136e16ffaf928e09906917f745f", "message": "update", "committedDate": "2021-01-14T03:32:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDAxNw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350017", "body": "Please revert this change.", "bodyText": "Please revert this change.", "bodyHTML": "<p dir=\"auto\">Please revert this change.</p>", "author": "yanghua", "createdAt": "2021-01-18T06:59:09Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -18,6 +18,7 @@\n \n package org.apache.hudi.config;\n \n+import org.apache.hadoop.hbase.io.compress.Compression;", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc3Mw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350773", "body": "please revert this change.", "bodyText": "please revert this change.", "bodyHTML": "<p dir=\"auto\">please revert this change.</p>", "author": "yanghua", "createdAt": "2021-01-18T07:01:16Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  ", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc5OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350798", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-18T07:01:22Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  ", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353013", "body": "We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?", "bodyText": "We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?", "bodyHTML": "<p dir=\"auto\">We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?</p>", "author": "yanghua", "createdAt": "2021-01-18T07:07:50Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM2MDA2NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559360064", "bodyText": "Now we have 4 types of CompactType, and compactable may be true or false.\nwhich means  it will have 8 situations.  If we add log in to each case, I think a little bit tedious. wdyt?", "author": "Karl-WangSK", "createdAt": "2021-01-18T07:26:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM2NDg0Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559364846", "bodyText": "so I discuss with wangxianghu , just give a summative statement.", "author": "Karl-WangSK", "createdAt": "2021-01-18T07:38:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM3NTc4NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559375785", "bodyText": "reasonable, The key point that I want to raise up is: it would be better to describe the trigger condition.", "author": "yanghua", "createdAt": "2021-01-18T08:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTQ0Mzk3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559443976", "bodyText": "ok, added !", "author": "Karl-WangSK", "createdAt": "2021-01-18T09:58:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzQ5OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353499", "body": "We may not tell why does not trigger compaction. It means nothing happened, right.", "bodyText": "We may not tell why does not trigger compaction. It means nothing happened, right.", "bodyHTML": "<p dir=\"auto\">We may not tell why does not trigger compaction. It means nothing happened, right.</p>", "author": "yanghua", "createdAt": "2021-01-18T07:09:10Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"", "originalCommit": "7d0453e72c7f9136e16ffaf928e09906917f745f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fbd87b4915f52d3010edeebe66b966855e1abfdc", "url": "https://github.com/apache/hudi/commit/fbd87b4915f52d3010edeebe66b966855e1abfdc", "message": "update", "committedDate": "2021-01-18T07:34:35Z", "type": "commit"}, {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c", "url": "https://github.com/apache/hudi/commit/96e596a8c14dc2a45606a5363e39b87474747b5c", "message": "add log in each case", "committedDate": "2021-01-18T09:56:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTg3Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501872", "body": "Still exists?", "bodyText": "Still exists?", "bodyHTML": "<p dir=\"auto\">Still exists?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:32:35Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTk3MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501971", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-18T11:32:49Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNjEzOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559506139", "body": "`COMMIT_NUM` and `NUM ` do not keep consistent. What about `NUM` or `COMMITS`?\r\n\r\nIMO, `CompactType` may make users confused. what about `CompactionTriggerStrategy` Or `CompactionScheduleStrategy`", "bodyText": "COMMIT_NUM and NUM  do not keep consistent. What about NUM or COMMITS?\nIMO, CompactType may make users confused. what about CompactionTriggerStrategy Or CompactionScheduleStrategy", "bodyHTML": "<p dir=\"auto\"><code>COMMIT_NUM</code> and <code>NUM </code> do not keep consistent. What about <code>NUM</code> or <code>COMMITS</code>?</p>\n<p dir=\"auto\">IMO, <code>CompactType</code> may make users confused. what about <code>CompactionTriggerStrategy</code> Or <code>CompactionScheduleStrategy</code></p>", "author": "yanghua", "createdAt": "2021-01-18T11:40:13Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum  CompactType {\n+    COMMIT_NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNzI3Nw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559507277", "body": "Revert this change, please.", "bodyText": "Revert this change, please.", "bodyHTML": "<p dir=\"auto\">Revert this change, please.</p>", "author": "yanghua", "createdAt": "2021-01-18T11:42:18Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwODczOQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559508739", "body": "IMO, `getLastDeltaCommitInfo` sounds better? Correct me, if it's not good for you.", "bodyText": "IMO, getLastDeltaCommitInfo sounds better? Correct me, if it's not good for you.", "bodyHTML": "<p dir=\"auto\">IMO, <code>getLastDeltaCommitInfo</code> sounds better? Correct me, if it's not good for you.</p>", "author": "yanghua", "createdAt": "2021-01-18T11:44:58Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5MDcwOA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559690708", "bodyText": "ok, better than mine", "author": "Karl-WangSK", "createdAt": "2021-01-18T16:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwODczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMDY3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559510676", "body": "`The commit number is larger than xxx, trigger compaction scheduler.` sounds better?", "bodyText": "The commit number is larger than xxx, trigger compaction scheduler. sounds better?", "bodyHTML": "<p dir=\"auto\"><code>The commit number is larger than xxx, trigger compaction scheduler.</code> sounds better?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:48:17Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMTk3NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559511975", "body": "`Timestamp` is not clear, it means `second` or `mills` or something else?", "bodyText": "Timestamp is not clear, it means second or mills or something else?", "bodyHTML": "<p dir=\"auto\"><code>Timestamp</code> is not clear, it means <code>second</code> or <code>mills</code> or something else?</p>", "author": "yanghua", "createdAt": "2021-01-18T11:50:35Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n+                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n+              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n     }\n+    return compactable;\n   }\n \n+  public Long parseToTimestamp(String time) {", "originalCommit": "96e596a8c14dc2a45606a5363e39b87474747b5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5NTcxNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559695716", "bodyText": "parsedToSeconds ? sry . Do u have any good name?", "author": "Karl-WangSK", "createdAt": "2021-01-18T16:51:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMTk3NQ=="}], "type": "inlineReview"}, {"oid": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "url": "https://github.com/apache/hudi/commit/a74ea8120c56e5b40de3491d0a4d61a93981011d", "message": "update", "committedDate": "2021-01-18T16:32:35Z", "type": "commit"}, {"oid": "4259478b945ea0440e0c28c56dc6290ee2df4442", "url": "https://github.com/apache/hudi/commit/4259478b945ea0440e0c28c56dc6290ee2df4442", "message": "Update HoodieWriteConfig.java", "committedDate": "2021-01-18T16:44:59Z", "type": "commit"}, {"oid": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "url": "https://github.com/apache/hudi/commit/3c3e12ef34026b9d5b89d3d537a37040046abae3", "message": "update", "committedDate": "2021-01-18T17:02:14Z", "type": "commit"}, {"oid": "a318c91498d1899107f886b73c3404cd5f50258f", "url": "https://github.com/apache/hudi/commit/a318c91498d1899107f886b73c3404cd5f50258f", "message": "Merge branch 'HUDI-1381' of github.com:Karl-WangSK/hudi into HUDI-1381", "committedDate": "2021-01-18T17:02:33Z", "type": "commit"}, {"oid": "b9a1a63910825d68b9a7e99cd9dd9995a4312448", "url": "https://github.com/apache/hudi/commit/b9a1a63910825d68b9a7e99cd9dd9995a4312448", "message": "update", "committedDate": "2021-01-19T01:49:36Z", "type": "commit"}, {"oid": "d720b4e939dab49e8493dbce073c7ee859b03594", "url": "https://github.com/apache/hudi/commit/d720b4e939dab49e8493dbce073c7ee859b03594", "message": "update", "committedDate": "2021-01-19T01:53:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560098640", "body": "how about renaming it to `hoodie.compact.inline.max.delta.seconds`, it seems more readable cc @yanghua ", "bodyText": "how about renaming it to hoodie.compact.inline.max.delta.seconds, it seems more readable cc @yanghua", "bodyHTML": "<p dir=\"auto\">how about renaming it to <code>hoodie.compact.inline.max.delta.seconds</code>, it seems more readable cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/yanghua/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanghua\">@yanghua</a></p>", "author": "wangxianghu", "createdAt": "2021-01-19T11:10:51Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.time\";", "originalCommit": "d720b4e939dab49e8493dbce073c7ee859b03594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIwMDQ3NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560200475", "bodyText": "sounds good~", "author": "yanghua", "createdAt": "2021-01-19T14:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIyNjI5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560226296", "bodyText": "edited.\nanything else  need to improve?", "author": "Karl-WangSK", "createdAt": "2021-01-19T14:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI1ODg1MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560258851", "bodyText": "thanks for your patient, will do a final check tomorrow.", "author": "yanghua", "createdAt": "2021-01-19T15:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA=="}], "type": "inlineReview"}, {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "url": "https://github.com/apache/hudi/commit/48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "message": "Update HoodieCompactionConfig.java", "committedDate": "2021-01-19T14:38:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTE0Mw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755143", "body": "Can we unify the constant name to `INLINE_COMPACT_TIME_DELTA_SECONDS_PROP ` so that we can align with `INLINE_COMPACT_NUM_DELTA_COMMITS_PROP ` and `withMaxDeltaTimeBeforeCompaction `", "bodyText": "Can we unify the constant name to INLINE_COMPACT_TIME_DELTA_SECONDS_PROP  so that we can align with INLINE_COMPACT_NUM_DELTA_COMMITS_PROP  and withMaxDeltaTimeBeforeCompaction", "bodyHTML": "<p dir=\"auto\">Can we unify the constant name to <code>INLINE_COMPACT_TIME_DELTA_SECONDS_PROP </code> so that we can align with <code>INLINE_COMPACT_NUM_DELTA_COMMITS_PROP </code> and <code>withMaxDeltaTimeBeforeCompaction </code></p>", "author": "yanghua", "createdAt": "2021-01-20T08:17:22Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTIwNQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755205", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:17:30Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -109,6 +112,8 @@\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n+  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDMwMA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760300", "body": "Do we also need to judge `NUM_OR_TIME `?", "bodyText": "Do we also need to judge NUM_OR_TIME ?", "bodyHTML": "<p dir=\"auto\">Do we also need to judge <code>NUM_OR_TIME </code>?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:26:05Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5ODkyNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560998926", "bodyText": "no need. only TIME_ELAPSED don't need deltaCommitsSinceLastCompaction", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDMwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDc5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760796", "body": "`return` -> `get`", "bodyText": "return -> get", "bodyHTML": "<p dir=\"auto\"><code>return</code> -&gt; <code>get</code></p>", "author": "yanghua", "createdAt": "2021-01-20T08:26:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MzAyNg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763026", "body": "Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  `threshold` is immutable.", "bodyText": "Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  threshold is immutable.", "bodyHTML": "<p dir=\"auto\">Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  <code>threshold</code> is immutable.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:30:27Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2Mzk5MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763990", "body": "IMO, we do not need all the `else` statement, right? It's too normal if we do not match the compaction strategy.", "bodyText": "IMO, we do not need all the else statement, right? It's too normal if we do not match the compaction strategy.", "bodyHTML": "<p dir=\"auto\">IMO, we do not need all the <code>else</code> statement, right? It's too normal if we do not match the compaction strategy.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:32:06Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NTQ4OQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560765489", "body": "Since you have defined a `compactable `, let's use `break` here and return it in the end.", "bodyText": "Since you have defined a compactable , let's use break here and return it in the end.", "bodyHTML": "<p dir=\"auto\">Since you have defined a <code>compactable </code>, let's use <code>break</code> here and return it in the end.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:34:35Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NjI3MQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560766271", "body": "`compact type` -> `compaction trigger strategy`.", "bodyText": "compact type -> compaction trigger strategy.", "bodyHTML": "<p dir=\"auto\"><code>compact type</code> -&gt; <code>compaction trigger strategy</code>.</p>", "author": "yanghua", "createdAt": "2021-01-20T08:35:47Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n+      case TIME_ELAPSED:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n+              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_OR_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_AND_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2ODgyNw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560768827", "body": "We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?", "bodyText": "We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?", "bodyHTML": "<p dir=\"auto\">We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:39:55Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5OTc4OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560999788", "bodyText": "good catch", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:26:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2ODgyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTQzNA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769434", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:40:47Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTYyMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769623", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:06Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTg0NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769845", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:27Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTkzMQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769931", "body": "ditto", "bodyText": "ditto", "bodyHTML": "<p dir=\"auto\">ditto</p>", "author": "yanghua", "createdAt": "2021-01-20T08:41:37Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnNumCommits() throws Exception {\n+    // Given: two commits, schedule compaction and its failed/in-flight\n+    HoodieWriteConfig cfg = getConfigBuilder(false)\n+        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n+            .withInlineCompaction(false)\n+            .withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+        .build();\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+    String instantTime2;\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Schedule compaction instant2, make it in-flight (simulates inline compaction failing)\n+      instantTime2 = HoodieActiveTimeline.createNewInstantTime();\n+      scheduleCompaction(instantTime2, writeClient, cfg);\n+      moveCompactionFromRequestedToInflight(instantTime2, cfg);\n+    }\n+\n+    // When: a third commit happens\n+    HoodieWriteConfig inlineCfg = getConfigForInlineCompaction(2, 60, CompactionTriggerStrategy.NUM);\n+    String instantTime3 = HoodieActiveTimeline.createNewInstantTime();\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(inlineCfg)) {\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(instantTime3, dataGen.generateUpdates(instantTime3, 100), writeClient, metaClient, inlineCfg, false);\n+    }\n+\n+    // Then: 1 delta commit is done, the failed compaction is retried\n+    metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+    assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    assertEquals(instantTime2, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnTime() throws Exception {\n     // Given: two commits, schedule compaction and its failed/in-flight\n     HoodieWriteConfig cfg = getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+            .withInlineCompaction(false)\n+            .withMaxDeltaTimeBeforeCompaction(5)\n+            .withInlineCompactionTriggerStrategy(CompactionTriggerStrategy.TIME_ELAPSED).build())\n         .build();\n-    List<String> instants = CollectionUtils.createImmutableList(\"000\", \"001\");\n+    String instantTime;\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      // Schedule compaction 002, make it in-flight (simulates inline compaction failing)\n-      scheduleCompaction(\"002\", writeClient, cfg);\n-      moveCompactionFromRequestedToInflight(\"002\", cfg);\n+      // Schedule compaction instantTime, make it in-flight (simulates inline compaction failing)\n+      Thread.sleep(10000);", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc3MDQxNQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560770415", "body": "Useless exception class?", "bodyText": "Useless exception class?", "bodyHTML": "<p dir=\"auto\">Useless exception class?</p>", "author": "yanghua", "createdAt": "2021-01-20T08:42:28Z", "path": "hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.exception;\n+\n+public class HoodieCompactException extends HoodieException {", "originalCommit": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTAwMDM4NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r561000385", "bodyText": "used in SparkScheduleCompactionActionExecutor", "author": "Karl-WangSK", "createdAt": "2021-01-20T14:27:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc3MDQxNQ=="}], "type": "inlineReview"}, {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "url": "https://github.com/apache/hudi/commit/c2a695a7fc90389ed68bedbd0677bea8820e47a0", "message": "update", "committedDate": "2021-01-20T14:44:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTczMzE0MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569733140", "body": "please add comments/java docs explaining what this controls ", "bodyText": "please add comments/java docs explaining what this controls", "bodyHTML": "<p dir=\"auto\">please add comments/java docs explaining what this controls</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:40:50Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0MjM1MA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569742350", "body": "rename to `NUM_COMMITS` and add a line of description for each?", "bodyText": "rename to NUM_COMMITS and add a line of description for each?", "bodyHTML": "<p dir=\"auto\">rename to <code>NUM_COMMITS</code> and add a line of description for each?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:56:47Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum CompactionTriggerStrategy {\n+    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NDI5Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569744296", "body": "can this call the method above or otherwise and reduce the code duplication?", "bodyText": "can this call the method above or otherwise and reduce the code duplication?", "bodyHTML": "<p dir=\"auto\">can this call the method above or otherwise and reduce the code duplication?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T20:59:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java", "diffHunk": "@@ -90,6 +90,20 @@ public static String createNewInstantTime() {\n     });\n   }\n \n+  /**\n+   * Returns next instant time that adds milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n+   */\n+  public static String createNewInstantTime(long milliseconds) {", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc3MTQzNA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570771434", "bodyText": "ok", "author": "Karl-WangSK", "createdAt": "2021-02-05T07:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NDI5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NTQ3Ng==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569745476", "body": "IIUC this block is just moved, no changes to code here within the if block?", "bodyText": "IIUC this block is just moved, no changes to code here within the if block?", "bodyHTML": "<p dir=\"auto\">IIUC this block is just moved, no changes to code here within the if block?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:01:48Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0ODQyMw==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570748423", "bodyText": "yes.just exchange the order.", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NTQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NjA1NA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569746054", "body": "can we use `Pair` instead of Tuple?", "bodyText": "can we use Pair instead of Tuple?", "bodyHTML": "<p dir=\"auto\">can we use <code>Pair</code> instead of Tuple?</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:02:54Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0ODQ1OA==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570748458", "bodyText": "sure", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NjA1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1MTk2Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569751962", "body": "I understand this is how it was. but overloading `lastCompactionTs` with the first delta commit and reusing this again is hard to grok. Can we atleast rename `lastCompactionTs` -> `latestInstantTs` or something more generic", "bodyText": "I understand this is how it was. but overloading lastCompactionTs with the first delta commit and reusing this again is hard to grok. Can we atleast rename lastCompactionTs -> latestInstantTs or something more generic", "bodyHTML": "<p dir=\"auto\">I understand this is how it was. but overloading <code>lastCompactionTs</code> with the first delta commit and reusing this again is hard to grok. Can we atleast rename <code>lastCompactionTs</code> -&gt; <code>latestInstantTs</code> or something more generic</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:10:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1Mjk1NQ==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569752955", "body": "can we always compute `deltaCommitsSinceLastCompaction` regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together", "bodyText": "can we always compute deltaCommitsSinceLastCompaction regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together", "bodyHTML": "<p dir=\"auto\">can we always compute <code>deltaCommitsSinceLastCompaction</code> regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together</p>", "author": "vinothchandar", "createdAt": "2021-02-03T21:12:23Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {", "originalCommit": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDc0NjU1Mg==", "url": "https://github.com/apache/hudi/pull/2260#discussion_r570746552", "bodyText": "yes. I think so. wdyt? @wangxianghu", "author": "Karl-WangSK", "createdAt": "2021-02-05T06:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1Mjk1NQ=="}], "type": "inlineReview"}, {"oid": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "url": "https://github.com/apache/hudi/commit/1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "message": "update", "committedDate": "2021-02-05T07:40:40Z", "type": "commit"}, {"oid": "b34c416c83f5d651cbf9d10fe35c0047d69fc964", "url": "https://github.com/apache/hudi/commit/b34c416c83f5d651cbf9d10fe35c0047d69fc964", "message": "update", "committedDate": "2021-02-05T08:28:56Z", "type": "commit"}]}